---
type: article
dhqtype: article
title: "Compounded Mediation: A Data Archaeology of the Newspaper Navigator Dataset"
date: 2021-12-07
article_id: "000578"
volume: 015
issue: 4
authors:
- Benjamin Lee
translationType: original
abstract: |
   The increasing roles of machine learning and artificial intelligence in the construction of cultural heritage and humanities datasets necessitate critical examination of the myriad biases introduced by machines, algorithms, and the humans who build and deploy them. From image classification to optical character recognition, the effects of decisions ostensibly made by machines compound through the digitization pipeline and redouble in each step, mediating our interactions with digitally-rendered artifacts through the search and discovery process. As a result, scholars within the digital humanities community have begun advocating for the proper contextualization of cultural heritage datasets within the socio-technical systems in which they are created and utilized. One such approach to this contextualization is the data archaeology, a form of humanistic excavation of a dataset that Paul Fyfe defines as recover[ing] and reconstitut[ing] media objects within their changing ecologies . Within critical data studies, this excavation of a dataset - including its construction and mediation via machine learning - has proven to be a capacious approach. However, the data archaeology has yet to be adopted as standard practice among cultural heritage practitioners who produce such datasets with machine learning. In this article, I present a data archaeology of the Library of Congress’s Newspaper Navigator dataset, which I created as part of the Library of Congress’s Innovator in Residence program . The dataset consists of visual content extracted from 16 million historic newspaper pages in the Chronicling America database using machine learning techniques. In this case study, I examine the manifold ways in which a Chronicling America newspaper page is transmuted and decontextualized during its journey from a physical artifact to a series of probabilistic photographs, illustrations, maps, comics, cartoons, headlines, and advertisements in the Newspaper Navigator dataset . Accordingly, I draw from fields of scholarship including media archaeology, critical data studies, science and technology studies, and the autoethnography throughout. To excavate the Newspaper Navigator dataset, I consider the digitization journeys of four different pages in Black newspapers included in Chronicling America, all of which reproduce the same photograph of W.E.B. Du Bois in an article announcing the launch of The Crisis, the official magazine of the NAACP. In tracing the newspaper pages’ journeys, I unpack how each step in the Chronicling America and Newspaper Navigator pipelines, such as the imaging process and the construction of training data, not only imprints bias on the resulting Newspaper Navigator dataset but also propagates the bias through the pipeline via the machine learning algorithms employed. Along the way, I investigate the limitations of the Newspaper Navigator dataset and machine learning techniques more generally as they relate to cultural heritage, with a particular focus on marginalization and erasure via algorithmic bias, which implicitly rewrites the archive itself. In presenting this case study, I argue for the value of the data archaeology as a mechanism for contextualizing and critically examining cultural heritage datasets within the communities that create, release, and utilize them. I offer this autoethnographic investigation of the Newspaper Navigator dataset in the hope that it will be considered not only by users of this dataset in particular but also by digital humanities practitioners and end users of cultural heritage datasets writ large.
teaser: "In this article, I consider the Library of Congress’s Newspaper Navigator dataset, which I created as part of the Library of Congress’s Innovator-in-Residence program."
order: 3
---



## I. An Introduction to the Newspaper Navigator Dataset

In partnership with LC Labs, the National Digital Newspaper Program, and IT Design & Development at the Library of Congress, as well as Professor Daniel Weld at the University of Washington, I constructed the _Newspaper Navigator_ dataset as the first phase of my Library of Congress Innovator in Residence project, _Newspaper Navigator_ .[^1] The project has its origins in _Chronicling America_ , a database of digitized historic American newspapers created and maintained by the National Digital Newspaper Program, itself a partnership between the Library of Congress and the National Endowment for the Humanities. Content in _Chronicling America_ is contributed by state partners of the National Digital Newspaper Program who have applied for and received awards from the Division of Preservation and Access at the National Endowment for the Humanities<a class="footnote-ref" href="#mears2014"> [mears2014] </a>. At the time of the construction of the _Newspaper Navigator_ dataset in March, 2020, _Chronicling America_ contained approximately 16.3 million digitized historic newspaper pages published between 1789 and 1963, covering 47 states as well as Washington, D.C. and Puerto Rico. The technical specifications of the National Digital Newspaper Program require that each digitized page in _Chronicling America_ comprises the following digital artifacts<a class="footnote-ref" href="#national2020"> [national2020] </a>:

A page image in two raster formats:
Grayscale, scanned for maximum resolution possible between 300-400 DPI, relative to the original material, uncompressed TIFF 6.0Same image, compressed as JPEG2000

Optical character recognition (OCR) text and associated bounding boxes for words (one file per page image)PDF Image with Hidden Text, i.e., with text and image correlatedStructural metadata (a) to relate pages to title, date, and edition; (b) to sequence pages within issue or section; and (c) to identify associated image and OCR filesTechnical metadata to support the functions of a trusted repository


Additional artifacts and metadata are contributed for each digitized newspaper issue and microfilm reel. All digitized pages are in the public domain and are available online via a public search user interface,[^2] making _Chronicling America_ an immensely rich resource for the American public.

The central goal of _Newspaper Navigator_ is to re-imagine how the American public explores _Chronicling America_ by utilizing emerging machine learning techniques to extract, categorize, and search over the visual content and headlines in _Chronicling America_ ’s 16.3 million pages of digitized historic newspapers. _Newspaper Navigator_ was both inspired and directly enabled by the Library of Congress’s _Beyond Words_ crowdsourcing initiative<a class="footnote-ref" href="#ferriter2017"> [ferriter2017] </a>. Launched by LC Labs in 2017, _Beyond Words_ engages the American public by asking volunteers to identify and draw boxes around photographs, illustrations, maps, comics, and editorial cartoons on World War I-era pages in _Chronicling America_ , note the visual content categories, and transcribe the relevant textual information such as titles and captions.[^3] The thousands of annotations created by _Beyond Words_ volunteers are in the public domain and available for download online. _Newspaper Navigator_ directly builds on _Beyond Words_ by utilizing these annotations, as well as additional annotations of headlines and advertisements, to train a machine learning model to detect visual content in historic newspapers.[^4] Because _Beyond Words_ volunteers were asked to draw bounding boxes to include any relevant textual content, such as a photograph’s title, this machine learning model learns during training to include relevant textual content when predicting bounding boxes.[^5] Furthermore, in the _Transcribe_ step of _Beyond Words_ , the system provided the OCR with each bounding box as an initial transcription for the volunteer to correct; inspired by this, the _Newspaper Navigator_ pipeline automatedly extracts the OCR falling within each predicted bounding box in order to provide noisy textual metadata for each image. In the case of headlines, this method enables the headline text to be directly extracted from the bounding box predictions. Lastly, the pipeline generates image embeddings for the extracted visual content using an image classification model trained on ImageNet.[^6] A diagram of the full _Newspaper Navigator_ pipeline can be found in Figure 1.

{{< figure src="images/image1.png" caption="A diagram showing the _Newspaper Navigator_ pipeline, which processed over 16.3 million historic newspaper pages in _Chronicling America_ , resulting in the _Newspaper Navigator_ dataset." alt="A diagram of newspaper screenshots"  >}}


Over the course of 19 days from late March to early April of 2020, the _Newspaper Navigator_ pipeline processed 16.3 million pages in _Chronicling America_ ; the resulting _Newspaper Navigator_ dataset was publicly released in May, 2020. The full dataset, as well as all code written for this project, are available online and have been placed in the public domain for unrestricted re-use.[^7] Currently, the _Newspaper Navigator_ dataset can be queried using HTTPS and Amazon S3 requests. Furthermore, hundreds of pre-packaged datasets have been made available for download, along with associated metadata. These pre-packaged datasets consist of different types of visual content for each year, from 1850 to 1963, allowing users to download, for example, all of the maps from 1863 or all of the photographs from 1910. For more information on the technical aspects of the pipeline and the construction of the _Newspaper Navigator_ dataset, I refer the reader to<a class="footnote-ref" href="#lee2020b"> [lee2020b] </a>




## II. Why a Data Archaeology?

As machine learning and artificial intelligence play increasing roles in digitization and digital content stewardship, the Libraries, Archives, and Museums (LAM) community has repeatedly emphasized the importance of ensuring that these emerging methodologies are incorporated ethically and responsibly. Indeed, a major theme that emerged from the “Machine Learning + Libraries Summit” hosted by LC Labs in September, 2019, was that “there is much more “human” in machine learning than the name conveys” and that transparency and communication are first steps toward addressing the “human subjectivities, biases, and distortions” embedded within machine learning systems<a class="footnote-ref" href="#lclabs2020"> [lclabs2020] </a>. This data archaeology has been written in support of this call for transparency and responsible stewardship, which is echoed in the Library of Congress’s Digital Strategy, as well as the recommendations in Ryan Cordell’s report to the Library of Congress “ML + Libraries: A Report on the State of the Field,” Thomas Padilla’s OCLC position paper “Responsible Operations: Science, Machine Learning, and AI in Libraries” , and the University of Nebraska-Lincoln’s report on machine learning to the Library of Congress<a class="footnote-ref" href="#congress2019"> [congress2019] </a>;<a class="footnote-ref" href="#cordell2020"> [cordell2020] </a>;<a class="footnote-ref" href="#padilla2019"> [padilla2019] </a>;<a class="footnote-ref" href="#lorang2020"> [lorang2020] </a>. I write this data archaeology from my perspective of having created the dataset, and although I am not without my own biases, I have attempted to represent my work as honestly as possible. Accordingly, I seek not only to document the construction of the _Newspaper Navigator_ dataset through the lens of data stewardship but also to critically examine the dataset’s limitations. In doing so, I advocate for the importance of autoethnographic approaches to documenting a cultural heritage dataset’s construction from a humanistic perspective.

This article draws inspiration from recent works in media and data archaeology, including Paul Fyfe’s “An Archaeology of Victorian Newspapers” ; Bonnie Mak’s “Archaeology of a Digitization” ; Kate Crawford and Trevor Paglen’s “Excavating AI: The Politics of Images in Machine Learning Training Sets” ; and, most directly, Ryan Cordell’s “Qi-jtb the Raven: Taking Dirty OCR Seriously,” in which Cordell traces the digitization of a single issue of the _Lewisburg Chronicle_ from its selection by the Pennsylvania Digital Newspaper Project to its ingestion into the _Chronicling America_ online database, with a focus on the distortive effects of OCR<a class="footnote-ref" href="#fyfe2016"> [fyfe2016] </a>;<a class="footnote-ref" href="#mak2017"> [mak2017] </a>;<a class="footnote-ref" href="#crawford2019"> [crawford2019] </a>;<a class="footnote-ref" href="#cordell2017"> [cordell2017] </a>. As argued by Trevor Owens and Thomas Padilla, it is essential to “document how digitization practices and how the affordances of particular sources … produce unevenness in the discoverability and usability of collections” <a class="footnote-ref" href="#owens2020"> [owens2020] </a>. Recent works within the machine learning literature have analogously emphasized the importance of documenting the collection and curation efforts underpinning community datasets and machine learning models. Reporting mechanisms include “Datasheets for Datasets,”  “Dataset Nutrition Labels,”  “Data Statements for NLP,”  “Model Cards for Model Reporting,” and “Algorithmic Impact Assessments” <a class="footnote-ref" href="#gebru2020"> [gebru2020] </a>;<a class="footnote-ref" href="#holland2018"> [holland2018] </a>;<a class="footnote-ref" href="#bender2018"> [bender2018] </a>;<a class="footnote-ref" href="#mitchell2019"> [mitchell2019] </a>;<a class="footnote-ref" href="#reisman2018"> [reisman2018] </a>. This case study adopts a similar framing in stressing the importance of reporting mechanisms, with a particular focus on the data archaeology in the context of cultural heritage datasets.

In the following sections, I trace the digitization process and data flow for _Newspaper Navigator_ , beginning with the physical artifact of the newspaper itself and ending with the machine learning predictions that constitute the _Newspaper Navigator_ dataset, reflecting on each step through the lens of discoverability and erasure. In particular, I study four different _Chronicling America_ Black newspaper pages published in 1910, each depicting the same photograph of W.E.B. Du Bois, as the pages move through the _Chronicling America_ and _Newspaper Navigator_ pipelines. All four pages reproduce the same article by Franklin F. Johnson, a reporter from _The Baltimore Afro-American_ <a class="footnote-ref" href="#farrar1998"> [farrar1998] </a>; the headline is as follows:

NEW MOVEMENT

BEGINS WORK

Plan and Scope of the Asso-

ciation Briefly Told.

Will Publish the Crisis.

Review of Causes Which Led to the

Organization of the Association in

New York and What Its Policy Will

Be-Career and Work of Professor

W.E.B. Du Bois

The article describes the creation of the National Association for the Advancement of Colored People (NAACP), details W.E.B. Du Bois’s background, and announces the launch of _The Crisis_ , the official magazine of the NAACP, with Du Bois as Editor-in-Chief. The four pages comprise the front page of the October 14th, 1910, issue of the _Iowa State Bystander_ <a class="footnote-ref" href="#iowa1910"> [iowa1910] </a>; the 16th page of the October 15th, 1910, issue of _Franklin’s Paper the Statesman_ <a class="footnote-ref" href="#franklin1910"> [franklin1910] </a>; and the 2nd and 3rd pages of the October 15th, 1910, and November 26th, 1910, issues of _The Broad Ax_ , respectively<a class="footnote-ref" href="#ax1910a"> [ax1910a] </a>;<a class="footnote-ref" href="#ax1910b"> [ax1910b] </a>. All four digitized pages are reproduced in the Appendix.




## III. _Chronicling America_ : A Genealogy of Collecting, Microfilming, and Digitizing

Any examination of _Newspaper Navigator_ must begin with the genealogy of collecting, microfilming, and digitizing that dictates which newspapers have been ingested into the _Chronicling America_ database. The question of what to digitize is, in practice, answered and realized incrementally over decades, beginning at its most fundamental level with the question of which newspapers have survived and which have been reduced to lacunae in the historical record<a class="footnote-ref" href="#hardy2019"> [hardy2019] </a>.[^8] Historic newspapers present challenges for digitization in part due to the ephemerality of the physical printed newspaper itself: many newspapers were microfilmed and immediately discarded due to a fear that the physical pages would deteriorate.[^9] Indeed, almost all of the pages included in _Chronicling America_ have been digitized directly from microfilm. In the next section, I will examine the microfilm imaging process in more detail; however, in most cases, librarians selected newspapers for collecting and microfilming decades before the National Digital Newspaper Program was launched in 2004. These selections were informed by a range of factors including historical significance - itself a subjective, nebulous, and ever-evolving notion that has historically served as the basis for perpetuating oppression within the historical record. In “Chronicling White America,” Benjamin Fagan highlights the paucity of Black newspapers in _Chronicling America_ , in particular in relation to pre-Civil War era newspapers [Fagan 2016]. It is imperative to remember that this paucity can directly be traced back decades to the collecting and preserving stages.[^10] 

In regard to collecting, the newspaper page is both an informational object (i.e., the newspaper page as defined by its content) and a material object (i.e., the specific printed copy of the newspaper page)<a class="footnote-ref" href="#owens2018"> [owens2018] </a>. At some point in time, librarians accessioned a specific copy of each printed page and microfilmed it or contracted out the microfilming. The materiality of that specific printed page is a confluence of unique ink smudges, rips, creases, and page alignment, much of which is captured in the microfilm imaging process. Though we may not make much of a crease or a smudge on a digitized page when we find it in the _Chronicling America_ database, it can very well take on a life of its own with a machine learning algorithm in _Newspaper Navigator_ . The machine learning algorithm might deem two newspaper photographs as similar simply due to the presence of creases or smudges, even if the photographs are easily discernible to the naked eye, or the smudges are of entirely different origin (i.e., a printing imperfection versus a smudge from a dirty hand).

It is only by foregrounding these subtleties of the collection, preservation, and microfilming processes that we can understand the selection process for _Chronicling America_ in its proper context. The grant-seeking process dictates selection criteria for _Chronicling America_ by which state-level institutions including state libraries, historical societies, and universities apply for two years of grant funding from the National Digital Newspaper Program via the Division of Preservation and Access at the National Endowment for the Humanities. With the awarding of a grant, a state-level awardee then digitizes approximately 100,000 newspaper pages published in their state for inclusion in _Chronicling America_ <a class="footnote-ref" href="#national2020"> [national2020] </a>;<a class="footnote-ref" href="#neh2020"> [neh2020] </a>. The grant-seeking and awarding process is nuanced, but salient points include that state-level applicants must assemble an advisory board including scholars, teachers, librarians, and archivists to aid in the selection of newspapers, and grants are reviewed by National Endowment for the Humanities staff, as well as peer reviewers.[^11] 

Regarding selection criteria for newspaper titles, the National Digital Newspaper Program defines the following factors for state-level awardees to consider for content selection after a newspaper is determined to be in the public domain<a class="footnote-ref" href="#national"> [national] </a>:


 * image quality in the selection of microfilm
 * research value
 * geographic representation
 * temporal coverage
 * bibliographic completeness of microfilm copy
 * diversity (i.e., “newspaper titles that document a significant minority community at the state or regional level”)
 * whether the title is orphaned (i.e., whether the newspaper has “ceased publication and lack[s] active ownership” [Chronicling America no date])
 * whether the title has already been digitized.


Though factors such as research value are considered by each state awardee’s advisory board, as well as by the National Endowment for the Humanities and peer review experts, the titles included in _Chronicling America_ are largely dictated by which exist on microfilm and are of sufficient image quality within a state-level grantee’s collection. Thus, the significance of the collection and microfilming practices of decades prior cannot be understated.

I also highlight that assessing microfilmed titles based on image quality is a complex procedure in its own right. The National Digital Newspaper Program has made publicly available a number of resources devoted specifically to this task, including documents and video tutorials<a class="footnote-ref" href="#barrall2005"> [barrall2005] </a>;<a class="footnote-ref" href="#meta"> [meta] </a>. They articulate factors such as the microfilm generation (archive master, print master, or review copy), the material (polyester or acetate), the reduction ratio, and the physical condition. The detailed resources made available by the National Digital Newspaper Program, the Library of Congress, and the National Endowment for the Humanities for navigating this process are testaments to the multidimensional complexity of the selection process for _Chronicling America_ <a class="footnote-ref" href="#national2019"> [national2019] </a>;<a class="footnote-ref" href="#national"> [national] </a>;<a class="footnote-ref" href="#neh2020"> [neh2020] </a>.

We have not yet investigated the topic of digitization, and we have already encountered a profusion of factors from collection to digitization that mediate which artifacts appear in _Chronicling America_ and thus _Newspaper Navigator_ . Let us now examine the microfilm itself.




## IV. The Microfilm

In “What Computational Archival Science Can Learn from Art History and Material Culture Studies,” Lyneise Williams shares a powerful anecdote of coming across a physical copy of a 1927 issue of the French sports newspaper _Match L’Intran_ that featured accomplished Black Panamanian boxer, Alfonso Teofilo Brown, on the front cover<a class="footnote-ref" href="#williams2019"> [williams2019] </a>. Williams describes Brown as “glowing. He looked like a 1920s film star rather than a boxer” <a class="footnote-ref" href="#williams2019"> [williams2019] </a>. Curious to learn more about the printing process, Williams discovered that the issue of _Match L’Intran_ was produced using rotogravure, a specific printing process that could “capture details in dark tones” <a class="footnote-ref" href="#williams2019"> [williams2019] </a>. However, when Williams found a version of the same newspaper cover that had been digitized from microfilm, it was apparent that the microfilming process had washed out the detail of the rotogravure, reducing Brown to a “flat black, cartoonish form” <a class="footnote-ref" href="#williams2019"> [williams2019] </a>. Williams relays the anecdote to articulate that the microfilming process itself is thus a form of erasure for communities of color<a class="footnote-ref" href="#williams2019"> [williams2019] </a>.

The grayscale saturation of photographs induced by microfilming is widely documented and recognizable to most researchers who have ever worked with the medium<a class="footnote-ref" href="#baker2001"> [baker2001] </a>; however, Lyneise Williams’s article affords us a lens into what precisely is lost amongst the distortive effects of the microfilming process. This erasure via microfilming can be seen in _Chronicling America_ directly. In Figure 2, I show the same photograph of W.E.B. Du Bois as it appears in 4 different _Chronicling America_ newspaper pages published during October and November of 1910 and digitized from microfilm<a class="footnote-ref" href="#iowa1910"> [iowa1910] </a>;<a class="footnote-ref" href="#franklin1910"> [franklin1910] </a>;<a class="footnote-ref" href="#ax1910a"> [ax1910a] </a>;<a class="footnote-ref" href="#ax1910b"> [ax1910b] </a>. The phenomenon described by Williams is immediately recognizable in these four images: Du Bois’s facial features are distorted by the grayscale saturation. In the case of the _Iowa State Bystander_ , Du Bois has been rendered into a silhouette.

Moreover, each digitized reproduction reveals unique visual qualities, varying in contrast, sharpness, and noise - a testament to the confluence of mediating conditions from printing through digitization that have rendered each newspaper photograph in digital form. Even in the case of the two images reproduced in the _The Broad Ax_ , which were digitized from the very same microfilm reel (reel #00280761059) by the University of Illinois at Urbana-Champaign Library, variations are still apparent. To understand how these subtle differences between images are amplified through digitization, we now turn to optical character recognition.

{{< figure src="images/image2.jpg" caption="The same image of W.E.B. Du Bois reproduced in 4 different digitized Black newspapers in _Chronicling America_ from 1910. Note that the combined effects of printing, microfilming, and digitizing have led to different visual effects in each image, ranging from contrast to sharpness." alt="Images of W.E.B. Du Bois"  >}}





## V. OCR

Optical character recognition, commonly called OCR, refers to machine learning algorithms that are trained to read images of typewritten text and output machine-readable text, thereby providing the bridge between an image of typewritten text and the transcribed text itself. Because OCR algorithms are “trained and evaluated using labeled data: examples with ground-truth classification labels that have been assigned by another means,” the algorithms are considered a form ofsupervised learningin the machine learning literature<a class="footnote-ref" href="#lee2019"> [lee2019] </a>. OCR engines are remarkably powerful in their ability to improve access to historic texts. Indeed, OCR is a crucial form of metadata for _Chronicling America_ , enabling keyword search in the search portal and making possible scholarship with the newspaper text at large scales.[^12] However, OCR is not perfect. Although humans are able to discern anEfrom anRon a digitized page even if the type has been smudged, an OCR engine is not always able to do so: its performance is dependent on factors ranging from the sharpness of text in an image to printing imperfections to the specific typography on the page.

In Figure 3, I show the same four images shown in Figure 2, along with OCR transcriptions of the captions provided by _Chronicling America_ . All four transcriptions fail to reproduce the true caption with 100% accuracy, differing from one another by at least one character. Consequently, a keyword search ofW. E. B. Du Boisover the raw text would not register the caption for any of the four photographs (the _Chronicling America_ search portal utilizes a form of relevance search to alleviate this problem). These examples reveal how sensitive OCR engines are to slight perturbations, or “noise,” in the digitized images, from ink smudges to text sharpness to page contrast. Though the NDNP awardees who contributed these pages may have utilized different OCR engines or chosen different OCR settings, the OCR for the two image captions from _The Broad Ax_ that have been digitized from the very same microfilm reel was in all likelihood generated using the same OCR engine and settings. Put succinctly, OCR engines amplify the noise from both the material page and the digitization pipeline.[^13] 

{{< figure src="images/image3.jpg" caption="The OCR transcriptions of the caption “W. E. B. DU BOIS, PH. D.” appearing in the image of W.E.B. Du Bois reproduced in 4 different digitized Black newspapers in _Chronicling America_ . These OCR transcriptions are provided by _Chronicling America_ ." alt="four images of W.E.B. Du Bois"  >}}


Though OCR engines have become standard components of digitization pipelines, it is important to remember that OCR engines are themselves machine learning models that have been trained on sets of transcribed typewritten pages. Like any machine learning model, OCR predictions are thus subject to biases encoded not only in the OCR engine’s architecture but also in the training data itself. Though it is often calledalgorithmic bias, this bias is undeniably human, in that the construction of training data machine learning models are imprinted with countless human decisions and judgment calls. For example, if an OCR engine is trained on transcriptions that consistently misspell a word, the OCR engine will amplify this misspelling across all transcriptions of processed pages.[^14] A recurring theme of algorithmic bias is that it is a force for marginalization, especially in the context of how we navigate information digitally. In _Algorithms of Oppression_ , Safiya Noble describes how Google’s search engine consistently marginalizes women and people of color by displaying search results that reinforce racism<a class="footnote-ref" href="#noble2018"> [noble2018] </a>. This bias is not restricted to Google: in _Masked by Trust: Bias in Library Discovery_ , Matthew Reidsma articulates how library search engines suffer from similar biases<a class="footnote-ref" href="#reidsma2019"> [reidsma2019] </a>. Despite the fact that knowledge of algorithmic bias in relation to search engines and image recognition tools is becoming increasingly widespread among the cultural heritage community, the errors introduced by OCR engines are often accepted as inevitable without critical inquiry from this perspective. However, algorithmic bias is a useful framework for examining OCR engines<a class="footnote-ref" href="#alpertabrams2016"> [alpertabrams2016] </a>.

Perhaps the most significant challenge to studying OCR engines is that the best-performing and most widely-used OCR engines are proprietary. Though ABBYY FineReader and Google Cloud Vision API offer high performance, the systems fundamentally are black boxes: we have no access to the underlying algorithms or the training data. The ability to audit a system is crucial to developing an understanding of how it works and the biases it encodes. The fact that many OCR engines are opaque prevents us from disentangling whether poor performance on a particular page is due to algorithmic limitations or due to a lack of relevant training data. The distinction is significant: the former may reflect an algorithmic upper bound, whereas the latter reflects decisions made by humans.

Indeed, algorithmic bias distorts and occludes the historical record, as it is made discoverable through OCR. Discrepancies in OCR performance for different languages and scripts is a consequence of human prioritization, from the collection of training data and lexicons to the development of the algorithms themselves. As articulated by Hannah Alpert-Abrams in “Machine Reading the _Primeros Libros_ ,”  “the machine-recognition of printed characters is a historically charged event, in which the system and its data conspire to embed cultural biases in the output, or to affix them as supplementary information hidden behind the screen” <a class="footnote-ref" href="#alpertabrams2016"> [alpertabrams2016] </a>. Alpert-Abrams’s work reveals how the OCR inaccuracies for indigenous languages recorded in colonial scripts perpetuate colonialism. For other languages such as Ladino, typically typeset in Rashi script, the lack of high-performing OCR has presented consistent challenges for digitization and scholarship.

In the case of _Chronicling America_ , the National Digital Newspaper Program is exemplary in its efforts to support OCR for non-English languages. In the Notice of Funding Opportunity for the National Digital Newspaper Program produced by the Division of Preservation of Access at the National Endowment for the Humanities, OCR performance in different languages is explicitly addressed: “Applicants proposing to digitize titles in languages other than English must include staff with the relevant language expertise to review the quality of the converted content and related metadata” <a class="footnote-ref" href="#neh2020"> [neh2020] </a>. I have included this discussion of OCR and algorithmic bias to offer a broader provocation regarding machine learning and digitization: how much text in digitized sources has been transmuted by this effect and thus effectively erased due to inaccessibility when using search and discovery platforms?




## VI. The Visual Content Recognition Model

I will now turn to the _Newspaper Navigator_ pipeline itself, in particular the visual content recognition model. Trained on annotations from the _Beyond Words_ crowdsourcing initiative, as well as additional annotations of headlines and advertisements, the visual content recognition model detects photographs, illustrations, maps, comics, editorial cartoons, headlines, and advertisements on historic newspaper pages.

As described in the previous section, examining training data is an essential component of auditing any machine learning model, from understanding how the dataset was constructed to uncovering any biases in the composition of the dataset itself. For the visual content recognition model, this examination begins with _Beyond Words_ . Launched in 2017 by LC Labs, _Beyond Words_ has collected to-date over 10,000 verified annotations of visual content in World War 1-era newspaper pages from _Chronicling America_ . The _Beyond Words_ workflow consists of the three steps listed below:


AMarkstep, in which volunteers are asked to draw bounding boxes around visual content on the page<a class="footnote-ref" href="#lclabs2017a"> [lclabs2017a] </a>. The instructions read as follows:


> In the Mark step, your task is to identify and select pictures in newspaper pages. For our project,picturesmeans illustrations, photographs, comics, and cartoons. You'll use the marking tool to draw a box around the picture using your mouse. After you have marked all pictures on the newspaper page, click the ‘DONE’ button. Skip the page altogether by clicking theSkip this pagebutton. If no illustrations, photographs, or cartoons appear on the page, click theDONEbutton. Not sure if a picture should be marked? Select theDone for now, more left to markbutton so another volunteer can help finish that page. Please do not select pictures within advertisements.


ATranscribestep, in which volunteers are asked to transcribe the caption of the highlighted visual content, as well as note the artist and visual content category (Photograph,Illustration,Map,Comics/Cartoon,Editorial Cartoon)<a class="footnote-ref" href="#lclabs2017b"> [lclabs2017b] </a>. The transcription is pre-populated with the OCR falling within the bounding box in question. The instructions for this step state:


> Most pictures have captions or descriptions. Enter the text exactly as you see it. Include capitalization and punctuation, but remove hyphenation that breaks words at the end of the line. Use new lines to separate different parts of captions and descriptions. You can zoom in for better looks at the page. You can also selectView the original pagein the upper right corner of the screen to view the original high resolution image of the newspaper.


An example of this step can be seen in Figure 4.

AVerifystep, in which volunteers are asked to select the best caption for an identified region of visual content from at least two examples; alternatively, a volunteer can add another caption<a class="footnote-ref" href="#lclabs2017c"> [lclabs2017c] </a>. The instructions state:


> Choose the transcription that most accurately captures the text as written. If multiple transcriptions appear valid, choose the first one. If the selected region isn't appropriate for the prompt, clickBad region.




{{< figure src="images/image4.png" caption="A screenshot showing an example of theTranscribestep of the _Beyond Words_ workflow. Note that the photograph caption is pre-populated using the OCR falling within the bounding box<a class=\"footnote-ref\" href=\"#lclabs2017b\"> [lclabs2017b] </a>." alt="screenshot of a newspaper article and a text box where the article has been transcribed"  >}}


For the purposes of _Newspaper Navigator_ , only the bounding boxes from theMarkstep and the category labels from theTranscribestep were utilized as training data; however, understanding the full workflow is essential because annotations are consideredverifiedonly if they have passed through the full workflow.

A number of factors contribute to which _Chronicling America_ pages were processed by volunteers in _Beyond Words_ . First, the temporal restriction to World War 1-era pages affects the ability of the visual content recognition model to generalize: after all, if the model is trained on World War 1-era pages, how well should we expect it to perform on 19th century pages? I will return to this question later in the section. Moreover, _Beyond Words_ volunteers could select either an entirely random page or a random page from a specific state, an important affordance from an engagement perspective, as volunteers could explore the local histories of states in which they are interested. But this affordance is also imprinted on the training data, as certain states - and thus, certain newspapers - appear at a higher frequency than if the World War-1 era _Chronicling America_ pages had been drawn randomly from this temporal range in _Chronicling America_ .

Furthermore, it should be noted that theMarkandTranscribesteps - specifically, drawing bounding boxes and labeling the visual content category - are complex tasks. Because newspaper pages are remarkably heterogenous, ambiguities and edge-cases abound. Should a photo collage be marked as one unit or segmented into constituent parts? What precisely is the distinction between an editorial cartoon and an illustration? How much relevant textual content should be included in a bounding box? Naturally, volunteers did not always agree on these choices. In this regard, the notion of a ground-truth, a set of perfect annotations against which we can assess performance, is itself called into question. Moreover, with thousands of annotations, mistakes in the form of missed visual content, as well as misclassifications, are inevitable.[^15] These ambiguities and errors are natural components of _any_ training dataset and must be taken into account when analyzing a machine learning model’s predictions.

A breakdown of _Beyond Words_ annotations included in the training data can be found in the second column of Table 1. I downloaded these 6,732 publicly-accessible annotations as a JSON file on December 1, 2019. Table 1 reveals an imbalance between the number of examples for each category; in the language of machine learning, this is called _class imbalance_ . While the discrepancy between maps and photographs is to be expected, the fact that so few maps were included was concerning from a machine learning standpoint: a machine learning algorithm’s ability to generalize to new data is dependent on having many diverse training examples. To address this concern, I searched _Chronicling America_ and identified 134 pages published between January 1st, 1914, and December 31st, 1918, that contain maps. I then annotated these pages myself.

In addition, during the development of the _Newspaper Navigator_ pipeline, I realized the value in training the visual content recognition model to identify headlines and advertisements. Consequently, I added annotations of headlines and advertisements for all 3,559 pages included in the training data. The statistics for this augmented set of annotations can be found in the third column of Table 1. Though I attempted to use a consistent approach to annotating the headlines and advertisements, my interpretation of what constitutes a headline is certainly not unimpeachable: I am not a trained scholar of periodicals or of print culture; even if I were, the task itself is inevitably subjective. Furthermore, I made decisions to annotate large grids of classified ads as a single ad to expedite the annotation process. Whether this was a correct judgment call can be debated. Lastly, annotating all 3,559 pages for headlines and advertisements required a significant amount of time, and there are inevitably mistakes and inconsistencies embedded within the annotations. My own decisions in terms of how to annotate, as well as my mistakes and inconsistencies, are embedded within the visual content recognition model through training. For those interested in examining the training data directly, the data can be found in the GitHub repository for this project<a class="footnote-ref" href="#lee2020a"> [lee2020a] </a>.
A breakdown of _Beyond Words_ annotations included in the training data for the visual content recognition model, as well as all annotations constituting the training data.CategoryBeyond Words AnnotationsTotal AnnotationsPhotograph4,1934,254Illustration1,0281,048Map79215Comic/Cartoon1,1391,150Editorial Cartoon293293Headline-27,868Advertisement-13,581 _Total_ 6,73248,409
Beyond the construction of the training data, I made manifold decisions regarding the selection of the correct model architecture and the training of the model. Because this discussion surrounding these choices is quite technical, I refer the reader to<a class="footnote-ref" href="#lee2020b"> [lee2020b] </a>for an in-depth examination. However, I will state that the choice of model, the number of iterations for which the model was trained, and the choice of model parameters are all of significant import for the resulting trained model and consequently, the _Newspaper Navigator_ dataset.

I will now turn to the visual content recognition model’s outputs in relation to the _Newspaper Navigator_ pipeline. The model itself consumes a lower-resolution version of a _Chronicling America_ page as input and then outputs a JSON file containing predictions, each of which consists of bounding box coordinates,[^16] the predicted class (i.e.,photograph,map, etc.), and a confidence score generated by the machine learning model.[^17] Cropping out and saving the visual content required extra code to be written. Because the high-resolution images of the _Chronicling America_ pages, in addition to the METS/ALTO OCR, amount to many tens of terabytes of data, questions of data storage became major considerations in the pipeline. I chose to save the extracted visual content as lower-resolution JPEG images in order to reduce the upload time and lessen the storage burden. Though the _Newspaper Navigator_ dataset retains identifiers to all high-resolution pages in _Chronicling America,_ the images in the _Newspaper Navigator_ dataset are altered by the downsampling procedure. This downsampling procedure should be free of any significant biasing effects.

For visual content recognition, “Newspaper Navigator” utilized an object detection model, which is a type of widely-used computer vision technique for identifying objects in images. The performance for computer vision techniques is regularly measured using metrics such as average precision. For “Newspaper Navigator” , the model’s performance on a specific page, as measured by average precision, is dependent on a confluence of factors. These factors include the page’s layout, artifacts and distortions introduced in the microfilming and digitization process, and - most importantly - the composition of the training data. Thus, each image isseendifferently by the visual content recognition model. In Figure 5, I show the four images of W.E.B. Du Bois, as identified by the visual content recognition model and saved in the _Newspaper Navigator_ dataset. Each image is cropped slightly differently. In the case of the image from the _Iowa State Bystander_ , extra text is included, while in the case of the images from _The Broad Ax_ , the captions are partially cut off. The loss in image quality is due to the aforementioned downsampling performed by the pipeline. This downsampling leads to artifacts such as the dots appearing on Du Bois’s face in the image from the _Iowa State Bystander_ , as well as the streaks in the image from _Franklin’s Paper the Statesman_ , that are not present in Figure 2.

Returning to the question of the visual content recognition model’s performance on pages published outside of the temporal range of the training data (1914-1918), it is possible to provide a quantitative answer by measuring average precision on test sets of annotated pages from different periods of time. In<a class="footnote-ref" href="#lee2020b"> [lee2020b] </a>, I describe this analysis in detail and demonstrate that the performance declines for pages published between 1875 and 1900 and further declines for pages published between 1850 and 1875. This confirms that the composition of the training data directly manifests in the model’s performance. While it is certainly the case that the _Newspaper Navigator_ dataset can still be used for scholarship related to 19th century newspapers in _Chronicling America_ , any scholarship with the 19th century visual content in the _Newspaper Navigator_ dataset must consider how the dataset may skew what visual content is represented.

{{< figure src="images/image5.jpg" caption="The four images of W.E.B. Du Bois, as identified by the visual content recognition model and included in the _Newspaper Navigator_ dataset<a class=\"footnote-ref\" href=\"#navigator1910a\"> [navigator1910a] </a>;<a class=\"footnote-ref\" href=\"#navigator1910c\"> [navigator1910c] </a>;<a class=\"footnote-ref\" href=\"#navigator1910e\"> [navigator1910e] </a>;<a class=\"footnote-ref\" href=\"#navigator1910g\"> [navigator1910g] </a>." alt="four images of W.E.B. Du Bois"  >}}


Let me conclude this section with a discussion of the act of visual content extraction itself in relation to digitization. While this extraction enables a wide range of affordances for searching _Chronicling America_ , it is also an act of decontextualization: visual content no longer appears in relation to the _mise-en-page_ . In the Appendix, the full pages containing the photographs of W.E.B. Du Bois are reproduced, showing each photograph in context. Only by examining the full pages does it become clear that the article featuring W.E.B. Du Bois was printed with a second article in the _Iowa State Bystander_ and _The Broad Ax_ , the headline of which reads: “ANTI-LYNCHING SOCIETY ORGANIZED IN BOSTON — Afro-American Women Unite For Active Campaign Against Injustice.” Furthermore, upon examination, the _Iowa State Bystander_ front page features the article on _The Crisis_ and W.E.B. Du Bois as the most prominent article of the issue. Though links between the extracted visual content and the original _Chronicling America_ pages are always retained, this decontextualization inevitably transmutes _how_ we perceive and interact with the visual content in _Chronicling America_ . Indeed, all uses of machine learning for metadata enhancement are a form of decontextualization, centering the user’s discovery and analysis of content around the metadata itself.




## VII. Prediction Uncertainty

Perhaps the most fundamental question to ask of the _Newspaper Navigator_ dataset is:How many photographs does the dataset contain?Because the dataset has been constructed using a machine learning model, predictions are ultimately probabilistic in nature, quantified by the confidence score returned by the model. This begs the question of what counts as an identified unit of visual content: a user is much more inclined to tally a prediction of a map if it has an associated confidence score of 99% rather than 1%. However, choosing this cut is fundamentally a subjective decision, informed by the user’s end goals with the dataset. In the language of machine learning, picking a stringent confidence cut (i.e., only counting predictions with high confidence scores) emphasizes _precision_ : a prediction of a photograph likely corresponds to a true photograph, but the predictions will suffer from false negatives. Conversely, picking a loose confidence cut (i.e., counting predictions with low confidence scores) emphasizes _recall_ : most true photographs are identified as such, but the predictions will suffer from many false positives. In this regard, the total number of images in the _Newspaper Navigator_ dataset is dependent on one’s desired tradeoff between precision and recall. In Table 2, I show the dynamic range of the dataset size, as induced by three different cuts on confidence score: 90%, 70%, and 50%. Figure 6 shows the effects of different cuts on confidence score for the page featuring W.E.B. Du Bois in the November 26,1910, issue of _The Broad Ax_ .
The number of occurrences of each category of visual content in the _Newspaper Navigator_ dataset with confidence scores above the listed thresholds (0.9, 0.7, 0.5).Category≥ 90%≥ 70%≥ 50%Photograph1.59 x 1062.63 x 1063.29 x 106Illustration8.15 x 1052.52 x 1064.36 x 106Map2.07 x 1054.59 x 1057.54 x 105Comic/Cartoon5.35 x 1051.23 x 1062.06 x 106Editorial Cartoon2.09 x 1056.67 x 1051.27 x 106Headline3.44 x 1075.37 x 1076.95 x 107Advertisement6.42 x 1079.48 x 1071.17 x 108 _Total_ 1.02 x 1081.56 x 1081.98 x 108
{{< figure src="images/Figure_6.jpg" caption="The same page of _The Broad Ax_ from November 26, 1910, along with predictions from the visual content recognition model, thresholded on confidence score at 5%, 50%, 70%, and 90%<a class=\"footnote-ref\" href=\"#navigator1910g\"> [navigator1910g] </a>;<a class=\"footnote-ref\" href=\"#navigator1910h\"> [navigator1910h] </a>. Note that red corresponds to a prediction ofphotograph, cyan corresponds to a prediction ofheadline, and blue corresponds to a prediction ofadvertisement." alt="screenshot of four newspaper pages"  >}}


Rather than pre-selecting a confidence score threshold, the _Newspaper Navigator_ dataset contains all predictions with confidence scores greater than 5%,[^18] allowing the user to define their own confidence cut when querying the dataset. However, the website for the _Newspaper Navigator_ dataset also includes hundreds of pre-packaged datasets in order to make it easier for users to work with the dataset. In particular, users can download zip files containing all of the visual content of a specific type with confidence scores greater than or equal to 90%, for any year from 1850 to 1963. I made this choice of 90% as the threshold cut for these pre-packaged datasets based on heuristic evidence from inspecting sample pre-packaged datasets by eye. However, as articulated above, based on different use cases, this cut of 90% may be too restrictive or permissive: relevant visual content may be absent from the pre-packaged dataset or lost in a sea of other examples. In Figure 7, I show the visual content recognition model’s confidence scores for the four images of W.E.B. Du Bois described throughout this data archaeology. The effect of a cut on confidence score can be seen here: selecting a cut of 95% would exclude the image from “Franklin’s Paper the Statesman” . I raise this point to emphasize that even this seemingly innocuous choice of 90% for the pre-packaged datasets alters the discovery process and thus can have an impact on scholarship.

{{< figure src="images/image8.jpg" caption="The visual content recognition model’s confidence score for each of the four images of W.E.B. Du Bois. Note how the model assigns a different confidence score to each identified image<a class=\"footnote-ref\" href=\"#navigator1910b\"> [navigator1910b] </a>;<a class=\"footnote-ref\" href=\"#navigator1910d\"> [navigator1910d] </a>;<a class=\"footnote-ref\" href=\"#navigator1910f\"> [navigator1910f] </a>;<a class=\"footnote-ref\" href=\"#navigator1910h\"> [navigator1910h] </a>." alt="Four images of W.E.B. Du Bois"  >}}


Just as the bounding box predictions themselves are affected by the training data, as well as newspaper page layout, date of publication, and noise from the digitization pipeline, so too are the confidence scores. In particular, the visual content recognition model suffers from high-confidence misclassifications, for example, crossword puzzles that are identified as maps with confidence scores greater than 90%. High-confidence misclassifications pose challenges for machine learning writ large, and the field of explainable artificial intelligence is largely devoted to developing tools for understanding this type of misclassification<a class="footnote-ref" href="#weld2019"> [weld2019] </a>. However, these high-confidence misclassifications can often be traced back to the composition of the training set. For example, the fact that the visual content recognition model sometimes identifies crossword puzzles as maps with high confidence is likely due to the fact that the training data did not contain enough labeled examples of maps and crossword puzzles for the visual content recognition model to differentiate them with high accuracy.

The questions surrounding confidence scores and probabilistic descriptions of items is by no means restricted to the _Newspaper Navigator_ dataset. I echo Thomas Padilla’s assertion that “attempts to use algorithmic methods to describe collections must embrace the reality that, like human descriptions of collections, machine descriptions come with varying measure of certainty” <a class="footnote-ref" href="#padilla2019"> [padilla2019] </a>. Machine-generated metadata such as OCR are also fundamentally probabilistic in nature; this fact is not immediately apparent to end users of cultural heritage collections because cuts on confidence score are typically chosen before surfacing the metadata. Effectively communicating confidence scores, probabilistic descriptions, and the decisions surrounding them to end users remains a challenge for content stewards.




## VIII. OCR Extraction

In the _Newspaper Navigator_ pipeline, a textual description of each prediction is obtained by extracting the OCR within each predicted bounding box. The resulting textual description is thus dependent on not only the OCR provided by _Chronicling America_ but also the exact coordinates of the bounding box: if the coordinates of a word in the localized OCR extend beyond the bounds of the box, the word is excluded. I experimented with utilizing tolerance limits to allow words that extend just beyond the bounds of the boxes to be included, but doing so ultimately introduces false positives as well, as words from neighboring articles or visual content were inevitably included some fraction of the time. Once again, the tradeoff between false positives and false negatives is manifest.

In Figure 8, I show the textual descriptions of the four images of W.E.B. Du Bois, as identified by the _Newspaper Navigator_ pipeline. Significantly, in the _Newspaper Navigator_ dataset, the OCR is stored as a list of words, with line breaks removed; these lists are what appear in Figure 8. These four examples provide intuition as to how the captions are altered. While the examples from the _Iowa State Bystander_ and _Franklin’s Paper the Statesman_ both have very similar captions as shown in Figure 3, the captions for both of the examples from _The Broad Ax_ are unrecognizable. Because the bounding boxes have clipped the caption, none of the characters from the proper OCR captions from Figure 3 are present. Furthermore, the captions contain OCR noise due to the OCR engine attempting to read text from the photographs. Consequently, the mentions of W.E.B. Du Bois are erased from the textual descriptions in the _Newspaper Navigator_ dataset. The visual content in the _Newspaper Navigator_ dataset is thus decontextualized not only in the sense that the visual content is extracted from the newspaper pages but also in the sense that the OCR extraction method further alters the textual descriptions. While the images from the _Iowa State Bystander_ and _Franklin’s Paper the Statesman_ are still recoverable with fuzzy keyword search, the two images from _The Broad Ax_ are impossible to retrieve with _any_ form of keyword search, revealing another instance in which employing automated techniques for collections processing affects discoverability.

{{< figure src="images/image9.jpg" caption="The textual descriptions of each image, as extracted from the OCR and saved in the _Newspaper Navigator_ dataset<a class=\"footnote-ref\" href=\"#navigator1910b\"> [navigator1910b] </a>;<a class=\"footnote-ref\" href=\"#navigator1910d\"> [navigator1910d] </a>;<a class=\"footnote-ref\" href=\"#navigator1910f\"> [navigator1910f] </a>;<a class=\"footnote-ref\" href=\"#navigator1910h\"> [navigator1910h] </a>." alt="four images of W.E.B. Du Bois"  >}}


Fortunately, visual content can still be recovered using similarity search over the images themselves; these methods are discussed in detail in the next section. However, in the case of headlines, the errors introduced by OCR engines and the subsequent OCR extraction have no recourse, as similarity search for images of headlines would only capture similar typography and text layout.[^19] 

To illustrate the effects of this OCR extraction on headlines, I reproduce in Table 3 the extracted OCR as it appears in the _Newspaper Navigator_ dataset for Franklin F. Johnson’s headline:

NEW MOVEMENT

BEGINS WORK

Plan and Scope of the Asso-

ciation Briefly Told.

Will Publish the Crisis.

Review of Causes Which Led to the

Organization of the Association in

New York and What Its Policy Will

Be-Career and Work of Professor

W.E.B. Du Bois
The extracted OCR associated with each of the four photographs of W.E.B. Du Bois<a class="footnote-ref" href="#navigator1910b"> [navigator1910b] </a>;<a class="footnote-ref" href="#navigator1910d"> [navigator1910d] </a>;<a class="footnote-ref" href="#navigator1910f"> [navigator1910f] </a>;<a class="footnote-ref" href="#navigator1910h"> [navigator1910h] </a>. _Iowa State Bystander_ (14 Oct. 1910) _Franklin’s Paper the Statesman_ (15 Oct. 1910) _The Broad Ax_ (15 Oct. 1910) _The Broad Ax_ (26 Nov. 1910)98.72%99.57%99.76%99.70%[NEW,MOVEMENT,BEGINS,WORK,and,Plan,Scope,of,the,Asso\u00ad,ciation,Briefly,Told.,WILL,PUBLISH,THE,CRISIS.,Review,of,Causae,Which,Lad,to,the,Organisation,of,the,Auooiation,In,Naw,York,and,JWhat,It*,Polioy,Will,Ba\u2014Career,and,Wark,of,Profeasor][NEW,MOVEMENT,BEGINS,WORK,Plan,and,Scope,of,the,Asso,ciation,Briefly,Told.,WILL,PUBLISH,THE,CRISIS.][NEW,MOVEMENT,BEGINS,WORK,Plan,and,Sep,if,the,Asso,ciation,Briefly,Told.,WILL,PUBLISH,THE,CRISIS,,Be,Career,nnd,Work,of,Professor,W.,E.,B.,Du,Bois.,Review,of,Causes,Which,Led,to,the,Oraanteallon,of,th.,A.Me!.!?n,i,i,New,York,and,What,IU,Policy,Will][NEW,MOVEMENT,BEGINS,WORK,Plan,and,Scope,of,the,Asso,ciation,Briefly,Told.,WILL,PUBLISH,THE,CRISIS.,Review,of,Causes,Which,Lad,to,tha,Organization,of,the\",Association,In,New,York,and,What,Its,Policy,Will]
The full pages are reproduced in the appendix for reference. Notably, all four extracted headlines contain OCR errors, as well as missing words due to the OCR extraction. The visual content recognition model consistently fails to include the last line of the headline, “W.E.B. Du Bois,” revealing another case in which Du Bois’s name is rendered inaccessible by keyword search in the _Newspaper Navigator_ dataset.




## IX. Image Embeddings

Animage embeddingcanonically refers to a low-dimensional representation of an image, often a list of a few hundred or a few thousand numbers, that captures much of the image’s semantic content. Image embeddings are typically generated by feeding an image into a pre-trained neural image classification model (i.e., a model that takes in an image and outputs a label ofdogorcat) and extracting a representation of the image from one of the model’s hidden layers, often the penultimate layer.[^20] Image embeddings are valuable for three reasons:

Image embeddings are remarkably adept at capturing semantic similarity between images. For example, images of dogs tend to be clustered together in embedding space, with images of bicycles in another cluster and images of buildings in yet another. These clusters can be fine-grained: sometimes, the red bicycles are grouped closer together than the blue bicycles.Image embeddings can be constructed by feeding images into an image classification model already trained on another dataset (such as ImageNet), meaning that generating image embeddings is a useful method for comparing images without having to construct training data by labeling images.Image embeddings are low-dimensional and thus much smaller in size than the images themselves (i.e., on the order of kilobytes instead of megabytes). As a result, image embeddings are much less computationally expensive to compare to one another when conducting similarity search, clustering, or related tasks. In short, image embeddings speed up image comparison.


Utilizing image embeddings to visualize and explore large collections of images has become an increasingly common approach among cultural heritage practitioners. Projects and institutions that have utilized image embeddings for visualizing cultural heritage collections include the Yale Digital Humanities Lab’s PixPlot interface<a class="footnote-ref" href="#yale2017"> [yale2017] </a>, the National Neighbors project<a class="footnote-ref" href="#lincoln2019"> [lincoln2019] </a>, Google Arts and Culture<a class="footnote-ref" href="#googlearts2018"> [googlearts2018] </a>, The Norwegian National Museum’s Principal Components project<a class="footnote-ref" href="#nasjonalmuseet2017"> [nasjonalmuseet2017] </a>, the State Library of New South Wales’s Aero Project<a class="footnote-ref" href="#geraldo2020"> [geraldo2020] </a>, the Royal Photographic Society<a class="footnote-ref" href="#vane2018"> [vane2018] </a>, The American Museum of Natural History<a class="footnote-ref" href="#foo2019"> [foo2019] </a>, and The National Library of the Netherlands<a class="footnote-ref" href="#lonij2017"> [lonij2017] </a>;<a class="footnote-ref" href="#weavers2020"> [weavers2020] </a>. These visualizations provide insights into broader themes in the collections, thereby allowing curators, researchers, and the public to explore collections at a scale previously only possible by organizing images by color or other low-level features.[^21] In this regard, image embeddings provide new affordances for searching over images that complement canonical faceted and keyword search.

Because these image embeddings enable these visualization approaches and open the door to similarity search and recommendation, I opted to include image embeddings as part of the _Newspaper Navigator_ pipeline. Indeed, these image embeddings power the similarity search functionality in the _Newspaper Navigator_ user interface and, in this regard, are crucial to the broader vision of the project<a class="footnote-ref" href="#lee2020c"> [lee2020c] </a>.[^22] To generate the embeddings, I utilized ResNet-18 and ResNet-50, two variants of a prominent deep learning architecture for image classification, both of which had already been pre-trained on ImageNet<a class="footnote-ref" href="#he2016"> [he2016] </a>.

ImageNet is perhaps the most well-known image dataset in the history of machine learning. Constructed by scraping publicly available images from the internet and recruiting Amazon Mechanical Turk workers to annotate the images, ImageNet contains approximately 14 million images across 20,000 categories<a class="footnote-ref" href="#deng2009"> [deng2009] </a>;<a class="footnote-ref" href="#imagenet2020a"> [imagenet2020a] </a>. Kate Crawford and Trevor Paglen’s essay “Excavating AI: The Politics of Images in Machine Learning Training Sets” offers a history and incisive critique of the classification schema of ImageNet; here, I will summarize the most salient critiques. First, many of the categories in the taxonomy utilized are themselves marginalizing<a class="footnote-ref" href="#crawford2019"> [crawford2019] </a>. Though many of the classes relating to people were removed in 2019, ImageNet had previously bifurcated theNatural Object `>` Body `>` Adult Bodycategory intoMale BodyandFemale Bodysubcategories. Second, ethnic classes were included, implying that 1) classification into rigid categories of ethnicity is possible and appropriate and 2) a machine learning system could learn how to classify ethnicity from these images. Diving deeper, the classifications become horrifying in their supposed granularity: until 2019, an image of a woman in a bikini was accompanied with the tags “slattern, slut, slovenly woman, trollop” <a class="footnote-ref" href="#crawford2019"> [crawford2019] </a>. Though many embedding models are pre-trained on subsets of ImageNet categories included in the ImageNet Large Scale Visual Recognition Challenge that elide these particularly troubling classifications, these classifications nonetheless necessitate a reckoning with our use of ImageNet writ large, especially in regard to how the semantics of ImageNet are projected onto any image embedding generated with such a model<a class="footnote-ref" href="#russakovsky2015"> [russakovsky2015] </a>.[^23] 

However, questions probing the data in ImageNet fail to critique the ethically questionable practices on which ImageNet is built. Though the researchers responsible for the dataset scraped all 14 million images from public URLs, ImageNet does not provide any guarantees on image copyright, as only the URLs are provided in the database: “The images in their original resolutions may be subject to copyright, so we do not make them publicly available on our server” <a class="footnote-ref" href="#imagenet2020b"> [imagenet2020b] </a>. It is highly unlikely that a photographer with an image in the dataset could have known that a photograph could be used this way, much less actively consent to the image’s inclusion, as is the case with subjects in the photographs. Furthermore, the labels themselves were collected using Amazon’s Mechanical Turk platform, which has been repeatedly criticized for its exploitative labor practices: as of 2017, workers earned a median wage of approximately $2 an hour on the platform<a class="footnote-ref" href="#haro2018"> [haro2018] </a>. Scholars including Natalia Cecire, Bonnie Mak, and Paul Fyfe have highlighted how outsourced marginalized labor underpins digitization efforts, and the reliance on Mechanical Turk for the production of ImageNet further entrenches the digitization and discovery process within a system of labor exploitation<a class="footnote-ref" href="#cecire2011"> [cecire2011] </a>;<a class="footnote-ref" href="#mak2017"> [mak2017] </a>;<a class="footnote-ref" href="#fyfe2016"> [fyfe2016] </a>. As cultural heritage practitioners and humanities researchers, we must acknowledge these exploitative practices, and we must reckon with how we perpetuate them through the use of ImageNet as a training source for image search and discovery.

In offering these critiques, my intention is not to dismiss ImageNet in a wholesale manner. Certainly, the benefits of utilizing ImageNet are manifold, as evidenced by widespread community adoption, as well as new affordances for searching cultural heritage collections enabled by the dataset that are shaping the contours of digital scholarship. In the case of my own scholarship with Newspaper Navigator, I have elected to utilize machine learning models pre-trained on ImageNet precisely for these reasons. I offer these provocations instead to question how we can do better as a community, not only in imagining alternatives but in bringing them to fruition. Classification is an act of interpretive reduction, whether by human or machine, and thus manifests all too often as an act of oppression.[^24] And yet, the structure imposed by classification constitutes the very basis for search and discovery systems. The salient question is thus not how we dispense of these systems but rather how we progressively realize a more inclusive vision of these systems, from the labor practices behind their construction to the very classification taxonomies themselves.

How, then, do image embeddings derived from ImageNet mediate our interactions with the photographs in _Newspaper Navigator_ ? Figure 9 shows a visualization of 1,000 photographs from the _Newspaper Navigator_ dataset published during the year 1910. This visualization was created using the ResNet-50 image embeddings, as well as a dimensionality reduction algorithm known as T-SNE<a class="footnote-ref" href="#vandermaaten2009"> [vandermaaten2009] </a>. With T-SNE, a cluster of photographs indicates that the photographs are likely semantically similar, but the size of the cluster and distances from other clusters bear no meaning<a class="footnote-ref" href="#wattenberg2016"> [wattenberg2016] </a>. With this in mind, we can examine the clusters. Despite the fact that the high-contrast, grayscale photographs in _Newspaper Navigator_ are markedly different, or “out-of-sample,” in comparison to the clear, color images in ImageNet, the clusters nonetheless capture semantic similarity. In Figure 9, we observe the clustering of photographs depicting crowds of people, as well as photographs depicting ships and the sea. This visualization technique with the image embeddings is thus powerful in helping to navigate large collections of photographs by their semantic content.

{{< figure src="images/image10.jpg" caption="A visualization of 1,000 photographs from the year 1910 in the _Newspaper Navigator_ dataset, generated using the _Newspaper Navigator_ ResNet-50 image embeddings." alt="image of a network graph"  >}}


What about the photographs of W.E.B. Du Bois? In Figure 10, I show the clusters containing these four photographs. This visualization affords us a lens into the limitations of image embeddings. First, it is evident that image embeddings are directly impacted by the distortions of the digitization process: while the three photographs from _Franklin’s Paper the Statesman_ and _The Broad Ax_ are clustered together with other portraits, the photograph from the _Iowa State Bystander_ is located in an entirely different cluster - a consequence of the fact that the _Iowa State Bystander_ photograph is saturated and that W.E.B. Du Bois’s facial features are obscured (notably, neighboring photographs suffer from similar distortions). A search engine powered with these image embeddings would in all likelihood return the three photographs from _Franklin’s Paper the Statesman_ and _The Broad Ax_ together, but the fourth photograph would effectively be lost. This algorithmic mediation is particularly troubling because, as described in Section IV, the microfilming digitization process causes newspaper photographs of darker-skinned people to lose contrast. While this loss in image quality is marginalizing in its own right, image embeddings perpetuate this marginalization: digitized newspaper portraits of darker-skinned individuals are more likely to suffer from saturated facial features, in turn resulting in these photographs being lost during the discovery and retrieval process, as is the case with the saturated _Iowa State Bystander_ photograph of W.E.B. Du Bois in Figure 10. Understanding these limitations of image embeddings are particularly relevant in the case of _Newspaper Navigator_ , as these image embeddings power the visual similarity search affordance within the publicly-deployed _Newspaper Navigator_ search application<a class="footnote-ref" href="#lee2020c"> [lee2020c] </a>. Though machine learning methods are often offered as panaceas for automation, this algorithmic erasure reminds us that traditional methods of scholarship and historiography, such as detailed analyses and close readings of Black newspapers in _Chronicling America_ , are more important than ever to counter algorithmic bias.

{{< figure src="images/image11.jpg" caption="The same visualization as in Figure 9, this time showing the locations of the four photographs of W.E.B. Du Bois." alt="image of a network graph focused on W.E.B. Du Bois"  >}}





## X. Environmental Impact

Any examination of a dataset whose construction required large-scale computing would be remiss in not investigating the environmental impact of the computation itself. The carbon emissions generated from training a state-of-the-art machine learning model such as BERT is comparable to a single flight across the United States; however, factoring in experimentation and tuning, the carbon emissions can quickly amount to the carbon emissions of a car over its entire lifetime, including fuel<a class="footnote-ref" href="#strubell2019"> [strubell2019] </a>. OpenAI’s GPT-3 model required several thousand petaflop/s-days to train; without specific numbers, the carbon emissions are not possible to calculate exactly, but they are nonetheless substantial<a class="footnote-ref" href="#brown2020"> [brown2020] </a>. In response, machine learning researchers have recommended ideas such asGreen AI,with the goal of encouraging the community to value computational efficiency and not just accuracy<a class="footnote-ref" href="#schwartz2019"> [schwartz2019] </a>.

In the case of _Newspaper Navigator_ , most of the compute time was devoted to processing all 16.3 million _Chronicling America_ pages with the visual content recognition model, as opposed to training the model itself. In Tables 4 and 5, I report details on training the model and running the pipeline, as well as the carbon emissions generated by each step, computed using the Machine Learning Impact Calculator<a class="footnote-ref" href="#lacoste2019"> [lacoste2019] </a>. In total, approximately 380 kg CO2 were emitted during the construction of the _Newspaper Navigator_ dataset, including development, experimentation, training, pipeline processing, and post-processing. It should be noted that this number is an estimate, as the statistics for experimentation and post-processing are difficult to quantify exactly. Nonetheless, this is approximately equivalent to the carbon emissions incurred by a single person flying from Washington, D.C. to Boston<a class="footnote-ref" href="#carbon"> [carbon] </a>. I include these numbers in the hope that cultural heritage practitioners will consider the environmental impact of utilizing machine learning and artificial intelligence for digital content stewardship. Doing so is essential to the data archaeology: given that climate change will disproportionately affect cultural heritage institutions in regions unable to develop proper infrastructure to withstand rapid temperature fluctuations and unprecedented flooding, even the environmental impacts of utilizing machine learning within digital content stewardship has the capacity to contribute to erasure and marginalization.
Carbon emissions from the GPU usage for _Newspaper Navigator_ , broken down by project component. Note that all computation was done on Amazon AWS g4dn instances in the zoneus-east-2. The carbon emissions were calculated using the Machine Learning Impact Calculator<a class="footnote-ref" href="#lacoste2019"> [lacoste2019] </a>.Activity# of NVIDIA T4 GPUsGPU Hours (each)Carbon EmissionsTraining1190.96 kg CO2Pipeline Processing8456144.56 kg CO2Experimentation for Training and Pipeline Processing (estimate)8247.66 kg CO2 _Total_ --153.18 kg CO2Carbon emissions from the CPU usage for _Newspaper Navigator_ , broken down by project component. Note that all computation was done on Amazon AWS g4dn instances in the zoneus-east-2. The CPU processors are all 2nd generation Intel Xeon Scalable Processors (Cascade Lake)<a class="footnote-ref" href="#amazon2020"> [amazon2020] </a>. The 48-core processor outputs approximately 350 W; the 4-core processor outputs approximately 104 W<a class="footnote-ref" href="#intel2020a"> [intel2020a] </a>;<a class="footnote-ref" href="#intel2020b"> [intel2020b] </a>. The carbon emissions were calculated using the Machine Learning Impact Calculator<a class="footnote-ref" href="#lacoste2019"> [lacoste2019] </a>. Note that the energy consumption by RAM is not factored in, but it is insignificant in comparison to the CPU and GPU energy consumption.ActivityCPU Processor (#)# Processor CPU CoresCPU Hours (each)Carbon EmissionsTraining14 CPUs191.13 kg CO2Pipeline Processing248 CPUs456181.9 kg CO2Experimentation for Training and Pipeline Processing ( _estimate_ )248 CPUs249.57 kg CO2Extra Computation (dataset post-processing, etc., _estimate_ )148 CPUs16833.52 kg CO2 _Total_ ---226.12 kg CO2



## XI. Conclusion

In this data archaeology, I have traced four _Chronicling America_ pages reproducing the same photograph of W.E.B. Du Bois as they have traveled through the _Chronicling America_ and _Newspaper Navigator_ pipelines. The excavated genealogy of digital artifacts has revealed the imprintings of the complex interactions between humans and machines. Indeed, the journey of each newspaper page through the _Chronicling America_ and _Newspaper Navigator_ pipelines is one of refraction, mediation, and decontextualization that is compounded upon with each step. Decisions made decades ago when microfilming a newspaper page inevitably affect how the machine learning models employed for OCR, visual content extraction, and image embedding generation ultimately process the pages, render them as digital artifacts in the _Newspaper Navigator_ dataset, and mediate their discoverability.

As articulated by Trevor Owens in _The Theory and Craft of Digital Preservation_ , machine learning and artificial intelligence are the “underlying sciences for digital preservation” <a class="footnote-ref" href="#owens2018"> [owens2018] </a>. Though machine learning techniques provide us with new affordances for searching and studying cultural heritage materials, they have the power to perpetuate and amplify the marginalization and erasure of entire communities within the archive. This erasure, coupled with the labor practices involved in creating training data as well as the environmental impact of training and deploying machine learning models in large-scale digitization pipelines, necessitates that we continue to examine the broader socio-technical ecosystems in which we participate. In doing so, we can work toward a more inclusive vision of the digital collection and the ways in which we render its contents discoverable.

How, then, is _Newspaper Navigator_ situated within this vision? In reimagining how we search over the visual content in _Chronicling America_ , one explicit goal of the project is to engage the public with the rich history preserved within historic American periodicals and thus build on _Chronicling America_ as a free-to-use, public domain resource for scholars, educators, students, journalists, genealogists, and beyond<a class="footnote-ref" href="#lee2021a"> [lee2021a] </a>.<a class="footnote-ref" href="#lee2021b"> [lee2021b] </a>. With _Newspaper Navigator_ , it is my belief that the new modes of interacting with _Chronicling America_ have the capacity to not only enable a breadth of new scholarship but also foster engagement in and reckoning with America’s multilayered history of oppression. In documenting the different components of the project with this data archaeology and corresponding technical paper<a class="footnote-ref" href="#lee2020b"> [lee2020b] </a>, as well as releasing the full dataset and all code into the public domain, I have intended to be as transparent as possible with the tools and methodologies employed. _Newspaper Navigator_ is not without its shortcomings, but my hope is that the project contributes to this vision of the digital collection through transparency and inclusivity, as well as the scholarship and pedagogy that it has enabled.

I offer this case study not only to contextualize the _Newspaper Navigator_ dataset but also to advocate for the autoethnographic data archaeology as a valuable apparatus for reflecting on a cultural heritage dataset from a humanistic perspective. Though the digital humanities community has yet to adopt the data archaeology as standard practice when creating and releasing cultural heritage datasets, doing so has the capacity to improve accountability and context surrounding applications of machine learning for both practitioners and end users. Given the manifold ways in which machine learning mediates access to the archive and perpetuates erasure, reflecting critically on these systems is not only urgent but essential for transparency and inclusivity.




## Sources of Funding

This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant DGE-1762114, as well as the Library of Congress Innovator in Residence Position.




## Acknowledgments

I would like to thank Eileen Jakeway, Jaime Mears, Laurie Allen, Meghan Ferriter, Robin Butterhof, and Nathan Yarasavage at the Library of Congress, as well as Molly Hardy and Joshua Ortiz Baco at the National Endowment for the Humanities, for their thoughtful and enlightening feedback on drafts of this article. I am grateful to my Ph.D. advisor, Daniel Weld, at the University of Washington, for his support, guidance, and invaluable advice with _Newspaper Navigator_ . In addition, I would like to thank Kurtis Heimerl and Esther Jang at the University of Washington for the opportunity to formulate and write early sections of this data archaeology as part of this Spring’s CSE 599: “Computing for Social Good” course.

Lastly, I would like to thank the following people who have shaped _Newspaper Navigator_ : Kate Zwaard, Leah Weinryb Grohsgal, Abbey Potter, Chris Adams, Tong Wang, John Foley, Brian Foo, Trevor Owens, Mark Sweeney, and the entire National Digital Newspaper Program staff at the Library of Congress; Devin Naar, Stephen Portillo, Daniel Gordon, and Tim Dettmers at the University of Washington; Michael Haley Goldman, Robert Ehrenreich, Eric Schmalz, and Elliott Wrenn at the United States Holocaust Memorial Museum; Jim Casey at The Pennsylvania State University; Sarah Salter at Texas A&M University-Corpus Christi; and Gabriel Pizzorno at Harvard University.




## Appendix:

{{< figure src="images/image12.png" caption="_Iowa state bystander_ . [volume] (Des Moines, Iowa), 14 Oct. 1910. Chronicling America: Historic American Newspapers. Lib. of Congress.[https://chroniclingamerica.loc.gov/lccn/sn83025186/1910-10-14/ed-1/seq-1/](https://chroniclingamerica.loc.gov/lccn/sn83025186/1910-10-14/ed-1/seq-1/)" alt="screenshot of a newspaper page"  >}}


{{< figure src="images/image13.png" caption="_Franklin's paper the statesman_ . (Denver, Colo.), 15 Oct. 1910. Chronicling America: Historic American Newspapers. Lib. of Congress.[https://chroniclingamerica.loc.gov/lccn/sn91052311/1910-10-15/ed-1/seq-16/](https://chroniclingamerica.loc.gov/lccn/sn91052311/1910-10-15/ed-1/seq-16/)" alt="screenshot of a newspaper page"  >}}


{{< figure src="images/image14.png" caption="_The broad ax._ [volume] (Salt Lake City, Utah), 15 Oct. 1910. Chronicling America: Historic American Newspapers. Lib. of Congress.[https://chroniclingamerica.loc.gov/lccn/sn84024055/1910-10-15/ed-1/seq-2/](https://chroniclingamerica.loc.gov/lccn/sn84024055/1910-10-15/ed-1/seq-2/)" alt="screenshot of a newspaper page"  >}}


{{< figure src="images/image15.png" caption="_The broad ax._ [volume] (Salt Lake City, Utah), 26 Nov. 1910. Chronicling America: Historic American Newspapers. Lib. of Congress.[https://chroniclingamerica.loc.gov/lccn/sn84024055/1910-11-26/ed-1/seq-3/](https://chroniclingamerica.loc.gov/lccn/sn84024055/1910-11-26/ed-1/seq-3/)" alt="screenshot of a newspaper page"  >}}



[^1]: More on the organizational considerations surrounding _Newspaper Navigator_ can be found in<a class="footnote-ref" href="#lee2021b"> [lee2021b] </a>.
[^2]: The public search interface is available at:[https://chroniclingamerica.loc.gov/](https://chroniclingamerica.loc.gov/)
[^3]: For more information on the _Beyond Words_ workflow, see<a class="footnote-ref" href="#lclabs"> [lclabs] </a>, as well as<a class="footnote-ref" href="#lee2020b"> [lee2020b] </a>.
[^4]: In particular, the annotations were used to finetune an object detection model that had been pre-trained on Common Objects in Context, a common dataset for benchmarking object detection algorithms.
[^5]: A screenshot of the workflow can be found later in this article in Figure 4.
[^6]: For those who are not familiar with image embeddings, a detailed description is provided in Section IX.
[^7]: For the dataset, see:[https://news-navigator.labs.loc.gov](https://news-navigator.labs.loc.gov); for the code, see[https://github.com/LibraryOfCongress/newspaper-navigator](https://github.com/LibraryOfCongress/newspaper-navigator).
[^8]: Indeed, compiling bibliographies of serials published after 1820 remains an immensely difficult task<a class="footnote-ref" href="#hardy2019"> [hardy2019] </a>.
[^9]: The extent to which newspaper microfilming was driven by credible fear of deterioration versus other factors, such as microfilm marketing, is an important question that is rightly debated. For more on this topic, see<a class="footnote-ref" href="#baker2001"> [baker2001] </a>.
[^10]: For example, a 2017 article describing the West Virginia University Libraries’ West Virginia & Regional History Center and its participation in the National Digital Newspaper Program states: “By August 2017, all known issues of West Virginia’s African-American newspapers from the 19th and early 20th centuries will have been digitized” <a class="footnote-ref" href="#maxwell2017"> [maxwell2017] </a>. The article describes Curator Stewart Plein’s efforts to locate surviving copies of three Black West Virginia newspapers in order to digitize and include them in _Chronicling America_ .
[^11]: For a thorough case study of this process, I direct the reader to “Qi-jtb the Raven,” in which Ryan Cordell walks through an example with the Pennsylvania Digital Newspaper Program<a class="footnote-ref" href="#cordell2017"> [cordell2017] </a>.
[^12]: For exemplary research collaborations that utilize the _Chronicling America_ bulk OCR, see the Viral Text Project and the Oceanic Exchanges Project<a class="footnote-ref" href="#cordell2017"> [cordell2017] </a>;<a class="footnote-ref" href="#oceanic2017"> [oceanic2017] </a>.
[^13]: For other examinations of how OCR mediates our interactions with digital archives, see<a class="footnote-ref" href="#hitchcock2013"> [hitchcock2013] </a>;<a class="footnote-ref" href="#milligan2013"> [milligan2013] </a>;<a class="footnote-ref" href="#strange2014"> [strange2014] </a>;<a class="footnote-ref" href="#traub2015"> [traub2015] </a>;<a class="footnote-ref" href="#wright2019"> [wright2019] </a>.
[^14]: For a concrete example of a similar phenomenon in the image domain, see<a class="footnote-ref" href="#lee2019"> [lee2019] </a>, in which a machine learning algorithm was trained to classify digitized images but consistently misclassified images that had been misoriented 180 degrees in the scanning bed - a consequence of the classifier not having seen enough instances of these misoriented scans during training.
[^15]: It should be noted that _Beyond Words_ was introduced by LC Labs as an experiment, with no interventions in workflow or community management.
[^16]: Bounding box coordinates refer to the positions of the corners of the predicted bounding box, relative to the image coordinates.
[^17]: The confidence score is examined in more detail in the next section.
[^18]: This modest cut is provided to remove the large number of predictions with confidence scores between 0% and 5%, which have high false-positive rates, and thus reduce the size of the _Newspaper Navigator_ dataset.
[^19]: The _Newspaper Navigator_ dataset does not retain the cropped images of headlines, as the textual content is more salient than visual snippets in the case of headlines.
[^20]: If these words are unfamiliar, the three takeaways listed are more important.
[^21]: For an introduction to some of these methods with lower-level features, see<a class="footnote-ref" href="#manovich2012"> [manovich2012] </a>.
[^22]: The search application can be found at:[https://news-navigator.labs.loc.gov/search](https://news-navigator.labs.loc.gov/search).
[^23]: The specific categories used in the challenge can be found at:[http://image-net.org/challenges/LSVRC/2010/browse-synsets](http://image-net.org/challenges/LSVRC/2010/browse-synsets).
[^24]: For more reading on this topic, see<a class="footnote-ref" href="#bowker2000"> [bowker2000] </a>.## Bibliography

<ul>
<li id="alpertabrams2016">Alpert-Abrams, H. “Machine Reading the Primeros Libros,”  _Digital Humanities Quarterly_ 10:4 (2016).
</li>
<li id="amazon2020"> “Amazon EC2 Instance Types - Amazon Web Services,” (2020) Amazon Web Services, Inc.<a href="https://aws.amazon.com/ec2/instance-types/">Available at:</a><a href="https://aws.amazon.com/ec2/instance-types/">https://aws.amazon.com/ec2/instance-types/</a>. (Accessed: 5 June 2020).
</li>
<li id="bailey2015">Bailey, M. “#transform(Ing)DH Writing and Research: An Autoethnography of Digital Humanities and Feminist Ethics,”  _Digital Humanities Quarterly_ 9:2 (2015).
</li>
<li id="baker2001">Baker, N. _Double Fold: Libraries and the Assault on Paper_ . Random House (2001).
</li>
<li id="barrall2005">Barrall, K. and Guenther, C. “Microfilm Selection for Digitization,” (2005). Available at:<a href="https://www.loc.gov/ndnp/guidelines/NEH_MicrofilmSelectionNDNP.pdf">https://www.loc.gov/ndnp/guidelines/NEH_MicrofilmSelectionNDNP.pdf</a>.
</li>
<li id="bender2018">Bender, E., and Friedman, B. “Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science.”  _Transactions of the Association for Computational Linguistics_ 6 (2018): 587–604.<a href="https://doi.org/10.1162/tacl_a_00041">https://doi.org/10.1162/tacl_a_00041</a>(Accessed 29 July 2021).
</li>
<li id="bowker2000">Bowker, G., and Star, S. _Sorting Things Out: Classification and Its Consequences_ . MIT Press, Cambridge (2000).
</li>
<li id="brown2020">Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei D. “Language Models Are Few-Shot Learners,”  _ArXiv:2005.14165 [Cs]_ (2020),<a href="http://arxiv.org/abs/2005.14165">Available at:</a><a href="http://arxiv.org/abs/2005.14165">http://arxiv.org/abs/2005.14165</a>(Accessed: 6 June 2020).
</li>
<li id="carbon"> “Carbon Footprint Calculator,” <a href="https://calculator.carbonfootprint.com/calculator.aspx?lang=en-GB&tab=3">Available at:</a><a href="https://calculator.carbonfootprint.com/calculator.aspx?lang=en-GB&tab=3">https://calculator.carbonfootprint.com/calculator.aspx?lang=en-GB&tab=3</a>. (Accessed: 6 June 2020).
</li>
<li id="cecire2011">Cecire, N. “Works Cited: The Visible Hand,”  _Works Cited_ (blog) (2011). Available at:<a href="http://nataliacecire.blogspot.com/2011/05/visible-hand.html">http://nataliacecire.blogspot.com/2011/05/visible-hand.html</a>.
</li>
<li id="chronicling"> “Chronicling America | Library of Congress,” Available at:<a href="https://chroniclingamerica.loc.gov/about/">https://chroniclingamerica.loc.gov/about/</a>(Accessed 3 July 2020).
</li>
<li id="cordell2017">Cordell, R. “‘Q i-Jtb the Raven’: Taking Dirty OCR Seriously,”  _Book History_ 20:1, pp. 188–225 (2017). Available at:<a href="https://doi.org/10.1353/bh.2017.0006">https://doi.org/10.1353/bh.2017.0006</a>.
</li>
<li id="cordell2020">Cordell, R. “Machine Learning + Libraries: A Report on the State of the Field” (2020). Available at:<a href="https://labs.loc.gov/static/labs/work/reports/Cordell-LOC-ML-report.pdf?loclr=blogsig">https://labs.loc.gov/static/labs/work/reports/Cordell-LOC-ML-report.pdf?loclr=blogsig</a>.
</li>
<li id="cordellsmith2017">Cordell, R., and Smith, D. _Viral Texts: Mapping Networks of Reprinting in 19th-Century Newspapers and Magazines_ (2017), Available at:<a href="http://viraltexts.org/">http://viraltexts.org</a>.
</li>
<li id="crawford2019">Crawford, K., and Paglen, T. “Excavating AI: The Politics of Training Sets for Machine Learning” (2019). Available at:<a href="https://excavating.ai">https://excavating.ai</a>(Accessed: 19 September 2019).
</li>
<li id="deng2009">Deng, J., Dong, W., Socher, R., Li, L., Li, K., and Fei-Fei, L. “ImageNet: A Large-Scale Hierarchical Image Database,” in _2009 IEEE Conference on Computer Vision and Pattern Recognition_ (2009), pp. 248–55,<a href="https://doi.org/10.1109/CVPR.2009.5206848">Available at:</a><a href="https://doi.org/10.1109/CVPR.2009.5206848">https://doi.org/10.1109/CVPR.2009.5206848</a>.
</li>
<li id="fagan2016">Fagan, B. “Chronicling White America.” American Periodicals: A Journal of History & Criticism 26:1, pp. 10-13 (2016). Available at:<a href="https://muse.jhu.edu/article/613375">https://www.muse.jhu.edu/article/613375</a>.
</li>
<li id="farrar1998">Farrar, H. _The Baltimore Afro-American, 1892-1950_ . Greenwood Publishing Group (1998).
</li>
<li id="ferriter2017">Ferriter, M. “Introducing Beyond Words | The Signal,” (2017). Available at:<a href="https://doi.org/blogs.loc.gov/thesignal/2017/09/introducing-beyond-words/">//blogs.loc.gov/thesignal/2017/09/introducing-beyond-words/</a>. (Accessed: 13 July 2020).
</li>
<li id="foo2019">Foo, B. “AMNH Photographic Collection,” (2020). Available at:<a href="https://amnh-sciviz.github.io/image-collection/about.html">https://amnh-sciviz.github.io/image-collection/about.html</a>(Accessed: 11 June 2020).
</li>
<li id="franklin1910">Franklin's paper the statesman. (Denver, Colo.), 15 Oct. 1910. _Chronicling America: Historic American Newspapers_ . Library of Congress. Available at:<a href="https://chroniclingamerica.loc.gov/lccn/sn91052311/1910-10-15/ed-1/seq-16/">https://chroniclingamerica.loc.gov/lccn/sn91052311/1910-10-15/ed-1/seq-16/</a>
</li>
<li id="fyfe2016">Fyfe, P. “An Archaeology of Victorian Newspapers,”  _Victorian Periodicals Review_ 49:4, pp. 546–77 (2016). Available at:<a href="https://doi.org/10.1353/vpr.2016.0039">https://doi.org/10.1353/vpr.2016.0039</a>.
</li>
<li id="gebru2020">Gebru, T., Morgenstern, J., Vecchione, B., Vaughan, J., Wallach, H., Daumé III, H., and Crawford, K. “Datasheets for Datasets.”  _ArXiv:1803.09010 [Cs]_ , March 19, 2020.<a href="http://arxiv.org/abs/1803.09010">http://arxiv.org/abs/1803.09010</a>(Accessed: July 29 2021).
</li>
<li id="geraldo2020">Giraldo, M. “Building Aereo,” DX Lab | State Library of NSW (2020). Available at:<a href="https://dxlab.sl.nsw.gov.au/blog/building-aereo">https://dxlab.sl.nsw.gov.au/blog/building-aereo</a>(Accessed: 2 July 2020).
</li>
<li id="googlearts2018"> “Google Arts & Culture Experiments - t-SNE Map Experiment” (2018). Available at:<a href="https://artsexperiments.withgoogle.com/tsnemap/">https://artsexperiments.withgoogle.com/tsnemap/</a>(Accessed: 11 June 2020).
</li>
<li id="hardy2019">Hardy, M., and DiCuirci, L. “Critical Cataloging and the Serials Archive: The Digital Making of ‘Mill Girls in Nineteenth-Century Print,’” Archive Journal, Available at:<a href="http://www.archivejournal.net/?p=8073">http://www.archivejournal.net/?p=8073</a>.
</li>
<li id="haro2018">Hara, K. et al. “A Data-Driven Analysis of Workers’ Earnings on Amazon Mechanical Turk,” in _Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems_ , CHI ’18 (Montreal QC, Canada: Association for Computing Machinery, 2018), pp. 1–14. Available at:<a href="https://doi.org/10.1145/3173574.3174023">https://doi.org/10.1145/3173574.3174023</a>.
</li>
<li id="he2016">He, K., Zhang, X., Ren, S., and Sun, J. “Deep Residual Learning for Image Recognition,” in _2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_ , 2016, pp. 770–78,<a href="https://doi.org/10.1109/CVPR.2016.90">Available at:</a><a href="https://doi.org/10.1109/CVPR.2016.90">https://doi.org/10.1109/CVPR.2016.90</a>.
</li>
<li id="holland2018">Holland, S., Hosny, A., Newman, S., Joseph, J., and Chmielinski, K. “The Dataset Nutrition Label: A Framework To Drive Higher Data Quality Standards.”  _ArXiv:1805.03677 [Cs]_ , May 9, 2018.<a href="http://arxiv.org/abs/1805.03677">http://arxiv.org/abs/1805.03677</a>(Accessed 29 July 2021).
</li>
<li id="hitchcock2013">Hitchcock, T. “Confronting the Digital,”  _Cultural and Social History_ 10:1. pp. 9–23 (2013). Available at:<a href="https://doi.org/10.2752/147800413X13515292098070">https://doi.org/10.2752/147800413X13515292098070</a>.
</li>
<li id="imagenet2020a"> “ImageNet,” Available at:<a href="http://image-net.org/index">http://image-net.org/index</a>(Accessed: 8 June 2020).
</li>
<li id="imagenet2020b"> “What about the images?” Available at:<a href="http://image-net.org/download-faq">http://image-net.org/download-faq</a>(Accessed: 8 June 2020).
</li>
<li id="intel2020a"> “Intel® Xeon® Platinum 9242 Processor (71.5M Cache, 2.30 GHz) Product Specifications,” Available at:<a href="https://ark.intel.com/content/www/us/en/ark/products/194145/intel-xeon-platinum-9242-processor-71-5m-cache-2-30-ghz.html">https://ark.intel.com/content/www/us/en/ark/products/194145/intel-xeon-platinum-9242-processor-71-5m-cache-2-30-ghz.html</a>(Accessed: 5 June 2020).
</li>
<li id="intel2020b"> “Intel® Xeon® Platinum 8256 Processor (16.5M Cache, 3.80 GHz) Product Specifications,” Available at:<a href="https://ark.intel.com/content/www/us/en/ark/products/192467/intel-xeon-platinum-8256-processor-16-5m-cache-3-80-ghz.html">https://ark.intel.com/content/www/us/en/ark/products/192467/intel-xeon-platinum-8256-processor-16-5m-cache-3-80-ghz.html</a>(Accessed: June 5, 2020).
</li>
<li id="iowa1910">Iowa state bystander. [volume] (Des Moines, Iowa), 14 Oct. 1910. _Chronicling America: Historic American Newspapers_ . Library of Congress. Available at:<a href="https://chroniclingamerica.loc.gov/lccn/sn83025186/1910-10-14/ed-1/seq-1/">https://chroniclingamerica.loc.gov/lccn/sn83025186/1910-10-14/ed-1/seq-1/</a>
</li>
<li id="lacoste2019">Lacoste, A., Luccioni, A., Schmidt, V., and Dandres, T. “Quantifying the Carbon Emissions of Machine Learning,”  _ArXiv:1910.09700 [Cs]_ (2019). Available at:<a href="http://arxiv.org/abs/1910.09700">http://arxiv.org/abs/1910.09700</a>.
</li>
<li id="lclabs2017a">LC Labs, “Beyond Words: Mark” Available at:<a href="http://beyondwords.labs.loc.gov/#/mark">http://beyondwords.labs.loc.gov/#/mark</a>(Accessed 5 June, 2020).
</li>
<li id="lclabs2017b">LC Labs, “Beyond Words: Transcribe,” Available at:<a href="http://beyondwords.labs.loc.gov/#/transcribe">http://beyondwords.labs.loc.gov/#/transcribe</a>(Accessed 5 June, 2020).
</li>
<li id="lclabs2017c">LC Labs, “Beyond Words: Veriffy,” Available at:<a href="http://beyondwords.labs.loc.gov/#/verify">http://beyondwords.labs.loc.gov/#/verify</a>(Accessed 5 June, 2020).
</li>
<li id="lclabs">LC Labs, Beyond Words | Experiments. Available at:<a href="https://labs.loc.gov/work/experiments/beyond-words/">https://labs.loc.gov/work/experiments/beyond-words/</a>(Accessed 5 June, 2020).
</li>
<li id="lclabs2020">LC Labs and Digital Strategy Directorate, “Machine Learning + Libraries Summit Event Summary” (2020). Available at:<a href="https://labs.loc.gov/static/labs/meta/ML-Event-Summary-Final-2020-02-13.pdf">https://labs.loc.gov/static/labs/meta/ML-Event-Summary-Final-2020-02-13.pdf</a>.
</li>
<li id="lee2019">Lee, B. “Machine Learning, Template Matching, and the International Tracing Service Digital Archive: Automating the Retrieval of Death Certificate Reference Cards from 40 Million Document Scans,”  _Digital Scholarship in the Humanities_ 34:3, pp. 513-535 (2019).<a href="https://doi.org/10.1093/llc/fqy063">Available at:</a><a href="https://doi.org/10.1093/llc/fqy063">https://doi.org/10.1093/llc/fqy063</a>.
</li>
<li id="lee2020a">Lee, B. _LibraryOfCongress/Newspaper-Navigator_ , GitHub Repository ( Library of Congress, 2020).<a href="https://github.com/LibraryOfCongress/newspaper-navigator">Available at:</a><a href="https://github.com/LibraryOfCongress/newspaper-navigator">https://github.com/LibraryOfCongress/newspaper-navigator</a>.
</li>
<li id="lee2020b">Lee, B., Mears, J., Jakeway, E., Ferriter, M., Adams, C., Yarasavage, N., Thomas, D., Zwaard, K., and Weld, D. “The Newspaper Navigator Dataset: Extracting And Analyzing Visual Content from 16 Million Historic Newspaper Pages in Chronicling America,” <a href="https://dl.acm.org/doi/proceedings/10.1145/3340531">CIKM '20: Proceedings of the 29th ACM International Conference on Information & Knowledge Management</a> _,_ pp. 3055–3062 (2020). Available at:<a href="https://doi.org/10.1145/3340531.3412767">https://doi.org/10.1145/3340531.3412767</a>.
</li>
<li id="lee2020c">Lee, B., and Weld, D. “Newspaper Navigator: Open Faceted Search for 1.5 Million Images,” <a href="https://dl.acm.org/doi/proceedings/10.1145/3379350">UIST '20 Adjunct: Adjunct Publication of the 33rd Annual ACM Symposium on User Interface Software and Technology</a>, pp. 120-122 (2020). Available at:<a href="https://doi.org/10.1145/3379350.3416143">https://doi.org/10.1145/3379350.3416143</a>.
</li>
<li id="lee2021a">Lee, B., Berson, I., and Berson, M. “Machine Learning and the Social Studies,”  _Social Education_ 85:2, pp. 88-92 (2021). Available at:<a href="https://www.socialstudies.org/social-education/85/2/machine-learning-and-social-studies">https://www.socialstudies.org/social-education/85/2/machine-learning-and-social-studies</a>.
</li>
<li id="lee2021b">Lee, B., Mears, J., Jakeway, E., Ferriter, M., and Potter, A. “Newspaper Navigator: Putting Machine Learning in the Hands of Library Users,”  _EuropeanaTech Insight_ 16 (2021). Available at:<a href="https://pro.europeana.eu/page/issue-16-newspapers">https://pro.europeana.eu/page/issue-16-newspapers</a>.
</li>
<li id="congress2019"> “Digital Strategy | Library of Congress,” Library of Congress (2019). Available at:<a href="https://www.loc.gov/digital-strategy/">https://www.loc.gov/digital-strategy/</a>(Accessed: 30 May 2020).
</li>
<li id="lincoln2019">Lincoln, M., Levin, G., Conell, S., and Huang, L. (2019) “National Neighbors: Distant Viewing the National Gallery of Art's Collection of Collections” (2019) Available at:<a href="https://nga-neighbors.library.cmu.edu/">https://nga-neighbors.library.cmu.edu</a>. (Accessed: 30 May 2020).
</li>
<li id="lonij2017">Lonij, J., and Wevers, M. (2017) SIAMESE. KB Lab: The Hague (2017). Available at:<a href="http://lab.kb.nl/tool/siamese">http://lab.kb.nl/tool/siamese</a>.
</li>
<li id="lorang2020">Lorang, E., Soh, L., Liu, Y., and Pack, C. “Digital Libraries, Intelligent Data Analytics, and Augmented Description: A Demonstration Project” (2020). Available at:<a href="https://digitalcommons.unl.edu/libraryscience/396/">https://digitalcommons.unl.edu/libraryscience/396/</a>.
</li>
<li id="mak2017">Mak, B. “Archaeology of a Digitization,”  _Journal of the Association for Information Science and Technology_ 65:8, pp. 1515–26 (2014). Available at:<a href="https://doi.org/10.1002/asi.23061">https://doi.org/10.1002/asi.23061</a>.
</li>
<li id="manovich2012">Manovich, L. “How to Compare One Million Images?,” in _Understanding Digital Humanities_ , ed. David M. Berry (London: Palgrave Macmillan UK, 2012), pp. 249–78. Available at:<a href="https://doi.org/10.1057/9780230371934_14">https://doi.org/10.1057/9780230371934_14</a>.
</li>
<li id="maxwell2017">Maxwell, M. “WVU Today | WVRHC Seeking Copies of Rare African-American Newspapers” (2017). Available at:<a href="https://wvutoday.wvu.edu/stories/2017/01/19/wvrhc-seeking-copies-of-rare-african-american-newspapers">https://wvutoday.wvu.edu/stories/2017/01/19/wvrhc-seeking-copies-of-rare-african-american-newspapers</a>. (Accessed 11 July 2020).
</li>
<li id="mears2014">Mears, J. _National Digital Newspaper Program Impact Study 2004-2014_ , National Endowment for the Humanities (2014). Available at:<a href="https://www.neh.gov/divisions/preservation/featured-project/neh-releases-national-digital-newspaper-program-impact-study">https://www.neh.gov/divisions/preservation/featured-project/neh-releases-national-digital-newspaper-program-impact-study</a>. (Accessed 29 May 2020).
</li>
<li id="meta"> “Meta | Morphosis: Tutorials,” National Digital Newspaper Program and the University of Kentucky Libraries. Available at:<a href="https://www.uky.edu/Libraries/NDNP/metamorphosis/tutorials.html">https://www.uky.edu/Libraries/NDNP/metamorphosis/tutorials.html</a>(Accessed 3 July 2020).
</li>
<li id="milligan2013">Milligan, I. “Illusionary Order: Online Databases, Optical Character Recognition, and Canadian History, 1997–2010,”  _Canadian Historical Review_ 94:4, pp. 540–69 (2013). Available at:<a href="https://doi.org/10.3138/chr.694">https://doi.org/10.3138/chr.694</a>.
</li>
<li id="mitchell2019">Mitchell, M., Wu, S., Zaldivar, A., Barnes, P., Vasserman, L., Hutchinson, B., Spitzer, E., Raji, I., and Gebru, T. “Model Cards for Model Reporting.”  _Proceedings of the Conference on Fairness, Accountability, and Transparency_ , January 29, 2019, 220–29.<a href="https://doi.org/10.1145/3287560.3287596">https://doi.org/10.1145/3287560.3287596</a>.
</li>
<li id="nasjonalmuseet2017"> “Project: «Principal Components»,” Nasjonalmuseet (2018). Available at:<a href="https://www.nasjonalmuseet.no/en/about-the-national-museum/collection-management---behind-the-scenes/digital-collection-management/project-principal-components/">https://www.nasjonalmuseet.no/en/about-the-national-museum/collection-management — -behind-the-scenes/digital-collection-management/project-principal-components/</a>(Accessed 11 June 2020).
</li>
<li id="national2019"> “About the Program - National Digital Newspaper Program (Library of Congress),” (2019). Available at:<a href="https://www.loc.gov/ndnp/about.html">https://www.loc.gov/ndnp/about.html</a>(Accessed 3 July 2020).
</li>
<li id="national2020">The National Digital Newspaper Program (NDNP) Technical Guidelines for Applicants 2020-22 Awards (2020). Available at:<a href="https://www.loc.gov/ndnp/guidelines/">https://www.loc.gov/ndnp/guidelines/</a>(Accessed 28 June 2020).
</li>
<li id="national"> “Content Selection - National Digital Newspaper Program (Library of Congress)” (2020). Available at:<a href="https://www.loc.gov/ndnp/guidelines/selection.html">https://www.loc.gov/ndnp/guidelines/selection.html</a>(Accessed 3 July 2020).
</li>
<li id="neh2020">Division of Preservation and Access (NEH), “Notice of Funding Opportunity, National Digital Newspaper Program” (2020). Available at:<a href="https://www.neh.gov/sites/default/files/inline-files/National-Digital-Newspaper-Program-NOFO-January-2020.pdf">https://www.neh.gov/sites/default/files/inline-files/National-Digital-Newspaper-Program-NOFO-January-2020.pdf</a>. (Accessed 28 June 2020).
</li>
<li id="navigator1910a">Image of W.E.B. Du Bois from the _Iowa State Bystander_ (14 October 1910). From the Library of Congress, Newspaper Navigator dataset: Extracted Visual Content from Chronicling America. Available at:<a href="https://news-navigator.labs.loc.gov/data/iahi_ames_ver01/data/sn83025186/00202198417/1910101401/1015/001_0_95.jpg">https://news-navigator.labs.loc.gov/data/iahi_ames_ver01/data/sn83025186/00202198417/1910101401/1015/001_0_95.jpg</a>.
</li>
<li id="navigator1910b">[Newspaper Navigator 1910b] _Newspaper Navigator_ metadata for the _Iowa State Bystander_ (14 October 1910). From the Library of Congress, Newspaper Navigator dataset: Extracted Visual Content from Chronicling America. Available at:<a href="https://news-navigator.labs.loc.gov/data/iahi_ames_ver01/data/sn83025186/00202198417/1910101401/1015.json">https://news-navigator.labs.loc.gov/data/iahi_ames_ver01/data/sn83025186/00202198417/1910101401/1015.json</a>.
</li>
<li id="navigator1910c">Image of W.E.B. Du Bois from _Franklin’s Paper the Statesman_ (15 October 1910). From the Library of Congress, Newspaper Navigator dataset: Extracted Visual Content from Chronicling America. Available at:<a href="https://news-navigator.labs.loc.gov/data/cohi_abbeyville_ver01/data/sn91052311/00279550730/1910101501/2272/001_0_93.jpg">https://news-navigator.labs.loc.gov/data/cohi_abbeyville_ver01/data/sn91052311/00279550730/1910101501/2272/001_0_93.jpg</a>
</li>
<li id="navigator1910d"> _Newspaper Navigator_ metadata for _Franklin’s Paper the Statesman_ (15 October 1910). From the Library of Congress, Newspaper Navigator dataset: Extracted Visual Content from Chronicling America. Available at:<a href="https://news-navigator.labs.loc.gov/data/cohi_abbeyville_ver01/data/sn91052311/00279550730/1910101501/2272.json">https://news-navigator.labs.loc.gov/data/cohi_abbeyville_ver01/data/sn91052311/00279550730/1910101501/2272</a><a href="https://news-navigator.labs.loc.gov/data/cohi_abbeyville_ver01/data/sn91052311/00279550730/1910101501/2272.json">.json</a>
</li>
<li id="navigator1910e">Image of W.E.B. Du Bois from _The Broad Ax_ (15 October 1910). From the Library of Congress, Newspaper Navigator dataset: Extracted Visual Content from Chronicling America. Available at:<a href="https://news-navigator.labs.loc.gov/data/iune_charlie_ver01/data/sn84024055/00280761059/1910101501/0538/002_0_98.jpg">https://news-navigator.labs.loc.gov/data/iune_charlie_ver01/data/sn84024055/00280761059/1910101501/0538/002_0_98.jpg</a>
</li>
<li id="navigator1910f"> _Newspaper Navigator_ metadata for _The Broad Ax_ (15 October 1910). From the Library of Congress, Newspaper Navigator dataset: Extracted Visual Content from Chronicling America. Available at:<a href="https://news-navigator.labs.loc.gov/data/iune_charlie_ver01/data/sn84024055/00280761059/1910101501/0538.json">https://news-navigator.labs.loc.gov/data/iune_charlie_ver01/data/sn84024055/00280761059/1910101501/0538.json</a>
</li>
<li id="navigator1910g">Image of W.E.B. Du Bois from The Broad Ax (26 November 1910). From the Library of Congress, Newspaper Navigator dataset: Extracted Visual Content from Chronicling America. Available at:<a href="https://news-navigator.labs.loc.gov/data/iune_charlie_ver01/data/sn84024055/00280761059/1910112601/0564/004_0_98.jpg">https://news-navigator.labs.loc.gov/data/iune_charlie_ver01/data/sn84024055/00280761059/1910112601/0564/004_0_98.jpg</a>
</li>
<li id="navigator1910h"> _Newspaper Navigator_ metadata for The Broad Ax (26 November 1910). From the Library of Congress, Newspaper Navigator dataset: Extracted Visual Content from Chronicling America. Available at:<a href="https://news-navigator.labs.loc.gov/data/iune_charlie_ver01/data/sn84024055/00280761059/1910112601/0564.json">https://news-navigator.labs.loc.gov/data/iune_charlie_ver01/data/sn84024055/00280761059/1910112601/0564.json</a>
</li>
<li id="noble2018">Noble, S. _Algorithms of Oppression: How Search Engines Reinforce Racism_ . NYU Press, New York (2018).
</li>
<li id="oceanic2017">Oceanic Exchanges Project Team. Oceanic Exchanges: Tracing Global Information Networks In Historical Newspaper Repositories, 1840-1914 (2017). Available at: 10.17605/OSF.IO/WA94S.
</li>
<li id="owens2018">Owens, T. _The Theory and Craft of Digital Preservation_ . Johns Hopkins University Press, Baltimore (2018).
</li>
<li id="owens2020">Owens, T., and Padilla, T. “Digital Sources and Digital Archives: Historical Evidence in the Digital Age,”  _International Journal of Digital Humanities_ (2020). Available at:<a href="https://doi.org/10.1007/s42803-020-00028-7">https://doi.org/10.1007/s42803-020-00028-7</a><a href="https://doi.org/10.1007/s42803-020-00028-7">https://doi.org/10.1007/s42803-020-00028-7</a>.
</li>
<li id="padilla2019">Padilla, T. _Responsible Operations: Data Science, Machine Learning, and AI in Libraries_ (2019). Available at:<a href="https://doi.org/10.25333/xk7z-9g97">https://doi.org/10.25333/xk7z-9g97</a>.
</li>
<li id="reidsma2019">Reidsma, M. _Masked by Trust: Bias in Library Discovery._ Litwin Books, Sacramento (2019).
</li>
<li id="reisman2018">Reisman, D., Schultz, J., Crawford, K., Whittaker, M. _Algorithmic Impact Assessments: A Practical Framework for Public Agency Accountability_ (2018). Available at:<a href="https://ainowinstitute.org/aiareport2018.pdf">https://ainowinstitute.org/aiareport2018.pdf</a>.
</li>
<li id="russakovsky2015">Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A., and Fei-Fei, L. “ImageNet Large Scale Visual Recognition Challenge,”  _International Journal of Computer Vision_ 115:3, pp. 211-252 (2015). Available at:<a href="https://doi.org/10.1007/s11263-015-0816-y">https://doi.org/10.1007/s11263-015-0816-y</a><a href="https://doi.org/10.1007/s11263-015-0816-y">https://doi.org/10.1007/s11263-015-0816-y</a>.
</li>
<li id="schwartz2019">Schwartz, R., Dodge, J., Smith, N., and Etzioni, O. “Green AI,” ArXiv:1907.10597 [Cs, Stat], (2019). Available at:<a href="http://arxiv.org/abs/1907.10597">http://arxiv.org/abs/1907.10597</a><a href="http://arxiv.org/abs/1907.10597">http://arxiv.org/abs/1907.10597</a>.
</li>
<li id="strange2014">Strange, C., McNamara, D., Wodak, J., and Wood, I. “Mining for the Meanings of a Murder: The Impact of OCR Quality on the Use of Digitized Historical Newspapers,”  _Digital Humanities Quarterly_ 8:1 (2014). Available at:<a href="http://www.digitalhumanities.org/dhq/vol/8/1/000168/000168.html">http://www.digitalhumanities.org/dhq/vol/8/1/000168/000168.html</a>.
</li>
<li id="strubell2019">Strubell, E., Ganesh, A., and McCallum, A. “Energy and Policy Considerations for Deep Learning in NLP,” ArXiv:1906.02243 [Cs] (2019). Available at:<a href="http://arxiv.org/abs/1906.02243">http://arxiv.org/abs/1906.02243</a>.
</li>
<li id="ax1910a">The broad ax. [volume] (Salt Lake City, Utah), 15 Oct. 1910. _Chronicling America: Historic American Newspapers_ . Library of Congress. Available at:<a href="https://chroniclingamerica.loc.gov/lccn/sn84024055/1910-10-15/ed-1/seq-2/">https://chroniclingamerica.loc.gov/lccn/sn84024055/1910-10-15/ed-1/seq-2/</a>
</li>
<li id="ax1910b">The broad ax. [volume] (Salt Lake City, Utah), 26 Nov. 1910. _Chronicling America: Historic American Newspapers_ . Library of Congress. Available at:<a href="https://chroniclingamerica.loc.gov/lccn/sn84024055/1910-11-26/ed-1/seq-3/">https://chroniclingamerica.loc.gov/lccn/sn84024055/1910-11-26/ed-1/seq-3/</a>
</li>
<li id="traub2015">Traub, M., van Ossenbruggen, J., and Hardman, L. “Impact Analysis of OCR Quality on Research Tasks in Digital Archives,” in _Research and Advanced Technology for Digital Libraries_ , ed. Sarantos Kapidakis, Cezary Mazurek, and Marcin Werla (Cham: Springer International Publishing, 2015), 252–263.
</li>
<li id="vandermaaten2009">van der Maaten, L., and Hinton, G. “Visualizing Data Using T-SNE,”  _Journal of Machine Learning Research_ 9, pp. 2579-2605 (2008). Available at:<a href="http://www.jmlr.org/papers/v9/vandermaaten08a.html">http://www.jmlr.org/papers/v9/vandermaaten08a.html</a>.
</li>
<li id="vane2018">Vane, O. “Visualising the Royal Photographic Society Collection: Part 2 • V&A Blog,”  _V&A Blog_ (2018). Available at:<a href="https://www.vam.ac.uk/blog/digital/visualising-the-royal-photographic-society-collection-part-2">https://www.vam.ac.uk/blog/digital/visualising-the-royal-photographic-society-collection-part-2</a>.
</li>
<li id="wattenberg2016">Wattenberg, M., Viégas, F., and Johnson, I. “How to Use T-SNE Effectively,”  _Distill_ 1:10 (2016). Available at:<a href="https://doi.org/10.23915/distill.00002">https://doi.org/10.23915/distill.00002</a>.
</li>
<li id="weavers2020">Wevers, M., and Smits, T. “The Visual Digital Turn: Using Neural Networks to Study Historical Images,”  _Digital Scholarship in the Humanities_ 35:1, pp. 194-207 (2020). Available at:<a href="https://doi.org/10.1093/llc/fqy085">https://doi.org/10.1093/llc/fqy085</a>.
</li>
<li id="weld2019">Weld, D., and Bansal, G. 2019. The challenge of crafting intelligible intelligence. Commun. ACM 62: 6, pp. 70–79 (2019). Available at:<a href="https://doi.org/10.1145/3282486">https://doi.org/10.1145/3282486</a>.
</li>
<li id="williams2019">Williams, L. “What Computational Archival Science Can Learn from Art History and Material Culture Studies,” in _2019 IEEE International Conference on Big Data (Big Data)_ , 2019, pp. 3153–55. Available at:<a href="https://doi.org/10.1109/BigData47090.2019.9006527">https://doi.org/10.1109/BigData47090.2019.9006527</a>.
</li>
<li id="wright2019">Wright, R. “Typewriting Mass Observation Online: Media Imprints on the Digital Archive,”  _History Workshop Journal_ 87, pp. 118–38 (2019). Available at:<a href="https://doi.org/10.1093/hwj/dbz005">https://doi.org/10.1093/hwj/dbz005</a>.
</li>
<li id="yale2017"> “Yale Digital Humanities Lab - PixPlot” (2020). Available at:<a href="https://dhlab.yale.edu/projects/pixplot/">https://dhlab.yale.edu/projects/pixplot/</a>(Accessed 11 June 2020).
</li>

</ul>
