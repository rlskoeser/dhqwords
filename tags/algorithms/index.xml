<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>algorithms on DHQwords</title><link>https://rlskoeser.github.io/dhqwords/tags/algorithms/</link><description>Recent content in algorithms on DHQwords</description><generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>cdh-info@princeton.edu</managingEditor><webMaster>cdh-info@princeton.edu</webMaster><copyright>This work is licensed under a Creative Commons Attribution-NoDerivatives 4.0 International License.</copyright><lastBuildDate>Thu, 20 Jul 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://rlskoeser.github.io/dhqwords/tags/algorithms/index.xml" rel="self" type="application/rss+xml"/><item><title>Tracing Toxicity Through Code: Towards a Method of Explainability and Interpretability in Software</title><link>https://rlskoeser.github.io/dhqwords/vol/17/2/000706/</link><pubDate>Thu, 20 Jul 2023 00:00:00 +0000</pubDate><author>David M. Berry</author><guid>https://rlskoeser.github.io/dhqwords/vol/17/2/000706/</guid><description>Introduction In the past decade, due to the perceived lack of accountability of algorithmic systems, particularly automated decision systems, a new explanatory demand has crystallized in an important critique of computational opaqueness and new forms of technical transparency calledÂ explainability. We see this, for example, in challenges to facial recognition technologies, public unease with algorithmic judicial systems and other automated decision systems. There have been new regulatory attempts to capture some of the ideas that stem from explainability such as the Algorithmic Accountability Act 2022 in the US, and the General Data Protection Regulation 2016/679 (GDPR) in the European Union.</description></item></channel></rss>