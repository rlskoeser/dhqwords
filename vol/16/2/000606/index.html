<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=theme-color content="#3D206C"><meta http-equiv=x-ua-compatible content="IE=edge"><link rel=apple-touch-icon sizes=180x180 href=/img/favicon/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/img/favicon/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/img/favicon/favicon-16x16.png><link rel=icon type=image/png sizes=384x384 href=/img/favicon/android-chrome-384x384.png><link rel=icon type=image/png sizes=192x192 href=/img/favicon/android-chrome-192x192.png><link rel=icon type=image/png sizes=150x150 href=/img/favicon/mstile-150x150.png><link rel="shortcut icon" href=/favicon.ico><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/img/favicon/safari-pinned-tab.svg color=#3d206c><link rel=schema.dc href=http://purl.org/DC/elements/1.0/><meta name=citation_public_url content="https://startwords.cdh.princeton.edu/vol/16/2/000606/"><meta name=citation_title content="Universal Dependencies and Author Attribution of Short Texts with Syntax Alone"><meta name=citation_date content="2022/04"><meta name=citation_author content="Robert Gorman"><meta name=citation_abstract content="1. Introduction Developing better methods of classifying short texts is becoming increasingly important in the field of computational stylometrics. Synoptic work (Eder 2015) suggests that many common approaches to at least one kind of text classification — Authorship Attribution — are unreliable when the text to be classified is less than a few thousand words. Maciej Eder’s study examines several classifiers (Burrows’ Delta, Support Vector Machine, and k-Nearest Neighbor) and finds that all suffer from a similar drop in effectiveness as the size of the target texts decreases."><meta name=citation_journal_title content="DHQwords"><meta name=citation_issn content="2694-2658"><meta name=citation_issue content="16.2"><meta name=citation_publisher content="Center for Digital Humanities, Princeton University"><meta name=DC.rights content="http://creativecommons.org/licenses/by/4.0/"><meta name=author content="Robert Gorman"><meta name=generator content="Center for Digital Humanities, Princeton University"><meta name=dcterms.created content="2022-04"><link rel=preload as=font type=font/woff2 href=/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-regular.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-italic.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-regular.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-300.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-700.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-300italic.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-500italic.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-700italic.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-500.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/fonts/Source_Sans_Pro/source-sans-pro-v13-latin-regular.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/fonts/Source_Sans_Pro/source-sans-pro-v13-latin-900.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/fonts/Source_Sans_Pro/source-sans-pro-v13-latin-300.woff2 crossorigin><title>Universal Dependencies and Author Attribution of Short Texts with Syntax Alone</title><meta name=description content="DHQwords Issue 16.2, April 2022. A research periodical irregularly published by the Center for Digital Humanities at Princeton. "><meta property="og:title" content="Universal Dependencies and Author Attribution of Short Texts with Syntax Alone"><meta property="og:description" content="1. Introduction Developing better methods of classifying short texts is becoming increasingly important in the field of computational stylometrics. Synoptic work (Eder 2015) suggests that many common approaches to at least one kind of text classification — Authorship Attribution — are unreliable when the text to be classified is less than a few thousand words. Maciej Eder’s study examines several classifiers (Burrows’ Delta, Support Vector Machine, and k-Nearest Neighbor) and finds that all suffer from a similar drop in effectiveness as the size of the target texts decreases."><meta property="og:type" content="article"><meta property="og:url" content="https://startwords.cdh.princeton.edu/vol/16/2/000606/"><meta property="og:image" content="https://startwords.cdh.princeton.edu/social.png"><meta property="article:section" content="vol"><meta property="article:published_time" content="2022-04-01T00:00:00+00:00"><meta property="article:modified_time" content="2023-08-04T13:14:15-04:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://startwords.cdh.princeton.edu/social.png"><meta name=twitter:title content="Universal Dependencies and Author Attribution of Short Texts with Syntax Alone"><meta name=twitter:description content="1. Introduction Developing better methods of classifying short texts is becoming increasingly important in the field of computational stylometrics. Synoptic work (Eder 2015) suggests that many common approaches to at least one kind of text classification — Authorship Attribution — are unreliable when the text to be classified is less than a few thousand words. Maciej Eder’s study examines several classifiers (Burrows’ Delta, Support Vector Machine, and k-Nearest Neighbor) and finds that all suffer from a similar drop in effectiveness as the size of the target texts decreases."><link rel=stylesheet href=/style.css><link rel=stylesheet href=/print.css media=print><script src=/js/polyfills.js></script><script defer src=/js/bundle.js></script></head><body class=article><header><nav class=main aria-label=main><ul><li class=home><a href=/>dhq</a></li><li class=issues><a href=/vol/>volumes</a></li></ul></nav></header><main><article><div class=grid><header><p class=number><a href=/vol/16/2/>Issue 16.2</a></p><h1>Universal Dependencies and Author Attribution of Short Texts with Syntax Alone</h1><p><ul class=authors><li><address>Robert Gorman</address></li></ul></p><p><time class=pubdate datetime=2022-04>April 2022</time></p><ul class="categories tags"></ul><ul class=tags></ul><p class=formats></p></header><section class=print-only><a class=first-page-header href=/ aria-label=Startwords><img alt=Startwords src=/pdf-logotype.svg></a>
<a class=page-header href=/ aria-label=Startwords><img alt=Startwords src=/pdf-logo.svg></a>
<a href=http://doi.org/ rel=alternate class=page-doi>doi:</a></section><div class=text-container><h2 id=1-introduction>1. Introduction</h2><p>Developing better methods of classifying short texts is becoming increasingly important in the field of computational stylometrics. Synoptic work (Eder 2015) suggests that many common approaches to at least one kind of text classification — Authorship Attribution — are unreliable when the text to be classified is less than a few thousand words. Maciej Eder’s study examines several classifiers (Burrows’ Delta, Support Vector Machine, and k-Nearest Neighbor) and finds that all suffer from a similar drop in effectiveness as the size of the target texts decreases. The same result is seen when considering the most usual independent variables (a.k.a.features) used as input to the various classifiers. These features include Most Frequent Words, character n-grams, and POS (part of speech) n-grams. Based on Eder’s results, one must conclude that success in classification of short texts needs improved algorithms and/or more informative input variables.</p><p>This study focuses on the second requirement and explores the value of morphological tagging and syntactic parsing to create a richly discriminative set of variables. Generally speaking, morphological and syntactic data are underused in Authorship Attribution as compared to lexical and character-based features. While a wide-ranging survey<a class=footnote-ref href=#stamatatos2009> [stamatatos2009] </a>was able to refer to a number of studies that used syntactic re-write rules or more complex morpho-syntactic features, an examination of references in a subsequent overview<a class=footnote-ref href=#swain2017> [swain2017] </a>indicates POS tags are almost the only such features in frequent use. However, a more recent experiment in Authorship Attribution relying entirely on morpho-syntactic variables<a class=footnote-ref href=#gorman2020> [gorman2020] </a>suggests that such features can significantly enrich the information available for text classification.</p><p>The general applicability of R. Gorman’s study is questionable for at least two reasons. First, it involves a morphologically complex language — ancient Greek. It is uncertain how its methods may be suited to a simpler morphological target such as English. Second, the study’s approach is based on a dependency syntax corpus in which morphology and dependency relations have been hand annotated by a single language expert. The corpus is unusually large to be annotated in this way, and the resulting accuracy and consistency is a luxury available in few languages.</p><p>The present investigation will further explore the viability of a morpho-syntactic approach to text classification. It will test the effectiveness of such variables in several languages representing a wide spectrum of morphological complexity (English, German, Spanish, Finnish, and Polish). In addition, the requisite morphological and dependency annotation will be generated automatically, using a freely available program. As a proof of concept, classification will be carried out with only the resultant morpho-syntactic information. This restriction will simplify interpretation of results by reducing possible confounding elements within the proposed feature set. In addition, an important consideration in authorship classification is to eliminate as far as possible the effects of topic and genre and to avoid relying on any features that could be easily imitated or manipulated. Syntactic features are thought to meet this requirement better than lexical ones. Since identifying syntactic information requires extra processing with possibly noisy results, most investigations fall back onfunction wordsas a proxy for truly syntactic features. By directly using computationally generated syntactic analyses instead, this study will explore the benefits of such pre-processing in comparison to the costs of the noise it inevitably introduces.</p><p>The remainder of this article has the following structure. Part 2 discusses the formation of the various language corpora. It then details the creation of the input variables. These variables are somewhat complicated, in that they are based on combinations of morpho-syntactic elements and, at the same time, seek to preserve as much flexibility as possible in the incorporation of these elements. Part 3 explains the several steps of the classification experiment itself. Part 4 presents the results of the classification process and explores their implications. It lays particular emphasis on the interpretability of the input variables used in this study. Arguably, descriptions of authorial style based on traditional morpho-syntactic categories will prove more persuasive to those outside the ranks of scholars of computational stylometrics.</p><h2 id=2-corpora-and-feature-extraction>2. Corpora and Feature Extraction</h2><p>This study involves corpora of texts in several languages: English, Finnish, German, Spanish, and Polish. An attempt was made to gather publicly available texts in a wider range of languages, but it was hindered by several factors. The design of the proposed experiment requires 20 single-author texts in a given language. All texts should be more than 20,000 tokens in length, and all texts in a language should be of the same type. Where possible, texts were to be drawn from a well-defined temporal range. These demands were most easily met by selecting works of fiction, in particular novels. The works were drawn from Project Gutenberg and the Computational Stylistics Group.<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></p><p>It may be surprising that a study interested in classification of short texts would use novels rather than, for example, tweets or news articles. However, the focus here is on the informational value of morpho-syntactic variables in various languages and not on any particular kind of text, and in this case the reasons for choosing novels are fairly compelling. As noted, the literature suggests that classification of more than a few texts becomes ineffective when the targets are shorter than a few thousand words. Thus, our investigation starts with texts c. 2000 words. Since data are traditionally split into a training set comprising approximately 90% of the material, with the remainder in the test set, texts of 20,000 words are in order. While it might be possible to manufacture texts of this length by concatenating tweets or the like, it would not be easy to find, for a range of languages, large sets of short texts that meet a second requirement: that single authorship of each 20,000-word text group can be assumed. Third, the use of novels minimizes, at least as compared to concatenated collections, possible confounding effects of genre and topic.</p><p>The core of this investigation is variable extraction and preparation. The first step is to generate morpho-syntactic annotations for each text. The processing is done with theudpipepackage for the R Software Environment<a class=footnote-ref href=#wijffels2019> [wijffels2019] </a>. The package includes functions to produce a universal dependency grammar analysis for texts in a wide variety of languages. Dependency grammar is an increasingly widely accepted approach to describing the structure of sentences. It offers an advantage to the present study in that, unlike phrase structure grammars, it deals well with non-projective grammatical sentence components — essentially, words closely related grammatically that are separated from each other by less closely related words. Non-projectivity occurs frequently in many languages, especially those which, unlike English, have a relatively complex system of morphology. The<a href=https://universaldependencies.org/>Universal Dependencies</a>framework<a class=footnote-ref href=#nivre2015> [nivre2015] </a>has been developed by an international cooperative of scholars to further cross-linguistic language study. One of its most important contributions is to establish a common set of tags for morphological features and syntactic relationships. This standard is a great step forward in natural language processing across languages, since,inter alia,the parsers (i.e., programs to analyze syntax) sponsored by the Universal Dependencies (UD) project produce mutually compatible output: a single algorithm can pre-process texts from a range of languages.</p><p>Raw text (.txt files) provided to the udpipe program gives an output in which each token is lemmatized, tagged for morphology, and parsed for universal dependency relationship. Formatted as a matrix, its most salient results look like this:<br>udpipe Annotation.token_idtokenlemmaupos1ErerPRON2lachtelachenVERB3vorvorADP4VergnügenVergnügenNOUN5,,PUNCT6sicher|es|siePRON7überüberADP8denderDET9KatechismusKatechismusNOUN10mokierenmokierenVERB11zuzuPART12könnenkönnenAUXudpipe Annotation (continued)token_idfeatshead_token_iddep_rel1Case=Nom|Gender=Masc|Number=Sing|Person=3|PronType=Prs2nsubj2Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin0root3Case=Nom|Gender=Neut|Number=Plur4case4Case=Nom|Gender=Neut|Number=Sing2obl5NA10punct6Case=Acc|Number=Sing|Person=3|PronType=Prs|Reflex=Yes10obj7Foreign=Yes9case8Case=Acc|Definite=Def|Gender=Masc|Number=Sing|PronType=Art9det9Case=Acc|Gender=Masc|Number=Sing10obl10VerbForm=Inf4xcomp11Case=Nom|Definite=Ind|Gender=Neut|Number=Sing|PronType=Ind10mark12VerbForm=Inf10aux<br>The text is part of a sentence from <em>Buddenbrooks</em> by Thomas Mann: “ Er lachte vor Vergnügen, sich über den Katechismus mokieren zu können … ” ( “He laughed with pleasure at being able to make fun of the catechism” ).</p><p>For each token the analysis gives the form as it appears in the text and its lemma. This information is not used in our approach to classification and will be ignored here. The remaining columns shown, however, are integral. Theuposcolumn contains the UD part-of-speech tags for each word. Thefeatscolumn gives the morphological analysis. Morphology information has the formTYPE=VALUE,with multiple features separated by a bar symbol (TYPE1=VALUE1|TYPE2=VALUE2). For example, token #2, <em>lachte</em> (laughed), is identified as Indicative in the category Mood (Mood=Ind), Singular in the category Number (Number=Sing), etc. There is no limit to the number of features that may be assigned to a single token.</p><p>Part of speech and morphology constitute what we may callshallowsyntactic features. Information of this type may allow us to infer some syntactical structures, but they do not represent them directly. In contrast, thehead_token_idanddep_relcolumns do constitute such a direct representation. The head token is the item that is the immediate syntacticparentof a given token. The dependency relation specifies the type of grammatical structure obtaining between parent and target. From these columns we may generate a description of the syntactic structure of the entire sentence in a representation of itsdeepsyntax. The structure revealed by these data points is perhaps most clearly illustrated with the corresponding dependency tree:</p><figure><img loading=lazy alt src sizes="(max-width: 768px) 100vw, 80vw" srcset="500w,
800w, w" class=landscape><figcaption><p>Dependency Tree of Example Sentence</p></figcaption></figure><p>As is apparent, the syntacticpathfrom the sentence root to the eachleaftoken is given by the combination of head id and dependency relationship.</p><p>Both shallow and deep syntactic information are used to create the input variables for this investigation. Before looking at those variables in detail, it is worth noticing that pre-processing with the udpipe program introduces a certain amount of noise through mistaken analyses. To focus on thefeatsoutput, token 4, the noun _ Vergnügen _ , is incorrectly assigned NOMINATIVE case (should be DATIVE). In addition, the three indeclinable tokens in this text — i.e., words with a fixed and static form whose morphology is usually not further analyzed — are treated very strangely indeed: the preposition _ vor _ and the particle _ zu _ are assigned gender, number, and case ( _ zu _ also gets classified for definiteness and pronoun type). The frequent preposition _ über _ is treated as a foreign word, perhaps as a dependent of the ecclesiastical (and ultimately ancient Greek) word _ Katechismus _ . Whatever the reason, the parser has introduced 4 errors in 12 tokens. Of course, the example text was chosen because it is short yet relatively complex; it may not be at all representative of the general error rate of the udpipe program. Nonetheless, it would be reasonable to worry that the noise introduced by erroneous morpho-syntactic annotations would undercut their value for classification input variables. However, as we will see below, bad tagging and parsing seems to make little or no practical difference in the classification results. We may evidently assume that these input errors are distributed randomly across the classified texts and that they have little effect as compared to the information carried by the input variables.</p><p>To turn now to the details of the features (i.e., input variables) themselves, there are two main principles guiding their creation: first, the set should include both shallow and deep syntactic information; second, thedegrees of freedompresent in the morpho-syntacticsystemof the target language should be, to the greatest extent possible, preserved and represented in the features.<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup></p><p>The first criterion is met by including, alongside the morphological information for each word, its dependency relation and, for each word that is not a sentence root, the morphology and dependency relation for the parent word of the target word. To illustrate from the German sentence given above, the variables for the word _ Katechismus _ would include the following:</p><ul><li>Self: POS = noun, case = accusative, gender = masculine, number = singular, dependency = oblique</li><li>Parent: POS = verb, verb form = infinitive, dependency = open clausal complement</li></ul><p>Including values for parent as well as target word creates a representation of the hierarchical structure that most linguistic theories assume for syntax. Of course, the dimensions of morpho-syntactic information produced by the udpipe tagger-parser vary sharply by language. The following table (Table 4) indicates the number ofbasicelements for each language.<br>Number ofSimplexVariablestarget tokentarget + parentEnglish1632Finnish2346German1530Polish3366Spanish1836<br>The second requirement mentioned above is meant to preserve the complexity of the morpho-syntax of the input texts. As we have noted, it is rare for an attribution attempt to take any morphological feature into account with the common exception of part of speech. In those studies that do include morphological details, the various values that make up the relevant information about a word seem to be coded in a single block. For example, Paulo Varela et al. (2018, 2016) incorporate in their variables a vector forflexion(number, gender, tense, etc.), but apparently combine the features of a word into a single value: thus the entry for <em>é</em> (Portugueseis) is reported asPR 3S IND VFIN,i.e, present indicative 3rd-person singular verb.<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup></p><p>Combining the morphological categories of a word into a single value obscures some of the stylistic information that the word may contain. For example, if we categorize a German noun into a consolidated unit of gender, number, and case (e.g., Masc Sing Nom), the computer can assign each noun in the corpus to one of the 24 resulting categories (3 genders x 2 numbers x 4 cases) and calculate a frequency distribution over them, but it cannot access the frequencies of, say, masculine nouns or of combinations of two categories such as feminine dative, etc.<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup> There is no strong reason to presume that any frequency ratio among the ternary variables is more discriminative for the texts in a corpus than, e.g., the ratio between Masc Nom and Fem Nom. We may usefully think of the morpho-syntactic elements that make up combinations ascharactersinsyntactic n-grams.<sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup> Just as the addition to word-based inputs of character n-grams with different values of <em>n</em> may improve classification, in the same way using a range of combination lengths for morpho-syntactic elements may be valuable. Thus, it is best <em>in principle,</em> to include variables made of allaritiesof morpho-syntactic categories, from one to the total number of such categories (i.e.,n1…nn).<sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup></p><p>Of course, such an extensive variable set is not feasible <em>in practice.</em> For example, the udpipe program for German returns 13 categories of morphological features. To these 13 must be added the part of speech and the dependency relationship, and since the characteristics of the parent word are to be combined with those of each target word, variable types will be combinations constructed from 30 categories. If all sizes of combination were included (i.e.,301…3030), the number of types would be essentially230—over a billion separate combinations! Nor is the combinatorial explosion the only problem. Each of the230combinations is a type, not a variable. We have seen that the ternary combination Gender-Number-Case may take one of 24 different values in German: MASC SING NOM, etc. Thus, the number of type-value pairs reaches truly astronomical values. Clearly, we must combine the generation of input features with a rigorous culling and reduction process.</p><p>The first step deals with the combinatorial dimension. Selecting grammatically sensible combinations such as Gender-Number-Case (and its unigram and bigram components) would be laborious and would require knowledge of the various languages to be classified. Thus, a naïve approach was preferred: an arbitrary maximum length was chosen. Given a set of 30 elements for combination, the length must necessarily be short.∑k=1430kalready gives 31,930 unique combinations. When we take into account that each combination will have multiple values, trouble with data sparsity and computational limitations can be expected. Therefore, the maximum was set at three elements per combination. This limit results in 4,525 combinations.</p><p>The next step populates each combination (or variable type) with its various values, as drawn from the basic morpho-syntactic elements produced by udpipe. This process is computationally slow for combinations of more than two elements, so we have used a smaller sample corpus for each language; it is composed of 500 tokens (punctuation excluded) from each text. When populated, the various combinations of types exhibit a large number of unique values. To take German as illustrative: the 30 unary types produce 161 values, the 435 binary types produce 5,579 values, and the 4,060 ternary combinations yield 56,349 values. As one would expect with linguistic data, the frequency distributions of these values are quite skewed. For example, 53,209 of the ternary values occur in 1% or fewer words and 19,116 occur only once in the sample corpus of 10,000 tokens. To avoid severe sparsity, another culling of variables is clearly in order.</p><p>Once again, since we do not know without prior investigation which combinations may be most distinctive for authors and texts, we have had recourse to a naïve approach. For each combination length, only those type-value pairs that occur in 5% of the tokens in the sample corpus have been included as input variables for classification. A separate set of variables has been identified in this way for each language. The size of each sub-set of variables is given in the following table (Table 3).<br>Number of Variable Subsets with at Least 5% FrequencyunarybinaryternarytotalEnglish456185191Finnish48226467932German58216318592Polish633317261,120Spanish46150189385<br>Because the variables may be difficult to conceptualize, the following tables give an illustration, including, for selected languages, the most frequent type-value pair as well as the least frequent pair to make the 5% minimum. Examples will be given in each category for target word characteristics (markedt:), parent word characteristics (marked “p:”), and combinations of target and parent (Tables 4, 5, and 6).<br>Examples of Simplex VariablestypevaluefrequencyEnglisht: NumSing29.80%p: POSVERB44.90%t: Relroot5.15%p: Relccomp5.56%Polisht: NumSing48.20%p: NumSing65.20%t: Relcc5.20%p: CaseLoc5.40%Examples of Binary VariablestypevaluefrequencyEnglisht: POS & t: NumNOUN/Sing13.80%p: POS & p: NumNOUN/Sing28%t: POS & t: VerbFormAUX/Fin5.50%p: Num & p: RelSing/nmod5.20%t: Num & p: POSSing/VERB16.30%t: POS & p: RelVERB/root5%Polisht: Num & t: GenderMasc/Sing21.20%p: POS & p: VoiceVERB/Act46.70%t: POS & t: AnimacyVerb/Hum5%p: Gender & p: AspectFem/Perf5.20%t: Num & p: NumSing/Sing33.70%t: Case & p: VerbFormAcc/Fin5%Examples of Ternary VariablestypevaluefrequencyEnglisht: POS & t: Num & t: PronTypePRON/Sing/Prs8.90%p: POS & p: VerbForm & p: MoodVERB/Fin/Ind19.90%t: Rel & t: Definite & t: PronTypedet/Def/Art5.50%p: Rel & p: VerbForm & p: Tenseroot/Fin/Past7.70%t: Num & p: POS & p: TenseSing/Verb/Past9.10%t: Rel & p: POS & p: Moodnsubj/Verb/Ind5.10%Polisht: Mood & t: VerbForm & t: VoiceInd/Fin/Act14%p: POS & p: VerbForm & p: MoodVERB/Fin/Ind41.30%t: POS & t: Animacy & t: VoiceVERB/Hum/Act5%p: Rel & p: Num & p: Aspectconj/Sing/Perf5%t: Num & p: POS & p: VerbFormSing/VERB/Fin20.60%t: Num & p: POS & p: RelSing/VERB/conj5%<br>It is important to emphasize that it is the <em>pairing</em> of the (possibly compound) types <em>and</em> their (possibly compound) values which constitute the input variables for this investigation. In Polish, for example, a type combining the Number of the target word, with the part of speech and dependency relation of the target word’s parent can have many possible values. But only those combinations of values that occur no less often than the 5% frequency of Singular-Verb-conjunct have been taken into account. All less frequent values for this type have been excluded from the variables and ignored.<sup id=fnref:7><a href=#fn:7 class=footnote-ref role=doc-noteref>7</a></sup></p><p>This generation and selection of variables produce, for each text in a language corpus, a matrix in which each row represents a token of the text and each column represents a type-value pair as described above. The total number of variable columns differs for each language (see Table 4). For each row, each cell is assigned a value of one if the token has the morpho-syntactic characteristic for that column; otherwise, zero is assigned. The design of the variables means any given token can have positive values in numerous columns. The German corpus, for example, has a mean of 48.8 positive columns per token (of a total 592 variables).</p><h2 id=3-classification>3. Classification</h2><p>The purpose of this study is to test whether automated Universal Dependency annotation, as supplied by the udpipe program, can be useful for authorship attribution and related problems. It seeks to understand whether such an input contains enough information to overcome the noise introduced by a level of mistakes, which can be relatively high as compared to the results of expert “hand annotation.” Since it is known that the information/noise ratio worsens for shorter input texts<a class=footnote-ref href=#eder2017> [eder2017] </a>, this investigation focuses on the degree to which classification performance degrades as the size of the input decreases. This process requires texts of various lengths, with all other factors held constant, insofar as this may be possible. This requirement is met by creating a series of smallertextsfor each author by sampling the full text.</p><p>At each stage of classification, each text in each language corpus was divided into smaller texts, and these smaller units were classified according to author.<a class=footnote-ref href=#gorman2020> [gorman2020] </a>has shown that — at least for expert-annotated ancient Greek prose — classification on the basis of combinations of morpho-syntactic characteristics is practically error-free for input texts larger than 500 tokens (> 99%). Exploratory tests for our study confirm this high level of accuracy for text sizes of between 2000 and 600 tokens. Thus, this paper will present only the data for the smallest texts in the investigation; it begins with texts of 500 tokens, and then decreases to 400, 300, 200, 100, and finally 50 tokens.</p><p>As is traditional in textual attribution studies, each text was treated as abag of wordsfor the purpose of division into samples. Each token was — naively — treated as independent of all others; no further account was taken of the context of an individual token in sentence, paragraph, or any other unit of composition. Thus, segments may contain tokens from many parts of the original text without regard for their original order.<sup id=fnref:8><a href=#fn:8 class=footnote-ref role=doc-noteref>8</a></sup></p><p>Once segments of the appropriate length have been generated, the next step is to aggregate the values for all variables in each segment. For example, a matrix with 20,000 rows representing as many separate tokens, each with a 1 or a 0 in its variable columns, is replaced by a new matrix with 40 rows, each representing 500 tokens. Each variable column now contains the sum of the relevant column for the underlying tokens. Columns values are then normalized so that sums are replaced by relative frequencies. Thesesegment matricesbecome the input for the classifying algorithm.</p><p>To present a difficult classification problem<a class=footnote-ref href=#luyckx2008> [luyckx2008] </a><a class=footnote-ref href=#luyckx2011>[luyckx2011] </a>, each language corpus contains one work apiece by 20 different authors. Given the number of suitable texts available in the public domain, this number represent an attempt to balance between including more languages with fewer authors for each and including fewer languages with more authors in each.</p><p>For the classification algorithm itself, logistic regression was chosen. This method has an ability to handle a large number of observations and variables. It is also able to function well in the presence of many co-linear variables. Specifically, the <em>LiblineaR</em> package for the <em>R Project for Statistical Computing</em> was chosen<a class=footnote-ref href=#fan2008> [fan2008] </a><a class=footnote-ref href=#helleputte2017>[helleputte2017] </a>. This package offers a range of linear methods; we selected the L-2 regularization option for logistic regression.<sup id=fnref:9><a href=#fn:9 class=footnote-ref role=doc-noteref>9</a></sup></p><p>For each input text size in each language, 90% of the data was used for training the classifier and the remaining 10% set aside for testing. Inclusion of a segment in the training or testing set was random. However, the amount of text by individual authors in some language corpora varies a great deal, and this situation may affect results, with the classifier, for example, strongly preferring the most frequent author in the training set. Thus, the random assignment into training and test sets was guided by associating a selection probability with each segment so that each of the <em>n</em> authors in a corpus was represented by approximately1nof the segments in the test set. This balance prevents undue bias in the classifier. To validate the results of the classification testing, we used Monte Carlo subsampling<a class=footnote-ref href=#simon2007> [simon2007] </a>applied at two levels. As a rule, the populating of the segments with randomly selected tokens was carried out ten times.<sup id=fnref:10><a href=#fn:10 class=footnote-ref role=doc-noteref>10</a></sup> For each of these partitionings to create text segments, 100 additional random divisions into a training set and a test set were made. As noted below, because the classification process becomes much slower as text size falls, fewer iterations of both kinds of subsampling were used for the smallest segments. The results of the tests at each text size were averaged. These steps minimize the effects of the internal make-up of particular segments or of their inclusion or exclusion from the training/testing groups.</p><h2 id=4-results-and-discussion>4. Results and Discussion</h2><p>For each language corpus, the results of classification according to the size of the input text segments are given in Table 8. Since the goal of this study is to evaluate for authorship attribution the sufficiency of machine-generated Universal Dependency data, no consideration is given to various specialized measurements of classification success. Instead, the simplest measure of accuracy is used: percentage of successful attributions. The mean accuracy over all attempts is given in the top row of each cell; the range is given below the mean.<br>Classification Accuracy50040030020010050Polish99.35(95.8-100)99.05(95.3-100)98.18(94.5-100)96.28(92.7-99.3)88.32(84.3-92.0)74.19(71.0-78.5)Finnish99.52(99.3-99.7)99.03(98.7-99.3)98.15(97.8-98.4)95.56(95.1-96.0)86.44(85.8-86.9)70.61(70.2-71.1)Spanish98.87(98.7-99.0)98.25(98.0-98.4)96.75(96.4-97.0)93.18(92.8-93.5)81.2(80.6-81.6)61.7(61.6-62.0)German96.7(93.2-99.2)95.4(91.6-98.4)92.85(87.9-95.5)87.99(84.8-91.1)76.95(71.7-83.0)56.07(54.5-58.2)English98.26(96.4-99.8)97.25(94.0-99.4)94.75(92.5-97.5)89.7(87.3-92.6)74.45(71.6-77.4)53.8(51.6-55.9)<br>These data make clear that morpho-syntactic variables derived from UD automated parsing suffice to identify author/work in a closed set of authors with a single work for each. For all tested languages, accuracy is greater than 90% with text segments larger than 300 tokens. Each language corpus contains 20 classes for attribution; 90% accuracy is roughly 18 times random chance since theno-information ratefor each iteration varies slightly from 5%.<sup id=fnref:11><a href=#fn:11 class=footnote-ref role=doc-noteref>11</a></sup> Such success offers promise for the feasibility of classification when vocabulary- and/or character-based variables are not appropriate.</p><p>A few more specific observations about these results are in order. In general, accuracy tracks well with the size of the variable set for each language (see Table 4). Languages with larger sets tend to have better accuracy. We may reasonably presume that this tendency is due in part to the correlation between the number of variables and the sparsity of the input matrices. Ordinarily, we expect sparsity — the number of cells of a matrix containing zeros — to increase as the number of variables becomes greater. And more sparsity in data for natural language processing usually means less accuracy. However, recall that in this study a single token may have positive values for many variables and that any variable which has positive values for fewer than 5% of tokens has been excluded. The combination of these two factors means that as the number of total variables increases, so too does the number of variables per token (Table 9). When the token matrices are aggregated to form representations oftextsof various lengths, the variable columns are summed. The presence of positive values in a good number of columns in the original matrix makes it unsurprising when relatively few columns in the aggregated matrix sum to 0. We may illustrate this with the example of Spanish. The average token in the Spanish matrix has 34.7 columns with a positive value. Forming, e.g., a 50-token segment by the aggregation of the matrix rows means that, on average, 1,735 (34.7 × 50) positive values will be distributed among the 385 variable columns in the resulting matrix. It stands to reason that, all else being equal, we may expect relatively few aggregated columns containing zeros. In fact, with the variables used in this study, sparsity is almost non-existent for texts larger than 100 tokens; the sparsity rates for 50-token texts are themselves unexpectedly small given the number of variables (Table 9). This low sparsity may go far to explaining the good accuracy of the classification experiment.<br>SparsityVariable typestotalMean VariablesPer tokenSparsity@50 tks.Polish1,120102.95.8%Finnish93269.73.8%Spanish38534.73.4%German59248.05.5%English19116.75.0%<br>A second aspect of the accuracy results reported in Table 3 needs closer elaboration. As is usual for author attribution studies that take text size into consideration, results in each language show a monotonic decline in accuracy as text size decreases. However, when viewed from the perspective of relative error rates, our morpho-syntactic variables seem quite robust against decreasing text size.</p><p>When considering classification accuracy of a range of text sizes, it is important to focus on the relationship between the change in accuracy and the change in text size. A 5% decrease in accuracy when text size drops from 2000 tokens to 1900 tokens is much more worrisome than the same decrease from 2000 to 1000 tokens. Table 10 gives the relevant information for this experiment. Specifically, it records the rates at which the error rate (1 – Accuracy) for each language increases as the text size is repeatedly halved: 400 to 200 tokens, 200 to 100, and 100 to 50.<br>Change in Error Rate by Text Size200/400100/20050/100Polish0.0372/.0095= 3.920.1168/.0372= 3.130.2581/0.1168= 2.2Finnish0.044/0.0097= 4.570.1356/0.044= 3.050.2939/0.1356= 2.16Spanish0.0682/0.0175= 3.890.188/0.0682= 2.750.383/0.188= 2.03German0.1201/.046= 2.610.2305/0.1201= 1.910.4393/0.2305= 1.90English0.103/0.0275= 3.740.2555/0.103= 2.480.462/0.2555= 1.81<br>The first row in each cell gives the error rate for a given text size divided by the error rate for the text that is double the size. For example, the Polish 200-token corpus had an average error rate of 3.72%, which is divided by the average error rate for the Polish 400-token corpus, 0.95%. The second line of the cell gives the corresponding result: while the text size decreases by a factor of 2, the error rate increases by a factor of 3.92. An overview of this table shows that for all languages the increase factor becomes significantly smaller as the input size itself decreases. While the absolute accuracy (or error) rate is still quite properly the metric of interest for literary and, most especially, forensic attribution methods, clearly presenting proportional relationships such as that of the rate of change in text size viz-à-viz the change in success rate should help us to better judge the effectiveness of different approaches to classification of small texts.</p><p>There are, of course, anomalies in the patterns observable in Table 8. For example, since the German matrix has both more total variables and more variables per token than does the Spanish, we might reasonably expect classification of German would outperform Spanish. This is not the case. Nor can an explanation be offered here.<sup id=fnref:12><a href=#fn:12 class=footnote-ref role=doc-noteref>12</a></sup> We can only note that some of the differences between the success rates of various languages must be due not to characteristics of the morpho-syntactic variables, but to particularities of the language corpora. Although an effort was made, within the limits of the texts available in the public domain, to assemble corpora that were roughly similar among languages, important aspects of the corpora must necessarily differ. It is inevitable, for example, that one corpus contains a higher proportion of texts that are more difficult to distinguish from each other than does another corpus. It is possible that this advantage, then, is so great that it may significantly affect the classification accuracy rates. Another complicating factor is introduced by the relative effectiveness of the udpipe annotation. Again, we must assume a possibly wide range between the most and the least accurate of the annotation algorithms. Accordingly, a relatively large amount of noise in the variables of an individual language may certainly be expected to suppress classification accuracy.</p><p>However, identifying such possible confounding elements and measuring their effects are not germane to the purpose of this study. Our experiments have been designed to test the hypothesis that machine-generated morpho-syntactic annotation can be used for accurate closed-set attribution, even for fairly small texts, in a range of languages. All comparisons among languages are merely illustrative and suggestive: greater morpho-syntactic complexity seems to correlate at least approximately with accuracy. Detailed investigation of this apparent relationship must be left to experts in the respective languages and literatures.</p><p>This investigation has set out to explore the discriminative value of a certain set of input features. It has advanced the hypothesis that, absent any vocabulary data, a mixture of a text’s shallow and deep morpho-syntactic characteristics, when combined in a way that preserves as manydegrees of freedomas possible, can produce strong results in closed-set classification. These results are significant, insomuch as most text classification methods rely primarily on vocabulary or character information, with a text’s syntax represented only by function words and POS tags. However, the quantitative study of language and texts has recently seen the rise in prominence of approaches such as deep neural networks. These are oftenend to endlearning systems; input consists ofrawtext with little pre-processing and no feature extraction. Results can be amazingly accurate. In the presence of such effective alternatives, the reader of this study may reasonably feel doubts about a method which requires syntactic parsing as well as significant effort given to feature engineering.</p><p>Yet, user-specified input features have their own advantages. The continuing efforts to find better methods of text classification is primarily driven, at least in the various disciplines of the humanities, by the desire to identify and describe the essential features of individual style. Investigations seek to confirm (or refute) the hypothesis that every user of language has a stylisticfingerprintorsignaturethat allows written or spokentextsfrom that source to be distinguished from all others. In order for studies of this sort to be plausible, however accurate the results may be, we must be able to give a clear interpretation for the variables on which the classification depends.<sup id=fnref:13><a href=#fn:13 class=footnote-ref role=doc-noteref>13</a></sup> Optimally, we must give this interpretation in terms of commonly accepted phenomena of language and communication. Generally speaking, hand-crafted input features meet this criterion. In contrast, end-to-end deep learning algorithms typically abstract their own features, and often it takes a great deal of effort to establish what phenomena the model is taking into account.<sup id=fnref:14><a href=#fn:14 class=footnote-ref role=doc-noteref>14</a></sup></p><p>The input features examined in this study are familiar in their basic elements and transparently interpretable. Anyone interested in the dimensions of authorial style presented here can quickly learn the outline of a language’s morphology and the elementary structures of Universal Dependency grammar. As a result, classification based on UD parsing and morphology tagging puts even beginning students of style in a position to explore clear distinctions between texts and authors.</p><p>Since even a classification technique as fundamental as logistic regression complicates the matter by adding a layer of learned weights to the input features, the interpretability of these morpho-syntactic variables is well illustrated if looked at through the lens of a simple distance measure. Since its publication, Burrows’s Delta has become one of the most widely used metrics for comparing text styles. In essence, Delta measures, for each feature of interest, how far the target text differs from the mean of the corpus, normalized by the standard deviation of each variable.<sup id=fnref:15><a href=#fn:15 class=footnote-ref role=doc-noteref>15</a></sup> In spite of its simplicity, Delta can produce very good classification results<a class=footnote-ref href=#eder2015> [eder2015] </a>. Conveniently, classification by this method is included in theStylopackage for R<a class=footnote-ref href=#eder2016> [eder2016] </a>. Table 11 demonstrates the effectiveness of our morpho-syntactic variables by showing the results for the classification of 33 Polish novels usingStylo.<sup id=fnref:16><a href=#fn:16 class=footnote-ref role=doc-noteref>16</a></sup><br>Classification Accuracy (Burrows’s Delta)Text SizeAccuracyText SizeAccuracyText SizeAccuracy200099.99(96.8-100)150099.98(97.87-100)90099.30(96.0-100)190099.98(97.3-100)140099.84(98.0-100)80098.82(95.2-100)180099.93(97.5-100)130099.9098.1-10070098.17(93.7-100)170099.97(95.0-100)120099.6893.0-10060097.1(90.3-100)1600100(100-100)110099.3(95.2-100)50095.90(90.1-100)100099.67(95.4-100)<br>Clearly, the variables used in this study can accurately distinguish texts without the addition of weights and calculations that may obscure interpretation. A very few brief examples should suffice to make this point.</p><p>According to the application of Burrows’s Delta to the morpho-syntactic variables presented here, the two stylistically most distinct works in our corpus of English fiction are James Fenimore Cooper’s <em>The Last of the Mohicans</em> (1826) and Mark Twain’s <em>The Adventures of Huckleberry Finn</em> (1885). The most distinctive feature in the set (where the two authors differ by almost four standard deviations) is Cooper’s fondness for an oblique dependent of a verb that itself is modified by its own dependent. In UD grammar, the oblique of a verb is a nominal dependent of a verb that is not an argument of the verb. Here is an example from Cooper’s first paragraph: “The hardy colonist, and the trained European who fought at his side ….” Here, <em>side</em> inat his sideis an oblique of <em>fought</em> and is modified by its dependent possessive <em>his.</em> If the annotations created byudpipeare to be trusted, Cooper used such structures to an unusual degree, while Twain avoided them. Comparing Cooper to the entire English corpus, we find that the author was also inordinately fond of modifying the object of a verb with a dependency.<sup id=fnref:17><a href=#fn:17 class=footnote-ref role=doc-noteref>17</a></sup> The unusually high frequency of this construction remains even if we control for the number of objects in general, as well as when we consider the total frequency of all verbs.</p><p>To examine a morphologically more complex language, in our German corpus the work with the greatest mean distance to all the others is Arthur Achleitner’s <em>Im grünen Tann</em> (1897?). One of the strongest <em>differentiae</em> is the author’s marked preference for verbs in the Present Indicative Active 3rd-person Singular. Because the data set contains values for simple morphological elements as well as combination, we can quickly see that Achleitner’s verbal tendency can be explained in terms of his very sharp relative avoidance of past tense verbs, alongside a more moderate relative deficit of plural verbs; we may assume that mood, voice, and person do not play a significant role in this stylistic peculiarity. With respect to nominal forms, Achleitner shows a relatively high number of nouns with a dependent definite article; this frequency is only partly explained by the relative number of nouns in general. At the same time, the frequency of pronouns is much lower than average. Accordingly, we might formulate, perhaps for our students, a “thumbnail” outline of Achleitner’s stylistic quirks: he loves <em>liebt</em> and hates <em>liebte</em> ; loves <em>der, die, das</em> and hates <em>er, sie, es.</em> It is a gross simplification to be sure, but it emphasizes the transparency of stylistic distinctions drawn on the basis of our feature set.</p><p>In conclusion, the evidence presented in this study has shown that morpho-syntactic features can form a basis for the successful classification of texts. Results are good across languages ranging from the morphologically complex (e.g., Polish) to the simple (e.g., English). And, while it is unlikely that the frequency of morpho-syntactic elements is unaffected by topic, genre, etc., we can reasonably suppose that it is less affected by such elements external to the author than are variables based primarily on aspects of vocabulary. Therefore, morpho-syntactic features seem essential to characterizing an author’s stylistic signature.</p><p>A second goal of the present work has been to demonstrate that we do not need to rely on expensive expert-annotated data to provide satisfactory syntactic information. Automated parsers such as udpipe are able to output Universal Dependency annotation that is sufficient to form the basis of effective variables representing both theshallowanddeepsyntax of a text. Because, with most variable sets used in the attribution literature, noise tends to overwhelm information when text size decreases sufficiently, this investigation has focused on short texts. Even with texts of 50 tokens, accuracy remains many times better than random chance (74%-53%). These results were achieved with the most naïve methods of feature selection and without any optimization of the parsing or classifying algorithms. We may thus suggest that further explorations of the value of UD parsing for classification promise to be productive.</p><p>Finally, we have argued that the morpho-syntactic features discussed here are advantageous, at least from the point of view of interpretability and pedagogical usefulness. Unlike n-grams, for example, morpho-syntactic variables represent traditional grammatical terms and categories. Unlike function words, such variables — at least when the preservation ofdegrees of freedomis emphasized in their construction — contain information based on every word in a text. As a result, stylistic traits formulated in terms of these variables are easily identified and illustrated in a way relatively more likely to be persuasive in the classroom or in the pages of a specialized literary journal.</p><p>In sum, this investigation of short text classification on the basis of machine-generated Universal Dependency annotation indicates that, at the very least, morpho-syntax should occupy a larger role in the future development of text attribution. Perhaps it should even take a central role.</p><h2 id=appendix-list-of-works-in-corpora>Appendix: List of Works in Corpora</h2><p>English, Finnish, German, and Spanish texts are taken from Project Gutenberg (<a href=https://www.gutenberg.org/>https://www.gutenberg.org/</a>) and are listed here by author and title. Polish texts are drawn from the web site of The Computational Stylistics Group (<a href=https://computationalstylistics.github.io/>https://computationalstylistics.github.io/</a>; texts at<a href=https://github.com/computationalstylistics/100_polish_novels>https://github.com/computationalstylistics/100_polish_novels</a>). No bibliographic data are provided on this site. Thus, Polish texts are listed by the file name used by the repository.</p><h2 id=english-corpus>English Corpus</h2><p>Alcott, <em>Little Women</em> Austen, <em>Pride and Prejudice</em> Barrie, <em>Peter Pan</em> Baum, <em>The Wonderful Wizard of Oz</em> Bronte, <em>Wuthering Heights</em> Cather, <em>My Antonia</em> Christie, <em>The Mysterious Affair at Styles</em> Conan Doyle, <em>The Hound of the Baskervilles</em> Cooper . <em>The Last of the Mohicans</em> Dickens, <em>A Christmas Carol</em> Eliot, <em>Middlemarch</em> Hardy, <em>Tess of the d&rsquo;Urbervilles</em> Joyce, <em>A Portrait of the Artist as a Young Man</em> London, <em>White Fang</em> Melville, <em>Moby Dick</em> Montgomery, <em>Anne of Green Gables</em> Shelly, <em>Frankenstein</em> Sinclair, <em>The Jungle</em> Twain, <em>Adventures of Huckleberry Finn</em> Wharton, <em>Ethan Frome</em></p><h2 id=finnish-corpus>Finnish Corpus</h2><p>Aho, <em>Hellmannin herra; Esimerkin vuoksi; Maailman murjoma</em> Airola, <em>Keksijän voitto</em> Alkio, <em>Eeva</em> Anttila, <em>Hallimajan nuoret</em> Canth, <em>Köyhää kansaa; Salakari</em> Elster, <em>Päivän valaisemia pilven hattaroita</em> Ervast, <em>Haaveilija</em> Gummerus, <em>Peritäänkö vihakin?</em> Haahti, <em>Valkeneva tie</em> Hahnsson, <em>Huutolaiset</em> Hannikainen, <em>Erakkojärveläiset</em> Heman, <em>Kysymysmerkkejä</em> Högman, <em>Merimiehen matkamuistelmia 1</em> Ivalo, <em>Aikansa lapsipuoli</em> Jääskeläinen, <em>Iloisia juttuja IV</em> Jahnsson, <em>Hatanpään Heikki ja hänen morsiamensa</em> Järnefelt, <em>Isänmaa</em> Järventaus, <em>Kaukainen onni</em> Järvi, <em>Harry</em> Kataja, <em>Lain varjolla</em></p><h2 id=german-corpus>German Corpus</h2><p>Achleitner, <em>Im grünen Tann</em> Ahlefeld, <em>Die Bekanntschaft auf der Reise</em> Aldersfeld-Ballestrem, <em>Die Falkner vom Falkenhof 1</em> Bechstein, <em>Der Dunkelgraf</em> Bernhard, <em>Die Glücklichen</em> Bonsels, <em>Eros und die Evangelien</em> Feuchtwanger, <em>Die häßliche Herzogin</em> Fontane, <em>Effi Briest</em> Gjellerup, <em>Der Pilger Kamanita</em> Goethe, <em>Wilhelm Meisters Lehrjahre 1</em> Grillparzer, <em>Das Kloster bei Sendomir</em> Hauff, <em>Der Mann im Mond</em> Hesse, <em>Unterm Rad</em> Hoffmann, <em>Klein Zaches, genannt Zinnober</em> Huch, <em>Der Fall Deruga</em> Mann, H., <em>Der Untertan</em> Mann, Th., <em>Buddenbrooks</em> Meyrink, <em>Der Golem</em> Spyri, <em>Heimatlos</em> Zweig, <em>Die Liebe der Erika Ewald</em></p><h2 id=spanish-corpus>Spanish Corpus</h2><p>Blasco Ibáñez, <em>La Catedral</em> Caro, <em>Amar es vencer</em> Carrere, <em>La copa de Verlaine</em> Conscience, <em>La niña robada</em> Delgado, <em>Angelina</em> Dourliac, <em>Liette</em> Espina, <em>Agua de Nieve</em> Fernández y González, <em>Los hermanos Plantagenet</em> Gil y Carrasco, <em>El señor de Bembibre</em> Halévy, <em>El Abate Constantin</em> Larra, <em>Si yo fuera rico!</em> Larreta, <em>La gloria de don Ramiro</em> Leumann, <em>Adriana Zumarán</em>  Mancey, <em>Las Solteronas</em> Ocantos, <em>Quilito</em> Ortega y Frías, <em>La Gente Cursi</em> Palacio, <em>La alegría del capitán Ribot</em> Pardo Bazán, <em>Una Cristiana</em> Pereda, <em>Al primer vuelo</em></p><h2 id=polish-corpus-20-texts-used-for-logistic-regression>Polish Corpus (20 texts used for logistic regression)</h2><p>balucki_burmistrz_1887.txtbeczkowska_gniezdzie_1899.txtdabrowska_nocednie2_1932.txtdmochowska_dwor_1903.txtdomanska_paziowie_1910.txtgodlewska_kwiat_1897.txtiwaszkiewicz_czerwone_1934.txtkaczkowski_olbrachtowi_1889.txtkossak_oreza_1937.txtkrzemieniecka_fatum_1904.txtkuncewiczowa_twarz_1928.txtmarrene_mezowie_1875.txtmostowicz_hanki_1939.txtnalkowska_romans_1923.txtprus_faraon_1897.txtrodziewicz_lato_1920.txtsienkiewicz_quo_1896.txtsygietynski_calvados_1884.txtzapolska_tagiejew_1905.txtzeromski_przedwiosnie_1924.txt</p><h2 id=polish-corpus-33-texts-used-for-classification-by-burrowss-delta>Polish Corpus (33 texts used for classification by Burrows’s Delta)</h2><p>balucki_burmistrz_1887.txtbeczkowska_bedzie_1897.txtberent_diogenes_1937.txtdabrowska_nocednie1_1931.txtdeotyma_panienka_1893.txtdmochowska_dwor_1903.txtdomanska_historia_1913.txtdygasinski_as_1896.txtgodlewska_kato_1897.txtgojawiczynska_dziewczeta_1935.txtiwaszkiewicz_czerwone_1934.txtkaczkowski_grob_1857.txtkorzeniowski_emeryt_1851.txtkossak_bog_1935.txtkraszewski_kordecki_1850.txtkrzemieniecka_fatum_1904.txtkuncewiczowa_cudzoziemka_1936.txtmakuszynski_basie_1937.txtmarrene_bozek_1871.txtmniszek_gehenna_1914.txtmostowicz_hanki_1939.txtnalkowska_granica_1935.txtorzeszkowa_gloria_1910.txtprus_emancypantki_1894.txtreymont_chlopi_1908.txtrodziewicz_lato_1920.txtsamozwaniec_ustach_1922.txtsienkiewicz_rodzina_1894.txtswietochowski_twinko_1936.txtsygietynski_wysadzony_1891.txtzapolska_tagiejew_1905.txtzarzycka_wiatr_1934.txtzeromski_syzyfowe_1897.txt</p><ul><li id=bittner2003>Bittner, D., Dressler, W. U., and Kilani-Schoch, M. (eds). _Development of Verb Inflection in First Language Acquisition: A Cross-Linguistic Perspective._ Mouton de Gruyter, Berlin (2003).</li><li id=eder2015>Eder, M. “Does Size Matter? Authorship Attribution, Small Samples, Big Problem” , _Digital Scholarship in the Humanities_ , 30.2 (2015): 167–82.</li><li id=eder2017>Eder, M. “Short Samples in Authorship Attribution: A New Approach,” _Digital Humanities 2017: Conference Abstracts_ . McGill University, Montreal (2017), pp. 221–24.<a href=https://dh2017.adho.org/abstracts/341/341.pdf>https://dh2017.adho.org/abstracts/341/341.pdf</a>.</li><li id=eder2016>Eder, M., Rybicki, J., and Kestemont, M. “Stylometry with R: a Package for Computational Text Analysis” , _R Journal_ , 8.1 (2016): 107–21.</li><li id=evert2017>Evert, S. Proisl, T., Fotis, J., Reger, I., Pielström, S., Schöch, Ch., and Vitt, Th. “Understanding and Explaining Delta Measures for Authorship Attribution, Digital Scholarship in the Humanities” , _Digital Scholarship in the Humanities_ 32.suppl. 2 (2017): ii4–ii16.<a href=https://doi.org/10.1093/llc/fqx023>https://doi.org/10.1093/llc/fqx023</a>.</li><li id=fan2008>Fan, R.–E., Chang, K.–W., Hsieh, C.–J., Wang, X.–R., and Lin, C.–J. “Liblinear: a Library for Large Linear Classification.” _Journal of Machine Learning Research_ , 9 (2008): 1871–74.</li><li id=gabay2021>Gabay, S. “Beyond Idiolectometry? On Racine’s Stylometric Signature” , _Computational Humanities Research Conference_ , November 17–19, 2021, Amsterdam, The Netherlands (2021): 359-76.<a href=http://ceur-ws.org/Vol-2989/long_paper39.pdf>http://ceur-ws.org/Vol-2989/long_paper39.pdf</a></li><li id=gorman2016>Gorman, R. and Gorman, V. “Approaching Questions of Text Reuse in Ancient Greek Using Computational Syntactic Stylometry” , _Open Linguistics_ , 2 (2016): 500–10.</li><li id=gorman2020>Gorman, R. “Author Identification of Short Texts Using Dependency Treebanks without Vocabulary.” _Digital Scholarship in the Humanities_ , 35.4 (2020): 812–25.<a href=https://doi.org/10.1093/llc/fqz070>https://doi.org/10.1093/llc/fqz070</a></li><li id=helleputte2017>Helleputte, T., Gramme, P., and Paul, J. _LiblineaR: Linear Predictive Models Based on the Liblinear C/C++ Library._ R package version 2.10–8 (2017).</li><li id=lakretz2020a>Lakretz, Y., Dehaene, S., and King, J-R. “What Limits Our Capacity to Process Nested Long-Range Dependencies in Sentence Comprehension?” _Entropy_ 22.4 (2020a): 446.<a href=doi.org/10.3390/e2204044>doi.org/10.3390/e2204044</a>6.</li><li id=lakretz2020b>Lakretz, Y., Hupkes, D., Vergallito, A., Marelli, M., Baroni, M., and Dehaene, S. “Exploring Processing of Nested Dependencies in Neural-Network Language Models and Humans.” Preprint: arXiv:2006.11098 [cs.CL] (2020b).<a href=https://www.researchgate.net/publication/342352527_Exploring_Processing_of_Nested_Dependencies_in_Neural-Network_Language_Models_and_Humans>https://www.researchgate.net/publication/342352527_Exploring_Processing_of_Nested_Dependencies_in_Neural-Network_Language_Models_and_Humans</a></li><li id=luyckx2008>Luyckx, K. and Daelemans, W. “Authorship Attribution and Verification with Many Authors and Limited Data.” In Donia Scott and Hans Uszkoreit (eds), _Proceedings of the 22nd International Conference on Computational Linguistics_ , Coling (2008), pp. 513–20.<a href="https://dl.acm.org/citation.cfm?id=1599146">https://dl.acm.org/citation.cfm?id=1599146</a>.</li><li id=luyckx2011>Luyckx, K. and Daelemans, W. “The Effect of Author Set Size and Data Size in Authorship Attribution” , _Literary and Linguistic Computing_ , 26.1 (2011): 35–55.</li><li id=marzi2019>Marzi, C., Ferro, M., and Pirrelli, V. “A Processing-Oriented Investigation of Inflectional Complexity” , _Frontiers in Communication_ , 4 (2019). doi:<a href=10.3389/fcomm.2019.00048>10.3389/fcomm.2019.00048</a></li><li id=nivre2015>Nivre, J. “Towards a Universal Grammar for Natural Language Processing.” In A. Gelbukh (ed.), _Computational Linguistics and Intelligent Text Processing._ CICLing 2015. Lecture Notes in Computer Science, 9041 (2015). Springer, Cham. DOI:<a href=10.1007/978-3-319-18111-0_1>10.1007/978-3-319-18111-0_1</a></li><li id=shannon1948>Shannon, C. E. “A Mathematical Theory of Communication” , _The Bell System Technical Journal_ , 27 (1948): 379-423, 623-56.</li><li id=sidorov2012>Sidorov, G., Velasquez, F., Stamatatos, E., Gelbukh, A., and Chanona–Hernández, L. “Syntactic Dependency-Based N-Grams as Classification Features” , _Lecture Notes on Computer Science_ , 7630 (2012): 1–11.</li><li id=simon2007>Simon, R. “Resampling Strategies for Model Assessment and selection.” In W. Dubitzky, M. Granzow, and D. Berrar. (eds.), _Fundamentals of Data Mining in Genomics and Proteomics_ . Springer, Boston (2007), pp. 173–86.</li><li id=stamatatos2009>Stamatatos, E. “A Survey of Modern Authorship Attribution Methods” , _Journal of the American Society for Information Science and Technology_ , 60.3 (2009): 538-556. doi:<a href=10.1002/asi.21001>10.1002/asi.21001</a></li><li id=swain2017>Swain, S., Mishra, G., and Sindhu, C. “Recent Approaches on Authorship Attribution Techniques — An Overview” , _International Conference of Electronics, Communication and Aerospace Technology_ (ICECA), Coimbatore (2017), pp. 557-66. doi:<a href=10.1109/ICECA.2017.8203599>10.1109/ICECA.2017.8203599</a>.</li><li id=varela2018>Varela, P. J., Albonico, M., Justino, E. J. R., and Bortolozzi, F. “A Computational Approach for Authorship Attribution on Multiple Languages,” 2018 _International Joint Conference on Neural Networks_ (IJCNN), Rio de Janeiro (2018), pp. 1-8, doi:<a href=10.1109/IJCNN.2018.8489704>10.1109/IJCNN.2018.8489704</a>.</li><li id=varela2016>Varela, P. J., Justino, E., Britto, A., and Bortolozzi, F. “A Computational Approach for Authorship Attribution of Literary Texts Using Sintatic Features,” 2016 _International Joint Conference on Neural Networks_ (IJCNN), Vancouver, BC (2016), pp. 4835-42. doi:<a href=10.1109/IJCNN.2016.7727835>10.1109/IJCNN.2016.7727835</a>.</li><li id=wijffels2019>Wijffels, J. “udpipe: Tokenization, Parts of Speech Tagging, Lemmatization and Dependency Parsing with the ‘UDPipe’ ‘NLP’ Toolkit” , (2019).<a href="https://CRAN.R-project.org/package=udpip">https://CRAN.R-project.org/package=udpip</a>e</li><li id=williams2010>Williams, P. and Beer, R. “Nonnnegative Decomposition of Multivariate Information.” Preprint: arXiv:1004.2515 [cs.IT] (2010).<a href=https://arxiv.org/pdf/1004.2515.pdf>https://arxiv.org/pdf/1004.2515.pdf</a>.</li></ul><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p><a href=https://www.gutenberg.org/>https://www.gutenberg.org/</a>and<a href=https://computationalstylistics.github.io/resources/>https://computationalstylistics.github.io/resources/</a>. The list of authors and works is given in the appendix.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>Here the metaphordegrees of freedomis drawn not from the statistical technical term but rather from traditional physics. Thus, the phrase is meant to call to mind the number of independent parameters that a mechanical system can have, such as the yaw, pitch, roll, etc. of a ship or airplane. Analogously, as discussed below, a German noun has three independent morphological parameters, gender, number, and case, each with multiple possible values. <em>Ex hypothesi</em> , any combination of these parameters can carry important information about an author’s style, and therefore all “degrees of freedom” should be taken into consideration.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>Varela et al. (2018, 2016) do not explicitly explain how the internal structure of the values in the flexion vector are handled in their approach<a class=footnote-ref href=#varela2018> [varela2018] </a><a class=footnote-ref href=#varela2016>[varela2016] </a>. However, the number of input variables in their analysis (179 reduced to 132) is not consistent with the preservation of all degrees of freedom in their five morpho-syntactic vectors.&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p>Certainly, an algorithm could be designed to recognize the internal structure of a composite variable and calculate the frequency of the components, but this procedure would be a more cumbersome route to the same result as including more granular variables from the beginning.&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5><p><a class=footnote-ref href=#sidorov2012>[sidorov2012] </a>explore what we might callword-levelsyntactic n-grams. The <em>n</em> in their conception represents the number of hierarchically contiguous words included, where hierarchy is determined by dependency and is analogous to linear order in the more familiar form of n-gram.<a class=footnote-ref href=#gorman2016> [gorman2016] </a>explore a similar procedure. In the present study, greater weight is given to intra-word characteristics alongside syntactic sequences between words. The analogy with character n-grams thus seems apt.&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:6><p>From the perspective of information theory,<a class=footnote-ref href=#shannon1948> [shannon1948] </a>has pointed out that entropy is sub-additive, in that the sum of the individual entropies of several variables is greater than the joint entropy of those variables, except in the case where the variables are statistically independent. While calculating the relative contribution to information of two variables is elementary, specifying the contributions of multiple variables is an open question<a class=footnote-ref href=#williams2010> [williams2010] </a>. It may therefore be more prudent to turn linguistic features into a larger number of more basic variables than a smaller number of consolidated inputs.&#160;<a href=#fnref:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:7><p>Note that it follows that one cannot pay attention to the values alone: the value Fem/Perf may be included in the variable set when it gives the values for the Gender and Aspect of a given word’s dependency parent, but the same morphological features may be excluded when they are the values of the target word’s <em>own</em> Gender and Aspect.&#160;<a href=#fnref:7 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:8><p>Because udpipe works sentence by sentence, all annotation must be completed before creating new smallertextsby subsampling since this random selection of course disrupts sentence structure. Thus, for strictly practical reasons, the sampling process is applied to the matrices of token plus annotation rather than to the raw text. This procedure seems to imply no theoretical implications beyond those entailed by the application of the bag-of-words method in general. My thanks to the anonymous reader for raising this question.&#160;<a href=#fnref:8 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:9><p>Exploratory testing revealed that the L-2 option was the quickest and most accurate of those available. L-2 regression (also calledRidge Regression) is also preferable for the way it handles collinearity. L-2 regression distributes the weight that collinear variables will have in a model across those collinear variables. The alternative method, L-1 orLasso Regression,by contrast, arbitrarily singles out one of the collinear variables and assigns it the full weight of those variables. L-2 appears more conducive for the interpretation of the importance of individual variables since L-1 may lead the unwary to dismiss the force of a large number of collinear inputs.&#160;<a href=#fnref:9 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:10><p>Processing becomes slow for text sizes of 100 tokens and fewer, depending on the morphological complexity of the language. The number of times the texts were partitioned was reduced accordingly.&#160;<a href=#fnref:10 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:11><p>We may takerandom chancehere to be equivalent of the reportedno-information rate.The no-information rate in a classification experiment is the occurrence rate of the most frequently appearing class in the data. For example, if texts by Thomas Mann made up 50% of our material, then a model that classified all test segments asMannwould achieve 50% accuracy simplyby chance.As indicated above, to minimize the no-information rate, our approach ensured that the representation of each class in the training set was roughly equal.&#160;<a href=#fnref:11 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:12><p>It should be recognized that the number of variables produced by UD parsing and the selection process outlined above is not a precise measure of a language’s morphological complexity. Some relevant studies identify the complexity of Spanish as greater than that of German<a class=footnote-ref href=#bittner2003> [bittner2003] </a><a class=footnote-ref href=#marzi2019>[marzi2019] </a>.&#160;<a href=#fnref:12 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:13><p><a class=footnote-ref href=#gabay2021>[gabay2021] </a>argues persuasively that stylometry should go beyond analysis of quantitative observations to focus on the exploration of “stylistic features with an interpretative yield.”&#160;<a href=#fnref:13 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:14><p>For example, one method of interpretation is to inactivate one at a time hidden nodes within a neural network model to see if theablationaffects the outcome of interest<a class=footnote-ref href=#lakretz2020a> [lakretz2020a] </a><a class=footnote-ref href=#lakretz2020b>[lakretz2020b] </a>.&#160;<a href=#fnref:14 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:15><p>∆Bur(Text1,Text2) =∑i=1n|zi(Text1) –zi(Text2)|, where <em>z</em> is thez-score: (observation value – population mean) / population standard deviation. As a sum of absolute values, Burrows’s Delta is a species of Manhattan Distance<a class=footnote-ref href=#evert2017> [evert2017] </a>.&#160;<a href=#fnref:15 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:16><p>The top line of eachAccuracycell gives the mean of all classifications for that size; the second line gives the range. For text sizes of 2000-600 tokens, data were partitioned into text segments five time; each such partition was then classified 100 times, with each classification using a different random test and training set. Because of slowing processing speed, the 500-token texts were classified only 50 times for each partitioning of segments.&#160;<a href=#fnref:16 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:17><p>In UD grammar, object does not refer only to a direct object of a transitive verb, but to the second argument of any verb. For example, inshe went to the store <em>store</em> would be considered the object of <em>went</em> on the assumption that, with that verb, an expression of goal is usually mandatory.## Bibliography&#160;<a href=#fnref:17 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div></div></article></main><footer><nav aria-label="footer links"><ul><li><a class=highlight-focus href=/tags/>Tags</a></li><li><a class=highlight-focus href=/about/>About</a></li><li><a class=highlight-focus href=/categories/>Keywords</a></li></ul></nav><div class=icons><a class="license highlight-focus" rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons Attribution No-Derivatives 4.0 International License" src=/img/logos/license.svg width=120 height=42></a></div></footer></body></html>