<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://gohugo.io/" version="0.116.0">Hugo</generator><link href="https://rlskoeser.github.io/dhqwords/vol/13/1/" rel="alternate" type="text/html" title="html"/><link href="https://rlskoeser.github.io/dhqwords/vol/13/1/index.xml" rel="alternate" type="application/rss+xml" title="rss"/><link href="https://rlskoeser.github.io/dhqwords/vol/13/1/atom.xml" rel="self" type="application/atom+xml" title="Atom"/><updated>2023-10-07T18:26:26+00:00</updated><rights>This work is licensed under a Creative Commons Attribution-NoDerivatives 4.0 International License.</rights><id>https://rlskoeser.github.io/dhqwords/vol/13/1/</id><entry><title type="html">Towards 3D Scholarly Editions: The Battle of Mount Street Bridge</title><link href="https://rlskoeser.github.io/dhqwords/vol/13/1/000415/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://rlskoeser.github.io/dhqwords/vol/13/1/000415/</id><author><name>Costas Papadopoulos</name></author><author><name>Susan Schreibman</name></author><published>2019-06-11T00:00:00+00:00</published><updated>2019-06-11T00:00:00+00:00</updated><content type="html"><![CDATA[<h1 id="heading"></h1>
<h2 id="1-introduction">1. INTRODUCTION</h2>
<p>In the last three decades, cultural heritage, history, and archaeology have made use of three-dimensional digital (re)constructions as tools in the process of knowledge production by making complex two-dimensional data more comprehensible and by simulating spatial and temporal aspects of the past that could not be addressed by using conventional methods. Despite the precariousness of working with 3D technologies in which research projects are reliant on software, hardware, operating systems, and the Internet itself that constantly change, we argue here that the benefits of translating physical environments into three-dimensional models - both the process of modelling as well as the models themselves - outweigh the problems caused by the medium.</p>
<p>Contested Memories: The Battle of Mount Street Bridge project focuses on a battle that took place on Wednesday, 26th April 1916 during the week of the Easter uprising in Dublin. This battle, between a small group of Irish rebels and a much larger force that the British high command sent to Dublin to put down the rebellion, was used to investigate to what extent a virtual world can enable alternative forms of research, help in the interpretive process, and assist knowledge production for both general audiences and specialists. More specifically, it explores how 3D (re)constructions can augment and enrich the palette of methodologies that we use to answer traditional historical questions by enhancing our ability to understand and interpret space as well as to map temporal dimensions within that space. In seeking to map both time and space, we encountered a common problem reported in 3D scholarship; that is the depiction of time in a meaningful way that complements the spatial dimension and enhances the perception of the virtual world <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>  <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>  <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>.</p>
<p>This paper will discuss the decision-making process of gathering and interpreting primary sources for the construction of a virtual world, issues of 3D modelling and representation, the challenges that the project faced, and its future directions. It will also problematise the term virtual world by referring to previous and current examples in heritage studies, also proposing a typology based on their use in teaching, research, and dissemination. It will argue that the process of (re)construction is a valuable tool that provides opportunities for experimentation and new insights, enabling specialists to better comprehend the multidimensional and ambiguous character of historical settings and events. Finally, by drawing from the theory and practice of Digital Scholarly Editions, this paper will suggest that 3D (re)construction projects can become knowledge sites in which the models are seen as text, and documentary evidence in the form of apparatus provides an enriched understanding of its content, thus giving credit and value to the products of 3D modelling as scholarship.</p>
<h2 id="2-historical-virtual-worlds-the-challenges-of-the-state-of-the-art">2. HISTORICAL VIRTUAL WORLDS: THE CHALLENGES OF THE STATE-OF-THE-ART</h2>
<p>In more than three decades of 3D heritage visualisation and simulation there have been immense technological advancements in both hardware and software, which have also driven the development of theoretical and methodological approaches, processes, and products; from the first schematic representations of buildings (see <sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup> to photorealistic renderings and predictive simulations of ancient structures (see <sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>); and from spatial analysis (see <sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>) and physics simulations (see <sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>) to interactive virtual worlds utilising online platforms (see <sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>) and game engines (see the projects carried out as part of the Humanities Virtual Worlds Consortium – <a href="http://virtualworlds.etc.ucla.edu/">http://virtualworlds.etc.ucla.edu/</a>).</p>
<p>In the last ten years, there have been several definitions for virtual worlds primarily reflecting Second Life and multiplayer game approaches; from Bell’s (2008)  “synchronous, persistent network of people, represented as avatars, facilitated by networked computers”  to Schroeder’s (2008)  “persistent virtual environments in which people experience others as being there with them – and where they can interact with them” . Girvan (2013) defines a virtual world as a  “persistent, simulated, and immersive environment facilitated by networked computers, providing multiple users with avatars and communication tools with which to act and interact in world and in real time” . Nevelsteen (2017) on the other hand describes a virtual world as a  “simulated environment where MANY agents can virtually interact with each other, act and react to things, phenomena and the environment; agents can be ZERO or MANY human(s), each represented by MANY‖ entities called a virtual self (an avatar), or MANY software agents; all action/reaction/interaction must happen in a real-time shared spatiotemporal nonpausable virtual environment; the environment may consist of many data spaces, but the collection of data spaces should constitute a shared data space, ONE persistent shard” . Trying to map these definitions to historical virtual worlds becomes futile, since the vast majority do not fulfil the necessary conditions to be classified as virtual worlds: avatars, multiple users and in-world interaction, real-time, persistency, communication tools, to name a few. Also, these definitions neglect more recent approaches in which such environments become tools in research and education. For this reason, we might either abandon the term and continue using the more generic terminology that is typically used interchangeably in such contexts, such as 3D visualisations, computer graphic simulations, and digital (re)constructions, or reconceive the definition of virtual worlds so that they can better encompass the historical environments created and their role and value in the process of knowledge production.</p>
<p>An abundance of virtual world projects in archaeology, history, and heritage-related subjects have been created, especially since the 2000s, particularly during the years that Second Life and Open Simulator were at their peak. Both platforms, and especially Open Simulator which is the free and open-source alternative to Second Life, democratised the process of creating virtual worlds as they did not require highly advanced 3D modelling skills. These online environments enabled a novel way of experiencing history/heritage and working in an online environment for pedagogical and research purposes unlike anything previously available <sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup>.</p>
<p>Second Life, as the name suggests, attempts to immerse its users in a world that has its own social and cultural systems, industry, and currency, while  “residents”  in order to build and maintain their second lives have to buy or rent a piece of land on the  “grid” . In the early years of Second Life, many educational institutions and projects purchased mainland regions or islands to build hubs that would enable new ways of online teaching, collaboration, and knowledge creation. However, due to the policy of Linden Labs to discontinue educational pricing, Second Life became unaffordable for many research groups and institutions. As a result, and due to the way that these virtual worlds functioned, e.g. in-world created 3D models and modalities of communication that could not be exported, most of these worlds have now vanished <sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup>. One of the most characteristic examples of this was the Okapi (Open Knowledge and the Public Interest) island built by the University of California Berkeley to create an online collaborative environment based on the excavation of Çatalhöyük, a Neolithic site in Anatolia, Turkey <sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>. Okapi island gained international media exposure and virtual learning awards, and soon became an exemplar of how such virtual worlds could function for educational and research purposes. However, due to the exigencies of the platform, Okapi island ceased to exist in February 2012. Several other projects developed in Second Life, such as the Virtual Rosewood <sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup> and Theatron <sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup> are no longer accessible since the platform’s maintenance costs were prohibitive for the research teams.</p>
<p>Despite the profound benefits of platforms, such as Second Life, it was the advent of Unity Technologies that boosted the production of heritage virtual worlds since they enabled the creation of highly-detailed models, the embedding of features found in game engines, such as real-time physics and lighting effects, while allowing in-browser experiences without the need to download additional software. They also allowed modellers to reuse their content created in specialised computer graphics packages, thus not requiring in-world modelling or the translation/modification of existing models. However, similar to the problems in Second Life, online virtual worlds developed in the game engine, Unity 3D, recently ceased to exist due to the Netscape Plugin Application Programming Interface (NPAPI) that was phased out by browsers in the period 2014-2017. The NPAPI is a technology that allows third party software developers to produce plugins and web extensions that can run software as a layer within html pages on internet browsers (e.g. Oracle’s Java plugin, Adobe Flash, and Apple Quicktime web extensions). Since these plugins became increasingly vulnerable to security flaws, browser manufacturers decided to replace the plugin functionality with HTML5 compliant approaches.</p>
<p>Considering that virtual worlds developed in Second Life or Unity 3D were intended to be available online, it is striking that at the time of writing this paper only a handful are still accessible. Contrary to the video game community that often builds emulators to overcome – even temporarily – problems of access <sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup>, heritage virtual worlds cannot cope with such demands. Many first generation projects are inaccessible and thus it is difficult for new projects to learn from previous work, especially when the experiential three-dimensional approach to space and time is only accessible via conventional, static, two-dimensional images. Therefore, the vast majority of the-state-of-the-art - at least in its original format - is lost. Online and interactive scholarship, even text-based, has always faced technological shifts and exigencies; however, even in such a fragile ecosystem, changes, successes, and failures enable alternative forms of research, inform the interpretive process, and assist knowledge production.</p>
<p>In the following paragraphs, we will refer to previous and current heritage virtual worlds projects dividing them into three broadly defined categories: Virtual Museums; Research Laboratories; and Teaching Environments, taking into account the features of each environment, intended purposes, as well as use by different audiences.<sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup>  Standalone, offline, 3D modelling projects have a much longer history than online environments and could also be categorised under the proposed typology as they were created to fulfil different purposes and for different audiences. However, given that these were not intended to be publicly available and were created to fulfill different goals, we will only refer here to online virtual worlds.<sup id="fnref:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup></p>
<p>Virtual Museums are environments that represent buildings, cityscapes, or landscapes at a moment of their existence or through time, allowing user navigation either using avatars or a first person view. We use the word museum since these environments are experienced in a similar way to museum exhibits; users can move around and observe the artefacts but cannot interact with, manipulate, or modify them. Such virtual worlds include little or no contextual information in-world, whereas contextual and interpretative material may be supplied in the form of accompanying online resources, e.g. website, or conventional publications. By definition, most virtual world projects fall under the Virtual Museums category since the minimum condition that has to be fulfilled is a 3D model accessible online. For example, the Zamani project (<a href="http://www.zamaniproject.org/">http://www.zamaniproject.org/</a>) has recorded highly-detailed and metrically accurate 3D models of Africa’s heritage sites that – among others – can be explored using the Unity Web player. Similarly, the Virtual Rosewood Research project (<a href="http://www.rosewood-heritage.net/">http://www.rosewood-heritage.net/</a>) that focuses on Rosewood, Florida, an African American town destroyed during the 1923 race riot (<sup id="fnref:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup>; also see <sup id="fnref1:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup> for previous reconstructions of Rosewood on Second Life), and Virtual Williamsburg 1776 (<sup id="fnref:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup>; <a href="http://research.history.org/vw1776/">http://research.history.org/vw1776/</a>), have made use of Unity to make these available online. Zamani, Rosewood, and Williamsburg projects are only comprised of 3D models and do not include any supplementary material in-world. Information that contextualises and provides additional information on these projects, including background, decision-making, and 3D modelling, is included on the websites that host the virtual worlds and in the referenced publications.</p>
<p>Research Laboratories on the other hand, do not only enable users, primarily researchers, to experience (re)constructions of and gain knowledge about past places, periods, and events but also provide them with experimentation opportunities by permitting the alteration of variables, thus allowing testing of hypotheses, new approaches to old data and research questions, and the construction of new narratives. These processes stimulate discussion and produce creative responses, thus having a transformative impact on historical sense-making, reasoning, and understanding. Therefore, virtual worlds become synonymous with knowledge production and the research process that leads to their creation, i.e. problematising sources, identifying variables, and justifying solutions, opens a dialogue that can generate new avenues of scholarship unlike traditional spatiotemporal approaches. For example, the Digital Hadrian’s Villa project <sup id="fnref:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup>, whose online version is implemented in Unity (not currently active), allows users to test archaeoastronomical theories, including the alignment of the sun with the tower of Roccabruna, in the summer solstices during Hadrian’s reign to discover celestial arrangements in the night sky as these would have been seen in the past (for a similar example on WebGL – currently active – see the Virtual Meridian of Augustus, <a href="http://cgi.soic.indiana.edu/~vwhl/VirtualMeridian/WebGL/index.html"> http://cgi.soic.indiana.edu/~vwhl/VirtualMeridian/WebGL/index.html</a>; also see <sup id="fnref:20"><a href="#fn:20" class="footnote-ref" role="doc-noteref">20</a></sup>). Similar attempts using the Virtual Reality Modelling Language (VRML) to create interactive visualisations with parameters that can be changed to demonstrate possibilities and variations in digital (re)constructions have been implemented since the early days of computer graphics (see for example <sup id="fnref:21"><a href="#fn:21" class="footnote-ref" role="doc-noteref">21</a></sup>&rsquo;s work on a Roman theatre in Canterbury). This category is the one with the fewest examples since there is no off-the-shelf platform that supports the interactive elements of such worlds as it requires many more resources in comparison to Virtual Museum projects since interactions have to be designed and pre-programmed by specialists with subject expertise and often to be enabled by Artificial Intelligence mechanisms (see for example <sup id="fnref:22"><a href="#fn:22" class="footnote-ref" role="doc-noteref">22</a></sup>).</p>
<p>In the Teaching Environments category, virtual worlds act as fora of communication, discussion, and outreach. It is not only about the virtual worlds themselves but also about how such worlds are used as a basis for teaching different groups and stakeholders, enquiry, synthesis, and critical analysis. Although there are a few cases where virtual classes are held within virtual worlds in which users can collectively attend teaching sessions and interact with each other and the instructor(s), as well as virtual worlds that were adapted to be used as teaching material (see for example <sup id="fnref:23"><a href="#fn:23" class="footnote-ref" role="doc-noteref">23</a></sup>; <sup id="fnref:24"><a href="#fn:24" class="footnote-ref" role="doc-noteref">24</a></sup> on the Crystal Palace Project – VW is inactive: <a href="http://slurl.com/secondlife/Sydenham%20Crystal%20Palace/158/201/23">http://slurl.com/secondlife/Sydenham%20Crystal%20Palace/158/201/23</a>); this category primarily includes virtual worlds built to provide an embodied and sensorial understanding and an immersive experience of a period, culture, or historical event by employing a narrative that guides users in their experience of the virtual world. For example, Oxford’s First World War Second Life project (still active: <a href="http://maps.secondlife.com/secondlife/Frideswide/219/199/646/">http://maps.secondlife.com/secondlife/Frideswide/219/199/646/</a>) that combines areas of the Western Front (1914-18) and digitised poetry from the First World War Poetry Digital Archive (<a href="https://www.oucs.ox.ac.uk/ww1lit/">https://www.oucs.ox.ac.uk/ww1lit/</a>), allows visitors to experience the poetry of the Great War in an evocative and emotive visualisation of the Western Front by providing access to archival material, such as veteran interviews, video clips, readings of poems, and manuscripts. In this category we also include novel approaches to teaching and learning that promote enquiry, teamwork, effective participation, self-management, reflective learning, and creative thinking, such as Okapi Island’s Machinima: The Hunt, a film made by students and faculty exclusively within the virtual world (<a href="https://youtu.be/n86eZOr-9xE">https://youtu.be/n86eZOr-9xE</a>), portraying hunting and burial ceremonies in everyday life at Çatalhöyük.</p>
<p>While the typologies of 3D scholarship outlined above make the models of use to a wider audience, they do not provide an integrated environment that brings to the fore the decision making process, for example by making available the materials collected that informed the research behind or steps taken to create the models so others - beyond the research team - can utilise the knowledge developed during the modelling process. As a result, this scholarship exists in a trifurcated information space; the original models are available to the individual or the team who worked on them, a version of the models - often downgraded - exists electronically (if technology allows), while the materials that informed decisions and the knowledge generated from them is written about in conventional publication formats or never become available beyond the research team. To overcome this, we are proposing a fourth, still relatively nascent typology, that of 3D Scholarly Editions. 3D DSEs are knowledge sites that provide hermeneutic richness <sup id="fnref:25"><a href="#fn:25" class="footnote-ref" role="doc-noteref">25</a></sup> that takes advantage of the interactivity of the medium and enables the communication of the process and results of that scholarship within a single spatiotemporal, immersive, and sensory environment.</p>
<p>The typologies explored here are offered as ways of creating broad categories for 3D (re)presentations in order to begin to develop, not only a shared vocabulary for discussing virtual worlds as knowledge production, but to begin to theorise this type of scholarship, not in terms of best practice but in terms of more consistency with other digital scholarship, including research goals, audiences addressed, methodologies and standards, and features that can be expected from each type of virtual world.</p>
<p>In the research trajectory of the Battle of Mount Street Bridge project, the 3D environment was one of several tools and resources that formed a Research Laboratory (per the typology enumerated above). Analogue and digital resources, spreadsheets and models, along with excursions to the battle site were utilised to build an increasingly nuanced and complex understanding of the battle. Once the article was published that explored our research findings <sup id="fnref:26"><a href="#fn:26" class="footnote-ref" role="doc-noteref">26</a></sup> we became dissatisfied that outside of our project team, the wider community interested in the battle could not access, evaluate, and reuse our decision-making process, nor could it use the 3D environment to posit alternative theories; hence our investigation of a fourth model, that of a Digital Scholarly Edition (DSE).</p>
<h2 id="3-towards-3d-scholarly-editions">3. TOWARDS 3D SCHOLARLY EDITIONS</h2>
<p>3D DSEs are 3D (re)constructions which include robust contextual information, metadata, and paradata either in the form of in-world annotations or supplementary side sources. In both cases the contextual information or annotation can be text or multimodal. Indeed, we would argue that the annotation needs to take advantage of the affordances of the medium to be truly effective in providing access to the creation process, background information about the world being modelled, and alternative versions of the (re)construction. This type draws from the theory and practice of Digital Scholarly Editing (DSE) which derives from a long history of practice in editing texts for print (See <sup id="fnref:27"><a href="#fn:27" class="footnote-ref" role="doc-noteref">27</a></sup>; <sup id="fnref:28"><a href="#fn:28" class="footnote-ref" role="doc-noteref">28</a></sup>; <sup id="fnref:29"><a href="#fn:29" class="footnote-ref" role="doc-noteref">29</a></sup>; <sup id="fnref:30"><a href="#fn:30" class="footnote-ref" role="doc-noteref">30</a></sup>; <sup id="fnref:31"><a href="#fn:31" class="footnote-ref" role="doc-noteref">31</a></sup>).</p>
<p>The digital has provided textual scholars with a wide palette with which to remediate, not only the textual record (from grave inscriptions to manuscripts of modernist texts to multiple editions of a work in print), but increasingly other mediums of knowledge transmission (e.g. images, audio recordings, maps), recording, (re)creating, and describing a wide variety of linguistic and non-linguistic features (verbal, visual, oral, and numeric). Textual scholarship includes not only the transmission of texts, but the social processes of their transmission <sup id="fnref:32"><a href="#fn:32" class="footnote-ref" role="doc-noteref">32</a></sup>. Thus a 3D scholarly edition should not be thought of as a defined object, but a methodological field in which a set of codes, not only the technological codes that govern the creation of the world, but the social, theoretical, and historical codes that its makers adopt in its creation, impose a prefiguring frame on the reality being created <sup id="fnref:33"><a href="#fn:33" class="footnote-ref" role="doc-noteref">33</a></sup>.</p>
<p>The construction of such an edition entails building an intertextual network composed of the primary text (in this case the 3D model) along with its accompanying annotation and apparatus providing a base from which the reader can actively engage in the knowledge creation process. Embedding the iconography of virtual worlds into what we might broadly describe as scholarly editing practice, opens up new vistas for scholarship and communicating the results of that scholarship within spatiotemporal environments that are immersive and multisensorial <sup id="fnref:34"><a href="#fn:34" class="footnote-ref" role="doc-noteref">34</a></sup>. If the goal of the modelled world is to create its own ecosystem to provoke and encourage evolving thought about the material, aesthetic, and cultures of the real-world events it simulates <sup id="fnref1:29"><a href="#fn:29" class="footnote-ref" role="doc-noteref">29</a></sup>, at the heart of this ecosystem is the text, the modelled world.</p>
<p>All edited texts are, at their core, dialogues <sup id="fnref1:31"><a href="#fn:31" class="footnote-ref" role="doc-noteref">31</a></sup>. Dialogues between the text and the editor and between the edited text and its readers. Scholarly editions are fundamentally, fabrications. They (re)present a work (or in the case of 3D, a world) as it never existed historically,<sup id="fnref:35"><a href="#fn:35" class="footnote-ref" role="doc-noteref">35</a></sup>  but in its (re)presentation, providing greater access to readers, not only to the textual record, but to the social, historical, and economic factors which led to its creation and subsequent use. This surrounding material, referred to as apparatus is annotative, providing critical, textual, and biographical notes. This can be seen in an online edition such as The Chymistry of Isaac Newton (<sup id="fnref:36"><a href="#fn:36" class="footnote-ref" role="doc-noteref">36</a></sup>; <a href="http://webapp1.dlib.indiana.edu/newton/">http://webapp1.dlib.indiana.edu/newton/</a>) that provides both diplomatic and normalised (modernised) transcriptions, alongside online tools, such as glossaries, indexes, and a guide to the symbols Newton used, so that the reader can better interpret the text. Depending on the theory with which the editor produces the text (typically described in the introductory material), the annotation, as well as edited text itself, follows standard editorial practices.<sup id="fnref:37"><a href="#fn:37" class="footnote-ref" role="doc-noteref">37</a></sup></p>
<p>Digital Scholarly Editions (as opposed to editions edited for print) have opened up new modalities for annotation to include, in addition to text, audio, video, images, and data linked from other sources. An example of this is The Walt Whitman Archive (<sup id="fnref:38"><a href="#fn:38" class="footnote-ref" role="doc-noteref">38</a></sup>; <a href="http://whitmanarchive.org/">http://whitmanarchive.org/</a>) that publishes Whitman’s published work alongside manuscripts (in both image and transcription formats), coupled with commentary, translations, audio recordings, and bibliographic information. In the case of 3D (re)constructions, annotation might include in-world sampling of sound in different sections of a renaissance church based on the position of the user in the virtual world, mechanisms to present alternative structural models according to written evidence and parallel sources, or a video in which a person describes some aspect of the text. We are calling these features annotations because, unlike the testing amongst multiple variables that users can undertake in Research Laboratories, their goal is to explicate as opposed to test. For example, the 3D (re)constructions may offer one version of a building; however, evidence that supports alternative versions of certain architectural features may be represented by other models accessible in-world through a pop-up box or by replacing the current version of a feature with other possible versions; areas of uncertainty may be rendered in different colours and shading to indicate hypotheses, sources, and surviving evidence; or, ambiguous features may be toggled on and off or replaced by alternative versions, also indicating how other elements will be affected by these changes (e.g. a larger door opening may indicate a lighter roof structure). Although such approaches in the creation of digital (re)constructions, especially in archaeological contexts, have been presented since the early years of the application of computer graphics in the field (see for example <sup id="fnref1:21"><a href="#fn:21" class="footnote-ref" role="doc-noteref">21</a></sup>; <sup id="fnref:39"><a href="#fn:39" class="footnote-ref" role="doc-noteref">39</a></sup>; <sup id="fnref:40"><a href="#fn:40" class="footnote-ref" role="doc-noteref">40</a></sup>; <sup id="fnref:41"><a href="#fn:41" class="footnote-ref" role="doc-noteref">41</a></sup>; <sup id="fnref:42"><a href="#fn:42" class="footnote-ref" role="doc-noteref">42</a></sup>; <sup id="fnref:43"><a href="#fn:43" class="footnote-ref" role="doc-noteref">43</a></sup>; <sup id="fnref:44"><a href="#fn:44" class="footnote-ref" role="doc-noteref">44</a></sup>; <sup id="fnref:45"><a href="#fn:45" class="footnote-ref" role="doc-noteref">45</a></sup>; <sup id="fnref:46"><a href="#fn:46" class="footnote-ref" role="doc-noteref">46</a></sup>), we believe that they haven’t been adequately supported by theoretical and methodological frameworks. In this paper we argue that following the paradigm of digital scholarly editions, the processes and results of digital (re)constructions for heritage datasets can be further informed, thus creating a more robust framework for considering 3D models and modelling as scholarship (also see <sup id="fnref1:34"><a href="#fn:34" class="footnote-ref" role="doc-noteref">34</a></sup>).</p>
<p>Several virtual world projects have followed the paradigm of what we call 3D Scholarly Editions, providing contextual information, including metadata and paradata, in the form of in-world textual and multimedia annotations, and/or supplementary side sources. For example, the Virtual Middletown Living Museum Project (<a href="http://idialab.org/virtual-middletown-living-museum-in-blue-mars/">http://idialab.org/virtual-middletown-living-museum-in-blue-mars/</a>) currently under development by IDIA Lab at Ball State University, which explores life in Muncie, Indiana in 1920-30s based on the seminal Middletown Studies by Robert and Helen Lynd, includes for the virtual world of the Ball Glass Factory, in-world interactive multimedia annotations to re-enact and inform visitors about the working life in a factory of the period.<sup id="fnref:47"><a href="#fn:47" class="footnote-ref" role="doc-noteref">47</a></sup>  Similarly, the Social Justice History Platform (originally built in Unity 3D<sup id="fnref:48"><a href="#fn:48" class="footnote-ref" role="doc-noteref">48</a></sup>  as part of The Soweto Historical GIS Project; <a href="http://www.dhinitiative.org/projects/shgis">http://www.dhinitiative.org/projects/shgis</a>) brings together spatial, temporal, and geographic data, archival material, and multimedia for Soweto under the South African apartheid regimes. Lastly, we should highlight VSim, the only off-the-shelf platform to date that was developed in an attempt to respond to the criticisms about the transparency of 3D modelling as a process and the concerns over the establishment of 3D as an accepted modality of scholarship by embedding annotations and links to sources, crafting narratives, and building arguments within the 3D models <sup id="fnref:49"><a href="#fn:49" class="footnote-ref" role="doc-noteref">49</a></sup>  <sup id="fnref:50"><a href="#fn:50" class="footnote-ref" role="doc-noteref">50</a></sup>. Some of the features supported in VSim are similar to those we describe for 3D Editions (e.g. textual annotations).</p>
<p>However, we believe that 3D scholarship could afford more dynamic and interactive annotative features that go beyond conventional textual and multimedia paradigms as outlined above.<sup id="fnref:51"><a href="#fn:51" class="footnote-ref" role="doc-noteref">51</a></sup>  Contested Memories: The Battle of Mount Street Bridge Virtual World (<a href="https://mountstreet1916.ie/">https://mountstreet1916.ie/</a>) has been experimenting with such an environment.</p>
<h2 id="4-contested-memories-an-introduction-to-the-battle-of-mount-street-bridge-project">4. CONTESTED MEMORIES: AN INTRODUCTION TO THE BATTLE OF MOUNT STREET BRIDGE PROJECT</h2>
<p>Contested Memories: The Battle of Mount Street Bridge (BMSB) project began in 2013 as part of the Humanities Virtual World Consortium (HVWC) (<a href="http://virtualworlds.etc.ucla.edu/"> http://virtualworlds.etc.ucla.edu/</a>), funded by the Andrew W. Mellon Foundation. The HVWC planning grant was funded in 2010 at what was arguably the height of academic interest in virtual worlds. Despite the excitement of the affordances of the technology, there was a growing awareness that for virtual worlds to flourish in heritage and academic settings, an alternative platform was necessary that allowed institutions greater control over their assets along with a security of tenure. As can be seen from section two, the final blow to many of the projects constructed in Second Life was the 2012 pricing structure which caused the vast majority of projects to abandon their assets and their lands.</p>
<p>The HVWC sought to intervene by exploring how online and interactive virtual worlds that provide tools and methods to approach space and time in three dimensions can enable collaborative networked approaches to cultural heritage, transform scholarly communication, evoke sensorial experience, and advance research practices in the humanities. These goals were in line with the definitions of virtual worlds referenced above by Bell, Schroeder, Girvan, and Nevelsteen, with the added ambition of advancing research practice in the humanities to provide a broader palette for researchers to ask and answer research questions in which the phenomenology of time and space could not be addressed using conventional (including more traditional digital) methods. In order to fulfil the aims of the Consortium, four projects were developed that covered a range of disciplines, including ancient and modern history, architecture, archaeology, and Tibetan studies, and a time frame from the Roman period to the early 20th century. The other projects of HVWC included RomeLab, led by Chris Johanson (UCLA); Lhasa, led by David Germano, Kurtis Schaeffer, and Tsering Gyalpo (University of Virginia); and, Hadrian’s Villa, led by Bernard Frischer (University of Indiana and John Fillwalk in the IDIA Lab of Ball State University). At the time, Unity 3D was selected as the platform to host the four virtual worlds. Although Unity 3D is a closed-source game development engine, it supported, contrary to other platforms evaluated in the planning stage of the grant (Second Life and OpenSimulator), highly detailed models and most features found in game engines, including real-time physics, lighting effects, and the ability to modify the Graphic User Interface. Unity would also allow tailoring the front-end to accommodate the four diverse projects and would support networked research and learning. Most of all, using a platform that seemed to emerge as the standard for virtual worlds would allow the partners to focus on scholarly content rather than dealing with technical challenges.</p>
<h2 id="5-the-battle-of-mount-street-bridge-a-brief-historical-background">5. THE BATTLE OF MOUNT STREET BRIDGE: A BRIEF HISTORICAL BACKGROUND</h2>
<p>The Easter Rising of 1916 is considered the single most important event in the fight for Irish independence from Great Britain. It began on Easter Monday, 24th April, with Patrick Pearse, the leader of the Rising, declaring an Irish Republic on the steps of the General Post Office in Dublin&rsquo;s city centre. Within minutes, the British were telegraphed about the insurrection and the following day began mobilising troops who were training in England for the open fields of the Western Front. In less than a week the rebels had been defeated and rounded up. By 12th May, 14 leaders had been executed. Although the Rising might be considered a failure militarily, it set the wheels in motion for Irish independence which was achieved in 1921.</p>
<p>The Irish (known as the Volunteers) took a number of key locations around Dublin on Monday afternoon, forming, in effect, a perimeter ring around the city centre, blocking the major routes of entry. One of the most important of these locations was to the south where the port of Kingstown (now Dún Laoghaire) is located. The 3rd Battalion of the Irish Volunteers took over Boland’s Mill on Grand Canal Dock on one of the southern routes into the city centre under the command of Éamon De Valera. It was from this location that the command of south inner city was to take place. A small detachment – 14 men – under the command of Lieutenant Michael Malone – occupied four buildings to the west of Boland Mills: 25 Northumberland Road, St. Stephen’s Schoolhouse, St. Stephen’s Parochial Hall, and Clanwilliam House in and around Northumberland Road. In effect, this blocked the coast road route to the city centre via Mount Street Bridge. Three additional men under the direct command of de Valera, were stationed on the rooftop of Robert’s Builders yard (<a href="#figure01">Figure 1</a>).</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/13/1/000415/resources/images/Figure1.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/13/1/000415/resources/images/Figure1_huba6643ac20183c87b2c6af75bf700cc8_6719525_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/13/1/000415/resources/images/Figure1_huba6643ac20183c87b2c6af75bf700cc8_6719525_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/13/1/000415/resources/images/Figure1_huba6643ac20183c87b2c6af75bf700cc8_6719525_1200x0_resize_q75_box.jpg 1200w,/dhqwords/vol/13/1/000415/resources/images/Figure1_huba6643ac20183c87b2c6af75bf700cc8_6719525_1500x0_resize_q75_box.jpg 1500w,/dhqwords/vol/13/1/000415/resources/images/Figure1_huba6643ac20183c87b2c6af75bf700cc8_6719525_1800x0_resize_q75_box.jpg 1800w,/dhqwords/vol/13/1/000415/resources/images/Figure1.jpg 10507w" 
     class="landscape"
     ><figcaption>
        <p>A schematic map of the battlefield and the five building occupied by 17 Irish Volunteers. The British troops arrived from the south (from the direction of Dun Laoghaire) and first came under gunfire from 25 Northumberland Road.
        </p>
    </figcaption>
</figure>
<p>On Monday afternoon British authorities began mobilising troops in England to reinforce the forces already in Ireland, many of them in training. On Tuesday evening, two battalions — the 2/7th and 2/8th — popularly known as the Sherwood Foresters sailed on a night boat from Liverpool to Kingstown. On Wednesday morning around 11.00 a.m. 26 April, some 1750 men marched up the coast road towards the city centre. At around 12.30 p.m. the forward company, Company C of the 2/7th met stiff resistance from 25 Northumberland Road where Malone and James Grace opened fire into the oncoming troops as they approached the junction. The first casualties included Captain Frederick Christian Dietrichsen and 2nd Lieutenant William Victor Hawken as they were easily distinguished from the other ranks as only officers carried pistols (regular troops were issued with rifles).</p>
<p>The battle raged well into the evening, with detachments of British troops making their way North on Northumberland Road, being met with gunfire, first from the four men in Parochial Hall, then shortly afterwards by fire from the eight Volunteers in Clanwilliam House. Other detachments made left and right flanking actions attempting to find alternative routes by which to take Mount Street Bridge. Troops that took the right flank found themselves under fire as they approached the canal from Robert’s Builders Yards, and those that approached from the left were fired upon by the Volunteers in Clanwilliam House (<a href="#figure02">Figure 2</a>). Eventually, the sheer numbers of the British troops, coupled with their superior firepower, allowed them to take the occupied buildings: 25 Northumberland Road fell first, with Malone killed and Grace escaping from the back of the house; the men in Parochial Hall ran out of ammo and escaped into the back garden; the British spent significant time in taking the School House, only to find it empty (it had been abandoned by the Volunteers Tuesday as being a poor position); Clanwilliam House was the last of the Volunteer posts to fall following a concerted rush by troops from the 2/8th and 2/7th Sherwood Foresters during which the house caught fire after successive bombing by the British: three of the eight men inside died (see <sup id="fnref1:26"><a href="#fn:26" class="footnote-ref" role="doc-noteref">26</a></sup> for a more thorough description of the battle).</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/13/1/000415/resources/images/Figure2.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/13/1/000415/resources/images/Figure2_hu155c236337affe31ed9d04a67ad0a33b_660662_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/13/1/000415/resources/images/Figure2_hu155c236337affe31ed9d04a67ad0a33b_660662_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/13/1/000415/resources/images/Figure2_hu155c236337affe31ed9d04a67ad0a33b_660662_1200x0_resize_q75_box.jpg 1200w,/dhqwords/vol/13/1/000415/resources/images/Figure2.jpg 1403w" 
     class="landscape"
     ><figcaption>
        <p>Aerial View of the 3D modelled Battlefield. Red arrows indicate the routes that the British troops followed.
        </p>
    </figcaption>
</figure>
<p>Militarily and in terms of casualties inflicted, the battle at Mount Street Bridge was the most successful Irish engagement of the Rising and accounted for a significant proportion of British casualties. And although the Rising has been the subject of a vast and growing historiography, there remained a number of significant questions surrounding this battle. Among the most contested is the extent of the casualties suffered by the British; the rebel casualties are clear – four of the volunteers were killed. Most commonly, historians have cited the figure produced for British casualties provided by General Sir John Maxwell (appointed general officer commanding in chief the forces in Ireland after the outbreak of the Rising) in a report compiled shortly after the event in May 1916: 234 casualties. 4 officers were killed, 14 wounded, and of Other Ranks 216 were killed and wounded (The National Archives, WO 32/9523 Maxwell to French, 25 Apr. 1916). These figures are usually reproduced uncritically but are problematic. For one, they contrast significantly with other British sources produced at the time and afterwards that offer casualty figures ranging from 155 to 196 (see <sup id="fnref2:26"><a href="#fn:26" class="footnote-ref" role="doc-noteref">26</a></sup>).</p>
<p>Given the potential range of figures, and the almost propagandistic nature of some of the writing on the battle itself, it was felt that this event lent itself to the types of methods outlined in this paper to create a more realistic narrative around the events on Mount Street. Moreover, this battle is of additional interest as it is one of the first that widely documents fighting in a built-up area; thus this research can demonstrate alternative ways for analysing battles using 3D technologies where extensive, albeit contradictory documentary evidence exists. Since at the time, the British Army only received training for field fighting in open environments (such as the one on the Western Front), it is important as an early example of urban warfare in understanding how a small number of strategically placed combatants, even those with a minimum of military training, as the Irish had, were able to engage and hold off a far superior force over the course of the day.</p>
<h2 id="6-modelling-the-virtual-world">6. MODELLING THE VIRTUAL WORLD</h2>
<h2 id="61-spatial-representations-of-battles-from-analogue-to-digital">6.1 Spatial Representations of Battles: From Analogue to Digital</h2>
<p>There is a long history of games and simulations invoking dynamic simulations of the physical world used in the training of the military, particularly in terms of strategic training for high ranking officers going back to the Roman Empire <sup id="fnref:52"><a href="#fn:52" class="footnote-ref" role="doc-noteref">52</a></sup>. Indeed, the game of chess is    “one of the most enduring expression of a battle game where two equal opposing forces meet on a board”   <sup id="fnref:53"><a href="#fn:53" class="footnote-ref" role="doc-noteref">53</a></sup>  . While chess is an example of an abstract game of strategy, in the late eighteenth century, Johann Christian Ludwig Hellwig developed a more representational war game which incorporated details of the terrain types (with each square representing two-thousand paces across) and reclassified chess pieces into branches of the military. Although Hellwig’s games were used by his contemporaries to fight both imaginary and past battles, ultimately, Hellwig realised that there existed an inverse relationship between introducing increasingly more realistic features into game play in an effort to represent what actually happens on the battlefield and the playability of the game. Hellwig’s insight, the trade-off between realism and playability, is still one of the fundamental challenges of wargame design <sup id="fnref1:53"><a href="#fn:53" class="footnote-ref" role="doc-noteref">53</a></sup>. Hence the next generation of war games began to introduce more abstract representations: dispensing with board and chess-like statuettes representing soldiers, replacing them with  “small, nondescript wooden blocks”  designed to    “‘occupy the exact dimension that troop formations would on the terrain scale’”   <sup id="fnref2:53"><a href="#fn:53" class="footnote-ref" role="doc-noteref">53</a></sup>  .</p>
<p>Concerns of time and space, and the trade-offs herein, still occupy the minds of the designers of war games or simulations created for pleasure, as a means to understand the past, or for training purposes (although these goals are not incompatible). In particular, in the case of representing historical events, there exists a tension between what actually occurred and the impossibility of representing a complex, multidimensional event with hundreds or thousands of actors, each making decisions, the vast majority of them lost to time. Nakamura argues that games that attempt to create simulations of past events can be considered  “correct”  if they present a    “‘reasonable image of the created world’”   <sup id="fnref:54"><a href="#fn:54" class="footnote-ref" role="doc-noteref">54</a></sup>  , embedding a narrative and internal consistency that somebody familiar with the event would recognise. Jettisoning what the designers of the simulation consider extraneous to the narrative is not unlike the choices film directors make when (re)constructing historical events to create a real-time aesthetic. Both film (through narration) and simulations (through interactivity) remediate events and hence the temporal engagement of their viewers/users in the construction of historical time <sup id="fnref:55"><a href="#fn:55" class="footnote-ref" role="doc-noteref">55</a></sup>, albeit through the cultural, political, and social lens of the time in which it was created <sup id="fnref:56"><a href="#fn:56" class="footnote-ref" role="doc-noteref">56</a></sup>.</p>
<p>There are, however, crucial differences in utilising virtual world technologies in the construction of war games in which the simulation typically allows outcomes that are inaccurate historically (see <sup id="fnref1:55"><a href="#fn:55" class="footnote-ref" role="doc-noteref">55</a></sup> and following; <sup id="fnref1:54"><a href="#fn:54" class="footnote-ref" role="doc-noteref">54</a></sup>, and <sup id="fnref:57"><a href="#fn:57" class="footnote-ref" role="doc-noteref">57</a></sup>), and the use of these methods for academic purposes in which a fidelity to historical accuracy is paramount. For example, war games may privilege users’ immersion in the virtual environment over accuracy.<sup id="fnref:58"><a href="#fn:58" class="footnote-ref" role="doc-noteref">58</a></sup>  They may also lack contextual evidence, again, in the interest of immersivity, although many war games do provide sources from which users make strategic decisions.<sup id="fnref:59"><a href="#fn:59" class="footnote-ref" role="doc-noteref">59</a></sup>  Having documentary sources, or annotation, embedded in-world, however, is an inherent feature of 3D Scholarly Editions. Here, the goal is not to have users suspend disbelief through an immersive environment, but to provide a contextualised environment to better understand, and hence draw their own conclusions, about the text, in this case, the created world.</p>
<p>Another key difference between modelling worlds for gaming as opposed to academic purposes is what might be typified as an absence of interaction between the user and the virtual world (e.g. the user is not able to shoot), be among simultaneous users, and the fact that the first-person perspective is not avatar-based. In other words, users cannot see their representation in the world. Computer games typically implement either a first-person avatar-based perspective through which players can see part of the avatar’s body, often including hands or a weapon, or a third-person perspective in which the player can see the body of the avatar (for an overview of avatar representation in cultural heritage applications see <sup id="fnref:60"><a href="#fn:60" class="footnote-ref" role="doc-noteref">60</a></sup>).</p>
<h2 id="62-modelling-the-ambiguity-of-primary-data">6.2 Modelling the Ambiguity of Primary Data</h2>
<p>Because of the constraints of modelling the complexity of real-world events as outlined above, the project sought to implement a    “reasonable image of the created world”   <sup id="fnref2:54"><a href="#fn:54" class="footnote-ref" role="doc-noteref">54</a></sup>  . To do this, we employed a wide range of research methods: conventional archival research and meetings with military historians were carried out to document different sources that provide evidence for the buildings that were occupied, participant accounts, and the accuracy of the weapons used. Since period guns were held by our collaborators, we were able to undertake controlled experiments, using shooters of various skill levels, which would have mirrored the situation of the Irish Volunteers. Period photographs and visits to the battlefield, now a peaceful leafy suburban street in Dublin, also enhanced our spatial conception of the event.</p>
<p>Due to the difficulties in mapping time in 3D representations, including technological constraints and the ambiguity of the sources, many virtual worlds are either atemporal, thus failing to depict the passing of time and how this affects spaces or events, or condense time, for example, by showing changes that occur over a period (from hours to years or even centuries), into short timeframes. Despite the amount of information gathered for the project from a wide range of contemporary and later sources, not surprisingly, the temporal dimension of the battle was, and remains, the most elusive. Although sources give some indication about the sequence of events (e.g. one house was attacked after another), they are rarely clear about the time that a particular event took place, and indeed, are frequently contradictory. Although the release into the public domain of archival material during the project period, including the Bureau of Military History (<a href="http://www.bureauofmilitaryhistory.ie/">http://www.bureauofmilitaryhistory.ie/</a>) and Military Pension records (<a href="http://www.militaryarchives.ie/collections/online-collections/military-service-pensions-collection/search-the-collection">http://www.militaryarchives.ie/collections/online-collections/military-service-pensions-collection/search-the-collection </a>), provided a wealth of information from the Irish perspective, this often contradicted (rather than confirmed) the British regimental histories written closer in time to the event<sup id="fnref:61"><a href="#fn:61" class="footnote-ref" role="doc-noteref">61</a></sup>   <sup id="fnref:62"><a href="#fn:62" class="footnote-ref" role="doc-noteref">62</a></sup>  <sup id="fnref:63"><a href="#fn:63" class="footnote-ref" role="doc-noteref">63</a></sup>. This is because the surviving testimonies carry biases and distortions, sometimes because of the time lag between the event and the recollection (the witness statements of the Irish volunteers were collected some 30 years after the event), other times due to the nature and purpose of the source itself, for example the British Battalion histories, the main sources for the British accounts. These accounts, written by the Officers of the Battalions (published in 1920 and 1921 for the 2/8th and 2/7th respectively) were far more circumspect in providing certain details, which is not surprising given the substantial casualties inflicted on the troops. Therefore, a significant aspect of the research was devoted to evaluating different accounts and problematising their reliability, acknowledging that despite the number of sources collated, a detailed timeline of the battle remains problematic.<sup id="fnref:64"><a href="#fn:64" class="footnote-ref" role="doc-noteref">64</a></sup></p>
<h2 id="63-designing-and-modelling-the-virtual-world">6.3 Designing and Modelling the Virtual World</h2>
<p>The project experimented with a number of representations of the world, from more schematic representations (e.g. omitting features such as textures on the buildings, road features, and natural light, <a href="#figure03">Figure 3</a>) to more detailed models that more closely mirrored the built and natural environment (<a href="#figure04">Figure 4</a>). Ultimately, the latter representation was chosen, due in some measure, to a feedback session with a group of military historians fairly early in the development process (see <a href="#figure03">Figure 3</a>) in which it was clear that the absence of realism (e.g. buildings, streets, foliage) as well as more schematic representations of soldiers, did not provide enough context for a primary audience of the research to utilise it.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/13/1/000415/resources/images/Figure3.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/13/1/000415/resources/images/Figure3_hu697f842107abc80af42ce62e78b53fb8_279853_500x0_resize_box_3.png 500w,
    /dhqwords/vol/13/1/000415/resources/images/Figure3_hu697f842107abc80af42ce62e78b53fb8_279853_800x0_resize_box_3.png 800w,/dhqwords/vol/13/1/000415/resources/images/Figure3_hu697f842107abc80af42ce62e78b53fb8_279853_1200x0_resize_box_3.png 1200w,/dhqwords/vol/13/1/000415/resources/images/Figure3.png 1380w" 
     class="landscape"
     ><figcaption>
        <p>Early prototype of the Battle of Mount Street Bridge in which buildings were schematically constructed and users were able to define the accuracy of the volunteers based on the guns used, i.e., rate of fire, reload rate, and accuracy hitting a target (courtesy of Joshua Savage).
        </p>
    </figcaption>
</figure>
<p>Feedback from the group also strengthened our resolve to have users enter the world disembodied. This decision was taken for several reasons. We had previously decided not to allow users adopt one of the personas engaged in the battle (either one of the named people, such as a Volunteer or an unnamed British soldier) because of the historiography of Irish participation in the Great War. We did not want to encourage users to ‘take sides’ by choosing an avatar, hence affiliating oneself with the British or the Irish. While online war games, even those based on real battles, allow hundreds of players making thousands of decisions, these games loosely follow the battle trajectory. On the other hand, our goal in modelling the battle was to better understand the trajectory of the battle and what conditions afforded the Volunteers such an advantage. We had no wish to allow users to ‘relive’ the battle, nor did we want to glorify it. We also did not want additional avatars in the scene for simultaneous users, even if dressed appropriately for the time, as this would have taken away from the historicity of the created world. The goal was not to allow users to play out a specific battle using Real Time Strategy (RTS) techniques or to provide an immersive experience utilising a First Person Shooter (FPS) perspective, but to use the 3D environment as a way to map the documentary sources onto a 3D plane, emphasising the dialectical and the tactical, allowing simultaneous temporal aspects of the battle to be better understood in a way that simple 2D mapping cannot.</p>
<p>Contrary to many digital (re)constructions of ancient spaces in which architecture is a fundamental research question that helps understand the relationship between people, artefacts, and movement, the area in which the Battle of Mount Street Bridge took place is more or less the same today as in 1916. Although there are buildings that were destroyed during the course of the battle and replaced by modern office blocks (such as Clanwilliam House and Robert’s Builders Yard) the rest of the buildings and the layout of the streets are virtually the same (the main difference being the trees on the street are 100 years older).</p>




























<figure ><img loading="lazy" alt="" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Photorealistic rendering of the battlefield in Autodesk 3dsMax including schematic representations of buildings in the adjacent streets.
        </p>
    </figcaption>
</figure>
<p>Unlike the challenges that many wargame designers have faced in terms of designing large battlefields that need to be compressed into a playing field, the Battle of Mount Street Bridge took place in a constrained environment with the majority of action taking place on one city block (Northumberland Road between the Grand Canal and Haddington Road), with forays down adjoining streets around the backs of buildings (<a href="#figure02">Figure 2</a>). Moreover, there exists fairly detailed cross-referenced sources about how the battle was waged spatially, which provided the BMSB team with quite accurate means for the (re)construction.</p>
<p>In order to develop the model a laser scan survey was conducted by Discovery Programme Centre for Archaeological Research and Innovation. Northumberland Road, where most of the events took place, was scanned in its entirety producing a highly detailed point cloud. The option of using this detailed point cloud for automatic mesh reconstruction was explored, but the models produced were found to be too noisy and computationally intensive for a real-time visualisation. Therefore, the detailed point cloud was only used as an accurate reference to simplify the modelling of the street from scratch.</p>
<p>Constructing the area that was not covered by the point cloud as well as the buildings and structures that are no longer present was achieved by using Google Earth imagery, the 1911 Ordnance Survey map of Dublin, photographs taken after the Rising (particularly of the burnt-out buildings), and site visits. In the case of features that do not exist, perspective and heights in the photographic records and comparison with existing features, such as lamp posts, were used to model these as accurately as possible. On the other hand, for existing structures of the wider area that were not recorded by the laser survey, the PhotoMatch feature in SketchUp was used to generate simple effective textured photogrammetry models from a range of photographs to provide the urban context of battlefield. Finally, for the buildings that had a crucial role in the battle but there is no photographic evidence, such as the Robert’s Builders Yard, schematic models rendered in grey were created from the 1911 Ordnance Survey map. The modelled scene was given texture coordinates to enable the accurate lighting and texture in the Unity Engine.</p>
<p>Once the 3D model was completed in 3dsMax, it was exported as an FBX and imported into Unity 3D Version 5.0 as an Asset to enable an in-browser 3D world for users to explore (Figure 5). The real-time interactivity that the platform provides with the free-roaming, user-directed camera contrary to static renderings and predefined animations allows users to explore the space at their own pace without predetermined paths, views, and orientations, thus enabling new discoveries that the modeller or the researchers involved in the decision-making process may not have considered. The camera views in Unity were based on the functionalities developed by the HVWC: Bird’s-eye view (from above); near-field view based on movement with WSAD keys; and third-view in which the camera is orbited using the mouse. Users can also  “Fly”  to observe the scene from different heights.</p>




























<figure ><img loading="lazy" alt="" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>The Battlefield in Unity 3D Webplayer
        </p>
    </figcaption>
</figure>
<p>The model for the project was developed to be viewed using the Unity Web Player, an NPAPI-based plugin. However, by the end of the first phase of the project in 2016, NPAPI, and consequently the Unity Web Player, was no longer supported by browsers. It was only Firefox that could run BMSB until Spring 2017, which also stopped supporting NPAPI later in the year.<sup id="fnref:65"><a href="#fn:65" class="footnote-ref" role="doc-noteref">65</a></sup>  For this reason, the BMSB, similarly to other Unity-based projects, stopped being accessible via a browser. Since the project was committed to providing a publicly-available, online version of our research, the model had to be optimised to run in Unity WebGL, which uses HTML5 technologies and the WebGL rendering API to run Unity content in a browser (<a href="#figure06">Figure 6</a>). Due to the geometric and textural complexity of the original models, it was necessary to simplify them and find ways to reduce the complexity of the scene, e.g. by reducing the buildings in the background to geometry  “instances”  that have a much lower memory footprint since they only consist of a reference to the original vertices and textures.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/13/1/000415/resources/images/Figure6.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/13/1/000415/resources/images/Figure6_hu9a73fa23601d9e1873be1a49ad3213de_2306283_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/13/1/000415/resources/images/Figure6_hu9a73fa23601d9e1873be1a49ad3213de_2306283_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/13/1/000415/resources/images/Figure6_hu9a73fa23601d9e1873be1a49ad3213de_2306283_1200x0_resize_q75_box.jpg 1200w,/dhqwords/vol/13/1/000415/resources/images/Figure6_hu9a73fa23601d9e1873be1a49ad3213de_2306283_1500x0_resize_q75_box.jpg 1500w,/dhqwords/vol/13/1/000415/resources/images/Figure6_hu9a73fa23601d9e1873be1a49ad3213de_2306283_1800x0_resize_q75_box.jpg 1800w,/dhqwords/vol/13/1/000415/resources/images/Figure6.jpg 3400w" 
     class="landscape"
     ><figcaption>
        <p>The 3D model of the Battlefield optimized for WebGL
        </p>
    </figcaption>
</figure>
<h2 id="64-implementing-a-3d-scholarly-edition">6.4 Implementing a 3D Scholarly Edition</h2>
<p>The version of Contested Memories: The Battle of Mount Street Bridge that we have been writing about here can be classified under the Virtual Museums category since it is comprised of a digital (re)construction of the battlefield with limited contextual information (e.g. buildings, people, and weapons) included in the website that hosted the virtual world (<a href="https://mountstreet1916.ie/">https://mountstreet1916.ie/</a>) and in a sidebar next to the world once launched. However, the project team decided to create bipartite instantiations of the model.</p>
<p>Given the issues in hosting the project online due to the deprecation of Unity’s web player, the optimised WebGL online version has now taken on the role of a more traditional abstract, a taster, or a more condensed version (<a href="#figure06">Figure 6</a>). This means that the online model could run smoothly on most browsers and on much slower computers contrary to the previous Webplayer version. To provide an overview of the battle, there is also a narrative-driven camera in which a voice over, featuring retired Commandant Billy Campbell, one of the researchers on the project, provides an evidence-informed interpretation of how the battle unfolded, while the camera moves concurrently to the various locations. Due to the limitations of the online model, during the narrative the user cannot interact with the scene.</p>
<p>On the other hand, the offline version which utilised the full Unity model is moving towards the paradigm of 3D Scholarly Editions, providing a framework to build a knowledge site, i.e. a single spatio-temporal environment that is immersive and multisensorial that provides the means to: 1) depict ambiguity and make the decision-making process transparent; 2) create annotations that would allow readers to make their own interpretations of the text; and, 3) depict both spatial and temporal aspects of the model. As this is part of a broader effort to develop a 3D edition framework that could be utilised by other Unity-based 3D worlds, the first step in the process was to develop a user-friendly interface that will include generalisable features so other projects can easily use them without the need to develop costly and unsustainable bespoke solutions. The prototype developed for BMSB makes use of clickable hotspots that include annotations for different aspects of the battle (e.g. buildings, volunteers etc.) on a side panel. This can accommodate not only textual information but also multimedia, such as videos recorded during our research in which military experts talk about different aspects of the battle, as well as sound. For example, in one of our visits to the shooting range we recorded the sounds of guns used in the battle. Incorporating these can also work as another cue in developing spatial awareness and understanding the chaos of echoing <sup id="fnref:66"><a href="#fn:66" class="footnote-ref" role="doc-noteref">66</a></sup>  <sup id="fnref:67"><a href="#fn:67" class="footnote-ref" role="doc-noteref">67</a></sup>, which could have been a critical factor in preventing the British soldiers from determining the exact location of the source of shooting by the Volunteers, especially at the beginning of the battle. Among the features of the 3D Edition is a timeline which indicates the moment an event happened (and/or its duration) also providing relevant annotations on the side panel (<a href="#figure07">Figure 7</a>).</p>
<p>In order to make the addition and updating of annotations easy to handle without the need of any technical expertise, a mechanism that allowed their creation outside the Unity Game Engine was developed. In the first experiments, an online Google spreadsheet was used, in which each line corresponds to a hotspot which has a unique ID and includes all the information that is pulled in the annotation panel. All multimedia are stored in a separate online folder and get connected to the hotspot based on their unique IDs. In such a way any changes in the annotations can be updated within the annotation panel without any Unity or other technical skills. We are currently reworking the spreadsheet into an annotation management system that makes entering and changing information more user friendly. Since the 3D edition framework is still in development and goes beyond the BMSB project, future iterations will also include ways to present alternative reconstructions with associated annotations as well as ways to show different levels of uncertainty, both of which are critical for 3D worlds based on inconsistent and ambiguous primary sources.</p>
<p>The offline version also provided opportunities to experiment with more computationally intensive annotations of the battle. For example, some of the Irish Volunteers described the scene when they saw the seemingly endless formation of British soldiers approaching them. In order to replicate that and due to technical constraints in rendering 1750 realistic soldier representations, we developed a more stylised view by deploying Artificial Intelligence (AI)-driven troop formations, using the Unity plugins Apex Utility AI (<a href="http://apexgametools.com/products/apex-utility-ai-2/">http://apexgametools.com/products/apex-utility-ai-2/</a>) and Apex Steer (<a href="http://apexgametools.com/products/apex-steer/">http://apexgametools.com/products/apex-steer/</a>). (<a href="#figure08">Figure 8</a>)</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/13/1/000415/resources/images/Figure7.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/13/1/000415/resources/images/Figure7_hu6fe4fad2b97e50b5ca65c8261e2dc83e_225149_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/13/1/000415/resources/images/Figure7_hu6fe4fad2b97e50b5ca65c8261e2dc83e_225149_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/13/1/000415/resources/images/Figure7_hu6fe4fad2b97e50b5ca65c8261e2dc83e_225149_1200x0_resize_q75_box.jpg 1200w,/dhqwords/vol/13/1/000415/resources/images/Figure7_hu6fe4fad2b97e50b5ca65c8261e2dc83e_225149_1500x0_resize_q75_box.jpg 1500w,/dhqwords/vol/13/1/000415/resources/images/Figure7.jpg 1591w" 
     class="landscape"
     ><figcaption>
        <p>The prototype of the 3D Scholarly Edition framework. Clickable hotspots provide multimedia annotations on a side panel, while a timeline indicates the duration or the moment when certain events happened.
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/13/1/000415/resources/images/Figure8.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/13/1/000415/resources/images/Figure8_hud7c79c8344eb99efd3dcd5542b240425_1250672_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/13/1/000415/resources/images/Figure8_hud7c79c8344eb99efd3dcd5542b240425_1250672_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/13/1/000415/resources/images/Figure8_hud7c79c8344eb99efd3dcd5542b240425_1250672_1200x0_resize_q75_box.jpg 1200w,/dhqwords/vol/13/1/000415/resources/images/Figure8_hud7c79c8344eb99efd3dcd5542b240425_1250672_1500x0_resize_q75_box.jpg 1500w,/dhqwords/vol/13/1/000415/resources/images/Figure8_hud7c79c8344eb99efd3dcd5542b240425_1250672_1800x0_resize_q75_box.jpg 1800w,/dhqwords/vol/13/1/000415/resources/images/Figure8.jpg 2560w" 
     class="landscape"
     ><figcaption>
        <p>A schematic view of the approaching British troops on Northumberland Road using Artificial Intelligence-driven troop formations.
        </p>
    </figcaption>
</figure>
<h2 id="7-conclusion">7. CONCLUSION</h2>
<p>The 3D world of the Battle of Mount Street Bridge enabled our research team to map our interpretation of documentary evidence and secondary sources on a three-dimensional terrain. Our goal was to stimulate a wider discussion and to enlarge the palette of creative responses to the event which had been long stuck in an Irish vs British, victor vs loser mode, along with hyperbolic and largely unproven claims. Our goal, as with many other research projects in employing these methods, was that introducing 3D modelling would have a transformative impact on sense-making, reasoning, and understanding through the process of problematising sources, identifying variables, and justifying solutions, opening a dialogue to generate new avenues of scholarship unlike the traditional spatiotemporal approaches that had been used in the past.</p>
<p>The technical challenges encountered by the BMSB project, and also the myriad of projects no longer accessible outlined above, highlight the precariousness of working with these technologies <sup id="fnref1:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup>  <sup id="fnref:68"><a href="#fn:68" class="footnote-ref" role="doc-noteref">68</a></sup>  <sup id="fnref:69"><a href="#fn:69" class="footnote-ref" role="doc-noteref">69</a></sup>). The instability of the research environment is a deterrent to more researchers working with such methods, since even when virtual worlds can be ported from one framework to another, this involves significant costs, downsampling and even rebuilding models, and repurposing content that cannot be fully migrated. Not only have we grown accustomed to expecting research outputs being available for hundreds of years in print-based environments, but for text-based digital scholarship, much of which has lasted decades, there exists models for preservation.</p>
<p>Despite these challenges, in the case of the BMSB project, there was no doubt amongst the project team, that it was only after we had the opportunity to navigate the photorealistic virtual world accompanied by a live narration by our collaborator Retired Commandant Billy Campbell describing the movements of the British soldiers that we managed to create a mental map of how and when the battle unfolded, which in turn affected our perception of the actual battlefield. The 3D world provided us a palette from which to map the battle in four dimensions, with actors moving concurrently in space over time. Early attempts to achieve this (before the 3D models were available) using more conventional methods (eg. spreadsheets and 2D maps) lacked an aspect of this multi-dimensionality, allowing us to only partially reconstruct spatial/temporal aspects of the battle. Moreover, despite having repeatedly walked the battlefield and extensively discussed the sequence and details of the various events, it was our experience of the 3D world with its vantage and viewpoints (whether flying above the world, looking out from a window in which a Volunteer stood, or being on the spot where the fighting broke out, without the distraction of present-day traffic and pedestrians) that provided us with a sense of presence <sup id="fnref:70"><a href="#fn:70" class="footnote-ref" role="doc-noteref">70</a></sup>  <sup id="fnref:71"><a href="#fn:71" class="footnote-ref" role="doc-noteref">71</a></sup> and solidified our understanding of the trajectory of the battle.</p>
<p>The model was crucial in answering the primary research question with which the project began: e.g. how many British casualties did the British forces suffer at this battle. The model in and of itself did not provide the answer: this was arrived at through exhaustive research into primary and contemporary secondary sources (for more details on this aspect of research see <sup id="fnref3:26"><a href="#fn:26" class="footnote-ref" role="doc-noteref">26</a></sup>). What the world provided us with, however, was an understanding of  <em>how</em>  this could happen. As stated above, while the figure for casualties (both killed and wounded) most cited is 234, given by General Sir John Maxwell, our research provided us with a more nuanced figure of 26 killed and 124 wounded. While this is not an insignificant figure, it is far lower than Maxwell’s more contemporary report, with a more precise number for those killed. Of the wounded, the discovery of a journal at the Bodleian Library of lists of casualties provided us with a more nuanced understanding of the range of reasons men presented with wounds: from wounds to the body to sprained ankles to shock. Typically, only body parts are listed in these reports: throat, hand, thigh, which in many instances we can presume were caused by bullets (throat, thigh); others, such as head, hand, ankle, are not as clear as these may have also been the result of sprains and falls <sup id="fnref:72"><a href="#fn:72" class="footnote-ref" role="doc-noteref">72</a></sup>. The very low ratio of wounded to dead may have also been due to the triage by medical personnel who were reported in multiple sources as being on site for the duration of the battle and able to provide immediate medical care (see for example, <sup id="fnref1:63"><a href="#fn:63" class="footnote-ref" role="doc-noteref">63</a></sup>; <sup id="fnref:73"><a href="#fn:73" class="footnote-ref" role="doc-noteref">73</a></sup>). Our much lower figure of troops that were killed also maps onto the preparedness of the Volunteers who had reasonable training, albeit not always with live ammo <sup id="fnref4:26"><a href="#fn:26" class="footnote-ref" role="doc-noteref">26</a></sup> coupled with an odd assortment of guns each with limited and not-interoperable ammo. Nevertheless, they were firing into, in military terms, a target rich environment with hundreds of troops within range.</p>
<p>The model, above all, provided us with a way to map, both temporally and spatially, the unfolding of the battle as the buildings held by the Volunteers were captured or vacated, as well as the complex movements of hundreds of British troops at any given time as various companies made flanking movements, attempting to find alternative routes to approach Mount Street Bridge, not knowing whether or where the Volunteers had occupied other buildings. The photorealistic rendering of the world provided us with clearer evidence than a two-dimensional map did of the terrain of the battlefield: eg., the lack of cover, the obstacles in the way of storming buildings (such as the high rails typically fronting each house). The high casualty figures may also be due to the British troops&rsquo; lack of experience. They were still in early days of training for the open fields of the Western Front which included no training for warfare in an urban or built-up environment. This dimensionality provided us with a tangible fourth dimension which helped us resolve conflicting information about how the battle unfolded in time. An example of this is the fall of 25 Northumberland Road, the first building captured by the British. The 2/7th battalion history claims the unit had taken the building soon after 2.45pm <sup id="fnref1:73"><a href="#fn:73" class="footnote-ref" role="doc-noteref">73</a></sup> while the sole survivor, Grace, placed it c. 8.30 pm, which is unrealistically late given other events for which we have more precise timings <sup id="fnref:74"><a href="#fn:74" class="footnote-ref" role="doc-noteref">74</a></sup>. Given the beginning of the battle around noon, and the accounts of other buildings north of 25 having vacated by early evening, the model provided us with an environment from which we could map and remap our evidence, arriving at a more comprehensive and source-driven picture of the event.</p>
<p>At the same time, the BMSB project has been used as a test case to explore how to integrate a 3D visualisation as the primary text of a digital scholarly edition, raising issues of how the phenomenology of place and space can be used to design a new language of scholarly editions, one that has the ability to model experience lost because of technological and evidentiary constraints <sup id="fnref2:29"><a href="#fn:29" class="footnote-ref" role="doc-noteref">29</a></sup>. This edition, like traditional DSEs, also brings together documentary evidence in the form of apparatus, reimagining digital textuality. We argue that the theories behind digital scholarly editing can be used as a theoretical and representational scaffolding to historical modelling in 3D to provide a framework from which to construct the world as well as for readers to understand the rationale and decision-making that underpins the creation of the world (for a discussion of the framework see <sup id="fnref2:34"><a href="#fn:34" class="footnote-ref" role="doc-noteref">34</a></sup>). At the same time, such a framework provides us with the means to valorise and to evaluate 3D visualisation scholarship, utilising similar principles to the ways in which we valorise and evaluate more established scholarly outputs <sup id="fnref:75"><a href="#fn:75" class="footnote-ref" role="doc-noteref">75</a></sup>.</p>
<p>Despite the constantly evolving landscape in which we do research, 3D (re)constructions provide both modellers and users with an advanced understanding of spatial and temporal dimensions of past environments. Although research is yet inconclusive regarding virtual world features that generate a sense of presence in these environments, e.g. textual annotation, storytelling, sound, visual realism etc. <sup id="fnref:76"><a href="#fn:76" class="footnote-ref" role="doc-noteref">76</a></sup>, we believe that by combining two seemingly incompatible paradigms, our work provides a framework for infusing 3D worlds with a hermeneutic richness <sup id="fnref1:25"><a href="#fn:25" class="footnote-ref" role="doc-noteref">25</a></sup> consisting of visual and auditory immersion and dynamic annotative features that will enhance the interactivity of the medium.</p>
<h2 id="acknowledgments">ACKNOWLEDGMENTS</h2>
<p>The authors acknowledge the many people who have contributed to the BSMB project. Particular thanks go to John Buckley for modelling the virtual world in Unity 3D and WebGL and to Almants Auksconis for his work on AI agents. Also, to Noho, and particularly Klemens Kopetzky and Niall Ó hOisín for their work on the 3D Scholarly Edition framework. Thanks go to Hugh Denard for his early work in conceptualising and framing the BMSB project. Thanks also go to Neale Rooney, Vinayak Das Gupta, and Roman Bleier for their help in developing the initial interface for the project, to Joshua Savage for his work on the early feedback session, and to Retired Commandant Billy Campbell, Brian Hughes, and Commandant Alan Kearney for their expertise in the historical and military aspects of the project. A special thanks goes to the Andrew W. Mellon Foundation who funded the initial development of the BMSB project as part of the Humanities Virtual Worlds Consortium, the Digital Scholarly Editions Initial Training Network (DiXIT) for supporting its redevelopment, and The National Historical Publications and Records Commission (NHPRC) and Andrew W. Mellon Foundation for funding our work on 3D Editions as part of the Scholarship in 3D project.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Zuk, T., Carpendale, S., and Glanzman, W.D.  “Visualizing Temporal Uncertainty in 3D Virtual Reconstructions.”  In M. Mudge, N. Ryan, R. Scopigno (eds), Proceedings of the 6th International conference on Virtual Reality, Archaeology and Intelligent Cultural Heritage - VAST'05 (2005), Eurographics Association, Aire-la-Ville, Switzerland, Switzerland, pp. 99-106, DOI: <a href="http://dx.doi.org/10.2312/VAST/VAST05/099-106">http://dx.doi.org/10.2312/VAST/VAST05/099-106</a>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Stefani, C, De Luca, L, Véron, P, Florenzano, M.  “Reasoning About Space-Time Changes: An Approach for Modeling the Temporal Dimension in Architectural Heritage.”  In Y. Xiao and E. ten Thij (eds) Proceedings of the IADIS International Conference on Computer Graphics and Visualization. Amsterdam, The Netherlands 22-27 July 200, pp. 287-292.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Laycock,  R. G., Drinkwater, D., and Day, A. M.  “Exploring Cultural Heritage Sites through Space and Time” , Journal on Computing and Cultural Heritage, 1. 2, Article 11 (2008), DOI: <a href="https://doi.org/10.1145/1434763.1434768">https://doi.org/10.1145/1434763.1434768</a>&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Reilly, P., Todd, S. and Walker, A.  “Rediscovering and Modernising the Digital Old Minster of Winchester” , Digital Applications in Archaeology and Cultural Heritage, 3, 2 (2016): 33–41, Retrieved November 5, 2017, DOI: <a href="https://doi.org/10.1016/j.daach.2016.04.001">https://doi.org/10.1016/j.daach.2016.04.001</a>&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>Dawson, P., Levy, R., Gardner, D. and Walls, M.  “Simulating the Behavior of Light Inside Arctic Dwellings: Implications for Assessing the Role of Vision in Task Performance” , World Archaeology, 39.1 (2007): 17–55, Retrieved November 5, 2017, DOI: <a href="http://dx.doi.org/10.1080/00438240601136397">http://dx.doi.org/10.1080/00438240601136397</a>&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>Paliou, E., Wheatley, D., and Earl, G.  “Three-dimensional Visibility Analysis of Architectural Spaces: Iconography and Visibility of the Wall Paintings of Xeste 3 (Late Bronze Age Akrotiri)” , Journal of Archaeological Science, 38.2 (2011): 375-386, Retrieved November 5, 2017, DOI: <a href="https://doi.org/10.1016/j.jas.2010.09.016">https://doi.org/10.1016/j.jas.2010.09.016</a>&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>Oetelaar, T.,  “CFD, Thermal Environments, and Cultural Heritage: Two Case Studies of Roman Baths.”  In IEEE 16th International Conference on Environment and Electrical Engineering (EEEIC), Florence, Italy, IEEE (2016), Retrieved November 5, 2017, DOI: <a href="http://doi.org/10.1109/EEEIC.2016.7555484">http://doi.org/10.1109/EEEIC.2016.7555484</a>&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>Sequiera M. L. and Morgado, L. C.  “Virtual Archaeology in Second Life and OpenSimulator” , Journal of Virtual Worlds Research 6.1 (2013): 1-16, Retrieved November 5, 2017 from: <a href="https://journals.tdl.org/jvwr/index.php/jvwr/article/view/7047/6310">https://journals.tdl.org/jvwr/index.php/jvwr/article/view/7047/6310</a>&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>Wankel, C. and Kingsley, J. (eds). Higher Education in Virtual Worlds: Teaching and Learning in Second Life. Bingley, UK, Emerald Group Publishing Limited (2009).&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>McDonough, J., Olendorf, R., Kirschenbaum, M., Kraus, K., Reside, D. et al. Preserving Virtual Worlds Final Report. Illinois, IDEALS, Illinois Digital Environment for Access to Learning and Scholarship (2010). Retrieved November 6, 2017 from: <a href="http://hdl.handle.net/2142/17097">http://hdl.handle.net/2142/17097</a>&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>Morgan, C.,  “(Re)Building Çatalhöyük: Changing Virtual Reality in Archaeology” , Archaeologies, 5 (2009): 468-487, Retrieved November 6, 2017, DOI: <a href="https://doi.org/10.1007/s11759-009-9113-0">https://doi.org/10.1007/s11759-009-9113-0</a>&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p>González-Tennant, E.  “New Heritage and Dark Tourism: A Mixed Methods Approach to Social Justice in Rosewood, Florida” , Heritage and Society, 6. 1 (2013): 62-88, Retrieved November 6, 2017, DOI: <a href="http://dx.doi.org/10.1179/2159032X13Z.0000000007">http://dx.doi.org/10.1179/2159032X13Z.0000000007</a>&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13">
<p>Beacham, R. THEATRON Final Report. London, King’s College London (2009). Retrieved November 6, 2017 from: <a href="http://cms.cch.kcl.ac.uk/theatron/fileadmin/templates/main/THEATRON_Final_Report.pdf">http://cms.cch.kcl.ac.uk/theatron/fileadmin/templates/main/THEATRON_Final_Report.pdf</a>&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14">
<p>Kraus, K. and Donahue, R.  “   Do You Want to Save Your Progress? : The Role of Professional and Player Communities in Preserving Virtual Worlds” , Digital Humanities Quarterly, 6, 2 (2012). Retrieved November 6, 2017 from: <a href="/dhqwords/vol/6/2/000129/">http://www.digitalhumanities.org/dhq/vol/6/2/000129/000129.html </a>&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:15">
<p>For an earlier classification of virtual worlds see Champion and Dave (2007)  “Dialing up the Past” , who divided virtual worlds into spatially visualised, activity-based, and hermeneutic in a pursuit of placemaking in such environments and in response to Kalay’s and Marx’s (2005) cyber-placemaking. Also see Anderson et al. (2010) on a typology for serious games.&#160;<a href="#fnref:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:16">
<p>For representative examples of standalone, offline 3D projects that have been used to analyse spatial and temporal features in past environments see: <sup id="fnref:77"><a href="#fn:77" class="footnote-ref" role="doc-noteref">77</a></sup>; <sup id="fnref:78"><a href="#fn:78" class="footnote-ref" role="doc-noteref">78</a></sup>; <sup id="fnref:79"><a href="#fn:79" class="footnote-ref" role="doc-noteref">79</a></sup>; <sup id="fnref1:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>; <sup id="fnref:80"><a href="#fn:80" class="footnote-ref" role="doc-noteref">80</a></sup>.&#160;<a href="#fnref:16" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:17">
<p>González-Tennant, E.  “Resurrecting Rosewood: New Heritage as Applied Visual Anthropology.”  In A. Gubrium, K. Harper and M. Otanez (eds), Participatory Visual and Digital Research in Action, Walnut Creek, California, Left Coast Press (2015), pp. 163-180.&#160;<a href="#fnref:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:18">
<p>Fischer, L.  “Visualizing Williamsburg: Modeling an Early American City in 2D and 3D.”  In F. Niccolucci, M. Dellepiane, S. Pena Serna, H. Rushmeier and L. Van Gool (eds), VAST11: International Symposium on Virtual Reality, Archaeology and Intelligent Cultural Heritage (2011), pp. 77-80. The Eurographics Association. Retrieved November 6, 2017, DOI: <a href="http://dx.doi.org/10.2312/PE/VAST/VAST11S/077-080">http://dx.doi.org/10.2312/PE/VAST/VAST11S/077-080</a>&#160;<a href="#fnref:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:19">
<p>Frischer, B., Zotti, G., Mari, Z., Capriotti Vittozzi, G.  “Archaeoastronomical Experiments Supported by Virtual Simulation Environments: Celestial Alignments in the Antinoeion at Hadrian&rsquo;s Villa (Tivoli, Italy),”  Digital Applications in Archaeology and Cultural Heritage, 3. 3 (2016): 55-79. Retrieved November 6, 2017, DOI: <a href="http://dx.doi.org/10.1016/j.daach.2016.06.001">http://dx.doi.org/10.1016/j.daach.2016.06.001</a>&#160;<a href="#fnref:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:20">
<p>Frischer B., Pollini, J., Cipolla, N., Capriotti, G., Murray, J. et al.  “New Light on the Relationship Between the Montecitorio Obelisk and Ara Pacis of Augustus.”  Studies in Digital Heritage, 1. 1 (2017). Retrieved November 6, 2017, DOI: <a href="https://doi.org/10.14434/sdh.v1i1.23331">https://doi.org/10.14434/sdh.v1i1.23331</a>&#160;<a href="#fnref:20" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:21">
<p>Roberts C. J. and Nick Ryan.  “Alternative Archaeological Representations within Virtual Worlds”  In R. Bowden (ed.) Proceedings of the 4th UK Virtual Reality Specialist Interest Group Conference – Brunel University, Brunel, Brunel University Printing Services (1997), pp. 179-188. Retrieved November 5, 2017 from <a href="https://www.cs.kent.ac.uk/pubs/1997/522/">https://www.cs.kent.ac.uk/pubs/1997/522/</a>&#160;<a href="#fnref:21" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:21" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:22">
<p>Vosinakis, S. and Avradinis, N.  “Virtual Agora: Representation of an Ancient Agora in Virtual Worlds using Biologically-inspired Motivational Agents” , Mediterranean Archaeology and Archaeometry, 16, 5 (2016): 29-41. Retrieved November 6, 2017, DOI: <a href="https://doi.org/10.5281/zenodo.204964">https://doi.org/10.5281/zenodo.204964</a>&#160;<a href="#fnref:22" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:23">
<p>Earle, N., Hales, S., Moseley, T. and Thorne, H. Crystal Palace Project and Chantry High School Evaluation Report. Bristol, University of Bristol (2011). Retrieved November 6, 2017 from: <a href="https://sydenhamcrystalpalace.files.wordpress.com/2011/10/evaluation-report-july-2011.pdf">https://sydenhamcrystalpalace.files.wordpress.com/2011/10/evaluation-report-july-2011.pdf</a>&#160;<a href="#fnref:23" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:24">
<p>Earle, N. and Hales, S.  “Pompeii in the Crystal Palace: Comparing Victorian and Modern Virtual, Immersive Environments”  Proceedings of Electronic Visualisation in the Arts Conference, Swindon, UK, BCS Learning &amp; Development Ltd. (2009), pp. 37-46.&#160;<a href="#fnref:24" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:25">
<p>Champion, E., 2015.  “Role-Playing and Rituals for Cultural Heritage-Oriented Games”  Diversity of play: Games – Cultures – Identities: Proceedings of DIGRA 2015, Luneberg, Germany, 14-17 May 2015.&#160;<a href="#fnref:25" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:25" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:26">
<p>Hughes, B., Campbell, B. and Schreibman, S.  “Contested Memories: Revising the Battle of Mount Street Bridge, 1916” , British Journal of Military History, 4.1 (2017): 2-22, Retrieved November 7, 2017 from: <a href="http://www.bjmh.org.uk/index.php/bjmh/article/view/192/154">http://www.bjmh.org.uk/index.php/bjmh/article/view/192/154</a>&#160;<a href="#fnref:26" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:26" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:26" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref3:26" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref4:26" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref5:26" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:27">
<p>Shillingsburg, P.. Scholarly Editing in the Computer Age: Theory and Practice. Third Edition. Ann Arbor, Michigan, University of Michigan Press (1996).&#160;<a href="#fnref:27" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:28">
<p>Apollon D., Bélisle, C. and Régnier, P. (eds) Digital Critical Editions. Urbana, Illinois, University of Illinois Press (2014).&#160;<a href="#fnref:28" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:29">
<p>Schreibman, S.  “Digital Scholarly Editing.”  In K.M. Price and R. Siemens (eds), Literary Studies in the Digital Age: An Evolving Anthology (2013). MLA Commons. Retrieved November 6, 2017, DOI: <a href="https://doi.org/10.1632/lsda.2013.4">https://doi.org/10.1632/lsda.2013.4</a>&#160;<a href="#fnref:29" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:29" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:29" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:30">
<p>Pierazzo, E. Digital Scholarly Editing: Theories, Models and Methods. Surrey, UK, Routledge (2015).&#160;<a href="#fnref:30" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:31">
<p>Driscoll J. M. and Pierazzo, E. (eds). Digital Scholarly Editing: Theories and Practices. Cambridge, Open Book Publishers (2016).&#160;<a href="#fnref:31" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:31" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:32">
<p>McKenzie, F. D. Bibliography and the Sociology of Texts. Cambridge, Cambridge University Press (1999).&#160;<a href="#fnref:32" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:33">
<p>Barthes, R. Image-Music-Text. Essays selected and translated by Stephen Heath. London, UK, Fontana (1977).&#160;<a href="#fnref:33" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:34">
<p>Schreibman, S. and Papadopoulos, C.  “Textuality in 3D: Three-dimensional (Re)constructions as Digital Scholarly Editions” . International Journal of Digital Humanities. Special Issue: Digital Scholarly Editing. Springer, DOI: <a href="https://doi.org/10.1007/s42803-019-00024-6">https://doi.org/10.1007/s42803-019-00024-6</a>&#160;<a href="#fnref:34" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:34" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:34" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:35">
<p>In the simplest case, in editing a manuscript for print publication an editor must make decisions in interpreting characters or words, in the case of deletions and additions, in what order the text should be presented, and in manuscripts that contain multiple texts, if portions should be considered part of the work being edited or of another work. More complex editing may involve creating the text from multiple extant versions (or witnesses) in which no single version carries final authorial intention.&#160;<a href="#fnref:35" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:36">
<p>Newman R. W. The Chymistry of Isaac Newton. Last Modified December 4, 2016. Retrieved November 6, 2017 from: <a href="http://webapp1.dlib.indiana.edu/newton/">http://webapp1.dlib.indiana.edu/newton/</a>&#160;<a href="#fnref:36" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:37">
<p>Editorial theory covers a wide range of practices and documentary types. For example, documentary editing is typically used for historical texts that exist in one version in which the manuscript is reproduced in print as accurately as possible (e.g. without correction of spelling, with additions and deletions, etc.). Genetic editing is typically used for manuscripts of literary texts in which the author has significant revision on the page. Here the goal of the editorial process is to understand the thought process of the author through the revision process.&#160;<a href="#fnref:37" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:38">
<p>Folsom, E. and Price, K.M. The Walt Whitman Archive. Last Modified September 2017. Retrieved November 6, 2017 from: <a href="http://whitmanarchive.org/">http://whitmanarchive.org/</a>&#160;<a href="#fnref:38" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:39">
<p>Strothotte, T., Masuch, M. and Isenberg, T.  “Visualizing Knowledge about Virtual Reconstructions of Ancient Architecture.”  In Proceedings of the International Conference on Computer Graphics - CGI &lsquo;99 (1999), Washington, DC: IEEE, pp. 36-43. Retrieved November 6, 2017 DOI: <a href="https://doi.org/10.1109/CGI.1999.777901">https://doi.org/10.1109/CGI.1999.777901</a>&#160;<a href="#fnref:39" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:40">
<p>Eiteljorg II H.  “The Compelling Image – A Double Edged Sword” , Internet Archaeology, 8. Retrieved November 6, 2017, DOI: <a href="https://doi.org/10.11141/ia.8.3">https://doi.org/10.11141/ia.8.3</a>&#160;<a href="#fnref:40" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:41">
<p>Djurcilov, S., Kwansik, K., Lermusiaux, P.F.J. and Pang, A.  “Volume Rendering Data with Uncertainty Information.”  In Ebert, D.S., Favre, J.M., and Peikert, R. (eds) Proceedings of the 3rd Joint Eurographics - IEEE TCVG Conference on Visualization, Aire-la-Ville, Switzerland: Eurographics Association Switzerland (2001), pp. 243-252 Retrieved November 6, 2017, DOI: <a href="http://dx.doi.org/10.2312/VisSym/VisSym01/243-252">http://dx.doi.org/10.2312/VisSym/VisSym01/243-252</a>&#160;<a href="#fnref:41" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:42">
<p>Frischer, B. and Stinson, P.  “The Importance of Scientific Authentication and a Formal Visual Language in Virtual Models of Archaeological Sites: The Case of the House of Augustus and Villa of the Mysteries.”  In N. A. Silberman and D. Callebaut (eds) Proceedings of the Interpreting the Past: Heritage, New Technologies and Local Development Conference on Authenticity, Intellectual Integrity and Sustainable Development of the Public Presentation of Archaeological and Historical sites and Landscapes (2003), pp. 49-83. Brussels, Flemish Heritage Institute. Retrieved November 6, 2017 from: <a href="http://www.iath.virginia.edu/images/pdfs/frischer_%20stinson.pdf">http://www.iath.virginia.edu/images/pdfs/frischer_ stinson.pdf</a>&#160;<a href="#fnref:42" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:43">
<p>Niccolucci, F. and Hermon, S. “A Fuzzy Logic Approach to Typology in Archaeological Research.” In M. Doerr and A. Sarris (eds), The Digital Heritage of Archaeology. Proceedings of Computer Applications and Quantitative Methods in Archaeology, Heraklion, Hellenistic Ministry of Culture, Greece (2002), pp 307-312. Retrieved November 6, 2017 from: <a href="https://bibliographie.uni-tuebingen.de/xmlui/bitstream/handle/10900/62210/03_Niccolucci_Hermon_CAA_2004.pdf?sequence=2&amp;isAllowed=y">https://bibliographie.uni-tuebingen.de/xmlui/bitstream/handle/10900/62210/03_Niccolucci_Hermon_CAA_2004.pdf?sequence=2&amp;isAllowed=y </a>&#160;<a href="#fnref:43" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:44">
<p>Pollini, J., Dodd, L.S., Kensek, K., Cipolla, N.  “Problematics of Making Ambiguity Explicit in Virtual Reconstructions: A Case Study of the Mausoleum of Augustus.”  In A. Bentkowska-Kafel, T. Cashen and H. Gardiner (eds) Theory and Practice. Proceedings of the 21st Annual Conference of CHArt: Computers and the History of Art, London, British Academy (2005).&#160;<a href="#fnref:44" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:45">
<p>Kensek, K.  “Survey of Methods for Showing Missing Data, Multiple Alternatives, and Uncertainty in Reconstructions” , CSA Newsletter, 19, 3. Retrieved November 6, 2017 from: <a href="http://csanet.org/newsletter/winter07/nlw0702.html">http://csanet.org/newsletter/winter07/nlw0702.html</a>&#160;<a href="#fnref:45" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:46">
<p>Papadopoulos, C. and Earl, G.P.  “Structural and Lighting Models for the Minoan Cemetery at Phourni, Crete.”  In Debattista, K., Perlingieri, C.,Pitzalis, D., and Spina, S. (eds), Proceedings of the 10th International conference on Virtual Reality, Archaeology and Cultural Heritage, Aire-la-Ville, Switzerland, Eurographics Association (2009), pp. 57-64. Retrieved November 6, 2017, DOI: <a href="http://dx.doi.org/10.2312/VAST/VAST09/057-064">http://dx.doi.org/10.2312/VAST/VAST09/057-064</a>&#160;<a href="#fnref:46" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:47">
<p>This project has been built and can be accessed by using the Blue Mars Virtual World Platform (<a href="http://www.bluemars.com/"> http://www.bluemars.com/</a>). At the moment of writing this paper the Blue Mars Client and consequently the Ball Glass Factory Virtual World was not accessible. John Fillwalk kindly provided a stand-alone version of the virtual world.&#160;<a href="#fnref:47" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:48">
<p>Due to the Unity Webplayer technology no any longer being supported by web browsers, the project cannot be accessed online. The PI, Angel D. Nieves, is currently working on a WebGL version as part of the  “Scholarship in 3D”  project funded by the National Historical Publications and Records Commission and the Andrew W. Mellon Foundation.&#160;<a href="#fnref:48" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:49">
<p>Snyder, L. M.  “VSim: Scholarly Annotations in Real-Time 3D Environments” . In P. Schmitz, L. Pearce, and Q. Dombrowski (eds) DH-CASE II: Collaborative Annotations on Shared Environments: Metadata, Tools and Techniques in the Digital Humanities (DH-CASE 2014), ACM, New York, NY, USA, Article 2, 8 pages. DOI: <a href="http://dx.doi.org/10.1145/2657480.2657483">http://dx.doi.org/10.1145/2657480.2657483</a>&#160;<a href="#fnref:49" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:50">
<p>Sullivan, E. A., and Snyder, L.M.  “Digital Karnak: An Experiment in Publication and Peer Review of Interactive, Three-Dimensional Content.”  Journal of the Society of Architectural Historians 76. 4 (2017): 464-482.&#160;<a href="#fnref:50" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:51">
<p>The authors currently are exploring the affordances of 3D Scholarly Editions as part of the  “Scholarship in 3D”  project that is creating a new paradigm for reconceiving 3D models as academic outputs by establishing a publishing cooperative with the necessary open-source infrastructure, workflows, and peer-reviewing guidelines.&#160;<a href="#fnref:51" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:52">
<p>Smith, R.  “The Long History of Gaming in Military Training” , Simulation &amp; Gaming, 41.1 (2010): 6-19. Retrieved November 6, 2017, DOI: <a href="https://doi.org/10.1177/1046878109334330">https://doi.org/10.1177/1046878109334330</a>&#160;<a href="#fnref:52" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:53">
<p>Peterson, J.  “A Game Out of All Proportions.”  In P. Harringan and M.G. Kirschenbaum (eds), Zones of Control: Perspectives on Wargaming, Cambridge, Massachusetts, MIT Press (2016), pp. 3-31.&#160;<a href="#fnref:53" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:53" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:53" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:54">
<p>Nakamura, T.  “The Gap Between Tabletop Simulations Games and the  “Truth” .” . In P. Harringan and M. G. Kirschenbaum (eds), Zones of Control: Perspectives on Wargaming, Cambridge, Massachusetts, MIT Press (2016), pp. 43-47.&#160;<a href="#fnref:54" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:54" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:54" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:55">
<p>Crogan, P. Gameplay Mode: War, Simulation, and Technoculture. Minneapolis, Minnesota, University of Minnesota Press (2011).&#160;<a href="#fnref:55" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:55" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:56">
<p>Jeremy, A.  “Struggling with Deep Play.”  In P. Harringan and M. G. Kirschenbaum (eds) Zones of Control: Perspectives on Wargaming (2016), Cambridge, Massachusetts, MIT Press, pp. 463-470.&#160;<a href="#fnref:56" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:57">
<p>McCall, Jeremiah.  “Teaching History with Digital Historical Games: An Introduction to the Field and Best Practices” , Simulation &amp; Gaming, 47, 4 (2016): 517-542. Retrieved November 6, 2017, DOI: <a href="https://doi.org/10.1177/1046878116646693">https://doi.org/10.1177/1046878116646693</a>&#160;<a href="#fnref:57" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:58">
<p>There are war games that have invested in both realism and accuracy. For example, the development of the multiplayer first-person shooter game, Verdun (<a href="https://www.verdungame.com/">https://www.verdungame.com/</a>), included historical research to accurately depict weapons, uniforms, and battlefields, as well as bullet physics.&#160;<a href="#fnref:58" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:59">
<p>In the war game, Battlefield 1 (<a href="https://www.battlefield.com/">https://www.battlefield.com/</a>), players have to complete certain challenges, called Codex Entries, to unlock information written by military and WWI historians. One of these, refers to  “Dicta Boelcke” , a list of aerial combat techniques for attack procedures and tactics written by the German flying Ace, Oswald Boelcke.&#160;<a href="#fnref:59" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:60">
<p>Octavian, M., Machidon, M. O., Duguleana, M., and Carrozzino, M.  “Virtual Humans in Cultural Heritage ICT Applications: A Review” . Journal of Cultural Heritage, 33 (2018): 249-260, Retrieved June 9, 2019, DOI: <a href="https://doi.org/10.1016/j.culher.2018.01.007">https://doi.org/10.1016/j.culher.2018.01.007</a>&#160;<a href="#fnref:60" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:61">
<p>Battalion histories for the 2/7th and the 2/8th Battalions of the Sherwood Foresters exist, each with extensive accounts of the battle.&#160;<a href="#fnref:61" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:62">
<p>Officers of the Battalions. The Robin Hoods. 1/7th, 2/7th &amp; 3/7th Battns. Sherwood Foresters: 1914-1918. Nottingham, J. &amp; H. Bell Limited (1921).&#160;<a href="#fnref:62" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:63">
<p>Written by Officers of the Battalions,  “The Robin Hoods: 1/7th, 2/7th &amp; 3/7th Battalions, Sherwood Foresters, 1914-1918” , Nottingham: J. &amp; H. Bell.&#160;<a href="#fnref:63" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:63" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:64">
<p>For a detailed historical account of the battle, see <sup id="fnref5:26"><a href="#fn:26" class="footnote-ref" role="doc-noteref">26</a></sup>.&#160;<a href="#fnref:64" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:65">
<p>Alternative web players, such as Gameload &lt;www.gameload.top&gt; can be installed as an extension to Chrome in Windows PCs, thus enabling an in-browser activation of a Unity game. Unity virtual worlds mentioned in Section 1  “Historical Virtual Worlds: The Challenges of the State-of-the-Art”  that were still available online were opened at the time of writing this paper using Gameload; however, due to the instability of the application (often needed to be reinstalled to run properly) navigation features were disabled in many of these.&#160;<a href="#fnref:65" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:66">
<p>Dinh, Q. H., Walker, N., Song, C., Kobayashi, A., and Hodges, F.L.  “Evaluating the Importance of Multi-sensory Input on Memory and the Sense of Presence in Virtual Environments”  Proceedings of the IEEE Virtual Reality - VR &lsquo;99 (1999). IEEE Computer Society, Washington, DC, USA, pp. 222-229.&#160;<a href="#fnref:66" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:67">
<p>Schmidt, M., Schwartz, S., and Larsen, J.  “Interactive 3-D Audio: Enhancing Awareness of Details in Immersive Soundscapes?” , Audio Engineering Society, Convention 133 (2012). Audio Engineering Society.&#160;<a href="#fnref:67" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:68">
<p>Ruan J. and McDonough, J.P.  “Preserving Born-digital Cultural Heritage in Virtual World.”  In IEEE International Symposium on IT in Medicine &amp; Education, Jina, China: IEEEA (2009), pp. 745-748. Retrieved November 6, 2017. DOI: <a href="http://ieeexplore.ieee.org/document/5236324/">http://ieeexplore.ieee.org/document/5236324/</a>&#160;<a href="#fnref:68" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:69">
<p>Archaeology Data Service Guide to Archiving Virtual Reality Projects (2011). <a href="http://guides.archaeologydataservice.ac.uk/g2gp/Vr_6-1">http://guides.archaeologydataservice.ac.uk/g2gp/Vr_6-1</a>.&#160;<a href="#fnref:69" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:70">
<p>Heeter, C.  “Being There: The Subjective Experience of Presence” , Presence: Teleoperators and Virtual Environments, 1.2 (1992): 262 -271.&#160;<a href="#fnref:70" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:71">
<p>Steuer, J.  “Defining Virtual Reality: Dimensions Determining Telepresence” , Journal of Communication, 42.4 (1992): 73–93.&#160;<a href="#fnref:71" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:72">
<p>Papers of Sir Matthew Nathan, Rebellion in Ireland, Vol. 1, MS 476 &amp; Vol. II, MS 477 Casualty lists by day, 26 April-5 May. Bodleian Library, Oxford.&#160;<a href="#fnref:72" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:73">
<p>Oates, W. C. The Sherwood Foresters in the Great War: 1914-1918. The 2/8th Battalion. Nottingham, J. &amp; H. Bell Limited.&#160;<a href="#fnref:73" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:73" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:74">
<p>Bureau of Military History Witness Statement. James Grace Witness Statement, BMH WS 310.&#160;<a href="#fnref:74" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:75">
<p>Sullivan, E, Nieves, A. D., and Snyder, L. M.  “Making the Model: Scholarship and Rhetoric in 3D Historical Reconstructions.”  In J. Sayers (ed.) Making Things and Drawing Boundaries: Experiments in the Digital Humanities. University of Minnesota Press.&#160;<a href="#fnref:75" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:76">
<p>Pujol, L.  “Did we just travel to the past? Building and evaluating with Cultural Presence different modes of VR-mediated experiences in Virtual Archaeology” , ACM Journal on Computing and Cultural Heritage, 12, 1 (2019), Retrieved March 18, 2019, DOI: <a href="https://doi.org/10.1145/3230678">https://doi.org/10.1145/3230678</a>&#160;<a href="#fnref:76" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:77">
<p>Gutierrez, D., Frischer, B., Cerezo, E., Gomez, A., Serona, F.  “AI and Virtual Crowds: Populating the Colosseum” , Journal of Cultural Heritage, 8 (2007): 176-185, Retrieved November 6, 2017, DOI: <a href="https://doi.org/10.1016/j.culher.2007.01.007">https://doi.org/10.1016/j.culher.2007.01.007</a>&#160;<a href="#fnref:77" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:78">
<p>Dylla, K., Frischer, B., Mueller, P., Ulmer, A., and Haegler, S.  “Rome Reborn 2.0: A Case Study of Virtual City Reconstruction Using Procedural Modeling Techniques.” , In B. Frischer, J. Webb Crawford, and D. Koller (eds), Making History Interactive. Computer Applications and Quantitative Methods in Archaeology (CAA). Proceedings of the 37th International Conference, Williamsburg, Virginia, United States of America, March 22-26 (BAR International Series S2079). Archaeopress, Oxford (2010), pp. 62-66.&#160;<a href="#fnref:78" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:79">
<p>Anderson, E.F., McLoughlin, L., Liarokapis, F., Peters, C., Petridis, P., de Freitas, S.  “Developing Serious Games for Cultural Heritage: A State-of-the-Art Review” . Virtual Reality 14.4: 255-275. Retrieved November 5, 2017, DOI: <a href="https://doi.org/10.1007/s10055-010-0177-3">https://doi.org/10.1007/s10055-010-0177-3</a>&#160;<a href="#fnref:79" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:80">
<p>Papadopoulos, C. and Earl, G.  “Formal Three-dimensional Computational Analyses of Archaeological Spaces.”  In E. Paliou, U. Lieberwirth and S. Polla (eds), Spatial Analysis and Social Spaces. Interdisciplinary Approaches to the Interpretation of Prehistoric and Historic Built Environments, Berlin, Boston, De Gruyter (2014), pp. 135-166.&#160;<a href="#fnref:80" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry><entry><title type="html">Curating Crowds: A Review of Crowdsourcing Our Cultural Heritage (Ashgate, 2014)</title><link href="https://rlskoeser.github.io/dhqwords/vol/13/1/000410/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://rlskoeser.github.io/dhqwords/vol/13/1/000410/</id><author><name>Victoria Van Hyning</name></author><published>2019-04-26T00:00:00+00:00</published><updated>2019-04-26T00:00:00+00:00</updated><content type="html"><![CDATA[<p><em>Crowdsourcing Our Cultural Heritage</em>  is an important collection for anyone working in cultural heritage or academia who is interested in the pros and cons of implementing crowdsourcing projects, whether for manuscript transcription, image or video tagging or crowd-curated exhibitions. Most essays in the collection, starting with the introduction by editor Mia Ridge, offer a definition of crowdsourcing, engage with some of the theoretical material pertaining to the topic such as James Suroweicki’s  <em>The Wisdom of Crowds</em>   <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>, and give an overview of the challenges that crowdsourcing can help GLAMs and academics overcome. Ridge is one of the most cogent advocates for, and careful critics of, crowdsourcing in cultural heritage industries, and she gets the volume off to a thought-provoking start with sections on Key Trends and Issues and Looking to the Future of Crowdsourcing in Cultural Heritage. While the essays themselves are somewhat repetitive in terms of definitions and theoretical ground, almost any one of them could be read on its own and provide insight into the broad issues surrounding crowdsourcing. This volume would be valuable as a teaching resource for a range of specialists, including GLAM practitioners such as archivists and curators, as well as educators, audiovisual specialists, art historians, web designers and developers, and sociological and history of science theorists interested in crowdsourcing. Most articles include robust bibliographies with references to grey and formal publications. Although this is a fast-moving area of research and practice, the volume is still remarkably up-to-date over four years on from its publication in 2014.</p>
<p>Part I contains eight case studies: seven from UK and USA-based GLAMs, and one case study of a video tagging project from the Netherlands. The titles of many of the former refer to text transcription or metadata extraction projects, in which volunteers are invited to transcribe or add tags to digital images of texts held in the online catalogues of diverse repositories. And while each case study delivers useful insights into the transcription and tagging projects mentioned in their titles, each at least touches on a wider range of public engagement and crowdsourcing activities undertaken by the authors’ respective GLAMs and/or universities or broadcasters. Some of these are in-person events such as roadshows, while the use of surveys by numerous authors helps to surface users’ or patrons’ voices. There is a good balance between quantitative and qualitative assessment of projects’ successes and failures as well as the reach and impact of digital initiatives. Most articles are illustrated with images of the web-based tools under discussion, and many include figures and tables communicating user participation and engagement.</p>
<p>Shelley Bernstein’s opening essay Crowdsourcing in Brooklyn, offers insights into the Brooklyn Museum’s strategies of digital and in-person engagement over the better part of a decade, including the process of curating and displaying an exhibition with input from members of the public —  <em>Click! A Crowd-Curated Exhibition</em>  (<a href="#https://www.brooklynmuseum.org/exhibitions/click">https://www.brooklynmuseum.org/exhibitions/click</a>). One of the strengths of Bernstein’s piece is her acknowledgement of the design influences and goals of Flickr whose founder, Caterina Flake, said You should be able to feel the presence of other people on the Internet, a principle Bernstein and her team translated for the GLAM setting:    “How could we highlight the visitor’s voice in a meaningful way and utilise technology and the web to foster this exchange?”   <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>  . She also engages with Surowiecki’s idea put forward in  <em>The Wisdom of Crowds</em>   <sup id="fnref1:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> that for crowds to be wise they must be diverse and their actors independent. Brooklyn attempted to foster both attributes in their open call for photographers to submit one image each on the theme of Changing Faces of Brooklyn, to be judged by a crowd for inclusion in a new exhibition. 389 entries were assessed by 3,344 evaluators in a thoughtfully created interface that attempted to minimize outside influence on evaluators. 410,089 evaluations were submitted; the top 20% of images were then displayed as the exhibition  <em>Click!</em> , drawing 20,000 visitors in six weeks. Bernstein provides a range of other useful statistics about user engagement with the evaluation interface and the exhibition. The remaining case studies include a Tinder-style app through which volunteers assess images quickly ( <em>Split Second: Indian Paintings</em> , <a href="#http://www.trevorowens.org/2012/03/crowdsourcing-cultural-heritage-the-objectives-are-upside-down/">https://www.brooklynmuseum.org/exhibitions/splitsecond</a>), and an open-studio tour program spanning 73 square miles and 67 neighbourhoods (Go). The article argues persuasively that GLAMs can work with visitors and volunteers to transform crowds into communities. Regional museums may be better suited to this approach than GLAMs such as the National Library of Wales, which is represented in a later chapter of the volume, whose authors remark on how relative regional isolation and poor public transportation have made online methods of crowdsourcing particularly useful and productive.</p>
<p>Chapter 2,  <em>Old Weather</em> : Approaching Collections from a Different Angle, by Lucinda Blaser (Royal Museums Greenwich), provides a valuable overview of a project in which volunteers transcribe historic ships’ logs in an effort to extract climatological data that will improve the British Met Office’s weather predication and climate models. As members of the  <em>Old Weather</em>  team have reported previously, volunteers’ interest in the historic information they encountered along the way has proved an unexpected hit, and has been a major reason for sustained engagement with the project over time (<a href="#https://blog.oldweather.org/">https://blog.oldweather.org/</a>). The project (<a href="#https://blog.oldweather.org/">https://www.oldweather.org/</a>) is a collaboration between  <em>Zooniverse</em>  (<a href="#https://blog.oldweather.org/">www.zooniverse.org</a>) – an online crowdsourcing research group based at the University of Oxford – the Adler Planetarium in Chicago, the University of Minnesota, the Met Office, National Maritime Museum and Naval-History.net, using original ship log sources held in the National Archives, UK (a number of spinoff projects since 2014 have added further material accessible through the same site). Blaser remarks that although the source material was not held at the National Maritime Museum, the museum held images of the ships and other material that    “could engage users further with links to historic photographs that would bring these vessels to life, making this project more than just a two-dimensional transcription project”   <sup id="fnref1:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>  . Moreover, she asks if this model could be more widely applied, and poses a rhetorical question:    “Do we have to be selfish and only think of ourselves in the results of crowdsourcing and citizen science projects, or is the ability to say that as an institution you have helped a large number of users engage with your subject matter in a meaningful way more than enough?”   <sup id="fnref2:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>  .</p>
<p>Blaser discusses some of the challenges and opportunities inherent in crowdsourcing, including the need to foster communities over the long term and incorporate crowdsourced results into content management systems. She is admirably attentive to the experience of volunteers, quoting a number of contributors in her essay, and reflecting on the ways in which volunteers’ engagement can result in new learning opportunities and a sense of fulfillment and ownership that can ultimately drive new research questions. She briefly mentions instances of crowd-curation’ of exhibitions at Royal Museums Greenwich, such as  <em>Beside the Seaside</em>  and  <em>Astronomy Photographer of the Year</em>      “where crowdsoured images and collection items share the same gallery space”   <sup id="fnref3:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>  . Blaser argues that    “crowdsourced displays will become more common”   <sup id="fnref4:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>   thus allowing volunteers to work directly with museum staff and feel greater ownership of collections. Her reflections link in well with the previous essay.</p>
<p>Tim Causer and Melissa Terras of University College London discuss the  <em>Transcribe Bentham</em>  project in chapter 3 of the volume.  <em>Transcribe Bentham</em>  invites members of the public to transcribe and apply TEI XML tags to Jeremy Bentham’s voluminous archives, which are slowly being edited by a team at UCL. The essay describes the original transcription interface — a customized MediaWiki web application — the initial call for engagement, updates to the interface, funding, staffing, cost-effectiveness, quality control, as well as future collaborations (now underway) to use the vetted Bentham transcripts as training data for Handwriting Recognition Technology. Significantly, the authors acknowledge that the difficulty of the transcription and marking tasks led to a narrowing of participation and reliance on a small cohort of seventeen Super Transcribers (the threshold at which one becomes a Super Transcriber is not specified). They aver that  <em>Transcribe Bentham</em>  might be better described as crowd-sifting, beginning with the traditional open call of crowdsourcing but resulting in the retention of a small group of highly dedicated individuals. Although the interface was tweaked to ease participation, the authors argue that it is worth attempting to attract more Super Transcribers than casual or short-term users. Other research teams, including Zooniverse, have attempted to lower barriers to participation by developing more granular approaches to text transcription.  <em>Shakespeare’s World</em>  and  <em>AnnoTate</em> , both launched in late 2015, are transcription projects built with GLAM partners on the Zooniverse platform, for which I served as project lead. These allow participants to transcribe as little as a word or line on a page and have resulted in higher levels of participation from a broader base. For example, as of May 2017, volunteers who worked on fewer than nine pages contributed 20% of  <em>Shakespeare’s World</em>  transcriptions overall, a significant contribution.</p>
<p>Case study 4,  “Build, Analyse and Generalise: Community Transcription of the  <em>Papers of the War Department</em>  and the Development of  <em>Scripto</em> ”  by Sharon M. Leon, describes how the creation of a particular project resulted in the release of  <em>Scripto</em>  (<a href="#www.scripto.org">www.scripto.org</a>)    “a customisable software library [built on WikiMedia] connecting a repository to an editing interface, and as extensions for three popular web-based content management systems”   <sup id="fnref5:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>   including Omeka, Drupal, and WordPress.  <em>Papers of the War Department</em>  (PWD) digitally assembles nearly 45,000 documents from archives in the US, Canada, Britain and France pertaining to the period 1784–1800. The papers had long been believed to be lost due to a fire in the War Office in 1800, which destroyed the central repository. Through the efforts of scholar Ted Crackel in the 1980s and 1990s, the whereabouts of copies and examples of the original correspondence were located and imaged, originally for the purposes of a printed edition, then a CD-ROM, and finally, in 2008, for  <em>PWD</em> , which invites members of the public to transcribe the sources. The sources were lightly catalogued by experts by 2010, but as Leon points out this only opened the corpus to researchers who knew precisely what they were looking for, while those    “with less concrete demands”   <sup id="fnref6:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>   found the early index less useful. Project funding was used to add more detailed metadata to a third of the collection, but could not stretch far enough to cover the whole. At this point, in 2013,  <em>PWD</em>  staff analyzed their site traffic and concluded they had a ready-made group of users who might be willing to contribute their own transcriptions and expertise back into the collection.</p>
<p>Before describing  <em>Scripto</em>  Leon gives an overview of some of the theoretical work and existing transcription tools and crowdsourcing platforms that inspired staff at the Roy Rosenzweig Center of History and New Media (RRCHNM) to engage with the public. She cites Max Evans’s    “2007 call for commons-based peer production as a way to create  Archives of the People, by the People, for the People ”   <sup id="fnref7:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>  : Wikipedia, Flickr Commons, Zooniverse and  <em>Transcribe Bentham</em> . Like many organisations that have harnessed crowdsourced transcription, RRCHNM realized that    “public contributions [could] provide transcriptions where there once were none, and where there likely would be none in the future”   <sup id="fnref8:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>   due to budgetary constraints and the sheer scale of the job. Moreover public contributions where volunteers choose what to transcribe    “can serve as a barometer of the most interesting materials within a particular collection”   <sup id="fnref9:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>   and perhaps have a bearing on editorial choices for print or digital editions in the future. Surely many publishers would be swayed by concrete evidence of this kind.</p>
<p>User testing of the early site led the team to implement a series of innovations to the standard MediaWiki transcription interface, for example showing the manuscript document at the top of the page and the transcription pane beneath. Login accounts are required and new users can wait a business day to be approved. I tested this on a working day, and was confirmed for a new account in less than twenty-four hours. The project team felt approved login was necessary to reduce vandalism and spam, but I would argue it probably acts as a deterrent to users who might feel motivated to engage, but are unwilling or unable to return to the project in future. The remainder of the case study traces the support, development, and editorial time devoted to DWP and the release and uptake of  <em>Scripto</em> , which has been particularly popular amongst university libraries <sup id="fnref10:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>.</p>
<p>Case study 5 returns us to New York, with an engaging piece titled  “ <em>What’s on the Menu?</em> : Crowdsourcing at the New York Public Library” , by Michael Lascarides and Ben Vershbow. The authors combine a detailed case study of a menus transcription and metadata extraction project launched in 2011 with up-to-date (and still relevant) analysis and insight into user motivation, usability, sustainability, and data ownership. Lascarides and Vershbow argue that it needs to be made very clear at the outset that your library entirely owns the newly created data to do with whatever it wants, and that the participant willingly relinquishes any ability to restrict those rights and that    “usually, you will want to share the [resulting] content that results from their labours as broadly as you can”   <sup id="fnref11:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>  . NYPL have perhaps been more explicit than others about the status of the data they collect. While most GLAMs want their data to be reusable and searchable through a web interface, and clearly state this in their mission statement and other materials geared towards potential volunteers, project owners could do more to highlight that any data produced through their interface will become the property of the institution.</p>
<p>After providing a clear overview of the site functionality, supported by images of the interface, the authors also highlight the depth of user engagement with  <em>What’s on the Menu</em>  (<a href="#http://menus.nypl.org/">http://menus.nypl.org/</a>) during its first sixteen months: 163,690 visits, four million page views, and an average of 6.36 minutes on the site, compared to just 2.38 on nypl.org <sup id="fnref12:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>, suggesting that transcription and other crowdsourcing interfaces offer patrons ways of engaging with and exploring collections that more traditional GLAM interfaces do not. In a What’s Next? section the authors advocate for crowdsourcing at scale in which a new generation of reusable tools that require less maintenance and serve a wider variety of purposes are deployed across most if not all NYPL domains, as opposed to building and attempting to maintain single stand-alone apps. Lascarides and Vershbow conclude with reflections on the gamification debate, arguing persuasively that participants are often motivated by the collections themselves and do not need an additional layer of play to become or remained involved in crowdsourcing. They cite a range of authorities including Trevor Owens (Library of Congress):</p>
<blockquote>
<p>When done well, crowdsourcing offers us an opportunity to provide meaningful ways for individuals to engage with and contribute to public memory. Far from being an instrument which enables us to ultimately better deliver content to end users, crowdsourcing is the best way to actually engage our users in the fundamental reason that these digital collections exist in the first place. [Owens, 2012, cited <sup id="fnref13:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>, 131]<br>
<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>   Like a number of other contributors to the volume, Lascarides and Vershbow emphasize the experimental and iterative nature of crowdsourcing projects at their institution (including others beyond  <em>What’s on the Menu?</em> , such as  <em>GeoTagger</em> , a geo-referencing project). They write about paying down their technical debt after a successful beta test of  <em>What’s on the Menu</em> , by overhauling the original application code, implementing new visual design and elements of the user interface, with better search and browsing features, and perhaps most significantly, a new public API    “to provide other application developers or digital researchers real-time data from the project”   <sup id="fnref14:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>  .</p>
</blockquote>
<p>Chapter 6,  “What’s Welsh for  Crowdsourcing ? Citizen Science and Community Engagement at the National Library of Wales” , by Lyn Lewis Dafis, Lorna M. Hughes and Rhian James, reports on two crowdsourcing projects undertaken at the National Library of Wales (NLW):  <em>The Welsh Experience of the First World War</em>  (<a href="#http://cymru1914.org/">http://cymru1914.org/</a>) collecting project and  <em>Cymru1900Wales</em>  (<a href="#http://cymru1914.org/">http://www.cymru1900wales.org/</a>), a place name gathering project in partnership between NLW, the University of Wales, People’s Collections Wales, the Royal Commission on the Ancient and Historical Monuments Wales and Zooniverse. They also describe the digitized collection of  <em>Welsh Wills Online</em> , a project with potential for adding crowdsourced transcription. The authors remark that the relatively remote location of the library has led the institution to focus on    “mass digitisation of core collections to support access, preservation, research and education”   <sup id="fnref15:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>  , as well as the provision of all tools and web services in both Welsh and English. Moreover, they argue that crowdsourcing    “can [&hellip;] be seen as the logical development of a long tradition of research and engagement based on the Library’s collections”   <sup id="fnref16:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>  .</p>
<p>Like others in the volume, the authors have recourse to the theoretical frameworks put forward by Jeff Howe of Wired magazine in regards to crowdsourcing and business practice. They conclude that crowdsourcing in the cultural heritage domain seeks to utilise the multiple perspectives of the crowd, a statement most clearly borne out in  <em>The Welsh Experience of the First World War</em>  which collected and digitized primary material provided by the public in a series of five roadshows held in geographically diverse parts of Wales. The roadshow format is not new, as the authors point out, citing the Oxford-based  <em>Great War Archive</em>  project,  <em>Europeana 1914–1918</em>  and the JISC-funded  <em>Welsh Voices of the Great War Online</em> . But the project is different in that it aimed to digitize materials that would fill particular gaps in existing collections. The authors provide a list of those organizations they contacted and the advertising deployed to recruit participants, and reveal that while the 350 items that were digitized were diverse, they did not succeed in gathering non-documentary or text-based items. They conclude that future marketing of roadshows would need to be much more targeted in order to capture other kinds of media.</p>
<p><em>Cymru1900Wales</em> , the library’s first crowdsourcing project, launched in September 2012 and asks volunteers to add local place name information to digitized Ordnance Survey maps from 1900. A number of research goals are referred to in broad strokes by the authors, for example the hope that the dataset will unlock social and linguistic history. Unlike the roadshows, this project was conducted entirely remotely, with academics and participants communicating via email, project blog, FaceBook and Twitter. This is particularly important for GLAMs that are remote from their patron base.  <em>Welsh Wills Online</em>  consists of 800,000 pages of wills and other legal documents collected in the Welsh ecclesiastical courts between the late-sixteenth century and 1858. Like so many of the text-based collections discussed throughout the volume, the materials here are not yet machine-readable, making manuscript transcription necessary if the contents of the images and original documents are to become word-searchable. At the time of writing, NLW had not yet embarked upon such a project and one does not appear to be under development at present.</p>
<p>The authors acknowledge that while crowdsourcing may have great promise, many GLAM and academic end-users, including those they surveyed, are anxious that projects be cost-effective. This manifests in two distinct but related anxieties: 1) that time spent setting up and maintaining projects should not exceed the amount of time it would take staff to do the core tasks associated with the project themselves, i.e. transcription and 2) that end-users, i.e. researchers, be able to make use of the results. Quality control and vetting, the authors argue, should not create a heavier burden than any work offset by the use of crowdsourcing. Like Causer and Terras above ( <em>Transcribe Bentham</em> ), the NLW team concludes that it might be best to attract and retain specialists or, to put it another way, a cohort of Super Transcribers. Again, drawing on my experience of  <em>Shakespeare’s World</em>  in which volunteers transcribe a range of early modern English manuscripts that share some of the difficulties of the Welsh wills corpus, a significant proportion of volunteers are able to make meaningful contributions to transcription when given some guidance in the form of handbooks, tutorials, shortcut keys for common abbreviations, and so on. But however much we lower barriers to participation, GLAMs and academics still need time, money and support to deal with both the process and the products of crowdsourcing. In this regard, the authors’ emphasis on the potential for crowdsourcing to save money is perhaps misleading, though even within the context of the present volume, it is a widely expressed view; one that may have its roots in crowdsourcing for business purposes. As Trevor Owens argues at the close of the volume, and as Blaser argues in chapter 2, crowdsourcing in GLAM and academic environments may save time on tasks such as transcription and metadata extraction, but ideally should create new roles dedicated to public engagement with collections and tweak existing roles and the ways in which GLAMs conceptualize their duties and interactions to patrons. GLAMs might, for example, spend more time nurturing public engagement projects and ingesting the products of crowdsourcing and other kinds of engagement projects into CMSs (content management systems), rather than having specialists add deeper metadata to a lightly catalogued collection.</p>
<p>In  “ <em>Waisda?</em> : Making Videos Findable Through Crowdsourced Annotations”  (chapter 7), Johan Oomen, Riste Gligorov and Michiel Hildebrand describe two pilot projects that resulted in the contribution of over one million tags to a corpus of video clips in the Netherlands Institute for Sound and Vision, which holds over 750,000 hours of audiovisual material as of 2014. The primary audience for the archive is not the equivalent of library patrons or museum-goers, but broadcasters and journalists who seek out reusable content. Secondary and tertiary audiences are comprised of researchers and students who use materials in a broad range of disciplines, and home users who access the materials for    “personal entertainment or a learning experience”   <sup id="fnref17:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>  . The opening pages of the article give an overview of the challenges of making non-machine readable datasets accessible through crowdsourcing, and describe various approaches to crowdsourcing and motivational factors for both GLAMs and end-users or participants. Many of these, such as    “increasing connectedness between audiences and the archive”   <sup id="fnref18:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>   are echoed in contributions throughout this volume.</p>
<p>Unlike most of the other projects here,  <em>Waisda?</em>  deploys gamification strategies to engage users, and the authors report successful outcomes from what they call serious game play <sup id="fnref19:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. As in the ESP Game, players of  <em>Waisda?</em>  accrue points if their tags match those of other players.  <em>Waisda?</em>  players can see their score relevant to other players and scorekeeping is split into a number of categories including    “fastest typers”   <sup id="fnref20:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>  . Evaluation of the results is provided for the first and second pilot studies, focusing on the overall usefulness of the tags created. As the authors indicate, some of these findings have been published at an earlier date, but an overview is offered here. Ultimately, they conclude that    “using only verified user tags (i.e. where there was mutual agreement) for search gives poorer performance than search based on all user tags”   <sup id="fnref21:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>   and that search functionality improves with the addition of more tags. Like the  <em>Transcribe Bentham</em>  and  <em>PWD</em>  teams, the authors advocate for finding super-taggers rather than creating broad appeal, but they do acknowledge that Zooniverse offers an alternative model, which they describe as relying on a    “sustainable  army  of users”   <sup id="fnref22:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>  . That said, they do not elaborate how that army might have been engaged in the first place nor how  <em>Waisda?</em>  might emulate Zooniverse to create broader engagement. However, the research team have reuse and sustainability on their agenda, having published their code on GitHub, and connected with the European Film Gateway and  <em>Europeana</em>   <sup id="fnref23:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. As Lascarides and Vershbow argue in chapter 5, reusability of apps is necessary for long-term sustainability.</p>
<p>Chapter 8,  “ <em>Your Paintings Tagger</em> : Crowdsourcing Descriptive Metadata for a National Virtual Collection”  by Kathryn Eccles and Andrew Greg, describes the  <em>Your Paintings</em>  site hosted by the British Broadcasting Corporation (BBC), containing over 200,000 images of paintings in The Public Catalogue Foundation, and a metadata extraction project called  <em>Your Paintings Tagger</em>  (<a href="#www.tagger.thepcf.org.uk">www.tagger.thepcf.org.uk</a> formerly). While many articles in  <em>Crowdsourcing our Cultural Heritage</em>  directly invoke Zooniverse’s Galaxy Zoo and other scientific projects,  <em>Your Paintings Tagger</em>  (YPT) was built in partnership with Zooniverse, and  <em>Galaxy Zoo</em>  user motivations have been compared directly with those of YPT participants by researchers at Oxford and the University of Glasgow (the current chapter builds on previous work undertaken by Greg and Eccles). Not all Zooniverse components were deployed in the YPT, for example this project does not have a social forum or other mechanism whereby volunteers and experts can interact. It was only through surveys that the project team learned of volunteers’ desire for a social space. Like Causer and Terras of  <em>Transcribe Bentham</em> , Greg and Eccles conclude that because most tags are contributed by a small cohort of volunteers, more should be done to engage and retain additional ‘super-taggers’, though the authors do gesture to the prospect that the threshold for agreement among taggers could be lowered and that paintings shown by some logic such as artist or time period might be more engaging than the default Zooniverse mechanism for presenting images, which is random.</p>
<p>The authors spend some early pages of the chapter discussing the complex negotiations between experts at the BBC, participating GLAMs and the University of Glasgow, who tried to pin down a suitable metadata format for  <em>Your Paintings</em> , before the introduction of a crowdsourced dimension. Other GLAM practitioners may find this account useful when considering the institutional barriers they may need to overcome when trying to make collections more discoverable. It is notable however, that while Greg and Eccles, like others in this volume, suggest that crowdsourcing is more cost effective than traditional metadata improvement projects, time and money are still needed to support communities. Indeed, even without a social forum feature, which generally entails more staff time to maintain than projects without a social dimension, YPT is currently unavailable due to a funding shortage. The project owners are keen to implement changes to the platform and a call for donations (a form of crowdfunding) is prominent on the home page. Rather than conceiving of crowdsourcing as a cheap alternative to metadata extraction, we should focus on other benefits, for example the prospect of engaging people in new ways with collections they might not otherwise encounter; and in the case of tagging developing alternative languages for searching that enable broader access to online collections. Greg and Eccles do in fact report on these benefits throughout their piece, and acknowledge that at the rate of tagging reported in 2013 it would take a long time for the project to come to completion. Project owners will continue to experience the same disappointments over cost effectiveness and speed so long as the (narrow) messaging around the value of crowdsourcing remains the same.</p>
<p>Part II,  “Challenges and Opportunities of Cultural Heritage Crowdsourcing” , contains four essays that address different aspects of the relationships between GLAMs and volunteers. Alexandra Eveleigh’s thoughtful and carefully balanced piece,  “Crowding Out the Archivist? Locating Crowdsourcing within the Broader Landscape of Participatory Archives” , acknowledges some of the common concerns of archivists and domain specialists in engaging with the crowd — notably concerns about authority and accuracy — while also advocating for careful engagement with online communities. Eveleigh examines the    “tension inherent between a custodial instinct to control context and authenticity, and a desire to share access and promote usage”   <sup id="fnref24:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>   of collections, and suggests that the reality of participatory archival practices will cause neither the demise of the archivist specialist nor the complete revolution of their role, but rather that the ever-changing landscape of participatory technologies and projects will enable the curator/gatekeeper role to evolve and to place greater emphasis on the perspective of the user/volunteer. Eveleigh’s piece brings many of the bubbling concerns from the case studies to the fore, and serves as a strong yet encouraging critique of GLAM practice with regards to crowdsourcing.</p>
<p>Stuart Dunn and Mark Hedges’  “How the Crowd Can Surprise Us: Humanities Crowdsourcing and the Creation of Knowledge”  is a follow on from their  “Crowd-Sourcing Scoping Study: Engaging the Crowd with Humanities Research”   <sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>. In the present essay they offer a series of definitions and typologies of crowdsourcing activities, which may be helpful for researchers interested in terminology and theories of crowdsourcing. They explore distinctions between crowdsourcing for business versus epistemic purposes, arguing that humanities crowdsourcing, while it may draw on mechanical micro-tasking approaches common in business crowdsourcing projects, can also provide the circumstances for knowledge co-creation, interpretation, creative responses, editing, investigation, and new research. They conclude that because a small number of people undertake the bulk of tasks in any given crowdsourcing project,    “successful uptake of contributor effort in humanities crowdsourcing will be dependent on finding pockets of enthusiasm and expertise for specific areas”   <sup id="fnref25:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>  .</p>
<p>The penultimate chapter is  “The Role of Open Authority in a Collaborative Web”  by Lori Byrd Phillips, which begins by quoting Jane McGonigal’s  “Gaming the Future of Museums”  lecture <sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>, and striking a note common to almost all of the other pieces: that there is pent-up knowledge in museums and pent-up expertise in the public that can be married up for the benefit of all involved. Perhaps more clearly than the other authors in the volume, Byrd Phillips argues that the increase in user-generated content created a    “renewed need for authoritative expertise in museums”   <sup id="fnref26:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>  . This argument essentially turns the more familiar paradigm — that there are collections that cannot be unlocked without volunteer effort — inside out. The piece echoes ideas put forward by Eveleigh and draws on additional theoretical perspectives, including the Reggio Emilia approach to learning, a child-led educational model that emerged in post-WWII Italy. Byrd Phillips argues that this model may be particularly useful for museums wishing to create    “opportunities for community learning and collaboration”   <sup id="fnref27:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>  .</p>
<p>The final essay, Trevor Owens’s  “Making Crowdsourcing Compatible with the Missions and Values of Cultural Heritage Organisations”  closes the volume on a confident and even utopian note, declaring that crowdsourcing should be a core function of the way in which GLAMs serve the public:    “crowdsourcing is one of the most valuable experiences we can offer our users”   <sup id="fnref28:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>  . He argues that crowdsourcing, when done well, can engage users with content in active and meaningful ways – not as mechanical transcribers for instance but as ‘authors of our historical record’, who contribute their passion and time to tasks that on the one hand open up collections for new kinds of investigation, and which also enable users to encounter primary material more deeply than if they were simply browsing an online catalogue.</p>
<p>Owens is a rhetorically skillful proponent of what he calls ethical crowdsourcing which is as much focused on the experience of patrons or volunteers as on cultural heritage outcomes. He touches on the work of Surowiecki, the examples of  <em>ReCaptcha</em> ,  <em>BabelZilla</em>  – an online community for developers and translators of extensions for Firefox web browser – and  <em>Galaxy Zoo</em> . Of the latter he argues:    “all the work of the scientists and engineers that went into those systems are part of one big scaffold that puts users in a position to contribute to the frontiers of science through their actions on a website, without needing the skills and background of a professional scientist”   <sup id="fnref29:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>  . His concept of scaffolding is particularly relevant in light of two new platforms, which enable anyone to create their own free project: <a href="#www.zooniverse.org/lab">www.zooniverse.org/lab</a> ( <em>Zooniverse Project Builder</em> ) and <a href="#https://crowdcrafting.org/">https://crowdcrafting.org/</a> ( <em>crowdcrafting</em> ), both launched after the publication of Ridge’s volume. Finally, as if in answer to some of the contradictory statements about cost-effectiveness and emerging modes of engaging with the public that have been put forward by various case study authors in Part I, Owens argues that    “in the process of developing [&hellip;] crowdsourcing projects we have stumbled onto something far more exciting than speeding up or lowering the costs of document transcription”   <sup id="fnref30:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>  . He closes with an example of transcription of Civil War diaries from the University of Iowa Libraries’ DIY history site <a href="http://diyhistory.lib.uiowa.edu/">http://diyhistory.lib.uiowa.edu/</a>, whose former head of Digital Library Services, Nicole Saylor, sees transcription as a wonderful by-product of a process of engaging the public with history. This model is a more realistic image of what GLAMs can hope to achieve by deploying crowdsourcing.</p>
<p><em>Crowdsourcing our Cultural Heritage</em>  has much to offer a range of researchers and GLAM practitioners both in terms of particular examples of projects focused on a diverse range of media, and in terms of the evolving and complex debates about the role of crowdsourcing and public engagement in GLAMs and academia. This is an excellent starting place for anyone interested in studying crowdsourcing or embarking upon or improving existing projects.</p>
<h2 id="acknowledgements">Acknowledgements</h2>
<p>This review was written while I was a British Academy Postdoctoral Fellow at the University of Oxford and Pembroke College, and the Zooniverse Humanities Principal Investigator in 2017, prior to my relocation to the Library of Congress, Washington DC, where I serve as Senior Innovation Specialist and Community Manager for By the People, a new crowdsourcing initiative (crowd.loc.gov).</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Surowiecki, James.  <em>The Wisdom of Crowds: Why the Many are Smarter than the Few and How Collective Wisdom Shapes Business, Economies, Societies, and Nations.</em>  Doubleyday and Co., New York (2004).&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Ridge, Mia (ed).  <em>Crowdsourcing Our Cultural Heritage</em>  Ashgate, Farnham (2014).&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref3:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref4:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref5:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref6:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref7:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref8:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref9:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref10:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref11:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref12:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref13:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref14:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref15:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref16:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref17:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref18:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref19:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref20:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref21:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref22:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref23:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref24:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref25:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref26:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref27:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref28:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref29:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref30:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Owens, Trevor.  “Crowdsourcing Cultural Heritage: The Objectives are Upside Down.”   <a href="#http://www.trevorowens.org/2012/03/crowdsourcing-cultural-heritage-the-objectives-are-upside-down/">http://www.trevorowens.org/2012/03/crowdsourcing-cultural-heritage-the-objectives-are-upside-down/</a> (2012).&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Hedges, Mark and Stuart Dunne.  “Crowd-Sourcing Scoping Study: Engaging the Crowd with Humanities Research.”   <a href="#https://kclpure.kcl.ac.uk/portal/en/publications/crowdsourcing-scoping-study(abf40e6d-7ece-4d76-94d2-36f91b5707ff).html">https://kclpure.kcl.ac.uk/portal/en/publications/crowdsourcing-scoping-study(abf40e6d-7ece-4d76-94d2-36f91b5707ff).html</a> (2012).&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>McGonigal, Jane.  “Gaming the Future Museums.”  Lecture presented at Newseum, Washington, D.C., hosted by American Alliance of Museums (AAM), <a href="#https://www.youtube.com/watch?v=zJ9j7kIZuoQ">https://www.youtube.com/watch?v=zJ9j7kIZuoQ</a> (2008, published 2012.)&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry><entry><title type="html">DH2018: A Space to Build Bridges</title><link href="https://rlskoeser.github.io/dhqwords/vol/13/1/000413/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://rlskoeser.github.io/dhqwords/vol/13/1/000413/</id><author><name>Molly Nebiolo</name></author><author><name>Gregory J. Palermo</name></author><published>2019-04-26T00:00:00+00:00</published><updated>2019-04-26T00:00:00+00:00</updated><content type="html"><![CDATA[<h1 id="dh2018-a-space-to-build-bridges">DH2018: A Space to Build Bridges</h1>
<h2 id="introduction">Introduction</h2>
<p>In June 2018, the Digital Humanities annual conference (DH2018) was held in the Global South for the first time. The hosts of the conference, the Red de Humanidades Digitales (RedHD), La Universidad Nacional Autónoma de México (UNAM) and El Colegio de México (Colmex), created an event that was refreshingly diverse in languages and perspectives that reflected the cultural richness of the conference’s location. Its theme, Puentes / Bridges, emphasized a reexamination of ongoing trends in archival work and academic work, with the hope of moving digital humanities a step toward inclusivity, accountability, and decolonization. DH2018 was the first time, as well, that the international DH conference featured keynote addresses delivered by two women of color working in the Global South: Janet Chávez Santiago and Schuyler K. Esprit, an indigenous language activist, engaged primarily beyond the academy, and a scholar of Caribbean studies, respectively. Chávez Santiago’s opening keynote addressed the conference’s themes by using digital humanities to bridge the gap between the documentation of modern languages and indigenous languages, particularly Zapotec. She noted how    “being digital means being tied to an ancestral world”   <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>  . This strongly depicts what most of the conference’s programming aimed to address when highlighting minority-centered digital projects and decolonizing digital spaces over the course of its four days (please see the conference program for more details). In her closing keynote, Esprit echoed these themes, talking about how digital humanists can collaborate with their communities to confront global narratives about climate change, and its locally-experienced effects, using methods of  “technological disobedience”  and community-sourced narratives <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>.</p>
<p>Other journals in the humanities have published conference reviews that attempt to amalgamate the work represented at meetings in those journals’ fields.  <em>ESQ</em> , for example, publishes an annual  “The Year in Conferences”  feature, in which 19th-century scholars of American Literature and Culture who attended the year’s meetings come together to relate an overview their happenings <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>. In the weeks leading up to the DH2018 conference, Northeastern University Professors Élika Ortega and Ryan Cordell secured funding from the NULab for Texts, Maps, and Networks at Northeastern University to support a similar effort for Digital Humanities Quarterly. Two graduate students were funded to attend DH2018, document their experiences, and write a report under Ortega and Cordell’s mentorship. [Editors’ note: DHQ welcomes proposals for reports on other conferences in digital humanities; please contact the editors at editors@digitalhumanities.org.]</p>
<p>Writing as the two graduate students who attended the conference, we hope to reflect on the conference’s events in order to narrate and analyze notable trends in the concepts and techniques that emerged. We come from the History and English departments at Northeastern University and are second-year and fourth-year PhD students, respectively. Having been involved with the NULab’s Graduate Certificate Program in Digital Humanities and various digital projects on campus, we offer different perspectives from these standpoints in the University’s digital humanities community. We engage in personal reflection and evaluation, as well as journalistic reporting and synthesis to determine where the field seems to be headed and where it may need improvement.</p>
<h2 id="molly-nebiolo">Molly Nebiolo</h2>
<p>As a historian of the early modern Atlantic world, I was constantly impressed by the incorporation of Latin American and non-western cultures into the conference program. No other conference that I have attended thus far in my academic career was structured like this. From the mixed language panels in English and Spanish, to completely non-English panels, I had the experience of being challenged with my knowledge of Spanish to understand and converse in what is, for me, a second language. I kept reflecting on the constant inclusivity of these panels to non-Spanish speakers: the encouragement to whisper to one another for translation, the motivation to speak up if something needed to be repeated or reworded. These moments left me thinking back to any other conference I have attended and how inclusivity of multiple languages for presenting academic work had never been present at the conference or presentation level for non-English speakers. It is commonly assumed that English is the only language to be used at an international academic conference. DH2018 directly challenged that (all presenters were encouraged to use the Translation Toolkit), humbling me as someone who never had to confront this truth before.</p>
<p>In the English language panels, the themes that repeatedly appeared were those of decolonization, indigeneity, and sustainability of projects. For my own work both as a historian and my work for the digital humanities program at NU, I gravitated towards panels on mapping and discussing the archive as a form of representation, like  “Maps, Networks and Data” ,  “Mapas y Territorios” , and  “Bridging Divides, Colonial Archives” . My reflections on these panels cover only a small segment of themes that were discussed as a whole at DH2018. In one of the earlier panels I attended,  “Bridging Cultures through Mapping Practices: Space and Power in Asia and America” , there was constant confrontation of the idea of creating a spatial temporal narrative, one that GIS tools could help define with georeferenced layers of data on a location placed over areas of geographical space. The panel covered mostly spatial understandings of Shanghai and Korean geographies that underlined the difficulties of defining space in a digital manner. There are discrepancies in what is present on a map, how things are defined, and whether or not these two rhetorics are explicit with keeping the integrity of diverse places. By noting the challenges and realities of creating digital maps, Cecile Armand, Christian Henriot, and Sora Kim in particular created discussion with their presentations, instead of solely presenting on aesthetically pleasing maps and data <sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>.</p>
<p>Other mapping panels, like  “Mapas y Territorios” , continued to ask this kind of question, as well. How can a digital snapshot of a place or space best represent the cultural layers, struggles, and interactions of boundaries and the people who made them? How important are boundaries in the construction of a digital archive around mapping? How can we make the understanding of boundaries more accurate to the complicated histories that they hold? These inquiries ran deep into the decolonizing panels, like  “Bridging Divides, Colonial Archives” , because there was this repetitive reflection on how to be more representative of non-white histories, voices and stories.</p>
<p>The panelists over the course of the conference tried to grapple with their own projects to better understand how digital humanities tools (mapping tools, archive creating sites, network analyses) can be harnessed to incorporate decolonized voices, particularly in  “Bridging Divides, Colonial Archives”  and  “Social Justice, Data Curation, and Latin American &amp; Caribbean Studies” . As someone who is routinely confronting the white history of the early Americas and struggles to find a comparable indigenous perspective, I was comforted that these spaces were made at DH2018 to discuss these decolonizing questions. However, the sheer size of the conference created many compressed panel sessions that made it difficult to digest and participate fully in such discussions. Commonly with four or even five presentations condensed into an hour and a half slot, many of these rich presentations felt more like flash-talks and less like a fully formed action plan or set of ideas to answer the questions they raised. There were so many people vying for time to present and make their voices heard that I left the conference full of thought-provoking questions, than real clarity on how the field could build bridges to fix the problem of the colonized archive and the digitization of white narrative.</p>
<p>There were some spaces that were able to cultivate these discusses outside of the presentation spaces. On the first night, there was a lively reception after the keynote speech that was a space for many introductions and discussions around upcoming panels, and a similar event abutted the end of the conference for last minute exchange of emails, questions, and networking. However, the poster session and the mid-week fiesta were the two instances where networking and conversation seemed richest. Two conference rooms were sandwiched with posters and presenters, which created a labyrinth-like environment for participants to weave through and absorb. I found that many of the posters<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>  focused on early-career and early-stage projects that were aiming to produce databases or markup languages for underrepresented cultures and languages in the digital humanities field, like  “Hispanic 18th Connect” ,  “Codicological Study of pre High Tang Documents from Dunhuang: An Approach using Scientific Analysis Data” , and  “Devochdelia: el Diccionario Etimolójico de las Voces Chilenas Derivadas de Lenguas Indíjenas Americanas de Rodolfo Lenz en versión digital”  (all from session 1). Another major theme was the fact that most of these graduate-student led presentations strongly pointed out ways they were hoping to extend and sustain their projects. In a Digital History class at Northeastern University led by Ben Schmidt, we ended the course grappling with this issue of sustainability of an adolescent digital humanities project on the web. How this situation is addressed is a part of how the field of digital humanities progresses into the future. It was great to see these conversations proliferate elsewhere. The fiesta was another space in which these conversations seemed to expound. Situated in the plaza of the Universidad del Claustro de Sor Juana, the event reverberated with the hum of conversations about the field, the torrential rain of a passing storm, toasts over tequila shots, and music from the mariachi band. While the conference presentations were the backbone to DH2018, it was the spaces for networking and conversation, like those of the poster session, keynote reception and fiesta, that fleshed out this experience for participants.</p>
<p>There is significance in hosting the first Global South digital humanities conference in Mexico City. The clashing and blending of cultures to create a stratum of various ethnicities and traditions over centuries has pushed the urgency of decolonizing academic, digital perspectives at the conference. The various language presentations, the emphasis of getting comfortable in these uncomfortable non-English spaces, and the Mexican culture that permeated the conference were necessary to make this conference as successful as it was. As a historian, I appreciated the integration of the commonly marginalized voices- as presenters, actors in the digital projects, and with their languages being digitally standardized. As a student just entering the world of digital humanities, the conference left me with hope that the field is progressing quickly in these aspects. There seemed to be plenty of participants, particularly those early in their careers, that will hold the annual conference accountable to make these themes sustainable in the future. The crowded schedule of many of the panels I sat in on did result in hurried and cramped presentations. In future conferences, it would be interesting to see if the conference could diversify the types of panels that are held, so there could be more room for discussion during the conference itself. For example, the History of Science Society’s annual conference offers a selection of one-hour long discussion round-tables around midday, lunchtime talks, and evening graduate student-led colloquia on field-specific pedagogy in addition to poster panels, classic presentations, and keynote receptions. This allowed for more opportunities for presenters to speak outside of the classic presentation style, and varied the schedule for many of the attendees, as well. Overall, I found DH2018 to be a promising turning point for how I understood and experienced digital humanities and academic conferences.</p>
<h2 id="gregory-palermo">Gregory Palermo</h2>
<p>As a digital humanist working in the US-based field of writing and rhetoric, I came to DH2018’s emphasis on bridging communities with certain disciplinary histories in mind. My research at the time, while writing my comprehensive exams, was on the rhetoric of disciplinarity: specifically, how members of fields reshape the stories<sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>  they tell about themselves by whom and how they cite in the same space. Both digital humanities and writing studies have long traditions of using digital tools and methods for research and pedagogy, while attending to their processes and ethics. The two fields, however, have remained largely insulated from one another, despite their resonant work and common values <sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>  <sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>. The exclusion of writing studies until fairly recently from  “big tent DH”  has inspired some hard feelings in the former, along with an accompanying impulse to draw boundaries around areas of research and claim these academic  “territories”   <sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup>.</p>
<p>Since DH2018 was the first international digital humanities conference to be held in the Global South, the conference’s planning materials drew particular critical attention to the borders dividing and constructing geopolitical territories. This frame for the conference was further amplified by its location, in Mexico City, and timing, just after news broke about the separation and detention of families at the US/Mexico border. As local conference organizer Isabel Galina noted in her opening address, now is the time to build bridges rather than walls, and our field can and should apply digital methods to impact international events. Conference attendees had the opportunity to intervene in the incident as part of two hackathons — held by Alex Gil and Roopika Risam and generously supported, with food and time, by the HASTAC community<sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup>  — that contributed to an early version of the Separados / Torn Apart project.<sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>  The project’s initial visualization of the infrastructural and financial footprints of US Immigration and Customs Enforcement was the product of volunteer<sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>  rapid-response prototyping, work undertaken the week or so immediately preceededing the conference. The project&rsquo;s visualizations and accompanying reflections narrate this team’s numerous rhetorical and ethical decisions: choices surrounding the access, curation, and reontologization of data, as well as its representation for multiple audiences and purposes.</p>
<p>While these hackathons were not a central or initially scheduled part of DH2018, I begin my discussion of the conference by mentioning them because they epitomize the conference’s focus on boundary-drawing and -crossing in all its forms: to quote the conference web site, of borders  “cultural, technological, political, and ideological” .<sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup>  The coming together of project&rsquo;s collaborators to work on a matter of emergent concern exemplifies a focus on local intervention that we increasingly value. This type of intervention requires literacies that draw from multiple experiences and backgrounds. Building a bridge, to invoke a term used in decolonial theory,  “enunciates”  the communities being bridged, rather than obscuring their difference under a totalizing tent. The choice of bridge-building by the conference organizers as a metaphor also — even more importantly, to my mind — calls our attention to the practical concerns of that work. If there were a single defining feature of the papers given and discussions happening across DH2018’s concurrent sessions, it was a focus on how academics might responsibly use digital methods to redraw boundaries.</p>
<p>A panel on  “Cultural Representation”  in digital work (PS-04) grappled with the role of researchers in serving the people with whom we are collaborating or whom we are studying. Presenters on their work with Somali, Haitian, Filipinx, and Latinx communities called attention to the time- and labor-intensive efforts to build a rapport and trust with them, in light of the context-dependent, multidirectional power differentials between the academy and the public. As Anelise H. Shrout put it succinctly on the conference’s Twitter backchannel, a common sentiment was that digital humanists  “are beholden to the communities we work with”   <sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup>. A panel on  “Digital Decolonizations”  convened by Allison Margaret Bigelow emphasized a need for  “data sovereignty”  in scholarship involving indigenous communities, asking who on the ground controls access to data and the purposes to which it is put (PS-24). In PS-04, Mahnke noted that some data should simply not be digitally accessible, a reality with which digital humanists should make peace despite our culture of openness <sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup>. It was promising to see her draw, at a digital humanities conference, from rhetoric’s body of reflective scholarship on community-engaged research;<sup id="fnref:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup>  her discussion of her work with a Filipinx community, sustaining their stories in digital spaces, signals a new generation of work that bridges rhetoric with DH.</p>
<p>Other conversations reflected on how we are beholden to our communities of colleagues and students. Deb Verhoeven, in her talk about using network analysis techniques to locate Gender Offenders in Australian digital research funding networks (Panel LP-13), termed a shift in digital research from mere  “counting”  to  “accountability” . For Verhoeven, the value of digital (and especially quantitative) methods is  “diagnostic” : to address and  <em>change</em>  the academic landscape in front of us as we describe its crises of representation and access <sup id="fnref:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup>. Brandon T. Locke, in a panel on undergraduate digital pedagogy (SP-05), broadened our usual understanding of access — e.g., to platforms and infrastructures — to include access to critical data literacy. Locke argued that it is not enough to make data available or point students to it without also teaching them about how to understand and manipulate its structures. Adding to Taylor Elyse Mills’ numerous and lucid recommendations for supporting undergraduate research in the classroom <sup id="fnref:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup>, Locke related his decision to give his students uncleaned data, exposing them in the classroom to the  “messy labor”  we know to go into data visualization and analysis. Locke’s poster on his effort to promote  “Civic Data Literacy”  with an  “Endangered Data Week”  joined multiple posters, in the conference’s two poster sessions, on related topics, such as data ontologies and linked open data (Locke). Data sustainability was a topic, as well, on a panel about  “Project Afterlives”  and  “Research Data for Pedagogical Use”  (PS-32). Presenter Megan Finn Senseney lamented the scarcity of raw data in a field with so much curated data — her pedagogical solution is to  “salt”  the data she uses with her students <sup id="fnref:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup>.</p>
<p>Still other conversations stressed the modes by which local interventions can create larger change in the academy. Danica Savonick and Lisa Tagliaferri, as part of a panel on  “Sustainability and Institutions”  (SP-18), used text analysis techniques to reveal patterns in the purposes for higher education that universities currently articulate in their mission statements <sup id="fnref:20"><a href="#fn:20" class="footnote-ref" role="doc-noteref">20</a></sup>; their own stated purpose was to provide fodder for  “advancing institutional change” .<sup id="fnref:21"><a href="#fn:21" class="footnote-ref" role="doc-noteref">21</a></sup>  In a panel addressing Institutional Infrastructures (LP-02), James W. Malazita brought science and technology studies’ understanding of  “knowledge structures”  and  “epistemic structures” , along with Michel de Certeau’s distinction between  “strategies”  and  “tactics”   <sup id="fnref:22"><a href="#fn:22" class="footnote-ref" role="doc-noteref">22</a></sup>, to attend to Matthew Kirschenbaum’s portrayal of  “digital humanities”  as a  “tactical term”   <sup id="fnref:23"><a href="#fn:23" class="footnote-ref" role="doc-noteref">23</a></sup>  <sup id="fnref:24"><a href="#fn:24" class="footnote-ref" role="doc-noteref">24</a></sup>. A panel on  “Precarious Labor”  convened by Arianna Ciula (PS-22) generated discussion on how we might tactically encourage and support the scholarship of those in job roles supporting others’ work — roles that universities too often impel us to casualize second to a narrow of production. A particularly productive panel discussion on project management in digital humanities (PS-14) covered the labor and turnover of projects’ largely contingent staff; Micki Kaufman led the charge, there, of providing pragmatic advice for project managers (many of whom are graduate students) and those who employ them. Lisa Rhody and her team’s poster on the  “The Digital Humanities Research Institute”  at CUNY offered similar infrastructural support, modeling an opportunity for bringing digital humanities education to one’s home campus <sup id="fnref:25"><a href="#fn:25" class="footnote-ref" role="doc-noteref">25</a></sup>. Matthew Gold et al. were as focused on getting a sense of people’s ongoing needs for their publishing platform Manifold as they were with presenting its current possibilities <sup id="fnref:26"><a href="#fn:26" class="footnote-ref" role="doc-noteref">26</a></sup>. Paige C. Morgan reminded us, however, that unbridled openness is not always a virtue within the academy either, drawing attention to the work that goes into data curation and from which others benefit; she conspicuously supports scholars, especially in precarious positions, who choose to keep their datasets initially private while working on them, in order to avoid being digital humanities’ version of  “scooped”   <sup id="fnref:27"><a href="#fn:27" class="footnote-ref" role="doc-noteref">27</a></sup>.</p>
<p>Counterpoints like these surfaced an ongoing tension in digital humanities around questions of openness and access: between the productive opportunities that access can bring and our recognition of the differential impact of that access on others. This tension is further complicated when the productive opportunity is, in fact, an opportunity for bridge-building. For example, Jin Gao offered to share her subtanital dataset when presenting on her team’s work using analyses of citation and social networks for broadly  “Visualizing the DH Community”  (LP-20; Gao et al. 2018). Others present at that panel, including Ciula, pointed out that this body of literature from journals published primarily in English represented only a subset of the community <sup id="fnref:28"><a href="#fn:28" class="footnote-ref" role="doc-noteref">28</a></sup>. Panel presenter Fabio Ciotti pointed to Domenico Fiormonte’s work <sup id="fnref:29"><a href="#fn:29" class="footnote-ref" role="doc-noteref">29</a></sup> on the geopolitics of knowledge in digital humanities, using it to critique the North American-centrism of the field’s received disciplinary historiography <sup id="fnref:30"><a href="#fn:30" class="footnote-ref" role="doc-noteref">30</a></sup>. Open data, here, could be used in our continual efforts de-center English-speaking digital humanities, as long as we attend to whose prerogative it is to assemble, use, and share the data.</p>
<p>The disproportionate space, especially digital, afforded to English-speaking conference attendees is one that Ernesto Priego quantified in his exploratory analysis of DH2018 tweets <sup id="fnref:31"><a href="#fn:31" class="footnote-ref" role="doc-noteref">31</a></sup>.<sup id="fnref:32"><a href="#fn:32" class="footnote-ref" role="doc-noteref">32</a></sup>  Some live tweeters devoted their efforts to translating panel presentations, in addition to providing the synthesis and talkback common to digital humanities conferences (see Ross, et al.). This digital intervention complemented the in-person  “whisper[ed]”  translation that Molly Nebiolo has noted above among panel attendees. It constituted an effort to somewhat temper the dominance of English in the conference’s archival record,<sup id="fnref:33"><a href="#fn:33" class="footnote-ref" role="doc-noteref">33</a></sup>  especially for members of the digital humanities community who could not make it physically to the conference. The ACH bolstered the existing, remotely organized initiatives to facilitate conference attendance: adding to ADHO’s graduate student travel bursaries so more young scholars to travel to experience the conference first-hand, while offering formalized mentorship programs like the Newcomer’s Dinners that pair conference  “veterans”  with  “newbies” . That said, we can continue to use our digital expertises to pluralize the conference’s digital spaces as well as its physical ones — spaces, like this conference review genre, that I hope will provide the means for a more inclusive future for the conference and the field.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Santiago, Janet Chávez. “Tramando La Palabra / Weaving the World.” DH 2018. Sheraton Maria Isabel, Mexico City, 26 Jun. 2018. Keynote Address. YouTube, ElClaustro TV, 1 Aug. 2018, <a href="https://youtu.be/mQzyDlTB070">https://youtu.be/mQzyDlTB070</a>.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Esprit, Schuyler K. “Ponencia de clausura: Experimentación Digital, Ciudadanía Valiente y Futurismo Caribeño / Closing Keynote: Digital Experimentation, Courageous Citizenship and Carribean Futurism.” DH 2018. Sheraton Maria Isabel, Mexico City, 26 Jun. 2018. Keynote Address. YouTube, ElClaustro TV, 1 Aug. 2018, <a href="https://youtu.be/K812bBfBUAI">https://youtu.be/K812bBfBUAI</a>.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Ach, Jada, et al. &ldquo;The Year in Conferences — 2017.&rdquo;  <em>ESQ: A Journal of Nineteenth-Century American Literature and Culture</em> , vol. 64 no. 1, 2018, pp. 133-197. doi:10.1353/esq.2018.0003&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Armand, Cecile, Henriot, Christian, and Sora Kim. “Bridging Cultures Through Mapping Practices: Space and Power in Asia and America.” DH2018. Sheraton Maria Isabel, Mexico City, 27 June 2018. Conference Presentation.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>For a full list of posters, see the conference program&rsquo;s listings of <a href="https://www.conftool.pro/dh2018/index.php?page=browseSessions&amp;abstracts=show&amp;form_session=368&amp;presentations=show">session 1 posters</a> and <a href="https://www.conftool.pro/dh2018/index.php?page=browseSessions&amp;abstracts=show&amp;form_session=402&amp;presentations=show">session 2 posters</a>.&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>Chávez Santiago’s keynote highlighted a Spanish-language pun that portrays storytelling as a mode of  “wefting”  about a textile’s warp, alternatively translated as  “plotting”  (7:17).&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>Ridolfo, Jim, and William Hart-Davidson, eds. “Introduction.”  <em>Rhetoric and the Digital Humanities</em> . University of Chicago Press, 2015, pp. 1-12.&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>Palermo, Gregory. “Transforming Text: Four Valences of a Digital Humanities Informed Writing Analytics.”  <em>Journal of Writing Analytics</em> , vol. 1, Sept. 2017. <a href="https://journals.colostate.edu/analytics/article/view/135">https://journals.colostate.edu/analytics/article/view/135</a>.&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>Carter, Shannon, et al. “Beyond Territorial Disputes: Toward a ‘Disciplined Interdisciplinarity’ in the Digital Humanities.”  <em>Rhetoric and the Digital Humanities</em> , edited by Jim Ridolfo and William Hart-Davidson, University of Chicago Press, 2015, pp. 33–48.&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>See <a href="https://www.hastac.org/blogs/danicasavonick/2018/07/08/torn-apartseparados-hackathon-hastac-meet-recap">https://www.hastac.org/blogs/danicasavonick/2018/07/08/torn-apartseparados-hackathon-hastac-meet-recap</a>.&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>See <a href="https://xpmethod.plaintext.in/torn-apart/index.html">https://xpmethod.plaintext.in/torn-apart/index.html</a>.&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p>In addition to Gil and Risam, this initial team included Manan Amed, Moacir P. de Sá Pereira, Sylvia A. Fernández, Merisa Martinez, and Linda Rodriguez. For more information see the project site: <a href="http://xpmethod.plaintext.in/torn-apart/credits.html">http://xpmethod.plaintext.in/torn-apart/credits.html</a>.&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13">
<p>See the conference site at <a href="https://dh2018.adho.org/en/acerca-de/">https://dh2018.adho.org/en/acerca-de/</a>.&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14">
<p>Shrout, Anelise. @AneliseHShrout. “Another #DH2018 theme is the obligation of researchers to the people they study/work with/are supported by. We are beholden to the communities we work with. #DH2018.” Twitter, 28 Jun. 2018, 5:50 p.m., <a href="https://twitter.com/AneliseHShrout/status/1012455596359061504">https://twitter.com/AneliseHShrout/status/1012455596359061504</a>&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:15">
<p>Mahnke, Stephanie. “The Collective in the Individual: Digital Collaboration and the Filipinx Community.” DH2018. Sheraton Maria Isabel, Mexico City, 27 Jun. 2018. Conference Presentation.&#160;<a href="#fnref:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:16">
<p>See, e.g., <sup id="fnref:34"><a href="#fn:34" class="footnote-ref" role="doc-noteref">34</a></sup>.&#160;<a href="#fnref:16" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:17">
<p>Verhoeven, Deb, et al. “Solving the Problem of the ‘Gender Offenders’: Using Criminal Network Analysis to Optimize Openness in Male Dominated Collaborative Networks.” DH2018. Sheraton Maria Isabel, Mexico City, 28 Jun. 2018. Conference Presentation.&#160;<a href="#fnref:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:18">
<p>Mills, Taylor E. “Next Generation Digital humanities: A Response to the Need for Empowering Undergraduate Researchers.” DH2018. Sheraton Maria Isabel, Mexico City, 27 Jun. 2018. Conference Presentation.&#160;<a href="#fnref:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:19">
<p>Posner, Miriam. @miriamkp. “One solution: “salting” datasets so they have inconsistencies (I’ve done this too!) #DH2018 #DH_PS32.” Twitter, 29 Jun. 2018, 5:57 p.m., <a href="https://twitter.com/miriamkp/status/1012817265081880578">https://twitter.com/miriamkp/status/1012817265081880578</a>.&#160;<a href="#fnref:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:20">
<p>Savonick, Danica and Lisa Tagliaferri. “The Purpose of Education: A Large-Scale Text Analysis of University Mission Statements.” DH2018. Sheraton Maria Isabel, Mexico City, 29 Jun. 2018. Conference Presentation.&#160;<a href="#fnref:20" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:21">
<p>See a Twitter thread by Aaron R. Hanlon for how university mission statements could be useful for tenured faculty when advocating for their contingent colleagues <sup id="fnref:35"><a href="#fn:35" class="footnote-ref" role="doc-noteref">35</a></sup>.&#160;<a href="#fnref:21" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:22">
<p>Certeau, Michel de. The Practice of Everyday Life. 1984. Translated by Steven F. Rendall, 3 edition, University of California Press, 2011.&#160;<a href="#fnref:22" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:23">
<p>Kirschenbaum, Matthew. “Digital Humanities As/Is a Tactical Term.”  <em>Debates in the Digital Humanities</em> , edited by Matthew K. Gold, University Of Minnesota Press, 2012, pp. 415–28.&#160;<a href="#fnref:23" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:24">
<p>Malazita, James W. “Epistemic Infrastructures: Digital Humanities in/as Instrumentalist Context.”DH2018. Sheraton Maria Isabel, Mexico City, 27 Jun. 2018. Conference Presentation.&#160;<a href="#fnref:24" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:25">
<p>Rhody, Lisa, et al. “Expanding Communities of Practice: The Digital Humanities Research Institute Model.” DH2018. Sheraton Maria Isabel, Mexico City, 27 Jun. 2018. Conference Poster.&#160;<a href="#fnref:25" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:26">
<p>Gold, Matthew, et al. “Manifold Scholarship: Hybrid Publishing in a Print/Digital Era.” DH2018. Sheraton Maria Isabel, Mexico City, 27 Jun. 2018. Conference Poster.&#160;<a href="#fnref:26" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:27">
<p>Morgan, Paige. @paigecmorgan. “For what it&rsquo;s worth, I understand people wanting to work with other folks&rsquo; datasets, but I also fully support people, esp. grad students &amp; other folks in precarious positions, hanging on to their data for a while to work with it before sharing it. #DH2018.” Twitter, 29 Jun. 2018, 3:54 p.m., <a href="https://twitter.com/paigecmorgan/status/1012786393205141504">https://twitter.com/paigecmorgan/status/1012786393205141504</a>.&#160;<a href="#fnref:27" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:28">
<p>Ciula, Arianna. @ariciula. “It&rsquo;s great but let&rsquo;s remember that&rsquo;s one DH landscape pls - i.e. the one represented by those selected 3 journals in English #DH2018 #LP20.” Twitter, 29 Jun. 2018, 3:47 p.m., <a href="https://twitter.com/ariciula/status/1012784706109177856">https://twitter.com/ariciula/status/1012784706109177856</a>.&#160;<a href="#fnref:28" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:29">
<p>Fiormonte, Domenico. “Digital Humanities and the Geopolitics of Knowledge.”  <em>Digital Studies/Le Champ Numérique</em> , vol. 7, no. 1, Oct. 2017. <a href="http://www.digitalstudies.org">http://www.digitalstudies.org</a>, doi:10.16995/dscn.274.&#160;<a href="#fnref:29" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:30">
<p>Ciotti, Fabio. “Dall’Informatica umanistica alle Digital Humanities. Per una storia concettuale delle DH in Italia.” DH2018. Sheraton Maria Isabel, Mexico City, 29 Jun. 2018.&#160;<a href="#fnref:30" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:31">
<p>Priego, Ernesto. “Tweets per user_lang in a #DH2018 archive.” Everything is Connected. June 30, 2018. <a href="https://epriego.blog/2018/06/30/tweets-per-user_lang-in-a-dh018-archive/">https://epriego.blog/2018/06/30/tweets-per-user_lang-in-a-dh018-archive/</a>.&#160;<a href="#fnref:31" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:32">
<p>Since it is substantially more difficult and expensive to query the historical Twitter API than collecting tweets at their moment of publication, Priego has promised to publish the dataset he collected once he has anonymized it.&#160;<a href="#fnref:32" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:33">
<p>One can access DH2018’s twitter backchannel by searching Twitter for “#DH2018” along with the hashtag of a conference panel (e.g., #LP20).&#160;<a href="#fnref:33" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:34">
<p>Cushman, Ellen, and Terese Guinsatao Monberg. “Re-Centering Authority: Social Reflexivity and Re-Positioning in Composition Research.”  <em>Under Construction: Working at the Intersections of Composition Theory, Research, and Practice</em> , edited by Christine Farris and Chris M. Anson, Utah State University Press, 1998. doi:10.2307/j.ctt46nrqf.&#160;<a href="#fnref:34" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:35">
<p>Hanlon, Aaron. @AaronRHanlon. “3) Bring out your university’s mission statement and US News write-up. Ask why the people on the front lines of achieving those lofty ambitions are treated as disposable or undeserving of benefits and salary.” Twitter, 11 Sep. 2018, 3:38 a.m., <a href="https://twitter.com/AaronRHanlon/status/1039417894684319745">https://twitter.com/AaronRHanlon/status/1039417894684319745</a>.&#160;<a href="#fnref:35" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry><entry><title type="html">Managing 100 Digital Humanities Projects: Digital Scholarship &amp; Archiving in King’s Digital Lab</title><link href="https://rlskoeser.github.io/dhqwords/vol/13/1/000411/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://rlskoeser.github.io/dhqwords/vol/13/1/000411/</id><author><name>James Smithies</name></author><author><name>Carina Westling</name></author><author><name>Anna-Maria Sichani</name></author><author><name>Pam Mellen</name></author><author><name>Arianna Ciula</name></author><published>2019-04-26T00:00:00+00:00</published><updated>2019-04-26T00:00:00+00:00</updated><content type="html"><![CDATA[<h1 id="heading"></h1>
<p>Digital Humanities (DH) research has reached an inflection point. On the one hand it appears to be in robust health, with an active community spread around the world, well-attended annual conferences, several well-established centres of excellence (be they labs, institutes, or departments), and new initiatives appearing on a regular basis. Activity is particularly strong in the United Kingdom, North America, and Europe, with significant work being done in Asia and Australasia, and important new initiatives developing in South America and Africa. University courses are proliferating at graduate and undergraduate level, and advances are being made to pedagogy <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. Intermittent criticism of the field is a sign of increasing intellectual vitality, as colleagues in neighbouring disciplines question its popularity and interrogate its intellectual, ideological, and ethical foundations <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>  <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>. This activity has appeared at the same time as the notion and utility of DH infrastructure has been questioned <sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>, and project teams have been forced to explore ways projects can not only be sustained but elegantly ended <sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>. Despite inheriting a relatively deep tradition, we are only beginning to understand the dense entanglements that accrue over time between digital humanists, administrators, and the web servers, programming languages, and tools, we use to produce our publications.</p>
<p>While technical digital humanities teams now have a vastly more sophisticated range of options than previous generations, including corporate-grade cloud services and free online services, this has done little to ease the problem of maintainability or sustainability - especially for high quality digital scholarship. Idiosyncratic solutions to specific research questions in this emerging field have left us with a legacy corpus developed from the 1990s into the 2000s, which raises new challenges in terms of sustainability. Problems that have been deferred for years, sometimes decades, have become pressing. A generation of legacy projects that need maintenance but are out of funding have reached critical stages of their lifecycles, an increasingly hostile security context has made DH projects potential attack vectors into institutional networks, heterogeneous and often delicate technologies have complicated the task of maintenance, and an increasing number of emerging formats have made archiving and preservation yet more difficult. This presents a significant, and growing, challenge for the community – and one that needs to be  resolved by raising awareness of the issues, evolved management of digital humanities infrastructure, attention to the full lifecycle of projects, and inventive approaches to funding that extend the life and impact of valuable research by sharing costs across funding agencies and institutions. This article aims to contribute to that process by initiating a conversation and explaining the experience and some solutions implemented in King’s Digital Lab (KDL) but does not aim to present a straight-forward  “How To”  guide for other teams. The realisation of robust and holistic approaches to the maintenance of digital research outputs is a matter of some urgency, but no single solution will work for every digital humanities team.</p>
<p>It is clear, however, that sustainable management of digital outputs that have survived beyond their initial funding has become a major problem. It is time to admit our problems and share our conceptual and procedural solutions. Such projects, although of central importance to the wider field of digital humanities and humanities scholarship generally, present a range of challenges. In the academic and financial year 2016-2017<sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup> , KDL worked on 6–8 funded projects at any one time and was involved in external grant proposals with a total value of £26m (GBP), together with collaborators across a wide variety of universities and cultural heritage organisations in London, the United Kingdom, Europe, and the United States. This constituted the lab’s primary activity and is at the core of its raison d&rsquo;être. In the same year, however, the lab completed assessment (followed by archiving, migration, or upgrade) of ~100 digital humanities projects undertaken over twenty years of activity at King’s College London and inherited from earlier instantiations of DH, including the Centre for Computing in the Humanities (CCH) and the Centre for eResearch in the Humanities (CeRch). Many projects were inherited from the Department of Digital Humanities (DDH), which the lab evolved from and has a close relationship to. This corpus of publications represents valuable and impactful research as well as significant investment from funding bodies, and research and heritage institutions. Humanities scholars rely on and make ongoing reference to the work contained in it, and it is increasingly being integrated into global Linked Open Data initiatives supported by libraries, archives, museums, and other digital humanities teams.</p>
<p>Finding a comprehensive and scalable approach to sustainable development in digital humanities labs is a non-trivial problem. Any solution must be tailored to the local environment and help support not only the complexity and range of digital scholarship, but financial and operational issues and more fundamental problems related to entropy of software systems and digital infrastructure. It also needs to allow for the fact that digital tools and infrastructure do not allow for perfect process, perfect archiving, or perfect security: at some point it is always necessary to retreat to principles of risk management and cost-benefit analysis. The work presented here involved coordination with technical specialists, researchers, administrative and financial university staff, and colleagues in IT and the library. The developed process helped KDL transition many digital humanities projects from an insecure to a sustainable basis, but the work is incomplete and will - in a fundamental sense - never end. Some projects, moreover, cannot be  “saved”  despite best intentions. Rather than aim for perfect process, KDL have chosen to accept archiving and sustainability as a permanent issue that requires ongoing care and attention. It has been added to the lab’s Software Development Lifecycle (SDLC) engineering process and is considered from our very first conversations with new project partners. Our experience is shared here to open a conversation and, rather than proposing simple solutions or demanding policy change, to invite discussion.</p>
<h2 id="kings-digital-lab-background">King’s Digital Lab: Background</h2>
<p>King’s Digital Lab (KDL) was launched in November 2015 at King’s College London. The lab evolved from the Centre for Computing and the Humanities (established 1995)<sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>  and the Centre for eResearch in the Humanities (established 2008), which merged to form the Department of Digital Humanities (DDH) in 2012. At the time of writing, DDH delivers 5 masters programmes, 1 PhD programme and an undergraduate programme to ~500 students and comprises ~40 academic staff. KDL was founded to increase digital capability and generate external grant income for digital projects within DDH, and across the Faculty of Arts &amp; Humanities as a whole: it is an independent Arts &amp; Humanities department in its own right, specialising in digital humanities software development but increasingly working with social scientists too. Team members sometimes act as Principal or Co-Investigators on grants but always work in unison with colleagues in other departments and/or institutions, implementing a model for digital humanities research at scale.</p>
<p>Unlike core academic departments, which engage in teaching as well as research, KDL is dedicated to research software engineering (RSE), and the implementation of the systems, infrastructure, tools, and processes that are needed to produce digital scholarly outputs. The lab has 12 permanent full-time staff to support these activities: research software analysts, engineers, designers, a systems manager, a project manager, and the director, and maintains its own server and network infrastructure. The team work in close collaboration with the university’s IT department and evolving University eResearch team. KDL’s research philosophy is evolving: it lies at the intersection of human research and technical systems, exploring and exploiting the creative synergies fostered by this encounter to push the boundaries of digital humanities forward. Taking an active interest in research methodology as well as inevitable business and technical realities, the lab embraces problems we believe are integral to the evolution and sustainability of the field.</p>
<p>Conflating the scholarly and operational aspects of the lab is both an overt act of historicisation - an acknowledgment of the reality of digital scholarship in early 21st century higher education - and a pragmatic response to the inherited and emergent issues outlined in this paper. The design and engineering of software and its supporting infrastructures is a problem that needs to be conceived as at once technical, political, economic, and  <em>human</em> . While the lab exists to engage in technical development, it is mandated to explore the epistemic and methodological implications of digital humanities development and can contribute to the broader field from a unique vantage point. Its institutional setting, technical expertise, and exposure to research problems that only time can generate positions it to explore fundamental issues of digital theory and method (including but not limited to digital entropy), while at the same time developing innovative methods for new research.</p>
<h2 id="legacy-portfolio">Legacy Portfolio</h2>
<p>The legacy portfolio supported by KDL is not unique, but significant for its range and scholarly value: it represents a key corpus in the history of digital humanities. Digital Humanities at King’s College is indebted to a group of people who were instrumental in developing a range of projects inherited by KDL. Colleagues like Harold Short, John Bradley, Willard McCarty, Charlotte Roueché, Marilyn Deegan, and Paul Spence, were involved in a remarkable array of projects of enormous scholarly value. In collaboration with PIs, both at King’s College and in partner institutions, their work provided the core of the lab’s inheritance including flagship projects such as  <em>Aphrodisias in Late Antiquity</em> ,  <em>Inscriptions of Roman Tripolitania</em> ,  <em>Henry III Fine Rolls</em> ,  <em>Jonathan Swift Archive</em> ,  <em>Jane Austen Manuscripts</em> ,  <em>The Gascon Rolls</em> ,  <em>The Gough Map</em> ,  <em>Inquisitions Post Mortem</em> ,  <em>Sharing Ancient Wisdoms</em> ,  <em>Prosopography of Anglo-Saxon England</em> ,  <em>Prosopography of the Byzantine World</em> ,  <em>The Complete Works of Ben Jonson</em> ,  <em>The Heritage Gazetteer of Cyprus</em> , and the  <em>Profile of a Doomed Elite</em> . Work in palaeography by Peter Stokes and Stewart Brookes has prompted a range of projects, including  <em>DigiPal</em> ,  <em>Models of Authority</em> ,  <em>Exon Domesday</em> , and the new  <em>Archetype</em>  framework.<sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>  This work was delivered in close collaboration with leading technical figures in digital humanities in the United Kingdom, including many who now work in, and with, King’s Digital Lab.<sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup></p>
<p>Of the 100 projects inherited by the lab, about half are either of exceptionally high quality or seminal in other ways. This is a sizeable  “estate”  to manage, but the authors are aware of at least one team managing considerably more projects, and more than one team who have suffered serious security breaches because of unmaintained applications. Teams struggling with the issues are located in the United States and Europe as well as the United Kingdom, suggesting any issues with policy and approach transcend national borders.<sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup>  Such circumstances entail a considerable moral bind: either ignore the demands of (some) project owners that their projects’ digital publications and data continue in perpetuity and turn them off (risking reputational damage and reducing the number of DH projects available to users, more often than not initially supported via public funding), introduce financial risk by maintaining them gratis (absorbing unfunded maintenance costs and undermining other activities), or do nothing and accept the existential risks that accompany a major security breach.</p>
<p>Little support is offered from the surrounding culture. Funding agencies might require data management plans to ensure content is gracefully handled, depending on the country of origin, but appear unable to deal with the complex issues associated with the systems that generate and store that data. Collaborators often become uneasy at the use of  “industry”  frameworks and  “business”  language, suggesting (understandably) that it detracts from academic research culture. Meanwhile, some critics of the digital humanities appear to be unaware that a universe of very high quality, bespoke, but at-risk digital scholarship exists far away from the values and commercial imperatives of Silicon Valley ideology.<sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>  In that sense, this paper is an account of a course charted between Charybdis and Scylla, seeking to protect a cargo of scholarship from technical and financial realities, the barbs of critics, the corporatisation of higher education, and gaps in national policy. Were it not for the fact that this is  <em>the precise set of operational tensions</em>  that drives the intellectual and creative culture of laboratories like KDL, and the support of an almost uniformly understanding group of project owners and stakeholders, the combined pressures would be insufferable. Given this, we view this article as an opportunity to articulate the issues facing teams like KDL, gesture towards some of our solutions, and make it easier for other teams to share their experiences and request the resources needed to mitigate issues.</p>
<p>The ~100 projects inherited by KDL range across several DH sub-disciplines, with a focus on Digital Classics (23 projects), Digital History (23 projects), and Digital Literary Studies (14 projects). Another group can be best described as Digital Humanities (20 projects), with smaller but important groups in Digital Musicology (5 projects), Cultural Studies (5 projects), and Spanish Studies (4 projects). A further 5 projects are best described as inter-disciplinary. New projects appear on a regular basis, of course, meaning the precise numbers constantly shift. Surprisingly, and accepting that five years is a long time in the digital world, 77 of the projects are less than 5 years old, with only 22 projects more than five years old. Of more concern is the fact that, when KDL was established, the majority of these projects were  “orphaned” , and left without funding for maintenance. In lieu of merely shutting them down, they had been kept live with little or no maintenance, resulting in some unacceptably old operating systems remaining in production. This is by no means out of step with the situation at many organisations (commercial or otherwise). It reflects an era in the history of computing when technological optimism was somewhat higher and security risks somewhat lower than they are today.</p>
<p>We would like to note, in this context, that our openness in publishing the details of the situation is relatively unusual and should indicate the importance we feel the subject holds for the global humanities and social science communities, and the library and archival teams that support them. We have a good degree of confidence the issues have been resolved, as far as is possible given today’s environment and the evolving security threats it presents, but – more importantly – feel it is time to have an open conversation about these issues. Teams like KDL struggle with issues presented by myriad pressures: it is neither fair nor productive for Principal Investigators (PIs), funding agencies, and the wider community, to have the reality of those pressures hidden from them. Significantly, rather than seeing such issues as embarrassments, to be hidden from administrators, funders, and colleagues inside and outside our institution, the lab recognises them as research opportunities for developing enhanced methodologies. It can be noted, too, that this attitude represents continuity with the history of digital humanities at King’s College rather than a departure from it. Previous generations of colleagues, including Harold Short, Marilyn Deegan, Lorna Hughes, and Sheila Anderson, tried to prompt policy change at a national level (most clearly through the Arts &amp; Humanities Data Service, but also through regular connection with national funding bodies and other organisations), but their efforts were not supported at crucial moments <sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>. Our goal is to empower similar teams to seek and secure the support needed to do their jobs, and contribute to the development of guidelines, standards and policies that can guide digital scholarship. This ambition needs to be seen in a wider context that includes issues of not only technical and financial sustainability but equitable career paths, ethical attribution, diversity, and DH in developing countries.</p>
<p>Although (again) by no means unusual, the details of KDL’s technical estate in late 2015 would give many systems administrators sleepless nights. KDL projects were running: Windows 2003 (2 servers); Windows 2008 (9 servers); Debian 4 (13 servers); Debian 5 (32 servers); Debian 6 (33 servers); Debian 7 (10 servers). The preponderance of Linux servers reduced risk significantly, but the age of many of them was enough to be a risk even before the potential impact of weaponised hacking tools on mainstream institutional systems became clear. All servers were backed up, onto older machines that were adequate but not entirely fit for purpose, and a significant security breach could have led to several days&rsquo; downtime while the systems were restored to their previous best-known state. It was initially difficult to communicate this to some project owners, who were unaware of the need for infrastructure maintenance, and the risks their servers posed. The WannaCry event prompted a marked change, however. PIs who had previously resisted sharing responsibility for their projects’ security immediately allowed KDL to turn off servers until emergency patching had been completed or (in the worst cases), both server and application had been rebuilt. The lab was close to taking this action unilaterally, regardless, for the good of everyone involved. The consciousness-raising that accompanied WannaCry, following its impact on the UK National Health Services (NHS) and other key digital infrastructures, made the process considerably easier <sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup>.</p>
<p>Solid security requires up to date and regularly patched servers, but also up to date and patched application frameworks (the body of code that enables the websites, databases, archives, and digital scholarly editions end-users interact with), which can be equally difficult to maintain. As with the use of Linux, decisions to build using open source tools lowered risk significantly but did not eliminate the need for basic ongoing maintenance. 26 of the oldest projects were built using Java, but 52 were built using the Python-based web framework Django, which has proven to be relatively secure. When coupled with the bespoke XML-based publishing solutions xMod and Kiln (used for digital scholarly editions), security risk and associated costs were almost entirely removed - but these tools could not be used for every project. The most problematic projects in the legacy portfolio were built using PHP-based frameworks such as WordPress and Typo 3, which were promptly removed from KDL servers wherever possible. Exceptions aside, analysis of the lab’s application security validated and renewed our focus on a more limited technology stack based on Linux, Python, Django, and associated supporting tools. Other labs might undertake similar analysis and conclude they should focus on a stack  <em>including</em>  Windows and PHP-based tools and  <em>excluding</em>  Linux and frameworks like Django (to better align to their technical history and capabilities): the issue is a matter of systems maintenance and security, not a reflection of the so-called    “programming language wars”   <sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup>  . Experimentation with a range of new technologies continues, particularly in emerging frameworks to support augmented and virtual reality, but long-term support is focused on the core tool set.</p>
<h2 id="policy-context">Policy Context</h2>
<p>It is not our intention to propose national policy change in this article (either in the UK or other countries), which requires more insight into the complexities of strategy and funding than we possess, but it is important to note that the projects inherited by KDL, and detailed in this paper, were developed using funding that only supported technical development and limited post launch hosting of projects. Limited or no support existed for significant post-funding system maintenance. In that sense, the funders  <em>themselves</em>  signalled that they did not expect (or were not prepared to support) the development of long-term or permanent digital resources: without the goodwill of colleagues and the host institution most of them would have been closed years ago. Their future was often only discussed tangentially, elided in conversations between technical teams and PIs during the development process, in the optimistic hope  “something”  would happen eventually, and that either the funding agencies would see the value of the scholarly assets being built, or a national solution would be implemented to protect them – or, in the absence of the realisation of such hopes, that hosting institutions would support them gratis in perpetuity. PIs shifted emotional responsibility onto technical teams, and vice-versa: actual contractual responsibility was normally left undefined.</p>
<p>With the benefit of hindsight this was unfortunate, but perhaps inevitable given the lack of knowledge about the many intersecting issues in play. Many of the projects hosted by KDL were produced during seminal years in the history of the field, when flagship digital humanities projects demonstrated the potential that digital tools and methods held for arts and humanities research, and they are consequently of considerable cultural and scholarly value. The spirit of 1990s cyber-utopianism - which assumed electronic media would be cheap and technically straightforward to maintain, and that libraries would develop subscription models able to support bespoke non-commercial projects - held back proactive funding of archiving and sustainability initiatives <sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup>. Funding agencies and researchers alike assumed that their role was to prompt expansion and illustrate possibilities, and that issues of maintenance and sustainability would be resolved in the future. This attitude was understandable, but it is having a serious impact on teams who have inherited multiple high profile (and now unfunded) projects that are well beyond their initial funding periods. That is not to suggest that earlier generations of digital humanists did nothing to plan for the future, however. UK colleagues often cite the defunding of the Arts &amp; Humanities Data Service<sup id="fnref:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup>  and the AHRC ICT Methods Network<sup id="fnref:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup>  at the start of the millennium as signal events that undermined the future of multiple projects.</p>
<p>It is reasonable to view this as an international problem. Other UK digital humanities teams report similar issues to KDL, and colleagues inform us that policy gaps have created similar problems in the United States. The problems exist in continental Europe but are less pronounced because of longer-term commitments to infrastructure development and better alignment to STEM-based initiatives that are actively exploring ways to improve Research Data Management (RDM) infrastructure and processes <sup id="fnref:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup>  <sup id="fnref:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup>. It is important at the outset to recognise that the issue runs deeper than straightforward problems of IT  “service delivery” , however. In large part the issues inherited by KDL are the result of a wider conceptual failure, and an inability (or unwillingness) to search    “for critical and methodological approaches to digital research in the humanities  <em>grounded in the nature of computing technology and capable of guiding technical development as well as critical and historical analysis</em> ”   <sup id="fnref:20"><a href="#fn:20" class="footnote-ref" role="doc-noteref">20</a></sup>  . If practical work in the digital humanities is to continue, this attitude needs to be fostered, and extended towards the ongoing maintenance, archiving and preservation of projects as well as their development. In an article in  <em>Aeon</em>  in 2017, historians of computing Andrew Russell and Lee Vinsel point out that the technology industry is so ideologically biased towards  “newness”  that it glosses over the need for maintenance despite it being a significant aspect of the contemporary digital world <sup id="fnref:21"><a href="#fn:21" class="footnote-ref" role="doc-noteref">21</a></sup>. More pointedly in the context of digital humanities, Paul Edwards et al note that  “ <em>sustainable knowledge infrastructures must somehow provide for the long-term preservation and conservation of data, of knowledge, and of practices</em> …”  and that this    “ <em>requires not only resource streams, but also conceptual innovation and practical implementation</em> ”   <sup id="fnref:22"><a href="#fn:22" class="footnote-ref" role="doc-noteref">22</a></sup>  .</p>
<p>The issue has cascading implications for digital humanists and policy makers alike. If digital humanities projects become known for not only soaking up valuable money that could be used in other disciplines, but using that money on unsustainable projects, the central raison d&rsquo;être of the wider tradition - using digital tools and methods to answer research questions in the humanities - will be undermined. However, there is no reason the worst scenarios (permanent loss of multiple flagship digital humanities projects) should come to pass. As Smithies has argued elsewhere, a wider view of digital humanities infrastructure, in its technical as well as intellectual and ethical dimensions, can provide perspectives that aid not only technical development and management, but the development of ethical perspectives, and greater purchase over business decision-making and funding policy <sup id="fnref1:20"><a href="#fn:20" class="footnote-ref" role="doc-noteref">20</a></sup>. Only by exploring this wider perspective can an appropriate understanding be gained, and supporting policy developed. Patrik Svensson takes a similar approach in his recent book about DH infrastructure <sup id="fnref:23"><a href="#fn:23" class="footnote-ref" role="doc-noteref">23</a></sup>. That book aligns well to emerging trends in critical infrastructure studies <sup id="fnref:24"><a href="#fn:24" class="footnote-ref" role="doc-noteref">24</a></sup>, platform studies <sup id="fnref:25"><a href="#fn:25" class="footnote-ref" role="doc-noteref">25</a></sup>, maker culture <sup id="fnref:26"><a href="#fn:26" class="footnote-ref" role="doc-noteref">26</a></sup>, minimal computing <sup id="fnref:27"><a href="#fn:27" class="footnote-ref" role="doc-noteref">27</a></sup>, and various critical and philosophical approaches perhaps best described as    “epistemologies of building”   <sup id="fnref:28"><a href="#fn:28" class="footnote-ref" role="doc-noteref">28</a></sup>  .</p>
<p>The problem is that this work tends to be only tangentially related to, or simply ignore, the seemingly pedestrian problems associated with technical design and development, archiving, and sustainability. Work on humanities research infrastructure is often written by people more invested in the easy development of new projects (and thus the easy availability of development teams and server and hosting infrastructure) than their ongoing maintenance, which hinders rather than helps the sustainability argument <sup id="fnref:29"><a href="#fn:29" class="footnote-ref" role="doc-noteref">29</a></sup>. The situation is further complicated by widespread cynicism about large-scale infrastructure development resulting from the failure of programmes such as Project Bamboo in the United States <sup id="fnref:30"><a href="#fn:30" class="footnote-ref" role="doc-noteref">30</a></sup>, which aimed to create a large national cyberinfrastructure in the humanities but foundered due to poor requirements elicitation, a focus on service-oriented architecture, and over-use of dominant STEM models. Geoffrey Rockwell is correct to suggest that digital humanities needs to assess its own requirements, and not assume that infrastructures designed for one purpose will fit another, but it is sensible to at least align the digital humanities to approaches in other fields <sup id="fnref1:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>.</p>
<p>The work of researchers like Deb Verhoeven and Toby Burrows, who explore the political and aesthetic implications of large-scale Research Infrastructures (RIs) alongside issues of sustainability and maintenance, provides a new model for thinking through these issues <sup id="fnref:31"><a href="#fn:31" class="footnote-ref" role="doc-noteref">31</a></sup>  <sup id="fnref:32"><a href="#fn:32" class="footnote-ref" role="doc-noteref">32</a></sup>. The value of such work issues from its connection of DH infrastructure development and its maintenance with sociological and anthropological work in infrastructure studies capable of normalising technical infrastructure as a  <em>human</em>  and  <em>community</em>  asset in need of maintenance and support, rather than a technical artefact in need of service management <sup id="fnref:33"><a href="#fn:33" class="footnote-ref" role="doc-noteref">33</a></sup>  <sup id="fnref:34"><a href="#fn:34" class="footnote-ref" role="doc-noteref">34</a></sup>. Well financed infrastructure combined with careful requirements analysis, tailored to the needs of humanities researchers and their local institutions, can dramatically increase the quality (and lower the costs) of digital humanities support, maintenance, and archiving, but lack of technical leadership has stymied development. Effort also needs to be directed towards the development of best practice and quality assessment frameworks for digital scholarship that include sustainability and maintenance at their core.</p>
<p>These perspectives are informed by changes in the policies guiding the development and management of STEM RIs which, although larger in scale, deal with many of the same issues and are not as focused on technology as sceptics might assume. A 2017 European Commission working paper on sustainable research infrastructures noted the centrality of both people and technology to the future of reproducible science <sup id="fnref1:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup>, and a number of reports on e-infrastructure at the European level and in the United Kingdom have made similar recommendations <sup id="fnref:35"><a href="#fn:35" class="footnote-ref" role="doc-noteref">35</a></sup>  <sup id="fnref:36"><a href="#fn:36" class="footnote-ref" role="doc-noteref">36</a></sup>  <sup id="fnref:37"><a href="#fn:37" class="footnote-ref" role="doc-noteref">37</a></sup>. The 2017  “State of the Nation”  report of the UK Research Software Engineering (RSE) association overtly positions permanent career paths at the core of both high-quality science, and technical sustainability <sup id="fnref:38"><a href="#fn:38" class="footnote-ref" role="doc-noteref">38</a></sup>. If there is a failure of post-millennium digital humanities, it could well be related to this human aspect, rather than anything overtly technical: setting aside all other considerations, permanent DH development teams will resolve most issues of sustainability and maintenance.</p>
<p>The experience of KDL suggests that the most effective strategy is to offer open-ended contracts and then embed archiving and maintenance deep into the culture of technical development, from requirements definition and the identification of digital research tools and methods, through to infrastructure design, deployment and maintenance. This is based on a conception of infrastructure that moves beyond material technical necessities, templates, and process documents (as essential as they are), towards one that acknowledges the centrality of people, funding, ethics, technology strategy, software engineering method, and data management to the long-term health of our research infrastructures. This becomes even more pressing if we acknowledge the wider epistemological and methodological shifts occurring across scientific and humanistic disciplines, related to the emergence of data science but also myriad new forms of research dissemination and product development. The community needs to recognise that high quality research requires attention to long-term digital sustainability if quality is to be maintained. This extends well beyond the specifically  <em>digital</em>  humanities, of course, and relates to all disciplines and interdisciplinary efforts that use digital tools and methods. Importantly, the failure (or sub-optimal performance) of previous large-scale infrastructure efforts supports the argument for greater attention to the need for investment in human capital and process maturity alongside capital investment. This suggests the need for a range of initiatives from institutions engaging in DH activity and funding agencies supporting it, from the development of viable technical career paths, to training in basic software development methods: archiving and sustainability is only one aspect.</p>
<h2 id="software-development-life-cycle-sdlc--infrastructure">Software Development Life-cycle (SDLC) &amp; Infrastructure</h2>
<p>A key part of KDL’s work concerns improvements to the engineering and procedural frameworks that enable digital scholarship. Much like research, software development rarely takes a linear path, and the relative volatility of the open web and rapid development of new technologies presents an ecosystem within which published work needs to be protected and maintained over time. Rather than presenting a pristine environment for artefacts, the digital environment, much like the physical one, presents challenges of an economic, political and entropic nature. The precarious existence of artefacts in the physical world, and the evolving responses from the research community to their preservation and documentation, therefore inform our digital practices. To this end, KDL uses an approach to the funding and management of research projects that considers the complexities of not just the research, but also software development and its ongoing sustainability in a changing digital landscape. While slightly increasing initial costs, the benefits of this approach accrue over time - particularly in relation to academic impact, but also medium and long-term maintenance, archiving and preservation.</p>
<p>To support this, the team have added System, Application, and Data Lifecycle Management to our Software Development Lifecycle (SDLC), along with Research Data Management. This has resulted in a process of analysis, development, and maintenance underpinned by Service Level Agreements (SLAs) defined in collaboration with PIs and management. The SDLC is based on the Agile DSDM® method <sup id="fnref:39"><a href="#fn:39" class="footnote-ref" role="doc-noteref">39</a></sup>, adapted for a research context. A range of archival products (static sites, removal of front-end, data migration, graceful shutdown, visualisation etc.) are now considered at the initial requirements gathering phase of projects, for implementation when funding ends. Although our concern here is with archiving, maintenance, and sustainability processes for the projects themselves, the work functions within a wider context of not only ongoing research activity but software engineering process and infrastructure management.<sup id="fnref:40"><a href="#fn:40" class="footnote-ref" role="doc-noteref">40</a></sup></p>
<p>The laboratory inherited significant infrastructure from the Department of Digital Humanities (DDH): rack servers supporting 400GB RAM, over 180 virtual machines, 27TB of data, and over 100 digital projects ranging from simple WordPress and Omeka sites to ground-breaking scholarly editions and historical prosopographies. At the time of writing a full infrastructure upgrade has been completed, including the deployment of new enterprise backup servers and core infrastructure that has upgraded capacity to ~1TB of RAM and additional disk space running on Solid State Drives (SSD). Network capacity has been upgraded from 1GB to 10GB. The new infrastructure has capacity for significantly more than 200 virtual machines, and planning has already started for a renewal cycle starting in 2023, to ensure continuity past the life of even the new infrastructure. This information is provided less as an advertisement for KDL, than as a reminder that sustainability requires maintenance of supporting hardware as well as the software that is the focus of this article. Coordinating maintenance of all levels of the technology stack requires considerable effort when it needs to support more than a handful of projects.</p>
<h2 id="principles">Principles</h2>
<p>Our experience suggests that, much as with traditional production and publication of research materials of archival quality, digital projects benefit from being planned and executed with their longevity in mind from the start. This often involves updating scholarly content, but always involves technical maintenance to ensure the publication remains accessible. This places additional importance on consistency and transparency in approach, supported by effective dissemination and internal peer review of technical documentation. Maintenance and ongoing hosting of a digitally published research project needs to be included in grant application budgets, reflecting the life-cycle of the project beyond the date of publication, with a set of maintenance milestones determined at the outset. This is not merely good operational practice, but an indication of the intellectual maturity of the project. Proper understanding of digital scholarship requires an acknowledgement of its entropic nature; the absence of forward planning implies a misunderstanding of the object being produced at a fundamental – perhaps ontological – level. KDL’s process is thus guided by a desire to enable high quality digital scholarship, balancing technical and financial issues with the intellectual and historical significance of the project alongside a consideration of its impact, future funding potential, and potential contribution to the UK Research Excellence Framework (REF). The process blends experience with common sense, rather than being anything particularly complicated. It also assumes, significantly, that not all projects should be maintained in perpetuity. Some are better conceived as short-term or even momentary interventions in the scholarly conversation, to be archived online for the historical record but not worth the intellectual, technical, and financial overhead of ongoing maintenance. Convincing PIs of this can sometimes be difficult but the more they consider the wider epistemological context of their work (and often more importantly, the methodological and even ontological purpose of the output they aim to produce) the more open they become. The realisation they don’t need to bind themselves to a project permanently - forever concerned about its future maintenance - usually comes as something of a relief.</p>
<p>KDL’s archiving process (see <a href="https://www.kdl.kcl.ac.uk/our-work/archiving-sustainability/">https://www.kdl.kcl.ac.uk/our-work/archiving-sustainability/</a>) has been, and is still being, developed in response to emergent tensions between the envisioned and manifest material, financial, and political conditions in which legacy projects exist. As Paul Conway has noted, transforming archiving and preservation practice entails fundamental decisions about how the practice is    “ <em>conceived, organised, and funded</em> ”   <sup id="fnref:41"><a href="#fn:41" class="footnote-ref" role="doc-noteref">41</a></sup>  . Our findings will ideally contribute to a conversation across the digital humanities community, funding agencies, and policy makers with a view to identifying and implementing (or at the least recommending) frameworks, infrastructures, and funding mechanisms that can ensure the sustainability of digital projects and their data in a way that shares the burden between universities and cultural heritage organisations, and funding agencies. While it is unreasonable to expect funding agencies to provide ongoing funding for all projects, it does seem reasonable to ask their support for projects that are managed according to transparent processes and accepted frameworks, that include a range of archival approaches, and integration into Research Data Management (RDM) systems that leading research agencies advocate greater use of <sup id="fnref1:37"><a href="#fn:37" class="footnote-ref" role="doc-noteref">37</a></sup>. As indicated earlier in this article, we do not feel it is our place to provide detailed recommendations here, however: the issue needs ongoing dialogue and careful consideration across the community.</p>
<p>By developing open approaches to the development of archiving and sustainability frameworks, even if they are merely the  “ <em>least bad</em> ”  option <sup id="fnref1:41"><a href="#fn:41" class="footnote-ref" role="doc-noteref">41</a></sup>, the digital humanities community might aspire to deliver on the promise of earlier initiatives like the Arts &amp; Humanities Data Service and safeguard the future of both the community and public investment in digital research projects. It is worth noting here that additional technical work has been initiated behind the scenes at King’s College, in a self-funded collaboration between the lab and the DH department. The goal is to create a  “data lake”  of metadata and digital objects collected over the history of DH at King’s, and comprising over 5 million digital objects, for use in teaching as well as research (see <a href="https://data.kdl.kcl.ac.uk/">https://data.kdl.kcl.ac.uk/</a>). This is part of a commitment to the implementation of deep infrastructure to support DH archiving, which will be aligned to institutional research data management infrastructure and made openly available to the wider community. Work is progressing slowly, as time and funding allows, but the goal is to create a suite of approaches that can be used in the future.</p>
<p>Digital curation has been described as a    “ <em>new discipline</em> ”   <sup id="fnref:42"><a href="#fn:42" class="footnote-ref" role="doc-noteref">42</a></sup>  , evolving from archives and libraries tasked with assessing digital material for collection, use, and preservation. KDL’s process for archiving inherited  “legacy”  projects reflects this. Rather than relying on rigid assessment matrices or requiring slavish attention to cost-benefit analyses, it is self-consciously oriented towards relatively subjective issues of  “scholarly and intellectual value”  and  “cultural heritage value” . These need to be balanced against hard operational and financial realities, but it was decided relatively early in the process that it would not be possible to create a procrustean assessment framework that could be applied rigorously to all projects: the heterogeneous nature of the projects (technically as well as intellectually), the frequent mismatch between scholarly value and straight-forward impact metrics such as web traffic, as well as uneven access to funding meant that a more holistic - but still consistent and transparent - process needed to be adopted.</p>
<p>KDL analysts therefore assess each project in terms of scholarly value, technical complexity, security risk, maintenance cost, infrastructure cost, PI engagement, institutional support, value to KDL, and value to King’s College London. Early assessments used a tabular matrix to guide analysis, but this was quickly abandoned as too limiting: recommendations are made in prose form, allowing quantitative and qualitative issues to be taken into account. A brief business case, including recommendations and costs, is then presented to the Vice Dean Research, Faculty of Arts &amp; Humanities, and a decision is made. Problematic cases can be referred to the Faculty Research and Impact Team (FRIT), and upwards to the Dean if necessary. The process, at this high level, works very well. Simon Tanner’s notion of a  “balanced view” , assessing value using both subjective and objective measures, allows the lab to act as liaison between the projects and University, and thus support the projects and the wider DH community <sup id="fnref:43"><a href="#fn:43" class="footnote-ref" role="doc-noteref">43</a></sup>. The key principle is that KDL acts as facilitator rather than decision-maker, providing professional digital humanities analysis to both PI and management. This requires resources to engage in due diligence, willingness to steward sometimes difficult conversations, and occasional recommendations that projects be archived rather than maintained in their live state, but the process ensures all stakeholders have equal access to information and that escalation paths exist.</p>
<h2 id="implementation">Implementation</h2>
<p>It is worth detailing the effort required to work through KDL’s archiving and sustainability issues. During the financial year 2016/2017, the lab undertook a complete audit of all projects held on its servers, including inherited legacy projects, and developed processes for realistic costing of their maintenance and hosting. In tandem, the lab set up contractual agreements that supported the reintegration of the updated legacy projects brought under Service Level Agreements (SLA) into the broader production processes of the lab. During the final four months of the financial year 2016/2017, one full-time member of staff was dedicated to the implementation of the new processes, with the intention to bring all prior legacy projects into current processes under SLA, migrate projects not suited to further managed hosting at KDL to the university’s IT department (ITS), external hosting, or a static legacy server, and archiving the remainder. A pilot phase was conducted using the portfolios of two prolific King’s College London researchers. Business cases for those projects were submitted to the Faculty of Arts &amp; Humanities, resulting in approval for 5 years’ support and maintenance.</p>
<p>It quickly became apparent that lengthy documents could be replaced with straightforward, fully itemised and costed Service Level Agreements (SLAs),<sup id="fnref:44"><a href="#fn:44" class="footnote-ref" role="doc-noteref">44</a></sup>  to clarify the extent and duration of KDL’s commitment. These are now issued as part of the release process of any project approaching finalisation and launch, and discussed with PIs in the earliest stages of project definition. As the pilot phase progressed, technical and supporting data about additional projects was gathered, including historical information about funding, PIs/Co-Is, external stakeholders, and infrastructure. This required the identification and synthesis of multiple historical sources but enabled KDL to gain an overview of the extent of the legacy projects, including dependencies and risks. The information was collated and included in documentation that supported the reintegration of each project into the lab’s active production cycle, whether that be via managed decommissioning, migration, or defined support and maintenance underwritten by key stakeholders. Based on this high-level assessment, 29 projects were dealt with almost immediately in a first phase that involved them being taken offline and archived by storing database dumps and content files in zip files, because they were incomplete, or incurred security risk out of all proportion to their scholarly value.<sup id="fnref:45"><a href="#fn:45" class="footnote-ref" role="doc-noteref">45</a></sup>  Others required only basic maintenance to make them secure. A further 35 were scheduled for Phase 2,<sup id="fnref:46"><a href="#fn:46" class="footnote-ref" role="doc-noteref">46</a></sup>  and 35 for Phase 3.<sup id="fnref:47"><a href="#fn:47" class="footnote-ref" role="doc-noteref">47</a></sup>  Only legacy projects that were no longer in active development were considered. Another class of project, inherited from DDH but still in active development, were dealt with using a different process. A second key document - the Statement of Work (SoW) - evolved to fit a subsidiary need: to detail and cost work required to bring projects up to an acceptable standard for ongoing hosting. That might only involve simple server upgrades, requiring half a day, or several weeks of active development to rebuild the site in its entirety.<sup id="fnref:48"><a href="#fn:48" class="footnote-ref" role="doc-noteref">48</a></sup></p>
<p>King’s College London Faculty of Arts &amp; Humanities approved all the business cases presented to it for support of ongoing maintenance and hosting of projects led by Principal Investigators at King’s College. It should be remembered that the approved SLAs are all finite - ranging from two to five years - but equally important to note that agreement was reached only after robust business cases were produced, detailing the scholarly and cultural heritage value, the significance to the Research Excellence Framework (REF), the  “brand”  value to the university, and the PI’s future career. This created a new, and essential, level of clarity and made the value of the projects more apparent. The process, which made cost, value, and mutual expectations transparent, was a necessary first step towards the faculty managing its digital assets in a more transparent and cost-effective way, and in alignment to its wider strategic direction. It is perhaps not an ideal solution, which would involve limitless funds and assurances of perpetual support, but it is practical and (we think) sensible given the complexities of long-term technology management and the need to accept competing needs for finite funds.</p>
<p>If some projects are eventually moved towards archiving, a decommissioning process is followed, aligned with wider university research data management requirements. It is highly unlikely now that any projects will simply disappear. At the very least their data and a public metadata record will be retained: the future of each project can be discussed on a case-by-case basis. Enhanced transparency has also facilitated co-funding arrangements (between College, Faculty, Department, external partner, and funder, for example), reducing the average SLA cost of ~£2000 GBP per year to an extremely reasonable level for each party. It is equally important to note that KDL is currently authorised to charge maintenance and hosting at cost recovery level, far below commercial rates (this is the case for normal project work too). This might need to be adjusted in future years, to manage demand if nothing else, but was a crucial element in explaining and justifying the archiving and sustainability projects to colleagues.</p>
<p>The process has made us keenly aware of gaps in contemporary funding models, which would ideally incentivise projects to manage their future according to similarly transparent and flexible models, but instead incentivise researchers to produce  “orphan”  projects with uncertain futures. If a tone of frustration is detected in this article it stems from the relatively common-sense nature of the solutions, coupled with the significant stress placed on teams like KDL by a lack of robust policy. This is not to criticise funding agencies, who have been learning about the implications of digital scholarship alongside the communities they serve (they do an excellent job, with limited resources) but it is important to recognise the human cost of poorly managed projects and infrastructure. It is concerning that recent updates to the UK Arts &amp; Humanities Research Council (AHRC) grant application process is likely to worsen rather than improve the situation in that country, by requiring data management plans but nothing related to system quality, infrastructure, or lifecycle management.</p>
<p>Conversations with PIs outside King’s College London were often the most difficult, as is to be expected given differences in administrative alignment and awareness of KDL as a new initiative. Expectations of ongoing hosting and maintenance were often ill-defined, and reliant on memory rather than crisp documentation: a result, again, of the loose requirements for archiving and sustainability in past years, as well as changes in personnel and restructuring. In many cases, the production of a SLA was all that was required for the PI to request support from their university (so that they had a simple document to present to administrative teams, usually with only a modest cost attached). If it could be demonstrated that a King’s staff member was closely involved in the project or stood to benefit from its ongoing maintenance, King’s College London would support a proportion of the SLA. Discussions could become difficult in more complex cases, such as when significant work needed to be undertaken to upgrade the project, or maintenance costs were above the average (normally due to significant use of disk space) but all PIs, internal and external, were offered three scenarios:<br>
Service Level Agreements, and (where appropriate) software updates, which guaranteed hosting, regular software maintenance, and server updates under renewable two to five-year contracts, costed on the basis of individual project requirements and including Statements of Work (SoWs), when required, for necessary additional upgrade work.   For non-King’s staff, migration to the partner institution for local hosting.   Archiving of websites no longer in active use. This option did not result in the destruction of research data and could entail rendering websites static for migration to a legacy server, or packaging for archival storage.</p>
<p>The last option can present problems, given the complexity of some of the projects and the state of the art in digital archiving. Technical issues abound. A range of  “archival solutions”  have been considered, ranging from removing complex front-end websites and archiving data, to software emulation, and packaging sites as virtual machines for offline use. The basic philosophy is to embrace heterogeneity of archival solutions, in line with the heterogeneity of the projects themselves. Bespoke approaches are developed on a case-by-case basis, although always in alignment with wider university, national and (where appropriate) international infrastructure initiatives.</p>
<p>Other initiatives are being considered too. At the time of writing, KDL is discussing an arrangement with the British Library National Web Archive to improve technical and procedural alignment. King’s Research Data Management system is likely to be used for preservation of raw research data along with the lab’s own server infrastructure. A project has been completed with the British Museum to produce static sites (more conducive to future archiving) from one of their legacy projects <sup id="fnref:49"><a href="#fn:49" class="footnote-ref" role="doc-noteref">49</a></sup>, and a collaboration with Stanford University Press is exploring new modes of digital publishing to balance advanced features with sustainability and maintainability <sup id="fnref:50"><a href="#fn:50" class="footnote-ref" role="doc-noteref">50</a></sup>. The internal project referred to above, with the Department of Digital Humanities, aims to aggregate Digital Humanities content stored at King’s College and making it publicly available for reuse so that even if some projects do lose their active web presence, their data will still be accessible. The lab is beginning to consider in some technical detail the different options available for archiving and preservation, including the difference between presentation and data layers, the possibility of preserving functionally limited but usable  “static”  websites rather than complete systems, the possibility of packaging publications into downloadable  “virtual machines”  that can be run on the desktop, and coupling all of these approaches with  “snapshots”  stored in the British Library National Web Archive and Internet Archive. The work described in this paper only becomes tractable through a range of solutions, in other words, conducted using a research-oriented frame of mind that seeks to embed archiving and preservation deeply within core digital humanities theory, method, practice, and policy. Improved policy and infrastructure at a national level would help significantly, but this is a multi-faceted issue that will require broad-based input and support.</p>
<p>When it was clear the best possible approach to assessment had been found, transparent processes were in place, and clear options determined, emails were sent to PIs en masse to accelerate phases Two and Three. It had become essential the assessment process not drag on, undermining the future of the lab, so there was a degree of nervousness about potential responses. In the initial email to project partners, a deadline for responses within 6 weeks from the sending date was given, after which Faculty would be notified of the status of the resource. After a further month, the permission would be sought from Faculty to archive the projects of non-responsive project partners. This timeline was clearly set out in the emails and followed to the letter. Responses were largely swift and positive, allowing mutually acceptable solutions to be identified in collaboration. Project partners generally responded to initial contact well within the stated time, and often immediately. Responses were broadly appreciative, and the rationale for putting older digital research outputs on secure footing appeared intuitively clear. This raises the question of whether resistance to adopting best practice across the wider research community is exaggerated: it is perhaps more the case that robust methods and clear processes are lacking, and funding policy acts against their development.</p>
<p>The lab’s attitude, enabled by decisions made within Faculty, prompted progress. King’s Digital Lab operates on a non-profit basis (with accordingly slim margins), so one of the most fundamental stages in assessment of the legacy projects was the audit of not just the digital resources held on KDL servers, but also defining the costs involved in their responsible ongoing management and hosting. In this sense the lab performed an administrative and communicative role, rather than acting as judge and jury. The inherently positive nature of the process made it more likely PIs would respond well and allowed the lab to streamline the further processing of legacy projects, and minimise detailed negotiation and problem solving for which there is limited resource. The aim was to conclude the financial year of 2016/2017 with no undocumented or out of contract legacy projects remaining on KDL servers, and all legacy projects that were neither migrated nor archived being brought under Service Level Agreements.</p>
<p>That was not completely achieved, but results were satisfactory. At the time of writing all assessment and decision­making has been completed, Service Level Agreements are in place for projects that are to remain hosted on KDL servers, migration has occurred or is scheduled for other projects, and archiving of the remainder will occur when time and resource allows. Given no perfect final state will ever be reached the initial task of rationalising and safeguarding the lab’s project inheritance can be considered to be complete. The newly established processes will be used to manage the lab’s project estate for the foreseeable future. Security risk has been brought within significantly more acceptable tolerances. At the end of the process dozens of once uncertain projects will have been given clarity, and a valuable corpus of digital humanities projects will have been brought under robust management. Surprisingly, given the anxiety that attended the start of the initiative, 46% of the projects were placed under Service Level Agreements, guaranteeing between three and five years of secure maintenance and hosting. Where the end date of projects passed less than five years ago, the lab issued backdated, zero-cost Service Level Agreements, itemised with future costs for each component, to clearly signpost future hosting and maintenance needs. This effectively gave several major projects no-cost extensions to their hosting and maintenance, as well as giving them time to consider their options and plan for the future. At least two significant sites will be rebuilt using new funding, and several others will be subject to follow-on funding proposals. It is worth mentioning that a very small subset of projects (five in total) await full resolution, while discussions around creative funding (e.g. crowdfunding and archiving options) continue.</p>
<p>In conjunction with the upgrade to KDL’s core infrastructure, this gives our community 3-5 years to continue seeking new options and align to evolving archiving and preservation efforts in the wider research data management and eResearch communities <sup id="fnref:51"><a href="#fn:51" class="footnote-ref" role="doc-noteref">51</a></sup>. 39% of the inherited projects have been archived in some form, 13% on a static HTML legacy server that allows their basic content to remain live but incurs no further maintenance, and 26% on local backup servers. No data, in the form of image files or otherwise, has been removed from potential circulation. Plans are in place to migrate the remaining 15% of the projects to other institutions, in a very pleasing move that signals that they also see the value in investing in the future of digital scholarship. 6% will be migrated to a WordPress service hosted in King’s College IT department, and 9% will be migrated to external hosting providers. The onward cost of our current project archiving services are negligible; local backup is supported from baseline operating costs, and the running cost of the two static HTML legacy servers is ~£600 per annum. The major costs, naturally, stem from the 12 months of effort, including 4 months with a dedicated full-time team member, to undertake assessment, produce documentation, and communicate with PIs. It is possible that significant additional costs will appear when more complex sites need to be archived, too, but these cases will appear in a staggered way and therefore be more manageable as part of the lab’s normal software development and maintenance process. The end result, in simple terms, is KDL’s new maintenance schedule: a list of ~50 projects, all covered by Service Level Agreements and generating modest internal and external income to offset costs. Concerns remain about some projects, and others remain in process, but that – in our estimation – is the best that can be expected: maintenance and archiving of digital scholarship is an iterative, continuous process, that does not allow for perfect endings.</p>
<h2 id="conclusion">Conclusion</h2>
<p>King’s Digital Lab has implemented pragmatic processes that take into account the human, as well as the technical, financial and political perspectives implicit in digital scholarship. It has reinforced the lab’s commitment to producing digital research within a holistic and scalable framework, supported by straightforward documentation to ensure mutual clarity about what can be expected from research partnerships. A key component of this framework includes the enhanced Software Development Lifecycle (SDLC) process, which is now implemented from the inception of a project, to align its development with post-publication maintenance and, where appropriate, archiving plans. Early clarity about the feasibility and cost of maintaining projects beyond the funded period allows all parties time to plan ahead, with sufficient time to accommodate the development and turnaround time of follow-on funding applications, negotiations with partner institutions, infrastructure resourcing and requisite allocation of staff time. In addition to optimising maintenance and management of legacy digital research outputs, this approach minimises ambiguity regarding responsibilities and expectations, and contributes to reputation risk management in more than one dimension. Contrary to what might have been expected, KDL’s experience of introducing the level of transparency and process described in this article was almost uniformly positive.</p>
<p>Successive generations of software (to support visualisation, AR/VR) and other efforts to enhance research methodologies and impact mean the urgency of the questions addressed in this paper is unlikely to diminish in future years, requiring ongoing interrogation of what is an  “ideal”  technology stack, and best practice. While experimentation with new technology is vital and the precise details of future process design cannot be rigidly determined, more attention to its sustainability, particularly where there is significant investment from public funds, will enhance the field, enhance the benefits of interdisciplinary collaboration outside of the Arts and Humanities, and strengthen arguments for robust funding of the digital humanities sector. Here, it is necessary to differentiate between established technologies and experimental ones. We need to accept brittle code and the possibility of failure in the shorter term for developing technologies but incubate emerging technologies within a context of legacy risk assessment informed by industry standards and including upfront analyses of wider infrastructures and technological limitations. Software sustainability will, in all likelihood, remain a pressing issue for the foreseeable future across all research disciplines. The broader conclusion from the experience of KDL is that entropic factors should be taken into account at early planning stages and be accepted by all parties to the project including PIs, developers, and funders. Here, the degree of orientation towards (or away from) archiving and sustainability are core concerns. Funding and associated policy is central to sustainable development, maintenance and archiving. Assuming that future technologies will make it easier or cheaper to solve problems associated with digital entropy is no longer adequate. Sustainable funding strategies need to be based on transparent costing that includes infrastructure and maintenance costs and made simpler and more reliable by established best practice. For this to be effective, realistic costing methods need to be developed and shared between product partners, and embedded within funding policy.</p>
<h2 id="acknowledgements">Acknowledgements</h2>
<p>King’s Digital Lab is indebted to the many PIs who initiated and led the projects described in this paper. Special thanks to Harold Short and colleagues (too many to list) in the Centre for Computing and the Humanities (CCH), Centre for eResearch (CERCH), and Department of Digital Humanities (DDH) for many years of ground-breaking work. We would also like to thank Paul Readman, and King’s College London Faculty of Arts &amp; Humanities, for their generous support and wise counsel in establishing King’s Digital Lab. Paul Readman, Tobias Blanke, Charlotte Roueché, Paul Spence, Simon Tanner, and anonymous  <em>DHQ</em>  reviewers provided valuable feedback on drafts of the article.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Hirsch, Brett D..  <em>Digital Humanities Pedagogy: Practices, Principles and Politics</em> . Open Book Publishers, 2012.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Eyers, Tom,  “The Perils of the  Digital Humanities : New Positivisms and the Fate of Literary Theory” ,  <em>Postmodern Culture</em> , Volume 23 - Number 2 - January 2013, <a href="http://www.pomoculture.org/2015/07/08/the-perils-of-the-digital-humanities-new-positivisms-and-the-fate-of-literary-theory/">http://www.pomoculture.org/2015/07/08/the-perils-of-the-digital-humanities-new-positivisms-and-the-fate-of-literary-theory/</a> (accessed February 21, 2018)&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Allington, Daniel, Brouillette, Sarah and Golumbia, David,  “Neoliberal Tools (and Archives): A Political History of Digital Humanities.”    <em>Los Angeles Review of Books</em> , May 1, 2016.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Rockwell, Geoffrey,  “As Transparent as Infrastructure: On the Research of Cyberinfrastructure in the Humanities” ,  <em>OpenStax CNS</em>  (April 2010). <a href="http://cnx.org/contents/fd44afbb-3167-4b83-8508-4e70885b6136@2/As_Transparent_as_Infrastructu">http://cnx.org/contents/fd44afbb-3167-4b83-8508-4e70885b6136@2/As_Transparent_as_Infrastructu</a> (accessed February 21, 2018).&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>Carlin, Claire, Czaykowska-Higgins, Ewa, Jenstad, Janelle, and Grove-White, Elizabeth,  “The Endings Project” ,  <em>The Endings Project</em> , 2016 -. <a href="https://projectendings.github.io">https://projectendings.github.io</a> (accessed April 27, 2018).&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>The financial year runs from August to July at our institution.&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>CCH itself evolved from an initiative known as the Research Unit in Humanities Computing, established in 1992. For more information see <sup id="fnref:52"><a href="#fn:52" class="footnote-ref" role="doc-noteref">52</a></sup>&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>Projects URLs with associated Principal Investigator (PI) acting as current signatory of the Service Level Agreement (SLA) with KDL: Charlotte Roueché (PI),  <em>Aphrodisias in Late Antiquity</em>  (2005) &lt;<a href="http://insaph.kcl.ac.uk/ala2004/index.html">http://insaph.kcl.ac.uk/ala2004/index.html</a>&gt;; Charlotte Roueché (PI),  <em>The Inscriptions of Roman Tripolitania</em>  (2009) &lt;<a href="http://inslib.kcl.ac.uk/irt2009/index.html">http://inslib.kcl.ac.uk/irt2009/index.html</a>&gt;; David Carpenter (PI),  <em>The Henry III Fine Rolls Project</em>  (2009), &lt;<a href="http://www.finerollshenry3.org.uk">http://www.finerollshenry3.org.uk</a>&gt;; James McLaverty (PI),  <em>Jonathan Swift Archive</em>  (2009) &lt;<a href="http://jonathanswiftarchive.org.uk/index.html">http://jonathanswiftarchive.org.uk/index.html</a>&gt;; Kathryn Sutherland (PI),  <em>Jane Austen’s Fiction Manuscripts Digital Edition</em>  (2010) &lt;<a href="http://www.janeausten.ac.uk/index.html">http://www.janeausten.ac.uk/index.html</a>&gt;; Anne Curry (PI),  <em>The Gascon Rolls project 1317-1468</em>  (2011), <a href="http://www.gasconrolls.org/en">http://www.gasconrolls.org/en</a>; Keith Lilley (PI),  <em>Linguistic Geographies: The Gough Map of Great Britain</em>  (2011), &lt;<a href="http://www.goughmap.org">http://www.goughmap.org</a>&gt;; Michael Hicks (PI),  <em>Mapping the Medieval Countryside Places, People, and Properties</em>  (2012), &lt;<a href="http://www.inquisitionspostmortem.ac.uk">http://www.inquisitionspostmortem.ac.uk</a>&gt;; Charlotte Roueché (PI),  <em>Sharing Ancient Wisdoms</em>  (2013), &lt;<a href="http://www.ancientwisdoms.ac.uk">http://www.ancientwisdoms.ac.uk</a>&gt;; John Martindale (PI),  <em>Prosopography of the Byzantine Empire</em>  (2014), &lt;<a href="http://www.pbe.kcl.ac.uk">http://www.pbe.kcl.ac.uk</a>&gt;; Martin Butler,  <em>The Cambridge Edition of the Works of Ben Jonson Online</em>  (2014), &lt;<a href="https://universitypublishingonline.org/cambridge/benjonson">https://universitypublishingonline.org/cambridge/benjonson</a>&gt;; Charlotte Roueché (PI),  <em>Heritage Gazetteer for Cyprus</em>  (2015), &lt;<a href="http://www.cyprusgazetteer.org">http://www.cyprusgazetteer.org</a>&gt;; Stephen Baxter (PI),  <em>Prosopography of Anglo-Saxon England</em>  (2010), &lt;<a href="http://www.pase.ac.uk">http://www.pase.ac.uk</a>&gt;; Dauvit Broun (PI),  <em>Models of Authority: Scottish Charters and the Emergence of Government</em>  (2017), &lt;<a href="https://www.modelsofauthority.ac.uk">https://www.modelsofauthority.ac.uk</a>&gt;; Julia Crick,  <em>The Conquerors&rsquo; Commissioners: Unlocking the Domesday Survey of SW England</em>  (2017), &lt;<a href="https://www.exondomesday.ac.uk">https://www.exondomesday.ac.uk</a>&gt;.&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>At the risk of forgetting important contributors we would like to acknowledge Miguel Vieira, Elliott Hall, Jamie Norrish, Paul Caton, Geoffroy Noel, Gabby Bodard, Arianna Ciula, Neil Jakeman, Charlotte Tupman, Ginestra Ferraro, Elena Pierazzo, Simona Stoyanova, Paul Vetch, Tamara Lopez, Gerhard Brey (†), Raffaele Viglianti, Valeria Vitale, Eleonora Litta, Zaneta Au, Hafed Walda, Richard Palmer, Alejandro Giacometti, Michele Pasin, Faith Lawrence, Peter Rose, John Lee, Jasmine Kelly, Artemis Papako-stoulis, Caroline Bearron, Osman Hankir, Felix Herrman, Martin Jessop, Tim Watts, Brian Maher, Andrew Wareham, Juan Garces.&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>We have chosen not to name them in this article, out of respect for their situation.&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>See Allington, Daniel, Brouillette, Sarah and Golumbia, David,  “Neoliberal Tools (and Archives): A Political History of Digital Humanities.”    <em>Los Angeles Review of Books</em> , May 1, 2016 for an example of the neoliberal critique of digital humanities, and Smithies, James,  <em>The Digital Humanities and the Digital Modern</em> . Basingstoke: Palgrave Macmillan, 2017 for a rejoinder.&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p>Rusbridge, Chris,  “Arts and Humanities Data Service Decision” ,  <em>Digital Curation Centre</em> , June 6, 2007. <a href="http://www.dcc.ac.uk/news/arts-and-humanities-data-service-decision">http://www.dcc.ac.uk/news/arts-and-humanities-data-service-decision</a> (accessed April 27, 2018).&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13">
<p>Cellan-Jones, Rory and Lee, Dave,  “Massive Cyber-Attack Hits 99 Countries” ,  <em>BBC News</em> , May 13, 2017, sec. Technology. <a href="http://www.bbc.co.uk/news/technology-39901382">http://www.bbc.co.uk/news/technology-39901382</a>.&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14">
<p>Stefik, Andreas and Hanenberg, Stefan,  “The Programming Language Wars: Questions and Responsibilities for the Programming Language Community” , in  <em>Proceedings of the 2014 ACM International Symposium on New Ideas, New Paradigms, and Reflections on Programming &amp; Software</em> . New York, NY, USA: ACM, 2014.&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:15">
<p>Turner, Fred,  <em>From Counterculture to Cyberculture: Stewart Brand, the Whole Earth Network, and the Rise of Digital Utopianism</em> . Chicago: University of Chicago Press, 2008.&#160;<a href="#fnref:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:16">
<p>Wikipedia contributors,  “Arts and Humanities Data Service,”     <em>Wikipedia, The Free Encyclopedia</em> ,  <a href="https://en.wikipedia.org/w/index.php?title=Arts_and_Humanities_Data_Service&amp;oldid=820259813">https://en.wikipedia.org/w/index.php?title=Arts_and_Humanities_Data_Service&amp;oldid=820259813</a>  (accessed March 1, 2018).&#160;<a href="#fnref:16" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:17">
<p>Arts &amp; Humanities Research Council,  “AHRC ICT Methods Network: Supporting the Digital Arts and Humanities” , 2005–2008. <a href="http://www.methodsnetwork.ac.uk/index.html">http://www.methodsnetwork.ac.uk/index.html</a>.&#160;<a href="#fnref:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:18">
<p>European Commission.  “Sustainable European research infrastructures. A call for action. Commission staff working document: long-term sustainability of research infrastructures.”  October 19, 2017 <a href="http://doi.org/10.2777/76269">http://doi.org/10.2777/76269</a>&#160;<a href="#fnref:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:19">
<p>Rosenthalter, Lukas, Fornaro, Peter, Immenhauser, Beat, and Böni, David,  <em>Final report for the pilot project  “Data and Service Center for the Humanities”  (DaSCH)</em> , Swiss Academies Reports 10, 1, 2015. Zenodo. <a href="http://doi.org/10.5281/zenodo.822918">http://doi.org/10.5281/zenodo.822918</a>.&#160;<a href="#fnref:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:20">
<p>Smithies, James,  <em>The Digital Humanities and the Digital Modern</em> . Basingstoke: Palgrave Macmillan, 2017.&#160;<a href="#fnref:20" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:20" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:21">
<p>Russell, Andrew and Vinsel, Lee,  “Hail the Maintainers.”    <em>Aeon</em> . 2016. <a href="https://aeon.co/essays/innovation-is-overvalued-maintenance-often-matters-more">https://aeon.co/essays/innovation-is-overvalued-maintenance-often-matters-more</a> (accessed February 21, 2018).&#160;<a href="#fnref:21" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:22">
<p>Edwards, P. N., Jackson, S. J., Chalmers, M. K., Bowker, G. C., Borgman, C. L., Ribes, D., Burton, M., &amp; Calvert, S. (2013)  <em>Knowledge Infrastructures: Intellectual Frameworks and Research Challenges</em> . Ann Arbor: Deep Blue. <a href="http://hdl.handle.net/2027.42/97552">http://hdl.handle.net/2027.42/97552</a> (accessed February 21, 2018)&#160;<a href="#fnref:22" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:23">
<p>Svensson, Patrik,  <em>Big Digital Humanities: Imagining a Meeting Place for the Humanities and the Digital</em> . Ann Arbor, MI: University of Michigan Press, 2016.&#160;<a href="#fnref:23" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:24">
<p>Liu, Alan et al.  “Critical Infrastructure Studies” . MLA 2018. New York, September 6 2018.&#160;<a href="#fnref:24" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:25">
<p>Montfort, Nick and Bogost, Ian,  <em>Racing the Beam the Atari Video Computer System</em> . Cambridge, MA: MIT Press, 2009.&#160;<a href="#fnref:25" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:26">
<p>Sayers, Jentery (ed.),  <em>Making Things and Drawing Boundaries</em> . Minneapolis: University of Minnesota Press, 2017.&#160;<a href="#fnref:26" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:27">
<p>Smithies, James.  “Full Stack DH: Building a Virtual Research Environment on a Raspberry Pi” , In  <em>Making Things and Drawing Boundaries: Experiments in the Digital Humanities</em> , edited by Jentery Sayers. Debates in DH. Minneapolis: University of Minnesota Press, 2018, pp.102-114.&#160;<a href="#fnref:27" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:28">
<p>Ramsay, Stephen and Rockwell, Geoffrey,  “Developing Things: Notes toward an Epistemology of Building in the Digital Humanities” , in Matthew K. Gold (ed.),  <em>Debates in the Digital Humanities</em> . Minneapolis: Michigan University Press, 2012, p. 75–84.&#160;<a href="#fnref:28" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:29">
<p>Anderson, Sheila,  “What are Research Infrastructures?” ,   <em>International Journal of Humanities and Arts Computing</em> , Volume 7 Issue 1-2, pp.4-23, (2013).&#160;<a href="#fnref:29" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:30">
<p>Dombrowski, Quinn.  “What Ever Happened to Project Bamboo?”    <em>Literary and Linguistic Computing</em>  29, no. 3 (September 1, 2014): 326–39.&#160;<a href="#fnref:30" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:31">
<p>Verhoeven, Deb, and Toby Burrows.  “Aggregating Cultural Heritage Data for Research Use: The Humanities Networked Infrastructure (HuNI).”  in Emmanouel Garoufallou, Richard J. Hartley, and Panorea Gaitanou (eds.),  <em>Metadata and Semantics Research</em> , Springer International Publishing [Communications in Computer and Information Science 544], 2015, pp. 417–23.&#160;<a href="#fnref:31" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:32">
<p>Verhoeven, Deb.  “As Luck Would Have It.”    <em>Feminist Media Histories</em>  2, no. 1 (2016): 7–28&#160;<a href="#fnref:32" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:33">
<p>Bowker, Geoffrey C.,  <em>Social Science, Technical Systems, and Cooperative Work: Beyond the Great Divide</em> . Lawrence Erlbaum Associates, 1997.&#160;<a href="#fnref:33" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:34">
<p>Dourish, Paul and Bell, Genevieve,  <em>Divining a Digital Future: Mess and Mythology in Ubiquitous Computing</em> . Cambridge, MA: The MIT Press, 2014.&#160;<a href="#fnref:34" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:35">
<p>European Science Foundation (ESF), Science Policy Briefing (SPB) on Research Infrastructures in the Digital Humanities, 2011 <a href="http://archives.esf.org/fileadmin/Public_documents/Publications/spb42_RI_DigitalHumanities.pdf">http://archives.esf.org/fileadmin/Public_documents/Publications/spb42_RI_DigitalHumanities.pdf</a>&#160;<a href="#fnref:35" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:36">
<p>Ciula, A, Nyhan, J, Moulin, C,  “Science Policy briefing on research infrastructures in the digital humanities: landscapes, ecosystems and cultures” .  <em>Lexicon Philosophicum</em> , 1, p. 277-287, 2013.&#160;<a href="#fnref:36" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:37">
<p>The Open Research Data Task Force,  “Research Data Infrastructures in the UK.”  Universities UK, 2017 <a href="http://www.universitiesuk.ac.uk/policy-and-analysis/research-policy/open-science/Pages/open-research-data-task-force.aspx#sthash.oMf7Po5F.dpbs">http://www.universitiesuk.ac.uk/policy-and-analysis/research-policy/open-science/Pages/open-research-data-task-force.aspx#sthash.oMf7Po5F.dpbs</a> (accessed February 21, 2018).&#160;<a href="#fnref:37" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:37" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:38">
<p>Alys, Brett, Croucher, Michael, Haines, Robert, Hettrick, Simon, Hetherington, James, Stillwell, Mark and Wyatt, Claire,  “Research Software Engineers: State of the Nation Report 2017.”  Zenodo, April 6, 2017. <a href="https://doi.org/10.5281/zenodo.495360">https://doi.org/10.5281/zenodo.495360</a> (accessed Febuary 21, 2018).&#160;<a href="#fnref:38" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:39">
<p>Agile Business Consortium,  “The DSDM Agile Project Framework (2014 Onwards)” , Agile Business Consortium, February 4, 2016. <a href="https://www.agilebusiness.org/resources/dsdm-handbooks/the-dsdm-agile-project-framework-2014-onwards">https://www.agilebusiness.org/resources/dsdm-handbooks/the-dsdm-agile-project-framework-2014-onwards</a> (accessed February 21, 2018).&#160;<a href="#fnref:39" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:40">
<p>Future articles are planned to detail our SDLC, which is too involved to describe in detail here. It is based on the Agile DSDM® method for those interested in exploring more. See <a href="https://www.kdl.kcl.ac.uk/how-we-work/why-work-us/">https://www.kdl.kcl.ac.uk/how-we-work/why-work-us/</a> for a high-level overview.&#160;<a href="#fnref:40" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:41">
<p>Conway, Paul,  “Preservation in the Age of Google: Digitization, Digital Preservation, and Dilemmas.”    <em>Library Quarterly</em>  80, no. 1 (January 2010).&#160;<a href="#fnref:41" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:41" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:42">
<p>Adams, Wright R.  “Archiving Digital Materials: An Overview of the Issues.”    <em>Journal of Interlibrary Loan, Document Delivery &amp; Electronic Reserve</em>  19, no. 4 (2009): 325–335.&#160;<a href="#fnref:42" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:43">
<p>Tanner, Simon,  <em>Measuring the Impact of Digital Resources: The Balanced Value Impact Model</em> . King’s College London, October 2012.&#160;<a href="#fnref:43" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:44">
<p>KDL’s SDLC templates are made progressively available in our GitHub repository: <a href="https://github.com/kingsdigitallab/sdlc-for-rse">https://github.com/kingsdigitallab/sdlc-for-rse</a>.&#160;<a href="#fnref:44" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:45">
<p>For phase 1, projects were selected on the basis of low research value and minimal complexity, e.g. underused blog sites, orphaned pilot sites, low relevance to King’s College and King’s Digital Lab (e.g. hosting of personal Wordpress sites, conference sites without updates since the main event, etc). A template email, offering four options (Service Level Agreement, migration to own host, migration to ITS microsite (if a King’s partner), or archiving) was sent to projects partners. Recipients were asked to respond within six weeks, after which we said that permission would be sought from Faculty to decommission the web resource. A further month from this, the site would be decommissioned. The majority of project partners responded in a timely fashion, and we successfully agreed on a future path for individual resources.&#160;<a href="#fnref:45" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:46">
<p>Phase 2, analysis of the remaining legacy projects (more complex digital research outputs, primarily REF-able and perceived to be of mid- to high research value) began with an investigation within the lab to unearth institutional memory of the affected projects. They were prioritised according to the categories SLA supported (where no moneys would be charged to the project in the contractual period), SLA paid (charged according to cost recovery including overheads for services rendered over the contractual period), migration (to static legacy server maintained gratis by KDL at low cost, or to another host), or archiving (with AWS Glacier or similar). Responses were requested within 6 weeks.&#160;<a href="#fnref:46" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:47">
<p>In the final phase, we aggregated preferred outcomes from the pilot, first and second phases.&#160;<a href="#fnref:47" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:48">
<p>Although we weren’t aware of it at the time, this approach aligns well to Matthew Addis’ notion of  “Minimal Viable Preservation” , which recommends an  “engineering approach”  to preservation that takes care of straight-forward issues before moving onto more complex (and therefore costly) cases. Matthew Addis.  “Minimum Viable Preservation - Digital Preservation Coalition.”  Digital Preservation Coalition, November 12, 2018. <a href="https://dpconline.org/blog/minimum-viable-preservation">https://dpconline.org/blog/minimum-viable-preservation</a> (accessed November 28, 2018).&#160;<a href="#fnref:48" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:49">
<p>Jakeman, Neil,  “A Low Bandwidth Solution for Cultural Heritage Web Content” ,  <em>King’s Digital Lab</em> , January 17, 2018. <a href="https://www.kdl.kcl.ac.uk/blog/african-rock-art/">https://www.kdl.kcl.ac.uk/blog/african-rock-art/</a> (accessed February 21, 2018).&#160;<a href="#fnref:49" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:50">
<p>Ciula, Arianna,  “Call for Expressions of Interest: Your Novel Idea of Publication” ,  <em>King’s Digital Lab</em> , November 22, 2017. <a href="https://www.kdl.kcl.ac.uk/blog/call-expressions-interest-your-novel-idea-publication/">https://www.kdl.kcl.ac.uk/blog/call-expressions-interest-your-novel-idea-publication/</a> (accessed February 21, 2018).&#160;<a href="#fnref:50" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:51">
<p>Nicholson, Craig.  “Keeping the Lights On” .  <em>Research Europe</em> , February 22 (2018): 13.&#160;<a href="#fnref:51" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:52">
<p>Short, Harold, Nyhan, Julianne, Welsh, Anne and Salmon, Jessica.  “ Collaboration Must Be Fundamental or It’s Not Going to Work : an Oral History Conversation between Harold Short and Julianne Nyhan.”    “Digital Humanities Quarterly”  6, no 3 (2012). Retrieved from <a href="/dhqwords/vol/6/3/000133/"> http://www.digitalhumanities.org/dhq/vol/6/3/000133/000133.html</a>&#160;<a href="#fnref:52" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry><entry><title type="html">Modelling Medieval Hands: Practical OCR for Caroline Minuscule</title><link href="https://rlskoeser.github.io/dhqwords/vol/13/1/000412/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://rlskoeser.github.io/dhqwords/vol/13/1/000412/</id><author><name>Brandon W. Hawk</name></author><author><name>Antonia Karaisl</name></author><author><name>Nick White</name></author><published>2019-04-26T00:00:00+00:00</published><updated>2019-04-26T00:00:00+00:00</updated><content type="html"><![CDATA[<h1 id="modelling-medieval-hands-practical-ocr-for-caroline-minuscule">Modelling Medieval Hands: Practical OCR for Caroline Minuscule</h1>
<h2 id="introduction">Introduction</h2>
<p>In an age replete with digital media, much of the content we access is the result of Optical Character Recognition (OCR), the rendering of handwritten, typed, or printed text into machine-readable form. On a more specific scale, OCR has increasingly become part of scholarly inquiry in the humanities. For example, it is fundamental to Google Books, the Internet Archive, and HathiTrust, corpus creation for large-scale text analysis, and various aspects of digital humanities. As a number of recent studies and projects have demonstrated, the results of OCR offer a wide range of possibilities for accessing and analyzing texts in new ways.<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></p>
<p>OCR has shifted the range and scope of humanistic study in ways that were not possible before the advent of computers. As stated by a team of scholars led by Mark Algee-Hewitt at the Stanford Literary Lab,    “Of the novelties produced by digitization in the study of literature, the size of the archive is probably the most dramatic: &hellip;now we can analyze thousands of [texts], tens of thousands, tomorrow hundreds of thousands”   <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>   (cf. <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>). Beyond literary study specifically, new possibilities due to the mass digitization of archives have emerged across the humanities. Historians of books, texts, and visual arts (to name just a few areas) now have ready access to many more materials from archives than previous generations. Among the new pursuits of humanities scholars, computer-aided studies often no longer focus only on a handful of texts but encompass large-scale corpora — that is, collections of hundreds or thousands of texts.<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>  Much of this is made possible by OCR.</p>
<p>Yet most studies and applications of OCR to date concern printed books (see, e.g., <sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>; <sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>; and <sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>). The majority of text-mining projects in the humanities focus on eighteenth- and nineteenth-century printed texts. <sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>  One way to expand the potential for humanistic studies even further is to apply OCR tools to extract data from medieval manuscripts, but this area of research has received much less attention.<sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup>  Indeed, the current situation with using OCR on medieval manuscripts is not much different from 1978, when John J. Nitti claimed that    “no OCR (Optical Character Recognition) device capable of reading and decoding thirteenth-century Gothic script was forthcoming”   <sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup>  . Now, as then, there has been little progress on using OCR to decipher Gothic or any other medieval script, regardless of type, date, or origin.</p>
<p>This article presents the results of a series of experiments with open-source neural network OCR software on a total of 88 medieval manuscripts ranging from the ninth through thirteenth centuries.<sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>  Our scope in these experiments focused mainly on manuscripts written in Caroline minuscule, as well as a handful of test cases toward the end of our date range written in what may be called  “Late Caroline”  and  “Early Gothic”  scripts (termed transitional when taken together).<sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>  In the following, we discuss the possibilities and challenges of using OCR on medieval manuscripts, neural network technology and its use in OCR software, the process and results of our experiments, and how these results offer a baseline for future research. Our results show potential for contributing to not only text recognition as such but also other areas of bibliography like paleographical analysis. In all of this, we want to emphasize the use of open-source software and sharing of data for decentralized, large-scale OCR with manuscripts in order to open up new collaborative avenues for innovation in the digital humanities and medieval studies.</p>
<h2 id="medieval-manuscripts-and-ocr">Medieval Manuscripts and OCR</h2>
<p>The field of medieval studies, of course, relies on the transcription and editing of texts for analysis. Work with OCR on medieval manuscripts is potentially useful considering how many medieval texts remain untranscribed or unedited. In many cases, this is because of the unwieldiness of editorial projects dealing with hundreds of witnesses. In other cases, texts remain obscure because they are overlooked or ignored in the shadow of more canonical works. While digitization has expanded the size of the archive for humanities scholarship in substantial ways, medieval studies still has work to do before this archive may be used for examinations of understudied texts or large-scale analysis.</p>
<p>In this respect, we see the potential to harness open-source OCR software for medieval studies in a number of ways. Using OCR could help contribute to areas such as: the speed, efficiency, and accuracy of text transcription and editing; cross-manuscript comparisons of texts in multiple respects; paleographical analysis; studies of textual transmission; as well as corpus creation, searchability, and subsequent macroanalysis. For example, with the output of OCR from medieval manuscripts, we could compare versions of texts for editorial purposes with collation tools like JuxtaCommons, or for intertextual parallels within larger corpora with tools like Tesserae. With other methodologies in mind, using OCR for massive corpus creation would allow for text-mining with tools like Lexos or other statistical analysis software like R (as in <sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup>). Using OCR with medieval manuscripts also opens up new avenues of research related to scribal practices such as scripts and their variant spellings, irregular spacing, abbreviations, and errors — encompassing local and general considerations through diachronic and synchronic lenses.</p>
<p>Certainly there are difficulties to working with OCR software on medieval manuscripts that do not apply to printed books. Medieval scribes did not work with Unicode, nor even with modern typefaces. A few reasons why OCR has not been widely applied to medieval manuscripts, even though it is evidently a good idea, include irregular features of handwriting, abbreviations, and scribal idiosyncrasies; variations in spellings; non-standard page layouts; and deteriorated pages — among other issues that might appear. All of these factors make traditional OCR based on print technology difficult. Recently assessing the field of OCR for corpus creation in medieval studies, Michael Widner writes,    “Medieval manuscripts are practically impervious to contemporary OCR”   <sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup>   (cf. <sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup>). However, with the availability of open-source Artificial Neural Network (ANN) technology, as well as an increasing amount of digitized sources available via open access, OCR for medieval manuscripts is actually becoming a greater possibility.</p>
<p>We believe that there is a significant point to using OCR software — rather than other tools for handwritten text recognition (HTR) — because of the relative regularity of medieval hands and scripts overall, which may be analogous in some ways to print typefaces in a general way.<sup id="fnref:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup>  Recent work has questioned the efficacy of OCR and turned to HTR as a preliminary way to analyze script types even before moving on to the process of recognition (e.g. by <sup id="fnref:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup>). The differences between OCR (a long-established technology) and HTR (in an early phase of development) are subtle but substantial. The biggest difference lies with layout analysis and segmentation for processing: this stage is usually built into the OCR engine, while it is a separate stage in HTR. OCR tends to be known for a focus on processing individual letter-forms or characters as isolated parts of the whole text or text lines and is more widely used on printed texts. HTR typically processes whole words or lines, and is upheld as the optimal technology for handwritten texts.</p>
<p>Yet OCR and HTR technologies and processes have recently converged to some degree. Major OCR engines are also used on handwritten texts in various ways. Indeed, there is now some amount of overlap between OCR and HTR in practice because of developments in machine learning; for example, engines like OCRopus allow for new methods and analysis and segmentation (see below, on the OCRopus engine). New modes of using OCR software with ANN technology shift beyond specific focus on characters. ANN technology has reached a point that makes this possible for several reasons, more than the previous type of technology. While both OCR and ANN technologies have been around for much longer than often acknowledged, we now have more computing power than ever before. The time seems right to use that technology for the challenges that medieval manuscripts pose to OCR.</p>
<p>At the outset, it is important to recognize that our goal is not to eliminate human analytical work with computers. But using open-source OCR tools for manuscripts has the potential to limit the time of editing and to increase the efficiency of dealing with large numbers of witnesses. As is obvious from other uses of OCR for text-mining, post-processing brings the need for cleaning up  “dirty OCR”  through means of what David Mimno has called    “data carpentry”   <sup id="fnref:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup>  . For print books, post-processing means, at the least, eliminating extraneous data such as chapter numbers, hyphens, page numbers, page headers and footers, and apparatus, as well as unwanted  “noise”  produced in the OCR process (see <sup id="fnref1:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>; and <sup id="fnref1:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup>). Manuscripts may include many of the same elements, as well as their own idiosyncrasies like glosses and marginalia. Surely the human is integral to this whole process.</p>
<h2 id="artificial-neural-networks-for-ocr">Artificial Neural Networks for OCR</h2>
<p>Artificial neural network (ANN) technology, also known as deep learning,  machine learning, or just AI has been around for a surprisingly long time.<sup id="fnref:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup>  While recognized as a powerful method which could address a large range of problems in computing, its uptake was limited for years, in large part due to the need for very large training sets and general speed issues. However, its use has exploded in recent years as storage and computer processing is much cheaper, and in some sectors massive labelled training sets can be assembled automatically using the labor of unknowing web users (see, e.g., <sup id="fnref:20"><a href="#fn:20" class="footnote-ref" role="doc-noteref">20</a></sup>; and <sup id="fnref:21"><a href="#fn:21" class="footnote-ref" role="doc-noteref">21</a></sup>). ANNs are useful in a large variety of applications and are particularly good at dealing with fuzziness and uncertainty in data, identifying patterns in a noisy world.</p>
<p>The basic idea behind ANNs is that they use algorithms to create a function that can perform some action, such as labelling different breeds of dogs, given a large training set, by repeatedly refining and testing different versions of a model. As this  “training”  process continues, the network becomes more and more accurate, until a point at which it plateaus and can no longer improve without more input data or changes in the initial configuration of the ANN. The model generated at each step is composed of many simple yes/no gates, automatically created by the training algorithms, which are given different weights through different iterations of the training process until an optimal configuration is found. These gates can be compared to biological neurons in the brain, which is where the name neural network comes from.</p>
<p>There are many types of ANNs, and each is appropriate for different uses. Deep neural networks (DNNs) use many hidden layers, which result in significantly more complex, but generally more accurate, recognition. Recurrent neural networks (RNNs) are a type of DNN in which different layers can refer back to each other, which has the effect of enabling some contextual memory — which is to say that later parts of recognition can make reference to earlier parts, and vice-versa. Finally, a type of RNN that has found much favor in OCR and many other applications is called Long Short Term Memory (LSTM). This is a clever configuration of a RNN such that contextual memory can endure for a long time when it is useful without skewing recognition results in other cases, which could happen with earlier RNN variants. The combination of a long contextual memory and a neural network makes LSTM very well suited to tasks like OCR and speech recognition, where the context (of earlier and later characteristics like pixels, sounds, characters, words, and beyond) is very helpful in inferring the correct result.</p>
<p>These ideas can be difficult to conceptualize in the abstract, so it helps to take a look at how they work in the case of OCR. The following description and accompanying images should help to make the process clearer. A training set will typically consist of many images, each containing a line of text, with accompanying  “labels”  that the ANN should somehow learn to associate with the parts of each image, until it can do this correctly even for unseen images. This can be done in different ways, so here we discuss how the open-source OCRopus engine works using a LSTM neural network.</p>
<p>First, the page image used for training the neural networks is split up into the lines that make up the text (see <a href="#figure01">Figure 1</a>). Each line image is matched with a text transcription as a label — the UTF-8 formatted text corresponding to the text in the image, as transcribed by a human in order to train the computer how to recognize the text. All matching pairs of image and text lines taken together make up the so-called  “ground truth”  which is used to train the OCR model. This stage of the process creates image-text pairs, between the segmented line image and the ground truth transcription (labels), for the purpose of training the engine.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/13/1/000412/resources/images/Figure_2.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/13/1/000412/resources/images/Figure_2_hub76c0d742a4ab44958e41ec9813d7ff2_154140_500x0_resize_box_3.png 500w,
    /dhqwords/vol/13/1/000412/resources/images/Figure_2_hub76c0d742a4ab44958e41ec9813d7ff2_154140_800x0_resize_box_3.png 800w,/dhqwords/vol/13/1/000412/resources/images/Figure_2_hub76c0d742a4ab44958e41ec9813d7ff2_154140_1200x0_resize_box_3.png 1200w,/dhqwords/vol/13/1/000412/resources/images/Figure_2.png 1400w" 
     class="landscape"
     >
</figure>
<p>In the next step, the actual training process, each ground truth image line is split into vertical lines 1 pixel wide and fed into the LSTM network in order (see <a href="#figure02">Figure 2</a>). In this case the  “memory”  of the LSTM network refers to the vertical lines before and after a point being considered. The LSTM training engine will go through the lines one-by-one, creating a model of how each line relates to those around it, with the lines further away generally having less weight than those closest (but still able to influence things, thanks to the  “long”  memory aspect of LSTM). The exact configuration and weighting of different nodes of the LSTM network is altered many times during this process, each time comparing the result of OCR with this test network on the ground truth image lines. After a few tens of thousands of iterations, this process gradually finds the configuration which produces the best results for the whole corpus of ground truth.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/13/1/000412/resources/images/Figure_1.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/13/1/000412/resources/images/Figure_1_hu8754603ba53e3f49dfa25b82e59818df_13533_500x0_resize_box_3.png 500w,
    /dhqwords/vol/13/1/000412/resources/images/Figure_1_hu8754603ba53e3f49dfa25b82e59818df_13533_800x0_resize_box_3.png 800w,/dhqwords/vol/13/1/000412/resources/images/Figure_1.png 659w" 
     class="landscape"
     >
</figure>
<p>By contrast, traditional OCR methods are based on rules defined by the designers of the system, defining algorithms for how best to split characters and match those characters to characters that have been previously seen. While intuitively we might expect human experts to do a better job at designing methods to do OCR than an RNN, recursive neural network systems consistently perform far better in practice. This is a well-recognized feature of recursive neural network systems <sup id="fnref:22"><a href="#fn:22" class="footnote-ref" role="doc-noteref">22</a></sup>. Again, perhaps surprisingly, neural networks are in particular very good at handling the fuzziness of real world inputs. Needless to say, this works in our favor for OCR in general, and for OCR of historical documents in particular.</p>
<p>While many medieval manuscripts are relatively regular in style and script, there is inevitably more variation in script than with printed documents. When coupled with the large number of variant spellings, irregular spacing, abbreviations, and errors used in the period, traditional OCR engines are not able to provide a good enough accuracy to be particularly useful. The power of OCR based on LSTM technology changes the situation, due to the significantly higher accuracy level in general, the ability to take as much context as is helpful into account, and the ability to better tolerate variation and other sorts of  “noise”  that crop up in centuries-old handwritten documents (see <sup id="fnref2:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>).</p>
<p>Of the RNN software currently available, our choice fell on OCRopus, an open-source program designed to train neural networks for optical character recognition. With respect to our purpose, OCRopus has some specific advantages vis-a-vis other open-source programs. First, it is distributed under the Apache License and freely usable by anyone under the outlined conditions. Second, OCRopus provides an easily modifiable set of commands that allows users to adapt the single steps of OCR (such as binarization, segmentation, training, and recognition) to their specific purposes. OCRopus is not a turn-key system, or software ready for use as is. Rather, this type of software requires the users to train their own models, the key input to the process; hence a degree of technical know-how is required. Even so, it offers the advantage that if any of the code has to be modified, or third-party programs used for discrete steps, this can be done in a simple, localized fashion without repercussions for the overall process.<sup id="fnref:23"><a href="#fn:23" class="footnote-ref" role="doc-noteref">23</a></sup></p>
<p>Unlike alternative OCR programs like Tesseract or Oculus, OCRopus’s neural networks are trained in a language-agnostic fashion — that is, exclusively focused on characters rather than words (see, e.g., <sup id="fnref:24"><a href="#fn:24" class="footnote-ref" role="doc-noteref">24</a></sup>; and <sup id="fnref:25"><a href="#fn:25" class="footnote-ref" role="doc-noteref">25</a></sup>). While OCRopus offers sets trained for specific languages like English, there is no corrective process interpreting strings of characters with the help of a set dictionary as a language-focused OCR program might. Although this sort of function can be of great use when adapting OCR for modern languages with a fixed orthography, this freedom from orthographic rules proves convenient when working with medieval manuscripts in Latin replete with variant spellings, irregular spacing, abbreviations, and errors. Indeed, for our purposes, this gives OCRopus an advantage over other OCR software and makes it a viable choice even in comparison with HTR technology.</p>
<p>Additionally, the documentation provided for OCRopus by both the original developers and users opens up the process significantly for further applications. One example is previous experimentation in using OCRopus with incunables by Uwe Springmann and Anke Lüdeling <sup id="fnref:26"><a href="#fn:26" class="footnote-ref" role="doc-noteref">26</a></sup>. Although incunables are printed, and of a greater regularity than expected from manuscripts, instructions and experiment results that Springmann and Lüdeling laid out provided an initial baseline for us to conduct experiments with.<sup id="fnref:27"><a href="#fn:27" class="footnote-ref" role="doc-noteref">27</a></sup></p>
<p>The open-source OCRopus engine uses a reasonably standard LSTM architecture. As mentioned above, it splits a page image into lines before passing the result to the LSTM engine, for either training or recognition. This is in contrast to most HTR systems, which cannot rely on lines being straight and mostly non-overlapping, and therefore have to rely on a more complex architecture. However, for printed texts and more palatable manuscript scripts like Caroline minuscule this is fortunately unnecessary, which means we can use a simpler architecture which is faster, smaller, and more accurate <sup id="fnref:28"><a href="#fn:28" class="footnote-ref" role="doc-noteref">28</a></sup>. The initial line-splitting is done with the tool ocropus-gpageseg,<sup id="fnref:29"><a href="#fn:29" class="footnote-ref" role="doc-noteref">29</a></sup>  which analyses a page image and outputs a series of lines in PNG format. One can then either perform recognition on these lines using an existing model, with the ocropus-rpred tool, or create a series of ground truth files that correspond to the lines (named, e.g., imgname.gt.txt) and train a new neural network model with the ocropus-rtrain tool.</p>
<h2 id="process-and-results">Process and Results</h2>
<h2 id="1-objective-and-theoretical-approach">1. Objective and theoretical approach</h2>
<p>The basic objective of our experiments with OCR software based on neural network technology was to develop a workable, ideally time-saving, but sufficiently accurate solution for recognizing text in medieval manuscripts using open-source software.</p>
<p>At the start of our research, there were no known efforts publicly documented that would demonstrate the success of applying OCR to medieval manuscripts. Our initial experiments were therefore designed to demonstrate whether neural networks can be trained on medieval manuscripts in the first place, using ground truth created from target manuscripts. Results over the course of our experiments yielded character and word accuracy rates over 90%, reaching 94% and 97% accuracy in some instances. With the benefit of hindsight, these findings concur with the excellent results achieved with digitized manuscripts from the Royal Chancellory in Paris by the HIMANIS project, launched in 2017, albeit not with open-source software <sup id="fnref:30"><a href="#fn:30" class="footnote-ref" role="doc-noteref">30</a></sup>. We might also compare these results to those of the Durham Priory Library OCR Project conducted using OCRopus <sup id="fnref:31"><a href="#fn:31" class="footnote-ref" role="doc-noteref">31</a></sup>.</p>
<p>The viability of developing an open-source OCR solution based on neural network technology for medieval manuscripts can be considered proven in point. Even more, a model trained on the ground truth from a specific, single manuscript can yield high-accuracy results with that particular manuscript. We see this in the case of our experiment with Arras, Bibliothèque municipale 764, for which we achieved a shocking 97.06% accuracy rate for OCR based on ground truth transcription from this single manuscript. With a view to the primary objective of large-scale, collaborative work on manuscripts, the involvement of open-source software would be a desirable, expedient, and time-saving approach — ideally by developing a turn-key model. Rather than focusing on whether manuscript OCR is possible, our experiments tried to ascertain some best practices.</p>
<p>The hypothesis to be tested in our experiments, therefore, was that size and diversity of the training pool would be directly proportional to the quality of the resulting model when tested on seen and unseen manuscripts within the realm of Caroline minuscule scripts. Seen in the context of this article refers to test manuscripts included in the training pool, albeit different pages; unseen refers to manuscripts not included in the training pool. While the results of seen manuscripts would be relevant in the research, developing a strategy for building a turn-key model for unseen manuscripts of a certain denomination is our ultimate goal. Among the specific types of application for neural networks, the strategic selection of training material seems to matter where there is no unlimited amount of data available, also depending on the type of material involved. That is, although the training pool can presumably never be big enough, the question remains about what kind of diversity is actually beneficial.</p>
<p>In the context of medieval manuscripts, the accuracy of the results is rarely satisfactory when a model trained on one specific manuscript is applied to an unseen test manuscript. This concurs with the result of Springmann and Lüdeling’s experiments with incunables <sup id="fnref1:26"><a href="#fn:26" class="footnote-ref" role="doc-noteref">26</a></sup>. In a further step, however, Springmann and Lüdeling’s experiments sought to combine the ground truth from different incunables to observe the outcome. The experiment was designed with a diachronic corpus of texts spanning several centuries and the range of training and testing manuscripts was significantly broad. The mixed models developed yielded reliably worse results than the pure models where the OCR training pool contained ground truth from the target incunable only. This problem can be expected to only exacerbate in the case of medieval manuscripts where it is not only the different  “font,”  so to speak (the script), but also the idiosyncratic execution of the scribe (the hand) that sets one manuscript apart from another. Since single-manuscript models do not perform well for other manuscripts, however, the mixing of ground truth from different manuscripts would be the only way to develop a turn-key model for unseen manuscripts. The question is whether the trade-off between a decreased accuracy for seen manuscripts and time saved by using a turn-key model would be made worthwhile by sufficiently good results for unseen manuscripts.</p>
<p>Conceptually, it is useful to liken the situation of manuscript OCR to that of speech recognition technology. Handwritten texts are deliberately similar within script categories, but without a definite blueprint they all refer to; conversely, there is no such thing as the perfectly pronounced natural speech, only variations of it (male-female voice, old-young, accent-native). Within the pool of the spoken sound, there are boundaries defined by languages: within each, specific sounds (phonemes) will consistently map to specific letter combinations (graphemes). Across a range of languages, however, with different relationships of phonemes and graphemes, this consistent mapping would get confused. Accordingly, training pools for speech recognition algorithms tend to be limited to data in one language only.</p>
<p>Comparing the case of medieval manuscripts, not phonemes but characters (abbreviations included) map directly to letters or letter combinations from the Latin alphabet — at least to the extent that the relationship is consistent within each manuscript. Although medieval manuscripts can be written in a variety of languages, the linguistic boundary seems less relevant than the typographic one: across a range of hands, some characters of different scripts resembling each other correspond with different letters of the Latin alphabet. The Visigoth  <strong>a</strong> , for example, looks more similar to a Caroline  <strong>u</strong>  than our expectation of an  <strong>a</strong> . Defining the boundary of the training pool should therefore exclude instances where two characters from separate medieval hands resemble each other, but map to different letters (whereas mapping one Latin letter to two different-looking characters would not necessarily pose a problem).</p>
<p>Following that analogy, we limited the training pool mainly to one script type. Given its spread in Western Europe, across a range of geographies and time periods, Caroline minuscule naturally suggested itself as a more easily read and widespread script. Eventually, we did include manuscripts that pushed beyond initial models based on Caroline minuscule. In our second phase of testing, we also incorporated manuscripts containing scripts with features exhibiting later developments. Such manuscripts contain transitional Late Caroline and Early Gothic scripts (see the Appendix). These manuscripts helped to expand our experiments somewhat to begin seeing further results about the diversity of our training pool.</p>
<p>The implicit goal was to develop a model that could deliver a character accuracy of at least 90% on unseen manuscripts. As per our experience, certain technical parameters are crucial to achieve good output: good quality, high-resolution images; a minimum line height, even if that significantly slows down OCRopus’s processing in the segmentation and recognition steps; and eliminating skewed results from large illustrations, difficult layouts, and discolorations. The quality of both testing and training manuscripts can skew the interpretation: for example, a testing manuscript of extremely high quality might compare all too favorably and skew expectations with regard to the average outcome. Within our overall framework, the goal was to establish tendencies rather than definite results.</p>
<p>For the current project, the expansion of medieval abbreviations has been treated as a post-processing problem and bracketed out from the experimentation objectives. With a view to future research, however, we believe there to be scope for LSTM models to be trained to correctly expand ambiguous abbreviations.</p>
<p>The overall experiment proceeded in two steps: testing models from training pools as small as 2-5 manuscripts, and pools of 50 manuscripts upwards.</p>
<h2 id="2-size-and-diversity-in-small-models">2. Size and diversity in small models</h2>
<p>In the first round of experiments, we combined single pages of a small number of manuscripts written in Caroline minuscule in a training pool and tested them on different pages from these seen manuscripts. In the process, models were built from 2-5 different manuscripts. This step very much resembled the experiments with mixed models done by Springmann and Lüdeling on incunables and our results concurred with theirs <sup id="fnref2:26"><a href="#fn:26" class="footnote-ref" role="doc-noteref">26</a></sup>. At the time, our assumption was that diversity within a training pool would be unequivocally beneficial in all cases. When tested on target manuscripts, however, these small, mixed models performed uniformly worse on seen manuscripts than pure models — that is, these results directly contradicted our assumption that a greater variety in a training pool would always lead to greater accuracy of the results.</p>
<p>Looking at the results more closely for both seen and unseen manuscripts brings more nuanced tendencies to the fore. Foremost, we encountered a phenomenon we termed relative preponderance in the context: the proportionally higher or lower representation of a manuscript or subgroup of manuscripts in the training pool and the subsequent effect on the accuracy of the respective test manuscript or subgroup. For example, when testing on seen manuscripts, i.e. manuscripts represented in the training pool, accuracy decreased with diversity of the training pool (<a href="#figure03">Figure 3</a>). In other words, the lesser the relative preponderance of the seen manuscript in the training pool, the lower the accuracy of the outcome.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/13/1/000412/resources/images/Figure_3.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/13/1/000412/resources/images/Figure_3_hue3bb6389d13015b8d53097d6689b9cd3_93628_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/13/1/000412/resources/images/Figure_3_hue3bb6389d13015b8d53097d6689b9cd3_93628_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/13/1/000412/resources/images/Figure_3_hue3bb6389d13015b8d53097d6689b9cd3_93628_1200x0_resize_q75_box.jpg 1200w,/dhqwords/vol/13/1/000412/resources/images/Figure_3.jpg 1423w" 
     class="landscape"
     >
</figure>
<p>An exception to this rule was a model composed of 100 lines of Arras, Bibliothèque municipale 764 (c.800-1100, France and England), and around 100 lines of Wolfenbüttel, Herzog August Bibliothek, Weissenburg 48 (c.840-860, Weissenburg?). With an error rate of 2.33% when tested on unseen pages of Arras 764 the model outperformed the model built from 300 lines of Arras 764 combined with 100 lines of Weissenburg 48 (3.97% error rate) and the original model on only 300 lines from Arras 764 on the same test pages (2.53% error rate). The error rates in this case were surprisingly good for Arras 764 and, with further experiments, proven exceptional rather than a rule. A further elaboration on the experiment combined the original ground truth of Arras 764 with pages of ground truth from other manuscripts. The result shows that the model created from a combination of Arras 764 and Weissenburg 48 was some kind of an outlier: in the majority of cases, the model formed from two different manuscripts did not outperform the original model trained exclusively on ground truth from the target manuscript.</p>
<p>A careful conclusion would have it that a variety of two or more manuscripts does not yield a better foundation for an OCR model for either target manuscript as a rule, but that specific combinations of manuscripts  <em>can</em>  yield exceptional results. In some cases, the reasons behind such results or the criteria for the respective manuscripts to be combined are not entirely clear. On the whole, mixing additional manuscripts into the training pool of a seen manuscript adversely affects the accuracy of the outcome.</p>
<p>When testing these models on unseen manuscripts, the trend was mostly the reverse: the more different manuscripts were included in the model, the better the results were for the unseen test manuscripts (<a href="#figure04">Figure 4</a>). As the graphed data shows, there is not a linear relationship between size and diversity of the training pool and the accuracy of the outcome. With so small a training and testing pool, the idiosyncrasies of each manuscript may also play a role. For the majority of the test cases, the results for unseen manuscripts were better for bigger training pools.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/13/1/000412/resources/images/Figure_4.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/13/1/000412/resources/images/Figure_4_hu478d6eb7b7b1c4150651eb87341d5b40_57767_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/13/1/000412/resources/images/Figure_4_hu478d6eb7b7b1c4150651eb87341d5b40_57767_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/13/1/000412/resources/images/Figure_4.jpg 745w" 
     class="landscape"
     >
</figure>
<p>The general trend confirms that models tested on seen manuscripts yield worse results the more different manuscripts are added to the training pool. This finding goes against the grain of the assumption that a greater quantity and diversity of manuscripts within the training pool would make for a better model. Our explanation is that at such a low number of manuscripts combined, the accuracy in the results is chiefly determined by the relative preponderance of a seen manuscript in the training pool. In other words, the more its presence is diluted through the addition of more manuscripts, the lesser the accuracy in the end result.</p>
<p>In the case of unseen manuscripts, the proportionality between quality of the model and diversity and size of the training pool holds. In this case, the relationship showed that the more different manuscripts were included in the training pool, the better were the results. Conceptually, our interpretation of this behavior is that while the relative preponderance of a seen manuscript is largely responsible for the accuracy for models of small size, the larger the diversity — and therefore complexity — in the training process, the better a model can deal with the unfamiliar hand of an unseen manuscript. Although the accuracy achieved with models of small size was not satisfactory, the findings suggest that the results would only improve with an increase in training material.</p>
<h2 id="3-size-diversity-and-accuracy-for-bigger-models">3. Size, diversity, and accuracy for bigger models</h2>
<p>With the ultimate objective of our experiments in mind — building a turn-key OCR model applicable to as large a range of unseen manuscripts as possible — the number of ground truth lines in the training pool were dramatically increased. The transcribed pages of 50 randomly chosen manuscripts of the time period between the ninth and eleventh century and written in Caroline minuscule or a transitional script were combined in one training pool and tested on seen manuscripts included in the training pool and unseen manuscripts not included in the training pool.</p>
<p>The results show that the relationship discovered between size of the training pool and the accuracy for the seen manuscript peaked (<a href="#figure05">Figure 5</a>). At some point, the relative preponderance of the target manuscript included in the training pool ceased to matter. Instead, the increase in size of the training pool was directly proportional to the accuracy achieved with the resulting model, almost uniformly.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/13/1/000412/resources/images/Figure_5.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/13/1/000412/resources/images/Figure_5_huf4600316736d6b4f602ce185a0bf37ca_46255_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/13/1/000412/resources/images/Figure_5_huf4600316736d6b4f602ce185a0bf37ca_46255_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/13/1/000412/resources/images/Figure_5.jpg 795w" 
     class="landscape"
     >
</figure>
<p>The relationship between the size of the training pool and accuracy of the outcome continued to hold for unseen manuscripts (<a href="#figure06">Figure 6</a>). Yet looking at the improvement of the error rate, the experiments show that, in a small training pool, the addition of one or two manuscripts makes a veritable difference, while in the case of larger pools the difference made by each addition becomes marginal. In other words, while the improvement of model accuracy is proportional to the quantity of manuscripts included, this is not a linear development but a flattening curve.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/13/1/000412/resources/images/Figure_6.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/13/1/000412/resources/images/Figure_6_hu590b03c897bb8238ccb02e38a375f9eb_51572_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/13/1/000412/resources/images/Figure_6_hu590b03c897bb8238ccb02e38a375f9eb_51572_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/13/1/000412/resources/images/Figure_6.jpg 829w" 
     class="landscape"
     >
</figure>
<p>Originally, the training and testing pool had been limited to Caroline minuscule. However, other than with languages, the definition of Caroline minuscule is not set in stone: there are many manuscripts that show transitional stages manifest in different character forms, a growing number of abbreviations, and changing layouts. In a way, the boundaries between  “pure”  and  “transitional”  scripts are also not absolute but very much subject to human discretion — a circumstance that curiously comes to the fore when testing computational approaches to the area of manuscript studies (cf. <sup id="fnref1:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup>).</p>
<p>In a similar vein, a degree of variety was represented in our training set, with some of the manuscripts containing Late Caroline and Early Gothic scripts. A direct comparison of the training sets compiled from Caroline minuscule and strictly excluding transitional forms, versus the same training set including some transitional forms, brought to the fore the result that the presence of transitional forms in the training pool uniformly improved the output, rather than making it worse. Whether this improvement was simply due to the increase of size or diversity of the training pool could not be established for sure at this stage.</p>
<p>Conversely, the diversity of the training pool had a greater influence on the results with unseen manuscripts outside the immediate boundary of Caroline minuscule, in this case with transitional scripts. In other words, the relative preponderance of  “outlier”  manuscripts in the big training pool showed the same effect as the relative preponderance of the  “seen”  manuscripts in a small training pool. When tested on an unseen Early Gothic manuscript, the results for a lower number of manuscripts in the pool showed better results than with further, strictly Caroline minuscule manuscripts added. Similar to the idea of relative preponderance of the seen manuscript in the training pool, the proportional representation of  “outlier”  scripts in the training pool affects the results with unseen Late Caroline and Early Gothic test manuscripts.</p>
<p>This hypothesis was confirmed in further experiments (<a href="#figure07">Figure 7</a>). To the training pool of fifty predominantly Caroline minuscule manuscripts we added ten Late Caroline and Early Gothic manuscripts for one model, ten Caroline manuscripts for another, and one model combining all three groups. The results showed that for the predominant manuscript group in the pool (with Caroline minuscule script) increase in size and diversity of the model almost uniformly improved the results, more than the increase of size with a view to uniformity. In this set of experiments, our best results achieved an accuracy rate of 94.22%.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/13/1/000412/resources/images/Figure_7.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/13/1/000412/resources/images/Figure_7_hu55b750094bd4472da6ba3db6d8bcdde5_98320_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/13/1/000412/resources/images/Figure_7_hu55b750094bd4472da6ba3db6d8bcdde5_98320_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/13/1/000412/resources/images/Figure_7_hu55b750094bd4472da6ba3db6d8bcdde5_98320_1200x0_resize_q75_box.jpg 1200w,/dhqwords/vol/13/1/000412/resources/images/Figure_7.jpg 1275w" 
     class="landscape"
     >
</figure>
<p>Differently said, in most cases, the addition of ten Late Caroline and Early Gothic manuscripts to the training pool (altogether combining a greater number of lines) made for better results than adding ten Caroline minuscule manuscripts. The difference of accuracy was very small in comparison but almost uniform across the Caroline minuscule test manuscripts. In all cases, the final, large model combining the original pool of fifty manuscripts, ten Late Caroline and Early Gothic, and ten Caroline minuscule performed best on all Caroline minuscule test manuscripts.</p>
<p>Conversely, the Late Caroline and Early Gothic test manuscripts uniformly performed best with the model combining the pool of fifty manuscripts with the ten additional Late Caroline and Early Gothic manuscripts, second best with the big combined model, and worst with the Caroline minuscule addition. Our conclusion is that the relative preponderance of Late Caroline and Early Gothic manuscripts in the training pool significantly influenced the outcome for  “outlier”  manuscripts, rather than the sheer increase in size of the training pool.</p>
<p>The resemblance in behavior with the example of seen manuscripts tested with small models suggests that there might be a similar peak whereby the relative preponderance of the outlier manuscripts in the training pool might cease to influence the outcome. Testing this hypothesis is not within the scope of this experiment but suggests an avenue for future research.</p>
<p>Lastly, it is notable that tests with  “outlier”  scripts other than Early Gothic — such as earlier manuscripts, for example — yielded neither remotely satisfactory results nor identifiable accuracy patterns across the test range of models. We found this to be the issue with two limit case examples from manuscripts written in Insular minuscule around the year 800. One example was a page from St. Gall, Stiftsbibliothek 761, a collection of medical texts, where none of our models achieved an error rate below 36%.<sup id="fnref:32"><a href="#fn:32" class="footnote-ref" role="doc-noteref">32</a></sup>  At this point in our research we have too little data to issue anything other than a hypothetical interpretation. Our conjecture is, however, that although overall stylistic distinctions between related scripts like Caroline minuscule and Insular (Caroline) minuscule are not considered immense according to paleographical accounts, even these smaller differences seem to have a big impact on the results with ANN OCR. The fact that these test cases failed to yield any substantial or regular OCR results suggests that the scope of the results of this experiment does not expand beyond the types of scripts used in the training data. These limit cases, then, demonstrate how a focus on script types is justified for further research; there is much more to be discovered in future experiments with this type of computer processing.</p>
<p>Given the regularity of behavior in the results with the Caroline and Early Gothic manuscripts, our experiments hint at how OCR technology based on neural networks might be used for further analytical purposes, with the potential to expand the same strategies beyond these script types. This became more apparent as we experimented with training and testing models made up of seen and unseen manuscripts, with a diversity of Caroline minuscule, Late Caroline, and Early Gothic scripts. Beyond pure text recognition (the main aim of using OCR software), results also indicate patterns that could help to better understand paleography and categories of scripts. While our investigations so far have mainly focused on manuscripts with Caroline minuscule script, the outliers we incorporated define another set of results with broader implications than text recognition as such.</p>
<p>One particular example of wider applicability may be seen with results from incorporating manuscripts written in scripts from the eleventh, twelfth, and thirteenth centuries. After all, script types during this period are often described as transitional, and paleographical distinctions between developments are difficult to classify (see <sup id="fnref:33"><a href="#fn:33" class="footnote-ref" role="doc-noteref">33</a></sup>; and <sup id="fnref:34"><a href="#fn:34" class="footnote-ref" role="doc-noteref">34</a></sup>). The experiments with different types of training and testing models based on more or less script diversity and greater or lesser relative preponderance of manuscripts groups showed up regular patterns in different contexts that could be employed for analytical purposes. Using these patterns for paleographical analysis is a tantalizing prospect. We hope to pursue these possibilities in the future, as we build more training and testing models to bear out results from further diversity in our experiments.</p>
<h2 id="conclusions">Conclusions</h2>
<p>Bibliography remains a fundamental basis of medieval studies, from traditional paleography to digital humanities databases. For example, research on scripts by paleographers like E. A. Lowe, Bernhard Bischoff, and R. A. B. Mynors formed the groundwork of the  <em>Codices Latini Antiquiores</em>  ( <em>CLA</em> ), which in turn gave rise to more recent endeavors in the digital age <sup id="fnref:35"><a href="#fn:35" class="footnote-ref" role="doc-noteref">35</a></sup> (cf. <sup id="fnref:36"><a href="#fn:36" class="footnote-ref" role="doc-noteref">36</a></sup>). Without the  <em>CLA</em> , we would not have examinations like Eltjo Buringh’s attempts to statistically quantify and come to terms with the implications of the production and loss of medieval manuscripts across the centuries <sup id="fnref:37"><a href="#fn:37" class="footnote-ref" role="doc-noteref">37</a></sup>. Similarly, without numerous bibliographical studies that came before, we would not have projects like  <em>The Production and Use of English Manuscripts 1060 to 1220</em>  by Orietta Da Rold, Takako Kato, Mary Swan, and Elaine Treharne, which provides valuable evidence about manuscripts, their contexts, and the history of books in early England <sup id="fnref:38"><a href="#fn:38" class="footnote-ref" role="doc-noteref">38</a></sup>.</p>
<p>Using OCR software on medieval manuscripts both rests on and extends the work of bibliographers with new possibilities brought about by digital tools. In all of this, it is important to acknowledge the human elements of digital humanities. These aspects include the starting point of research questions asked from a fundamentally humanistic perspective as well as the labor involved in data curation and carpentry, manuscript transcription and processing, and interpreting results to consider the payoff and possibilities that they provide for future pursuit. After all, digital humanities approaches are only possible through the combination of computers with the types of critical, analytical research questions that drive the humanities. OCR extends the investigation of books while also connecting bibliography with other fields for greater possibilities of innovative pursuits. It is our hope that using OCR with medieval manuscripts offers a new range of questions built on bibliography but with fresh implications for future research using computers.</p>
<p>Using OCR with medieval manuscripts also helps to confirm and feed back into traditional bibliographical analysis in fields like paleography. One result of using digital software on medieval manuscripts is the confirmation that scripts as paleographers have described and identified them do appear and function distinctly (cf. <sup id="fnref2:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup>). The process of using OCRopus for our experiments has reinforced that certain groups of manuscripts with different script types (for example, Caroline minuscule, Late Caroline, and Early Gothic scripts) do differ from each other within general categories, not only in traditional paleographical assessments but also in the ways that the software handles them. OCRopus works differently in processing manuscripts exhibiting earlier Caroline minuscule features than it does with manuscripts exhibiting later Late Caroline and Early Gothic features, demonstrating that the software process is not the same in every case. In other words, how OCR software handles manuscripts differently within and across certain paleographical categories (even if they are relatively fluid) further justifies previous knowledge about such categories and their uses in medieval scribal practice.</p>
<p>It is clear that using OCR software with medieval manuscripts proves to be useful, but currently this is an area of bibliographical scholarship not well-served. Open-source software based on ANN technology can change this. Our experiments show that, given certain strategies, good OCR results can be achieved, even with a source pool of none-too-shocking size. We have initially experimented using mainly manuscripts written in Caroline minuscule, but our results from expanding to include others written in Late Caroline and Early Gothic indicate that these strategies could be feasibly replicated across a larger corpus of manuscripts and shared alike with other processes. Some of the most exciting results from our experiments were in the best accuracy rates achieved with our corpus of manuscripts: 97.06% accuracy rate for results on the seen manuscript Arras 764 based on ground truth from only Arras 764; and 94.13% accuracy rate for results on unseen manuscripts based on ground truth from all training manuscripts. Our hope is that future work can build on these results to make OCR processing more efficient, more accurate, and more applicable to a wider range of medieval manuscripts.</p>
<p>The aim of future projects and collaboration using OCR with medieval manuscripts should be to share not only results but also data for the whole process so that others are able to build upon research for future improvements. While these are already fundamental aspects of many endeavors in digital humanities, the case of OCR for medieval sources underscores the necessity of open sharing. After all, our experiments and results are only one starting point, and our hope is that future research will improve the possibilities presented here. Furthermore, while we have used OCRopus for the experiments presented in this article, collaborative, open sharing allows for use with other OCR engines and different techniques. None of our results would be possible without open-source software, collaboration, open access to digitized manuscripts as facilitated by libraries, and, in some cases, the previous work of those experimenting with OCR. Like other ANN software, OCRopus does not come with training built in, so users need to train it. To build on OCR results, researchers need large training sets that have already been created and tested, so that they do not need to start from scratch every time. The future of this type of work necessarily calls for a collaborative approach to sharing data and results. We believe that this is where open-source software is more useful than proprietary software, which does not lend itself to building upon for collaborative work.</p>
<p>This sort of collaboration will require an amount of digital infrastructure and commitment to distribute training sets. Because training data for all of the popular ANN OCR software today is simply made up of pairings between source images (segmented lines) and ground truth text (transcriptions), they are easy to share. Sharing data in open access repositories is the most sensible approach, so that (for example) collaborators can access and use the data, identify and correct errors in training sets, and upload new batches of training data for different types of source analyses. In some ways, possibilities for this kind of sharing already exist in repositories that enable collaboration — like Github, Gitlab, and Zenodo.<sup id="fnref:39"><a href="#fn:39" class="footnote-ref" role="doc-noteref">39</a></sup>  This has also been our goal with sharing our own results in a Github repository. Our process using open-source OCR software with neural network technology should allow many people to participate collaboratively in decentralized fashion and on a much larger scale over time. Our intention with this process is to bring together different scholars, tools, and methodologies to build a robust, collaborative approach to OCR for medieval manuscripts.</p>
<h2 id="appendix-a-note-on-manuscript-dates-origins-and-scripts">Appendix: A Note on Manuscript Dates, Origins, and Scripts</h2>
<p>For data about manuscript dates, origins, and scripts in our data sets, we have generally followed standard bibliographic descriptions in catalogues and online repositories. All of the data gleaned from paleographical analyses and descriptions are necessarily fluid (and often subjective). With this in mind, paleographical data should be understood as somewhat fuzzy:    “meaning is under constant negotiation”   <sup id="fnref3:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup>  . For example, all date ranges given are approximate, and any specific date given should be understood as in or about that year. Details that justify date ranges may be found in catalogues and other secondary literature about specific manuscripts. Our system for dating aligns with other bibliographic metadata standards, especially the  “Principles of Description”  in  <em>The Production and Use of English Manuscripts 1060 to 1220</em>  by Orietta Da Rold, Takako Kato, Mary Swan, and Elaine Treharne <sup id="fnref1:38"><a href="#fn:38" class="footnote-ref" role="doc-noteref">38</a></sup>. Dates for manuscripts in our data set are necessarily approximate.</p>
<p>The same need to account for fuzzy data is also true of script types, since the history and development of medieval handwriting is full of ambiguity. Classifications of scripts in our data are based on paleographical standards in bibliographic descriptions, but these are not always straightforward. Distinctions between script types, their characteristic features, and terminology are contested and rather difficult to pin down between about 1075 and 1225 (see details in <sup id="fnref1:33"><a href="#fn:33" class="footnote-ref" role="doc-noteref">33</a></sup>; and <sup id="fnref1:34"><a href="#fn:34" class="footnote-ref" role="doc-noteref">34</a></sup>). Nonetheless, certain distinctive features do emerge during this period, and older forms drop out of use. Despite being slow and gradual, a transition does occur. For this reason, we take the view that Caroline minuscule was transitional in this period, and we use this term (following Kwakkel) throughout.</p>
<p>More specifically, we use two terms for scripts during this transitional period (both in this article and in our data sets): those exhibiting a balance of earlier features from Caroline minuscule, often before about 1200, are identified as Late Caroline; those exhibiting a balance of later features more like Gothic, often after about 1200, are identified as Early Gothic. Justification for these decisions is found in behavioral differences for how OCR software handles different types of scripts. These differences are most prominently seen in the way that adding different types of manuscripts (in Caroline minuscule, Late Caroline, and Early Gothic, as well as seen and unseen) affect the accuracy rates of OCR results (see our discussion in the section on  “Process and Results” ). While more detailed discussion of the implications of our results for paleographical analysis is beyond the scope of the present article, we hope to pursue these issues further in the future.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>There are many more than can be cited here, but see esp. results from the Stanford Literary Lab; <sup id="fnref:40"><a href="#fn:40" class="footnote-ref" role="doc-noteref">40</a></sup>; <sup id="fnref:41"><a href="#fn:41" class="footnote-ref" role="doc-noteref">41</a></sup>; <sup id="fnref:42"><a href="#fn:42" class="footnote-ref" role="doc-noteref">42</a></sup>; and <sup id="fnref:43"><a href="#fn:43" class="footnote-ref" role="doc-noteref">43</a></sup>.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Algee-Hewitt, Mark, Sarah Allison, Marissa Gemma, Ryan Heuser, Franco Moretti, and Hanna Walser.  “Canon/Archive. Large-scale Dynamics in the Literary Field.”  Pamphlets of the Stanford Literary Lab, Pamphlet 11, January 2016.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Moretti, Franco.  “Patterns and Interpretation.”  Pamphlets of the Stanford Literary Lab, Pamphlet 15, September 2017.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>See previous references, esp. the overview and examples in <sup id="fnref1:43"><a href="#fn:43" class="footnote-ref" role="doc-noteref">43</a></sup>.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>Rydberg-Cox, Jeffrey A.  “Digitizing Latin Incunabula: Challenges, Methods, and Possibilities.”    <em>Digital Humanities Quarterly</em>  3.1 (2009).&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>Strange, Carolyne, Daniel McNamara, Josh Wodak, and Ian Wood.  “Mining for the Meanings of a Murder: The Impact of OCR Quality on the Use of Digitized Historical Newspapers.”    <em>Digital Humanities Quarterly</em>  8.1 (2014).&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>Alpert-Abrams, Hannah.  “Machine Reading the Primeros Libros.”    <em>Digital Humanities Quarterly</em>  10.4 (2016).&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>For projects using OCR on early modern printed texts, see, e.g., <sup id="fnref1:31"><a href="#fn:31" class="footnote-ref" role="doc-noteref">31</a></sup>; and <sup id="fnref:44"><a href="#fn:44" class="footnote-ref" role="doc-noteref">44</a></sup>.&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>Some recent endeavors to render machine-readable data from medieval materials (manuscripts and incunables) stand out, although not all of these use OCR software: e.g., <sup id="fnref:45"><a href="#fn:45" class="footnote-ref" role="doc-noteref">45</a></sup>; <sup id="fnref:46"><a href="#fn:46" class="footnote-ref" role="doc-noteref">46</a></sup>; <sup id="fnref:47"><a href="#fn:47" class="footnote-ref" role="doc-noteref">47</a></sup>; <sup id="fnref:48"><a href="#fn:48" class="footnote-ref" role="doc-noteref">48</a></sup>; <sup id="fnref1:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup>; <sup id="fnref:49"><a href="#fn:49" class="footnote-ref" role="doc-noteref">49</a></sup>; <sup id="fnref3:26"><a href="#fn:26" class="footnote-ref" role="doc-noteref">26</a></sup>; the Rescribe project <sup id="fnref2:31"><a href="#fn:31" class="footnote-ref" role="doc-noteref">31</a></sup>; <sup id="fnref:50"><a href="#fn:50" class="footnote-ref" role="doc-noteref">50</a></sup>; <sup id="fnref4:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup>; and the recently launched HIMANIS project <sup id="fnref1:30"><a href="#fn:30" class="footnote-ref" role="doc-noteref">30</a></sup>, built on proprietary software by Teklia, not open-source tools that could be more widely applied.&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>Nitti, John J.  “Computers and the Old Spanish Dictionary.”  In  <em>Medieval Studies and the Computer</em> , ed. Anne Gilmour-Bryson, special issue of  <em>Computers and the Humanities</em>  12 (1978): 43-52.&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>Data for our experiments may be found at <a href="https://github.com/rescribe/">https://github.com/rescribe/</a>.&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p>On terminology and other issues, see our Appendix: A Note on Manuscript Dates, Origins, and Scripts.&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13">
<p>Jockers, Matthew L.  <em>Text Analysis with R for Students of Literature</em> . Switzerland: Springer International Publishing, 2014.&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14">
<p>Widner, Michael.  “Toward Text-Mining the Middle Ages: Digital Scriptoria and Networks of Labor.”  In  <em>The Routledge Research Companion to Digital Medieval Literature</em> , ed. Jennifer E. Boyle and Helen J. Burgess. New York: Routledge, 2018: 131-44.&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:15">
<p>Hawk, Brandon W.  “OCR and Medieval Manuscripts: Establishing a Baseline.”    <em>brandonwhawk.net</em> . April 20, 2015.&#160;<a href="#fnref:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:16">
<p>On distinctions between OCR and HTR, and critique of the former, see <sup id="fnref5:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup>; and, e.g., <sup id="fnref:51"><a href="#fn:51" class="footnote-ref" role="doc-noteref">51</a></sup>, with documentation about both OCR and HTR at the project wiki, <a href="https://transkribus.eu/wiki/index.php/Main_Page">https://transkribus.eu/wiki/index.php/Main_Page</a>.&#160;<a href="#fnref:16" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:17">
<p>Kestemont, Mike, Vincent Christlein, and Dominique Stutzmann.  “Artificial Paleography: Computational Approaches to Identifying Script Types in Medieval Manuscripts.”    <em>Speculum</em>  92/S1 (2017), S86-109.&#160;<a href="#fnref:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref3:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref4:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref5:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:18">
<p>David Mimno.  “Data carpentry is a skilled, hands-on craft which will form a major part of data science in the future.”    <em>The Impact Blog</em> , September 1, 2014.&#160;<a href="#fnref:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:19">
<p>On ANN technology and its connection to OCR as discussed in this section, see esp. <sup id="fnref1:28"><a href="#fn:28" class="footnote-ref" role="doc-noteref">28</a></sup>; <sup id="fnref:52"><a href="#fn:52" class="footnote-ref" role="doc-noteref">52</a></sup>; and <sup id="fnref:53"><a href="#fn:53" class="footnote-ref" role="doc-noteref">53</a></sup>.&#160;<a href="#fnref:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:20">
<p>Taigman, Yaniv, Ming Yang, Marc’ Aurelio Ranzato, and Lior Wolf.  “DeepFace: Closing the Gap to Human-Level Performance in Face Verification.”  Presented at the  <em>Conference on Computer Vision and Pattern Recognition</em>  (2014).&#160;<a href="#fnref:20" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:21">
<p>“Google reCaptcha.”  2016. <a href="https://www.google.com/recaptcha/intro/index.html">https://www.google.com/recaptcha/intro/index.html</a>.&#160;<a href="#fnref:21" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:22">
<p>Karpathy, Andrej.  “The Unreasonable Effectiveness of Recurrent Neural Networks.”    <em>Andrej Karpathy Blog</em> , May 21, 2015.&#160;<a href="#fnref:22" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:23">
<p>In our specific case, we chose to process the binarization with an alternative open-source program, ScanTailor, in order to maintain a higher resolution than OCRopus would produce. We also amended the numbering procedure for the segmented lines from hexadecimal to decimal as we realized that the initial hexadecimal order would confuse the order of lines in the raw text output.&#160;<a href="#fnref:23" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:24">
<p>Baumann, Ryan.  “Latin OCR for Tesseract.”   <a href="https://ryanfb.github.io/latinocr/">https://ryanfb.github.io/latinocr/</a>.&#160;<a href="#fnref:24" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:25">
<p>White, Nick.  “Training Tesseract for Ancient Greek OCR.”    <em>Eutypon</em>  28-29 (2012): 1-11.&#160;<a href="#fnref:25" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:26">
<p>Springmann, Uwe, and Anke Lüdeling.  “OCR of historical printings with an application to building diachronic corpora: A case study using the RIDGES herbal corpus.”    <em>Digital Humanities Quarterly</em>  11.2 (2017).&#160;<a href="#fnref:26" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:26" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:26" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref3:26" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:27">
<p>Our main sources for installation and training process, apart from OCRopus’s own documentation on Github, was Dan Vanderkam’s very useful contribution about installing OCRopus on Apple iOS <sup id="fnref:54"><a href="#fn:54" class="footnote-ref" role="doc-noteref">54</a></sup> and Springmann’s detailed description of a training process for incunables <sup id="fnref1:49"><a href="#fn:49" class="footnote-ref" role="doc-noteref">49</a></sup>.&#160;<a href="#fnref:27" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:28">
<p>Ul-Hasan, Adnan, Breuel, T.M.  “Can we build language-independent OCR using LSTM networks?”  In  <em>Proceedings of the 4th International Workshop on Multilingual OCR</em> . Washington, DC: MOCR, 2013: article 9.&#160;<a href="#fnref:28" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:28" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:29">
<p>This tool, as are the others mentioned here, is built into OCRopus.&#160;<a href="#fnref:29" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:30">
<p><em>HIMANIS project</em> . Teklia. <a href="http://www.himanis.org/">http://www.himanis.org/</a>&#160;<a href="#fnref:30" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:30" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:31">
<p><em>Rescribe Ltd</em> . <a href="https://rescribe.xyz/">https://rescribe.xyz/</a>.&#160;<a href="#fnref:31" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:31" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:31" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:32">
<p>See description and digital facsimile at  <em>e-codices</em> , <a href="https://www.e-codices.unifr.ch/en/list/one/csg/0761">https://www.e-codices.unifr.ch/en/list/one/csg/0761</a>&#160;<a href="#fnref:32" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:33">
<p>Derolez, Albert.  <em>The Palaeography of Gothic Manuscript Books: From the Twelfth to the Early Sixteenth Century</em> . Cambridge Studies in Palaeography and Codicology 9. Cambridge: Cambridge University Press, 2003.&#160;<a href="#fnref:33" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:33" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:34">
<p>Kwakkel, Erik.  “Biting, Kissing and the Treatment of Feet: The Transitional Script of the Long Twelfth Century.”  In  <em>Turning Over a New Leaf: Change and Development in the Medieval Book</em> , ed. Erik Kwakkel, Rosamond McKitterick, and Rodney Thomson. Leiden: Leiden University Press, 2012: 79-125.&#160;<a href="#fnref:34" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:34" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:35">
<p>Lowe, E. A., ed.  <em>Codices Latini Antiquiores: A Palaeographical Guide to Latin Manuscripts Prior to the Ninth Century</em> . 12 vols. Oxford: Clarendon Press, 1934-1972.&#160;<a href="#fnref:35" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:36">
<p>Stansbury, Mark.  <em>Earlier Latin Manuscripts</em> . NUI Galway. 2015-2018. <a href="https://elmss.nuigalway.ie/">https://elmss.nuigalway.ie/</a>.&#160;<a href="#fnref:36" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:37">
<p>Buringh, Eltjo.  <em>Medieval Manuscript Production in the Latin West: Explorations with a Global Database</em> . Global Economics History Series 6. Leiden: Brill, 2011.&#160;<a href="#fnref:37" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:38">
<p>Da Rold, Orietta, Takako Kato, Mary Swan, and Elaine Treharne.  <em>The Production and Use of English Manuscripts 1060 to 1220</em> . University of Leicester, 2010; last update 2013.&#160;<a href="#fnref:38" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:38" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:39">
<p>See, e.g., the Github repository by <sup id="fnref:55"><a href="#fn:55" class="footnote-ref" role="doc-noteref">55</a></sup>; and the Zenodo repository by <sup id="fnref:56"><a href="#fn:56" class="footnote-ref" role="doc-noteref">56</a></sup>.&#160;<a href="#fnref:39" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:40">
<p>Moretti, Franco.  <em>Graphs, Maps, Trees: Abstract Models for Literary History</em> . London: Verso, 2005.&#160;<a href="#fnref:40" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:41">
<p>Moretti, Franco.  <em>Distant Reading</em> . London: Verso 2012.&#160;<a href="#fnref:41" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:42">
<p>Jockers, Matthew L.  <em>Macroanalysis: Digital Methods and Literary History</em> . Urbana, IL: University of Illinois Press, 2013.&#160;<a href="#fnref:42" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:43">
<p>Jockers, Matthew L., and Ted Underwood.  “Text-Mining in the Humanities.”  In  <em>A New Companion to Digital Humanities</em> , ed. Susan Schreibman, Ray Siemens, John Unsworth. Malden, MA: Wiley-Blackwell, 2016: 291-306.&#160;<a href="#fnref:43" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:43" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:44">
<p><em>Early Modern OCR Project (eMOP)</em> . Texas A&amp;M University. <a href="http://emop.tamu.edu/">http://emop.tamu.edu/</a>.&#160;<a href="#fnref:44" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:45">
<p>Edwards, Jaety, Yee Whye Teh, David Forsyth, Roger Bock, Michael Maire, and Grace Vesom.  “Making Latin Manuscripts Searchable using gHMM’s.”     <em>Advances in Neural Information Processing Systems</em>   17 (2004): 385-392.&#160;<a href="#fnref:45" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:46">
<p>Boschetti, Frederico, Matteo Romanello, Alison Babeu, David Bamman, and Gregory Crane.  “Improving OCR Accuracy for Classical Critical Editions.”   In  <em>Research and Advanced Technology for Digital Libraries</em> , ed. Maristella Agosti, et al., Lecture Notes in Computer Science 5714. Heidelberg: Springer, 2009: 156-67.&#160;<a href="#fnref:46" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:47">
<p>Fischer, Andreas, Markus Wuthrich, Marcus Liwicki, Volkmar Frinken, Horst Bunke, Gabriel Viehhauser, and Michael Stolz.  “Automatic Transcription of Handwritten Medieval Documents.”    <em>Proceedings of the 2009 15th International Conference on Virtual Systems and Multimedia</em> . Washington, DC: IEEE COmputer Society, 2009: 137-42.&#160;<a href="#fnref:47" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:48">
<p>Leydier, Yann, Véronique Églin, Stéphane Brès, and Dominique Stutzmann.  “Learning-Free Text-Image Alignment for Medieval Manuscripts.”  In   <em>Proceedings: 14th International Conference on Frontiers in Handwriting Recognition</em> . Los Alamitos, CA: IEEE Computer Society, 2014: 363-68.&#160;<a href="#fnref:48" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:49">
<p>Springmann, Uwe.  “Ocrocis: A high accuracy OCR method to convert early printings into digital text – a tutorial.”   <a href="http://cistern.cis.lmu.de/ocrocis/tutorial.pdf">http://cistern.cis.lmu.de/ocrocis/tutorial.pdf</a>.&#160;<a href="#fnref:49" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:49" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:50">
<p>Camps, Jean-Baptiste.  “Homemade manuscript OCR (1): OCRopy.”    <em>Sacré Gr@@l: Histoire, philologie, programmation et statistiques</em> , February 6, 2017.&#160;<a href="#fnref:50" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:51">
<p><em>Transkribus</em> . READ project. <a href="https://transkribus.eu/Transkribus/">https://transkribus.eu/Transkribus/</a>.&#160;<a href="#fnref:51" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:52">
<p>Simistira, Fotini, Adnan Ul-Hassan, Vassilis Papavassiliou, Basilis Gatos, Vassilis Katsouros, and Marcus Liwicki.  “Recognition of Historical Greek Polytonic Scripts Using LSTM Networks.”  Presented at the  <em>13th International Conference on Document Analysis and Recognition</em>  (2015).&#160;<a href="#fnref:52" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:53">
<p>Ul-Hasan, Adnan.  <em>Generic Text Recognition using Long Short-Term Memory Networks</em> . Unpublished PhD dissertation, Technische Universität Kaiserslautern, 2016.&#160;<a href="#fnref:53" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:54">
<p>Vanderkam, Dan.  “Extracting text from an image using Ocropus.”    <em>danvk.org</em> . January 9, 2015. <a href="https://www.danvk.org/2015/01/09/extracting-text-from-an-image-using-ocropus.html">https://www.danvk.org/2015/01/09/extracting-text-from-an-image-using-ocropus.html</a>.&#160;<a href="#fnref:54" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:55">
<p>Kestemont, Mike.  “Code for the DeepScript Submission to ICFHR2016 Competition on the Classification of Medieval Handwritings in Latin Script.”   <a href="https://github.com/mikekestemont/DeepScript">https://github.com/mikekestemont/DeepScript</a>.&#160;<a href="#fnref:55" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:56">
<p>Springmann, Uwe, Christian Reul, Stefanie Dipper, and Johannes Baiter.  “GT4HistOCR: Ground Truth for training OCR engines on historical documents in German Fraktur and Early Modern Latin.”  August 12, 2018. <a href="https://zenodo.org/record/1344132">https://zenodo.org/record/1344132</a>.&#160;<a href="#fnref:56" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry><entry><title type="html">Velvet Evolution: A Review of Lev Manovich's Software Takes Command (Bloomsbury Academic, 2013)</title><link href="https://rlskoeser.github.io/dhqwords/vol/13/1/000409/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://rlskoeser.github.io/dhqwords/vol/13/1/000409/</id><author><name>Alan Bilansky</name></author><published>2019-04-26T00:00:00+00:00</published><updated>2019-04-26T00:00:00+00:00</updated><content type="html"><![CDATA[<p>When I started writing this review, a story had just gone viral in social media and then moved to traditional media: a teacher in Kenya had to prepare his students to take a standard exam that included using computers, but had no computers in his classroom, so he drew the user-interface of Microsoft Word on the blackboard, in exquisite detail <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. This story caught attention because it pointed out the digital divide of course, but we were also fascinated by the detail - striking, how we are all able to check the accuracy of his drawing from memory. It suggests a central claim of Lev Manovich&rsquo;s  <em>Software Takes Command</em> : we now live in    “a software society and our culture can be justifiably called a software culture — because today software plays a central role in shaping both the material elements and many of the immaterial structures that together make up  “culture” ”   <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>  . To argue for this claim and follow through on its implications, his work spans categories such as visual arts, media studies, and software studies.</p>
<p>Manovich combines three rhetorical threads in this book. First is an encomium to software as something close to a force in human evolution: at the end of the twentieth century humans have added a fundamentally new dimension to everything that counts as    “culture”   <sup id="fnref1:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>  . Second, within the historical trajectory leading from Xerox PARC to the Macintosh to iPads, Manovich studies the writings of some of the better-known computer scientists involved to discover ideas of remediation that are central to his analysis. Third, he produces close reading of two pieces of software that are products of that school of thought:    “desktop applications for media authoring most widely used today,”   <sup id="fnref2:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>   namely Adobe Photoshop and After Effects.</p>
<p>We think it is entirely fitting that a text like  <em>Frankenstein</em> , so germinal in our culture that we see signs of it everywhere, should be the subject of so much scholarly attention, read by multiple schools of criticism. A text like Microsoft Word is no less central to our lived experience and ephemeral and lasting culture, and deserves scholarly attention as well. Manovich coined the term    “software studies,”   <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>   in his earlier important work,  <em>Language of New Media</em> , and he fully explores it here.</p>
<p>A budding academic field has branched off to do software studies, which we might generalize as centering on the study of code as writing. Manovich follows his own path here: software as a force in culture. He makes the case strenuously that we cannot consider any aspect of contemporary culture — from art to entertainment to work and the economy, to the production, distribution, and use of knowledge — without considering software, because software as a movement has brought with it an aesthetic and epistemology. And Manovich is interested in software, not code:    “I am interested in how software appears to users — i.e. what functions it offers . . . the interfaces used to present these functions, and assumptions and models about a user, her/his needs, and society encoded in these functions and their interface design”   <sup id="fnref3:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>  .</p>
<p>To cite an example he keeps returning to: many examples of software include the functions of select, copy, cut, and paste. Manovich has the technical sophistication to know that the algorithms behind these functions can have nothing to do with each other. In fact, when I use them in a program like Adobe Illustrator, I might be invoking entirely different modules of code if I select and copy a block of text or the outline of a building or a bit-map image. Manovich would have us dispense with these distinctions. What is important for his argument is that they create the same experience for the user, the same way of dealing with information and with the world; and the conventions that unify the movement of software in expressly the same way as a particular linear perspective can unify a school of visual art. He is most comfortable dealing with the visual user interface as a seamless surface and he provides virtuoso close readings reminiscent of New Critics like Cleanth Brooks or I. A. Richards.</p>
<p>Manovich&rsquo;s survey of the history of ideas centers on the computer as simulation. Computers are remediation machines, borrowing the term remediation from Jay Bolter and Richard Grusin:    “the representation of one medium in another”   <sup id="fnref4:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>  . Older media are metaphorized in the visual interface of software. Files are viewed inside images of manila folders that we can drag them into and out of, and we add color to a shape with an image of a paint bucket. This is the central aesthetic and the germinal idea animating all the software Manovich focuses on. Software has made computers into a metamedium, a wide range of already-existing and not-yet-invented media.</p>
<p>Most important to Manovich&rsquo;s narrative, the innovators at Xerox PARC and then Apple Computer developed applications for media manipulation and creation that incorporated representations of older media (e.g, scissors, paint brushes, envelopes) into their graphic user interfaces. He moves from one well-known historical episode to another — Douglas Engelbart&rsquo;s mother of all demos, Alan Kay and others at Xerox PARC (the direct ancestor of the Macintosh&rsquo;s interface) — and suggests a genealogy that leads us to the present of Photoshop and After Effects and Terminator II. Although these writers’ texts are readily available and Engelbart&rsquo;s demo is on YouTube, any serious attention paid to them is worth praise. Manovich points out the continuity of ideas and argues that they culminate in the software that media professionals use in the present.</p>
<p>Diving into the recent history of software is still rare enough to always be worth reading, and I hope  <em>Command</em>  inspires more and deeper work. What alternative ideas of computing died on the vine? Is it significant that all the figures Manovich touches on are white men with similar educational backgrounds and institutional positioning? How did economic and institutional pressures affect the ideas these figures chose to pursue? Did anything connect these figures aside from ideas? Other than theories of modern art, were there any ideas circulating in the 1960s and 1970s that may have found their way into their thinking? The writers he surveys were interested in hardware as well as software (and not just because software was not unbundled in the way that it is now; Engelbart spends a good deal of time on office furniture); can we really speak of one without the other? He asks why these writers did not find more interest in the academy and why capital is more interested in these media technologies — an excellent question! — but quickly hedges:    “The systematic answer to this question will require its own investigation”   <sup id="fnref5:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>  ; that question would indeed make for an interesting study.</p>
<h2 id="the-potential-of-softwarization">The Potential of Softwarization</h2>
<p>It is through close readings of cultural software that Manovich fleshes out his ideas of software as re-mediation. When a medium undergoes the process of softwarization, it is first metaphorized from the physical. E.g., reading a book on an iPad, we can flip pages and leave a bookmark much as we can with a physical codex. Then by virtue of the software simulating the bound paper book, new features are added. We can zoom in on details of a page and copy and paste text directly rather than having to do it manually, and the creators can add features like hyperlinks. Thus the simulation both reproduces and augments the older forms. It is a process he demonstrates in his close reading of the interface of Photoshop. A paintbrush is simulated, but the simulation software gives greater control than any artist ever had.</p>
<p>Manovich effectively refutes the facile premise most of us, myself included — have probably voiced, that capabilities are added to media — greater ease of creation, manipulation, duplication, distribution and so on — by virtue of their being digital. Rather, our new capabilities with new media arise from the software we use. It is not by virtue of being digital that I can string-search a digital copy of  <em>Frankenstein</em>  or copy it without effort or loss in quality. Rather, I can search the text and copy it because software allows me to.</p>
<p>But wait, there&rsquo;s more. The next stage in the process of softwarization is when chunks of code simulating different tools and different media communicate with each other behind the scenes and are presented to us in the same interface, so that media combine seamlessly. In his reading of the After Effects interface to serve as illustration, Manovich argues that now since one can author in multiple media at the same time, combining video, 3D animation, textual effects and so on, the end result is not simply multi-media. Rather,    “the new media of 3D computer animation has  eaten up  the dominant media of the industrial age — lens-based photo, film and video recording”   <sup id="fnref6:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>  , and a new medium is created where live-action video is not separable from animation:    “the most fundamental assumptions of different media forms and traditions, are brought together resulting in new media gestalts. That is, they merge together to offer a coherent new experience different from experiencing all the elements separately”   <sup id="fnref7:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>  . This process of softwarization is described in the terms of biological evolution:    “The already simulated mediums started exchanging properties and techniques. As a result, the computer metamedium came to contain endless new species”   <sup id="fnref8:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>  .</p>
<p>To name the process of simulation and softwarization and remediation he adopts the term Velvet Revolution, from Czechoslovakia&rsquo;s nonviolent political change, to refer to the revolution of software, and although he never follows through, he asserts,    “Although it may seem presumptuous to compare political and aesthetics transformations . . . it is possible to show that the two revolutions are actually related”   <sup id="fnref9:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>  . This is the point in the book I most needed Manovich to flesh out. He goes on to rely on the term, but never returns to define what the political implications are. It&rsquo;s hard to guess at what the parallels are. Of course, we should note that Manovich grew up and received his first training in art in the Soviet Union, where in recent history aesthetics had very clear political causes and effects. But if he sees political effects of this movement in software, he does not explicitly explore it. What are the political implications of this Velvet Revolution?</p>
<p>Manovich views films like  <em>Terminator 2</em> ,  <em>The Matrix</em> ,  <em>300</em> , and television commercials as the apotheosis of Velvet Revolution: the    “logic is also the same as that which we observe in the creation of new hybrids in biology. That is, the result of the hybridization process is not simply a mechanical sum of the previously existing parts but a new  species  — a new kind of visual aesthetics that did not exist previously”   <sup id="fnref10:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>  .</p>
<p>Software has enabled filmmakers to combine 3D graphics with live photography so that they are indistinguishable, and characters played by actors interact seamlessly with computer-generated characters, looking just as realistic so that we can forget about the difference between them. We can extend the argument into the present with films like  <em>Guardians of the Galaxy</em>  and  <em>Rogue One: A Star Wars Story</em> . The 2016 film  <em>Rogue One</em>  created some controversy <sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup> for blurring the line until it is meaningless, imposing the faces of a young Carrie Fisher and Peter Cushing on other actors. Thus Carrie Fisher could have a simulated cameo as a younger version of herself and Cushing, the deceased legend of stage and screen who died in 1994, could play a crucial supporting role in this film, in 2016.</p>
<h2 id="software-and-human-progress">Software and Human Progress</h2>
<p>Manovich&rsquo;s greatest contribution is an ode to software:    “our contemporary society can be characterized as a software society and our culture can be justifiably called a software culture — because today software plays a central role in shaping both the material elements and many of the immaterial structures that together make up  culture ”   <sup id="fnref11:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>   [Manovich 2013, 33].</p>
<p>His voice is as strong as it was in  <em>The Language of New Media</em> . Manovich&rsquo;s work is indispensable for the language he provides, an optimistic — ecstatic even-view of the progress already achieved and the further promise of digital technologies, unmoderated by critical perspectives but also unsullied by the tendency of voices centered in Silicon Valley to view the potential for profit as grounds for praise.</p>
<p>Software is a force in human evolution:    “In my view, this ability to combine previously separate media techniques represents a fundamentally new stage in the history of human media, human semiosis, and human communication, enabled by its  softwarization ”   <sup id="fnref12:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>  . Additionally, software follows its own teleology, best described as a virus released in the wild:    “The makers of software used in media production usually do not set out to create a revolution. On the contrary, software is created to fit into already existing production procedures, job roles, and familiar tasks. But software applications are like species within the common ecology — in this case, a shared environment of a digital computer. Once  released , they start interacting, mutating, and making hybrids”   <sup id="fnref13:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>  . Human agency, collective or individual, is not important to this narrative.</p>
<p>Speaking broadly, we can view those who, analyze technology in the world as divided between those whose first impulse is to celebrate every innovation of technology as historic progress, and those whose first impulse is to be critical, skeptical, suspicious. Many of those in the former group, if they are not directly associated with venture capitalists, often tend to see the economics of technology as itself justifying it. Try to imagine praise for Facebook that makes no reference to market success. This is not true of Manovich. He is ecstatic about progress demonstrated and the further potential of new information technologies, without relying on facile economic justifications. His work needs to be read, as it serves as a corrective to critical approaches to our digital environment. He writes like a visionary and a user of computers, not like a venture capitalist.</p>
<p>He gives us the language to discuss the promise of software, but no help criticizing it. To return to Manovich&rsquo;s central point about software and the aesthetic of  <em>The Matrix</em> , we might reflect on recent history and the importance of recordings made public. During the last presidential campaign (2016), a recording (whose veracity was confirmed) surfaced of the future President confessing to sexual assault. There is talk of him possibly being blackmailed with a tape involving prostitutes in a foreign country. In the campaign for Governor in my home state of Illinois, a recording (also confirmed) was leaked of a Democratic candidate having a conversation with a past corrupt governor. A video claiming to show the Democratic nominee for President is circulating now on the internet in conservative circles. That one is obviously fake, but the examples cited above show that a  “fake”  video can look very real. Rogue One is entertaining fiction but it is not hard to imagine dangerous fictions flowing from the Velvet Revolution. How do we cope with the same techniques used to bring Peter Cushing back from the dead being used to show Barack Obama or Robert Mueller committing a crime? Manovich&rsquo;s work celebrates the software that makes possible such a dangerous falsehood, but leaves it to us to decide how to deal with the consequences.</p>
<p>Manovich&rsquo;s comfort with software as a sealed commodity, as shown in the example of copy and paste as a topos rather than code, would seem at odds with the approaches of many in the audience of this journal. There is a very different ethos valued by many digital humanists that stresses the transparency and comprehensibility of the software tools we use, e.g., their ideal is to publish not just the visualized product, but also the raw dataset and the code that produced it and the version of R it ran on, in case a bug is discovered by the R community in the future. Commitment to the work of understanding the software tools used is not fun, but is generally considered empowering and honest. Manovich is clear that he is interested in the experience of most users, and we know that most users are happy to use seamless black boxes. This leads to an honest question: is it a problem if most users don&rsquo;t have an empowered attitude toward the software they rely on?</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Gharib, Malaka.  “Computer Teacher with No Computer Chalks Up Clever Classroom Plan.”    <em>NPR All Things Considered.</em>  (March 2018).&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Manovich, Lev.  <em>Software Takes Command</em>  Bloomsbury Academic, New York (2013).&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref3:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref4:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref5:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref6:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref7:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref8:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref9:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref10:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref11:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref12:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref13:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Manovich, Lev.  <em>The Language of New Media</em>  MIT Press, Cambridge (2001).&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Shoard, Catherine.  “Peter Cushing is dead. Rogue One&rsquo;s resurrection is a digital indignity.”    <em>The Guardian</em>  (December 2018).&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry><entry><title type="html">Music Scholarship Online (MuSO): A Research Environment for a More Democratic Digital Musicology</title><link href="https://rlskoeser.github.io/dhqwords/vol/13/1/000381/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://rlskoeser.github.io/dhqwords/vol/13/1/000381/</id><author><name>Timothy C. Duguid</name></author><author><name>Maristella Feustle</name></author><author><name>Francesca Giannetti</name></author><author><name>Elizabeth Grumbach</name></author><published>2019-04-12T00:00:00+00:00</published><updated>2019-04-12T00:00:00+00:00</updated><content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>This paper describes the work to date on Music Scholarship Online (MuSO), an online research environment for digitized resources and digital scholarly outputs relating to music that inscribes itself within the federated model of the Advanced Research Consortium (ARC). With the project now in its third year, MuSO has reached an inflection point where it has developed a music-centered RDF schema and demonstrated the potential for federated searching across ARC nodes by crosswalking eighteenth-century music content from Europeana into ARC. The case study presented here outlines the dissemination role that MuSO proposes to play within the music research community, the history of MuSO in relation to ARC, the Europeana test case, and future steps for the continued development of MuSO. By facilitating the discovery of curated digital music content, and providing a virtual environment for music researchers, MuSO will promote data reuse, strengthen community standards in music representation, and create possibilities for cross-disciplinary exchange. We propose that by leveraging the connections between digital music resources and digital humanities research technologies, MuSO will facilitate new research that expands the musicological discipline.</p>
<h2 id="background">Background</h2>
<p>Researchers of music have come to depend on digital tools and information resources in their work, similar to scholars in other humanistic disciplines. Digital indexes and databases such as the Digital Image Archive of Medieval Music (DIAMM), Chopin’s First Editions Online (CFEO), and the British Library’s Early Music Online have radically simplified the task of finding scholarly resources.<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>  Meanwhile, this ever-growing corpus of digitized primary source material has transformed an erstwhile preoccupation of scarcity into what is at least an appearance of abundance, extending on Rosenzweig&rsquo;s remarks on born-digital records <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. Music researchers must now stay current with a proliferation of new online resources to ensure that they overlook nothing of significance to their subdiscipline <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>. They increasingly prefer digital tools and resources over print and other physical formats, even while voicing concerns over incompleteness and the superficiality of working with digitized materials <sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>. There is, moreover, a consensus that digital technologies have had, and continue to have, a transformative effect on scholarly networks and the work of interpretation. The ease and speed of access to digital research inputs and outputs, and the shifts in methodological scope from close to distant reading, though not yet broadly shared in musicological circles, open significant new research prospects <sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>  <sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>  <sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>.</p>
<p>Even so, systems of scholarly production, review, and dissemination have not fully adapted to the digital realm. The infrastructures of print and publication that have dominated musicological dissemination continue to shape the discipline in the digital domain, both in the content that is produced and in the formats of that content. Traditional ideas about what is worthy of study have influenced digitization decisions and consequently the digital research that is now being undertaken on those resources. If, as Hooper notes, the allowable topics of musicological enquiry have diversified over the past few decades, the digital offers an even greater opportunity for researching underrepresented areas in musicological canons <sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>. As Johnson has argued, these marginalized voices are valuable, acting as the blind spots on the map, the dark continents of error and prejudice that    “carry their own mystery”   <sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup>  . Without the compulsion to sell enough printed stock to offset the costs of producing that stock, digital scholarship offers, at least in theory, an opportunity to further democratize music history, contrasting well-known exemplars with lesser-known, non-canonical voices. But in an academy in which recognition and reward remain tied closely to the published word, and therefore to the pressures of producing content that aligns with established cultural preferences, digital recovery work is noticeably scarce <sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup>.</p>
<p>Beyond promoting existing content biases, the majority of digital publishing has also simply shifted formats from an analog monograph or journal article to an epub or pdf. The rich multimedia capabilities of the digital are therefore largely underutilized in a music scholarship whose digital outputs still tend towards the static, text-centric restraints of the printed page. To be fair, there are digital projects in music that do display virtuosic technical accomplishment, but the content of these projects rarely receives the same level of theoretical attention.</p>
<p>The traditionalist skew of digital scholarly media belies the fact that the use of digital tools across the musical domain is complex and overlapping. The boundaries between the study of musical texts (scores) on the one hand and the study of musical sounds (performances) on the other, let alone the various aims supported by digital technologies such as training, analysis, dissemination, and music making, are persistent even while fuzzy <sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>. In addition, the radical potential of digital technologies to unify music scholarship through shared resources and methodologies remains unrealized. Siloed projects, isolated research communities, and solitary work cultures have constrained the impact of such technologies to a relative few.</p>
<p>This mirrors the state of literary scholarship over ten years ago when Jerome McGann and Bethany Nowviskie produced a founding document for the Networked Infrastructure for Nineteenth-Century Electronic Scholarship (NINES). In it, they described the state of the field of aggregation for digital humanities scholarship:    “what you see now on the web is what you get: an agglomeration of sites and projects whose content is atomized and whose scholarly and educational value is indeterminate”   <sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>  . NINES was founded to explore the informational design for scholarly work at a global scale; the goal was to develop a structured online environment that would promote access and repurposing of digital scholarly outputs <sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup>. For NINES, scholar-produced digital projects included early digital humanities efforts such as  <em>The William Blake Archive</em>  and  <em>The Rossetti Archive</em> .</p>
<h2 id="a-musical-solution">A musical solution</h2>
<p>This  “atomized”  digital content, first described by McGann and Nowviskie within a literary context, is becoming more prevalent within music scholarship. Resources such as  <em>The Lost Voices Project</em>  or Digital DuChemin,  <em>Hearing Wagner</em> , and  <em>Songs of the Victorians</em>  are examples of the ways interdisciplinary and multimodal research outputs can utilize and extend content available through digital library and archives. <sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup>  The valuable research materials offered by these projects, which include biophysical data, newly accessible digitized and annotated scores, and dynamic digital editions and visualization tools, are not readily findable to those scholars because their status as self-published scholarly outputs precludes their inclusion among many library- and publication-based digital catalogs and search engines.</p>
<p>Music Scholarship Online (henceforth MuSO) is working to improve the dissemination of music scholarship by making digital scholarly outputs discoverable alongside digitized music resources.<sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup>  With the term digital scholarly output, we refer to content that was created primarily for digital dissemination, denoting a shift away from a unidimensional print paradigm. This content may include a variety of data, media, and formats, but at its core it is content that has been transformed intellectually through the actions of experts. To give an example,  <em>Freischütz Digital</em>  is a digital edition of the opera  <em>Der Freischütz</em> . Not only does this resource provide access to digitized manuscripts and printed editions of the opera and its libretto, it offers multitrack audio recordings, XML-based digital editions inclusive of editorial interventions made by Carl Maria von Weber, Friedrich Kind, and various copyists working with the creators, as well as the researchers’ analytic commentary that extends upon the data provided by these sources. On occasion, digital scholarly outputs include software tools, datasets, and computer code, in addition to a web presentation for the general public. The Centre for the History and Analysis of Recorded Music (CHARM), for instance, provides a search interface for its sound files as well as a suite of software tools for conducting analyses of recorded sound.<sup id="fnref:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup>  Similar to MuSO in its aims, the Virtual Library of Musicology (ViFaMusik) is an information resource that provides access to a range of materials that includes primary and secondary resources.<sup id="fnref:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup>  It also promotes networking opportunities with a database of music scholars, which is at this time oriented towards the German-speaking musicological world <sup id="fnref1:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>  <sup id="fnref:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup>. Moreover, although ViFaMusik lists digital scholarly outputs like  <em>Freischütz Digital</em> , these resources are not treated with the same level of granularity as the digitized content. For instance, one cannot search for a particular musical edition or digital object made available by the  <em>Freischütz Digital</em>  project. MuSO seeks to expose the constituent parts of digital scholarly outputs, making them as discoverable as digitized resources, and thus to maximize the potential for large-scale digital research.</p>
<p>MuSO began in 2015 as a Digital Humanities Start-up Grant funded by the National Endowment for the Humanities. At the initial planning stage, music librarians, music encoders, and musicologists gathered to discuss issues surrounding aggregation and peer review for digital scholarly outputs in music <sup id="fnref:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup>. During this meeting, the group decided to follow in the footsteps of McGann and Nowviskie’s NINES by becoming a member of the Advanced Research Consortium (ARC). As a consequence, MuSO will build on ARC’s expertise in curating digital scholarly outputs alongside digitized collections. Through advisory and peer review activities, MuSO will identify and evaluate specialized music resources, and aggregate selected resources together with any associated data and software. In doing so, MuSO will strengthen community standards in the representation of music data and promote data reuse. Through federated searching across ARC nodes, MuSO&rsquo;s users will be able to discover high-quality, vetted scholarly resources that have been curated by experts in other humanistic disciplines, and thereby conduct thorough, multidisciplinary research. With these efforts, MuSO will advocate for open, collaborative, and cross-disciplinary research practices in music.</p>
<h2 id="arc-a-collaborative-for-digital-research-in-the-humanities">ARC: A collaborative for digital research in the humanities</h2>
<p>ARC is a federation of scholarly organizations that identify, review, and curate digital content most relevant to the community of scholars each organization serves, e.g. NINES serves nineteenth-century scholars and 18thConnect, eighteenth-century scholars. Socially, the ARC organization is made up of representatives from digital research environments like NINES and MuSO. Technologically, ARC is the home of the  “ARC Index”  — the physical computing and software infrastructure that aggregates reviewed and curated digital content into one single catalog that then provides high-quality scholarly resources for humanities scholars.</p>
<p>ARC is not a digital repository (like Bepress, DSpace, or Fedora Commons) or a publishing platform (such as the Manifold digital publishing platform<sup id="fnref:20"><a href="#fn:20" class="footnote-ref" role="doc-noteref">20</a></sup> ), nor is ARC a digital library environment, like the Perseus Digital Library or HathiTrust Digital Library, as ARC does not host or publish digital content. This means that, instead of hosting resources, such as digitized page images, scholarly presentations and papers, or book publications, ARC ingests metadata  <em>about</em>  digital resources that already exist on the web and have been reviewed or curated by ARC’s scholarly organizations.</p>
<p>ARC’s research environments are digital aggregators, and each research environment prescribes a set of peer review guidelines for the inclusion of a resource. Usually, scholar-created digital projects undergo blind peer-review and databases of digitized content undergo community approval processes. When a digital resource, project, or database has been approved, metadata about the  “holdings”  (or discrete digital items) is ingested into the  “ARC index.”  Once ingested into the index, this metadata allows each research environment to provide a search experience that aggregates records describing respected, scholarly digital resources into one federated search and discovery system. Search is open, whether across ARC nodes, or within individual ARC research environments, but each ARC node may choose to aggregate content that is behind a paywall according to its prescribed guidelines or review policies.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/13/1/000381/resources/images/figure01.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/13/1/000381/resources/images/figure01_hu242ddb5856aa342eaf0664cf3b042304_271905_500x0_resize_box_3.png 500w,
    /dhqwords/vol/13/1/000381/resources/images/figure01_hu242ddb5856aa342eaf0664cf3b042304_271905_800x0_resize_box_3.png 800w,/dhqwords/vol/13/1/000381/resources/images/figure01_hu242ddb5856aa342eaf0664cf3b042304_271905_1200x0_resize_box_3.png 1200w,/dhqwords/vol/13/1/000381/resources/images/figure01_hu242ddb5856aa342eaf0664cf3b042304_271905_1500x0_resize_box_3.png 1500w,/dhqwords/vol/13/1/000381/resources/images/figure01_hu242ddb5856aa342eaf0664cf3b042304_271905_1800x0_resize_box_3.png 1800w,/dhqwords/vol/13/1/000381/resources/images/figure01.png 3300w" 
     class="landscape"
     ><figcaption>
        <p>An ARC workflow showing the stages of peer review and curation, and discovery and dissemination via ten contributing nodes: 18thConnect, the Canadian Writing Research Collaboratory (CWRC), Great Lakes Aggregator (GLA), Medieval Electronic Scholarly Association (MESA), Modernist Networks (ModNets), Musical Scholarship Online (MuSO), Networked Early American Resources (NEAR), NINES, Renaissance Knowledge Network (ReKN), and Studies in Radicalism Online (SiRO).
        </p>
    </figcaption>
</figure>
<p>When a scholar searches the ARC index, the experience is similar to searching within an institution’s library search interface or the Digital Public Library of America, in which searches populate a list of results that point to a physical (in the stacks of a library) or digital space (in a database like JSTOR) where the item lives. Like DPLA and library search interfaces, using ARC for research purposes will return a list of results that point to where digital content can be located. Unlike other scholarly research portals online, ARC also points scholars to curated digital scholarly outputs created by and relevant to humanities researchers. These digital scholarly outputs, which range from annotated digital editions to complex archives, are indexed within ARC, and the digital content findable within ARC’s catalog contains not only a single metadata record pointing to the project, but many records describing the content available within the project.</p>
<p>ARC’s digital aggregation efforts have resulted in a catalog of over two million curated digital objects, spanning the medieval period and up to copyright, in federated digital research environments such as those listed on the right-hand side of Figure 1. The resulting federated ARC network allows scholars to search for content in previously  “atomized”  scholar-produced projects. Once spread far and wide across the internet, the digital projects aggregated by ARC are now discoverable via a single, scholarly, and scholar-designed search interface.<sup id="fnref:21"><a href="#fn:21" class="footnote-ref" role="doc-noteref">21</a></sup></p>
<p>ARC intends to reflect the interdisciplinary nature of digital humanities. Since digital work is often collaborative and interdisciplinary, an aggregated body of digital research inputs and outputs should consequently be findable and accessible to scholars from various disciplines. For digital methods to be fully integrated as humanities research methodologies, the outputs of these methods must be familiar to all humanists: all of the multidisciplinary, multilingual and international scholars that our institutions serve. Against the backdrop of this rich history of interdisciplinary and non-hierarchical work, MuSO joins ARC as a research community that seeks to build on ARC’s engagement with humanities scholars, by serving music scholars.</p>
<h2 id="muso-and-europeana">MuSO and Europeana</h2>
<p>At the conclusion of the NEH start-up grant, the MuSO team recognized the need for a new way to describe digital objects in music that would bring digital scholarly outputs in music together with digitized resources. At the same time, the schema and its descriptive ontologies needed to be accurate, while still broad enough to ensure interoperability across the boundaries of discipline and format within ARC. This discovery-level schema would not be intended to supplant the valuable, highly descriptive preservation-level data being generated by libraries, as MuSO would still send users to the resource and its accompanying preservation-level metadata to conduct more in-depth analysis. Rather, MuSO’s metadata schema would need to draw out the most important pieces of information that researchers need for discovery. To help with the creation of such a schema, MuSO chose to rely on yet another digital aggregator, Europeana.</p>
<p>Europeana    “is the organisation tasked by the European Commission with developing a digital cultural heritage platform for Europe”   <sup id="fnref:22"><a href="#fn:22" class="footnote-ref" role="doc-noteref">22</a></sup>  . Since its launch in 2008, Europeana has been working with libraries, museums, and other organizations in the cultural heritage sector throughout Europe to aggregate their digital collections into a single catalog and online interface. In the autumn of 2016, Europeana instituted a new grant scheme, entitled the Europeana Research Grant scheme, for scholars to work with its aggregated collections. Building on Europeana’s expertise in standardizing metadata descriptions of widely disparate collections, this grant would allow MuSO to construct an initial metadata schema, which could bridge the gap between Europeana’s metadata schema, the Europeana Data Model (EDM), and the ARC RDF schema while maintaining and incorporating the long-established cataloging standards of the music research community.</p>
<p>The new MuSO schema would allow music scholars to use MuSO for its music-specific content, but they could then expand their search to encompass the period-specific offerings of 18thConnect, the node of the Advanced Research Consortium (ARC) dedicated to eighteenth-century studies. MuSO’s federated searching capability therefore would become an opportunity to widen and bridge research fields as well as to promote knowledge of digital humanities research tools and methodologies within the musicological community <sup id="fnref1:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>. In other words, MuSO would provide users with what they are increasingly demanding: the ability to isolate content relating to Haydn’s  “The Flowers of Edinburgh,”  Hob. XXXIa:90 (for instance), regardless of whether it is in the context of a website covering eighteenth-century folk songs, Scottish literature, or a complete Haydn digital edition.</p>
<h2 id="aggregating-eighteenth-century-material-from-europeana-music">Aggregating Eighteenth-Century Material from Europeana Music</h2>
<p>In the first phase of the Europeana Grant, the team, consisting of a subset of the original MuSO team, inspected the metadata standards of over a dozen other projects and organizations, including the Digital Image Archive of Medieval Music (DIAMM), Répertoire International de Littérature Musicale (RILM), Répertoire International des Sources Musicales (RISM), Répertoire international de la presse musicale (RIPM), the Library of Congress’ Music Treasures Consortium, and others in order to identify common traits which might inform our decisions, as well as more customized metadata requirements which might also be useful. The overarching categories identified from these and other metadata standards included repository identifier, item identifier, language, data, physical description, title, and statements of responsibility.</p>
<p>With this information, the team entered the prototyping phase of the project. The team met for two days to discuss and formulate an initial metadata schema based on the information that was gathered in phase one. We distinguished where existing standards such as Dublin Core provided adequate structure, and where it might be necessary to propose specialized fields.</p>
<p>The third phase of development was necessarily iterative, as the team&rsquo;s abstract model made contact with actual data. During this phase, the team  “crosswalked”  eighteenth-century content in the Europeana Music Collection over to ARC using the new MuSO schema. A crosswalk is designed to show users or database managers how to translate data from one descriptive formatting to another. Because multiple metadata schemas exist in the humanities and cultural heritage sector, e.g. Dublin Core, MARC, TEI, RDF, such an intellectual and technical mapping must take place to allow search and indexing technologies to speak to each other. In general, the workflow followed a classic Deming cycle: Plan, Do, Study, Act. In general, the workflow followed a classic Deming cycle: Plan, Do, Study, Act. The workflow was progressively refined as implementation produced new information to incorporate: some elements which seemed desirable in the planning stages were not practical to implement in this phase or were not germane to the actual content of the data.</p>
<p>For instance, one element that seemed highly desirable was notation form, in order to distinguish systems like Gregorian chant notation, and lute tablature from modern standard notation. However, retrieving such information would have required item-level examination, as notation was inconsistently described in the dc:description field. And while the Resource Description and Access vocabulary for notation type was integrated into the MuSO metadata schema, the team chose to defer full notational description in view of time considerations.<sup id="fnref:23"><a href="#fn:23" class="footnote-ref" role="doc-noteref">23</a></sup></p>
<p>The team had also hoped to use the Library of Congress Genre/Form Project vocabularies in its subgenre field for maximum interoperability, but that project is still in progress and has not yet published an official vocabulary. In its absence, the team carried over Europeana fields that contained information on musical form and genre. This information will be retained until a standardized vocabulary is established.</p>
<p>Variations and areas for interpretation inevitably arose, even in a standardized structure such as Europeana’s. One of these areas was an unforeseen complication: institutions with multiple RISM sigla. RISM sigla have become the standard in the music community for identifying cultural heritage institutions (libraries, archives, museums, etc.) that hold music source materials. These sigla consist of a two-letter country code followed by a unique 3-4 letter institution designation.<sup id="fnref:24"><a href="#fn:24" class="footnote-ref" role="doc-noteref">24</a></sup>  Though the sigla are intended to be unique, some institutions have become consolidated since their sigla were assigned. This has led to many institutions such as the Bibliothèque nationale de France having several sigla assigned to them. The MuSO team therefore had to determine which sigla to use. In another example of the data variations in the Europeana dataset, certain art forms required consensus due to their multidisciplinary nature, such as ballet, which incorporates music, dance, and drama. Some cases also pointed to a clear need for new genre terms in the ARC vocabularies, which the team suggested: e.g. dance, pedagogy, ethnography, and sculpture/architecture.</p>
<p>Similarly, a consensus was necessary on the English translation and labeling of terms describing roles and responsibilities, as with  <em>Éditeur scientifique</em>  and  <em>Compositeur prétendu</em> .<sup id="fnref:25"><a href="#fn:25" class="footnote-ref" role="doc-noteref">25</a></sup>  This process was also iterative, as team members conferred and reconciled interpretations and decisions across datasets. Retrospective editing was not difficult, as the facet and clustering function in OpenRefine, which is an open source tool for working with messy data, aided finding and replacing terms as needed.</p>
<p>With the data successfully crosswalked, the fourth phase focused on ingesting the data into 18thConnect. During this time, the team worked with the development team at 18thConnect to ensure that the data was consistent and that it displayed properly on the 18thConnect website. Furthermore, the team launched a new MuSO website that would highlight MuSO’s schema and aggregated content.<sup id="fnref:26"><a href="#fn:26" class="footnote-ref" role="doc-noteref">26</a></sup></p>
<h2 id="looking-forward">Looking Forward</h2>
<p>Despite the advances of the MuSO project, much work remains in generating a digital portal that is fully capable of curating scholarly research and materials. During the phases of the Europeana grant and the initial Start-up Grant from the National Endowment for the Humanities, the MuSO community of scholars identified four significant tasks for the project. These include building a search platform for MuSO, developing interdisciplinary project review and data integration workflows within the ARC community, identifying and incorporating more digitized collections into MuSO’s catalog of musical resources, and finally promoting digital outputs and metadata standards among music scholars and humanists.</p>
<p>Thanks to the efforts of 18thConnect, MuSO has been able to make a selection of the content found in Europeana Music interoperable with the existing ARC catalog. This is an important step, as it allows scholars to discover articles about Mozart alongside the actual letters and scores that those articles cite. Nevertheless, ARC’s reliance on text-based metadata means that its current search interfaces are completely biased towards the search and discovery of textual information, even if that information is describing some non-textual content. Although music can be notated and encoded in ways similar to text, it is also heavily reliant on sonic and non-textual manifestations, and a number of researchers are working on these issues of musical search for both audio and symbolic formats. Given MuSO’s commitment to interdisciplinarity, its search interface must bring these efforts in non-textual search together with existing textual search technologies, expanding ARC’s search capabilities to allow scholars to search and discover melody, harmony, and rhythm in addition to text.</p>
<p>MuSO’s collaboration with the ARC community will likely result in more than an expansion of search capabilities. ARC was originally organized according to temporal boundaries such as nineteenth-century studies, medieval studies, and modernist studies. MuSO is one of the latest in a number of recent additions to ARC that is dedicated to a particular topic or discipline. MuSO’s efforts to promote digital scholarly outputs alongside digitized musical resources are spurring the ARC community to re-evaluate its curatorial practices. It is not enough to append a new music-based research environment to ARC’s current structure. MuSO will invariably aggregate content of relevance to ARC’s other research communities (i.e. Wagner-related content for nineteenth-century scholars). MuSO’s future efforts will therefore include partnering with the existing research communities in ARC to ensure that digitized musical resources are discoverable in the relevant research environments outside of the MuSO portal so that Wagner-related content can be discovered in NINES along with MuSO. Moreover, MuSO will work with the ARC community to develop methods of conducting both period-specific and discipline-specific review of digital scholarly outputs that will provide relevant peer-review for contributors while assuring users that the content made available through ARC and its nodes has been vetted by experts in relevant disciplinary and cultural-temporal fields.</p>
<p>MuSO will continue to identify and aggregate existing digital collections into its catalog. Resources such as Europeana, HathiTrust, and the DPLA continue to grow, and MuSO will do the same, identifying digitized primary sources and secondary scholarship for inclusion in the MuSO catalog. MuSO will also work to include digital scholarly outputs in its catalog. It has already identified a number of digital projects that meet technical and scholarly standards that would likely pass peer review and could be incorporated into the MuSO catalog. MuSO will reach out to these projects and ones of similar quality that arise in the future, encouraging them to consider digital peer review.</p>
<p>This goal is closely related to MuSO’s final future goal to promote digital outputs and metadata standards among music scholars. MuSO will work closely with digital projects to ensure they follow best practices for describing their digital outputs, and it will promote those outputs by making them available for the scholarly community to search and discover alongside other relevant scholarly resources. The criteria for inclusion in MuSO remain under development in anticipation of the continued evolution of digital scholarship in music. In addition to accommodating emerging technologies and methods, our intention to democratize participation in digital scholarship favors an inclusive approach to proposed projects. Therefore thinking  “outside the box”  may find a place as  “a feature, not a bug,”  where traditional modes of scholarly communication in music research retain a formidable pedigree. The basic criteria for inclusion established in the January 2016 meeting for MuSO continue to apply: Does the project make a worthwhile contribution to research in its subject area? Is its methodology sound? Does the project achieve its own goals? Are there major obstacles to usability? Are its existence and accessibility sustainable? <sup id="fnref1:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup></p>
<p>Scholars are increasingly turning to digital resources for conducting and disseminating their research. Professional societies such as the Modern Language Association <sup id="fnref:27"><a href="#fn:27" class="footnote-ref" role="doc-noteref">27</a></sup> and the American Historical Association <sup id="fnref:28"><a href="#fn:28" class="footnote-ref" role="doc-noteref">28</a></sup> are setting standards for digital outputs, and governmental standards initiatives such as the United Kingdom’s Research Excellence Framework are making allowances for digital outputs. However, current outlets for reviewing music scholarship are optimized for static print and therefore struggle to assess multi-modal and multidisciplinary digital scholarly outputs. Amidst a growing set of high-quality digital projects, the musical community demands not only a set of standards for assessing born-digital scholarship but it demands a community with expertise in assessing that scholarship <sup id="fnref1:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup>. Without a central location afforded by traditional publishing platforms such as journal articles and monographic series, digital scholarly outputs demand digital aggregation platforms that ensure they are discoverable by the scholarly community. MuSO is poised to step into this void, promoting quality digital scholarship in music while giving it an equal voice alongside the traditional platforms of dissemination.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>For DIAMM, see <a href="https://www.diamm.ac.uk/">https://www.diamm.ac.uk/</a>; for CFEO, see <a href="http://www.chopinonline.ac.uk/cfeo/">http://www.chopinonline.ac.uk/cfeo/</a>; for Early Music Online, see <a href="https://www.royalholloway.ac.uk/music/research/earlymusiconline/home.aspx">https://www.royalholloway.ac.uk/music/research/earlymusiconline/home.aspx</a>.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Rosenzweig, R.  “Scarcity or Abundance? Preserving the Past in a Digital Era.”    <em>The American Historical Review,</em>  108.3 (2003): 735–62.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Hope, H.  “Review of Virtual Library of Musicology (ViFaMusik), (<a href="http://www.vifamusik.de">http://www.vifamusik.de</a>),”    <em>Music Theory Online,</em>  20.3 (2014): 32–35.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Inskip, C. &amp; Wiering, F. (2015)  “In their own words: using text analysis to identify musicologists’ attitudes towards technology.”  In  <em>Proceedings of the 16th International Society for Music Information Retrieval Conference</em> . [online]. Malaga, Spain: International Society for Music Information Retrieval, 1-7. <a href="http://ismir2015.uma.es/index.html">http://ismir2015.uma.es/index.html</a> [Accessed 2 October 2017]&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>Pugin, L.  “The Challenge of Data in Digital Musicology.”    <em>Frontiers in Digital Humanities,</em>  2 (2015). [online]. <a href="https://doi.org/10.3389/fdigh.2015.00004">https://doi.org/10.3389/fdigh.2015.00004</a>. [Accessed 2 October 2017]&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>Kent-Muller, A. (2017)  “Big Musicology: A Framework for Transformation.”  In  <em>Proceedings of the 4th International Digital Libraries for Musicology Workshop</em> . [online]. Shanghai, China: ACM ICPS, 1-2. <a href="http://www.transforming-musicology.org/resources/documents/KENT-MULLER-Big-Musicology.pdf">http://www.transforming-musicology.org/resources/documents/KENT-MULLER-Big-Musicology.pdf</a>. [Accessed 2 October 2017]&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>Urberg, M.  “Pasts and Futures of Digital Humanities in Musicology: Moving Towards a ‘Bigger Tent’.”    <em>Music Reference Services Quarterly</em> , 20.3-4 (2018): 134–50. <a href="https://doi.org/10.1080/10588167.2017.1404301">https://doi.org/10.1080/10588167.2017.1404301</a>. [Accessed 24 April 2018]&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>Hooper, G.  <em>The Discourse of Musicology</em> . London and New York, NY, Routledge (2016).&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>Johnson, S.  <em>The Ghost Map: The Story of London’s Most Terrifying Epidemic - and How it Changed Science, Cities, and the Modern World.</em>  New York, NY, Riverhead Books (2006).&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>Duguid, T.  “Revolutionaries Needed: peer review in early music digital scholarship and editions.”    <em>Early Music,</em>  42.4 (2014): 619–22.&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>Duffy, C.  “Mapping the Use of ICT in Creative Music Practice.”  In T. Crawford and L. Gibson (eds),  <em>Modern Methods for Musicology: Prospects, Proposals, and Realities</em> . Farnham, Surrey Burlington, VT, Ashgate (2009): 73–91.&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p>Nowviskie, B. And McGann, J. (September 2005).  <em>NINES White Paper</em>  [online]. <a href="http://www.nines.org/about/wp-content/uploads/2011/12/9swhitepaper.pdf">http://www.nines.org/about/wp-content/uploads/2011/12/9swhitepaper.pdf</a>. [Accessed 29 October 2017].&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13">
<p>McGann, J.  “On Creating a Usable Future” ,  <em>Profession</em> , (2011): 182-95.&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14">
<p>See <a href="http://digitalduchemin.org/">http://digitalduchemin.org/</a> for  <em>The Lost Voices Project</em> ; <a href="http://www.transforming-musicology.org/news/2014-11-13_hearing-wagner-at-being-human/">http://www.transforming-musicology.org/news/2014-11-13_hearing-wagner-at-being-human/</a> for  <em>Hearing Wagner</em> ; <a href="http://www.songsofthevictorians.com/">http://www.songsofthevictorians.com/</a> for  <em>Songs of the Victorians</em> .&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:15">
<p><a href="http://muso.arts.gla.ac.uk/index.html">http://muso.arts.gla.ac.uk/index.html</a>&#160;<a href="#fnref:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:16">
<p><a href="http://www.charm.kcl.ac.uk/index.html">http://www.charm.kcl.ac.uk/index.html</a>&#160;<a href="#fnref:16" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:17">
<p><a href="https://www.vifamusik.de">https://www.vifamusik.de</a>&#160;<a href="#fnref:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:18">
<p>Platt, H.  “Electronic Resource Review: Virtual Library of Musicology.”    <em>Nineteenth Century Music Review</em> , 10.2 (2013): 359–64. <a href="https://doi.org/10.1017/S1479409813000372">https://doi.org/10.1017/S1479409813000372</a>. [Accessed 30 October 2017]&#160;<a href="#fnref:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:19">
<p>Duguid, T. (31 August 2016).  <em>MuSO: Aggregation and Peer Review in Music</em>  [online] <a href="http://oaktrust.library.tamu.edu/handle/1969.1/157548">http://oaktrust.library.tamu.edu/handle/1969.1/157548</a>. [Accessed 1 November 2017].&#160;<a href="#fnref:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:20">
<p><a href="https://manifoldapp.org/">https://manifoldapp.org/</a>&#160;<a href="#fnref:20" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:21">
<p>For instance, see BigDiva, <a href="http://www.bigdiva.org">http://www.bigdiva.org</a>; and Collex, <a href="https://github.com/collex">https://github.com/collex</a>.&#160;<a href="#fnref:21" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:22">
<p>Kenny, E. (2017)  “Who we are,”    <em>Europeana Pro: Transforming the world with culture</em> . [online]. <a href="https://pro.europeana.eu/our-mission/who-we-are">https://pro.europeana.eu/our-mission/who-we-are</a>. [Accessed 9 April 2018]&#160;<a href="#fnref:22" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:23">
<p><a href="http://www.rdaregistry.info/termList/MusNotation/">http://www.rdaregistry.info/termList/MusNotation/</a>&#160;<a href="#fnref:23" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:24">
<p><a href="http://www.rism.info/en/sigla.html">http://www.rism.info/en/sigla.html</a>&#160;<a href="#fnref:24" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:25">
<pre><code>_Éditeur scientifique_  ordinarily designates an editor of a scholarly monographic series or journal, however in the case of the eighteenth-century score anthologies held in the Bibliothèque nationale de France, the term was used to describe the role of the person responsible for compiling and editing the songs published therein. Consequently, the MuSO team agreed to use the  “Compiler”  relator code for this term.  _Compositeur prétendu_  described a circumstance in which a work’s creative attribution was later called into question; the MuSO team preserved this term in its metadata as  “Attributed name.” 
</code></pre>
&#160;<a href="#fnref:25" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></li>
<li id="fn:26">
<p><a href="http://muso.arts.gla.ac.uk">http://muso.arts.gla.ac.uk</a>&#160;<a href="#fnref:26" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:27">
<p>Modern Language Association (2017).  <em>Guidelines for Evaluating Work in Digital Humanities and Digital Media</em> . [online] Modern Language Association. Available from: <a href="https://www.mla.org/About-Us/Governance/Committees/Committee-Listings/Professional-Issues/Committee-on-Information-Technology/Guidelines-for-Evaluating-Work-in-Digital-Humanities-and-Digital-Media">https://www.mla.org/About-Us/Governance/Committees/Committee-Listings/Professional-Issues/Committee-on-Information-Technology/Guidelines-for-Evaluating-Work-in-Digital-Humanities-and-Digital-Media</a>. [Accessed 2 October 2017]&#160;<a href="#fnref:27" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:28">
<p>American Historical Association (2015).  <em>Professional Evaluation of Digital Scholarship in History.</em>  Available from: <a href="https://www.historians.org/teaching-and-learning/digital-history-resources/evaluation-of-digital-scholarship-in-history">https://www.historians.org/teaching-and-learning/digital-history-resources/evaluation-of-digital-scholarship-in-history</a>. [Accessed 2 October 2017]&#160;<a href="#fnref:28" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry></feed>