<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=theme-color content="#3D206C"><meta http-equiv=x-ua-compatible content="IE=edge"><link rel=apple-touch-icon sizes=180x180 href=/img/favicon/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/img/favicon/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/img/favicon/favicon-16x16.png><link rel=icon type=image/png sizes=384x384 href=/img/favicon/android-chrome-384x384.png><link rel=icon type=image/png sizes=192x192 href=/img/favicon/android-chrome-192x192.png><link rel=icon type=image/png sizes=150x150 href=/img/favicon/mstile-150x150.png><link rel="shortcut icon" href=/favicon.ico><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/img/favicon/safari-pinned-tab.svg color=#3d206c><link rel=schema.dc href=http://purl.org/DC/elements/1.0/><meta name=citation_public_url content="https://rlskoeser.github.io/dhqwords/vol/13/3/000426/"><meta name=citation_title content="Textension: Digitally Augmenting Document Spaces in Analog Texts"><meta name=citation_date content="2019/10"><meta name=citation_author content="Adam James Bradley"><meta name=citation_author content="Victor Sawal"><meta name=citation_author content="Sheelagh Carpendale"><meta name=citation_author content="Christopher Collins"><meta name=citation_abstract content="INTRODUCTION Printed books still remain persistent in the workflow of scholars even though there is a plethora of digital options available that afford great power and flexibility to the user. Word processors and other applications have been completely integrated into people’s daily lives and have started to replace pen and paper as a modality for interacting with the written word; when it comes to books, the affordances offered by digital platforms such as search and copy are considered paradigm shifting additions to the act of reading."><meta name=citation_journal_title content="DHQwords"><meta name=citation_issue content="13.3"><meta name=citation_publisher content="Center for Digital Humanities, Princeton University"><meta name=DC.rights content="http://creativecommons.org/licenses/by/4.0/"><meta name=author content="Adam James Bradley, Victor Sawal, Sheelagh Carpendale, Christopher Collins"><meta name=generator content="Center for Digital Humanities, Princeton University"><meta name=dcterms.created content="2019-10"><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-regular.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-italic.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-regular.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-300.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-700.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-300italic.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-500italic.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-700italic.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-500.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/Source_Sans_Pro/SourceSans3VF-Roman.otf.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/Source_Sans_Pro/SourceSans3VF-Italic.otf.woff2 crossorigin><title>Textension: Digitally Augmenting Document Spaces in Analog Texts</title><meta name=description content="DHQwords Issue 13.3, October 2019. an experiment republishing DHQ articles with Hugo & Startwords theme. "><meta property="og:title" content="Textension: Digitally Augmenting Document Spaces in Analog Texts"><meta property="og:description" content="INTRODUCTION Printed books still remain persistent in the workflow of scholars even though there is a plethora of digital options available that afford great power and flexibility to the user. Word processors and other applications have been completely integrated into people’s daily lives and have started to replace pen and paper as a modality for interacting with the written word; when it comes to books, the affordances offered by digital platforms such as search and copy are considered paradigm shifting additions to the act of reading."><meta property="og:type" content="article"><meta property="og:url" content="https://rlskoeser.github.io/dhqwords/vol/13/3/000426/"><meta property="og:image" content="https://rlskoeser.github.io/dhqwords/social.png"><meta property="article:section" content="vol"><meta property="article:published_time" content="2019-10-14T00:00:00+00:00"><meta property="article:modified_time" content="2019-10-14T00:00:00+00:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://rlskoeser.github.io/dhqwords/social.png"><meta name=twitter:title content="Textension: Digitally Augmenting Document Spaces in Analog Texts"><meta name=twitter:description content="INTRODUCTION Printed books still remain persistent in the workflow of scholars even though there is a plethora of digital options available that afford great power and flexibility to the user. Word processors and other applications have been completely integrated into people’s daily lives and have started to replace pen and paper as a modality for interacting with the written word; when it comes to books, the affordances offered by digital platforms such as search and copy are considered paradigm shifting additions to the act of reading."><link rel=stylesheet href=/dhqwords/style.css><link rel=stylesheet href=/dhqwords/print.css media=print><script src=/dhqwords/js/polyfills.js></script><script defer src=/dhqwords/js/bundle.js></script></head><body class=article><header><nav class=main aria-label=main><ul><li class=home><a href=/dhqwords/>dhq</a></li><li class=issues><a href=/dhqwords/vol/>volumes</a></li></ul></nav></header><main><article><div class=grid><header><p class=number><a href=/dhqwords/vol/13/3/>Issue 13.3</a></p><h1>Textension: Digitally Augmenting Document Spaces in Analog Texts</h1><p><ul class=authors><li><address>Adam James Bradley</address></li><li><address>Victor Sawal</address></li><li><address>Sheelagh Carpendale</address></li><li><address>Christopher Collins</address></li></ul></p><p><time class=pubdate datetime=2019-10>October 2019</time></p><ul class="categories tags"></ul><ul class=tags></ul><p class=formats></p></header><section class=print-only><a class=first-page-header href=/ aria-label=Startwords><img alt=Startwords src=/pdf-logotype.svg></a>
<a class=page-header href=/ aria-label=Startwords><img alt=Startwords src=/pdf-logo.svg></a>
<a href=http://doi.org/ rel=alternate class=page-doi>doi:</a></section><div class=text-container><h1 id=heading></h1><h2 id=introduction>INTRODUCTION</h2><p>Printed books still remain persistent in the workflow of scholars even though there is a plethora of digital options available that afford great power and flexibility to the user. Word processors and other applications have been completely integrated into people’s daily lives and have started to replace pen and paper as a modality for interacting with the written word; when it comes to books, the affordances offered by digital platforms such as search and copy are considered paradigm shifting additions to the act of reading. But, even though these tools exist, scholars still write on paper and still have books on their bookshelves. There is a tension that exists between these new digital formats and our history. We often create digital tools to mimic the affordances of books, but while they improve steadily, the weight, smell, and sounds of a book are still unique to bound paper and ink. It is important to note that it is still not known how these affordances affect cognition, and for literary scholarship, interpretation. Mehta et al. studied fourteen literary critics and found that each had idiosyncratic methods of marking up a literary document, but most importantly, all of them engaged in some form of annotation when working with poetic texts <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>. Our own domain expert, a modernist literary critic, confirmed the findings of that study and presented the idea to us for a system that allows scholars to quickly digitize a document for augmentation but retain the look and feel of the original.</p><figure><img loading=lazy alt="An annotated digital capture of a printed page." src=/dhqwords/vol/13/3/000426/resources/images/figure01.png sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/13/3/000426/resources/images/figure01_hub73a88c5528da284e7be80074e6a269b_304982_500x0_resize_box_3.png 500w,
/dhqwords/vol/13/3/000426/resources/images/figure01_hub73a88c5528da284e7be80074e6a269b_304982_800x0_resize_box_3.png 800w,/dhqwords/vol/13/3/000426/resources/images/figure01_hub73a88c5528da284e7be80074e6a269b_304982_1200x0_resize_box_3.png 1200w,/dhqwords/vol/13/3/000426/resources/images/figure01_hub73a88c5528da284e7be80074e6a269b_304982_1500x0_resize_box_3.png 1500w,/dhqwords/vol/13/3/000426/resources/images/figure01.png 1515w" class=landscape><figcaption><p>The five document spaces that can be used for placement of interactive elements: Word Space, Line Space, Margin Space, Occlusion Space, Canvas Space. In this image, the line space has been automatically expanded and sparklines inserted.</p></figcaption></figure><p>Beyond the affordances of physical texts, many older texts used for research often do not have reliable digital versions, and many corpora are still digitized as images such as Early English Books Online <sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>. The solution we offer is a combination of techniques that bring together the paper and ink history of our past, with the digital affordances of our present. To demonstrate, we present a web-based interface, geared toward researchers, that allows for the quick digitization and augmentation of paper documents. By using any web-based device with a camera or uploading an existing image, Textension allows the user to create interactive digital objects from analog texts that retain the look and feel of the originals.</p><p>To start this project, we asked ourselves the question: How can we allow users the ability to interact with both analog and digital text at the same time? Not knowing exactly what we lose when we digitize a text in terms of interpretation or cognition is a much larger problem, and we wanted to address how to interact with these texts without completely discarding the originals. While there are great efforts to digitize the world’s books, such as the Google books projects <sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>, large-scale OCR projects are difficult to implement. With the growth of digitized text repositories that leverage these technologies, two problems are still outstanding: digitally supporting books that have not yet been digitized, and enabling better use of books that have been digitized as images and are not currently interactive. The flexibility and freedom of digital writing and reading are leading to increasing pressure to digitize texts. However, most of these solutions are costly, time-consuming, and never seem to reach the document of current interest. There is a need for a quick, direct and simple way to gain these freedoms with the document you currently have in hand — whether it is a hand-written letter, an old book, or the newspaper. What we have laid out here is a method for achieving this goal, while still having and keeping the original text close at hand.</p><p>In this paper, we present a framework that extends the power of the digital to physical books in near real-time. Our contribution is bringing together ideas studied in digital document spaces and existing word-scale visualizations to demonstrate how these known quantities can be leveraged to bridge analog and digital reading and writing. Our framework is informed by previous results describing document spaces and the different ways they are used with analog documents. We outline each of the document spaces and describe how they can be used in tool design, and we implement a prototype to demonstrate the robustness of this framework (see <a href=#figure01>Figure 1</a>).</p><p>Through Textension we offer quick access, applicable to the analog text at hand, to an integrated digital/analog environment, only requiring common equipment such as the camera in a phone and a preferred web-enabled device. Simply photograph or upload an existing picture of a document, display it in Textension on a web-browser and start interacting. By making paper documents interactive on mobile devices Textension allows for a smooth transition between our history and our present by allowing users a quick way to digitize documents while working on-site in places like libraries. Our system produces in-line visualizations and interactive elements directly on the newly built digital document, allowing for work to continue while having an augmented digital document at the ready.</p><h2 id=related-work>RELATED WORK</h2><p>While we present a prototype in this paper, we see our main contribution as a discussion and amalgamation of what is possible when bringing together analog and digital affordances as it relates to text. We see Textension as a way to leverage past studies and computational approaches to natural language to quickly create digital documents from analog texts using readily accessible mobile technology.</p><h2 id=paper-and-digital-functionality>Paper and Digital Functionality</h2><p>Bridging the affordances of paper documents and digital technologies is not a new pursuit. Many projects have attempted to cross the boundaries between these two modalities to leverage what is best in both.</p><p>Early work on Fluid Documents <sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup> <sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup> has provided inspiration for working with both analog and digital documents. Bondarenko and Janssen provide an overview of what we can learn from paper to improve digital documents <sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup>, specifically important to this framework is their distinction between the affordances of paper versus digital texts in terms of task management. Supporting active reading using hybrid interfaces has been addressed by Hong et al. <sup id=fnref:7><a href=#fn:7 class=footnote-ref role=doc-noteref>7</a></sup>, and Shilit et al. report methods for supporting active reading with freeform digital ink annotations <sup id=fnref:8><a href=#fn:8 class=footnote-ref role=doc-noteref>8</a></sup>, we leveraged these findings as we designed and built our prototype. More recently, Metatation was presented as a pen-based project that supports close reading annotation work on physical paper with a digital interface <sup id=fnref1:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> and concluded that pen-based annotations were necessary for the workflow of literary critics. The difference between active and close reading is small but important. Active reading is often discussed as a way to engage in cognitive offloading i.e. note taking, while close reading is a task that is specific to literary criticism where critics use analysis techniques to decipher meanings within a text. We developed the Textension framework to be robust enough to allow for both activities.</p><p>Pen-based systems that cross between physical and digital interfaces have also been explored by Weibel et al. <sup id=fnref:9><a href=#fn:9 class=footnote-ref role=doc-noteref>9</a></sup>, and with the introduction of gestures in RichReview <sup id=fnref:10><a href=#fn:10 class=footnote-ref role=doc-noteref>10</a></sup>. Gesture-based interactions have also been addressed in Papiercraft, a system which enables gesture-based commands on interactive paper <sup id=fnref:11><a href=#fn:11 class=footnote-ref role=doc-noteref>11</a></sup>, Pacer, a method for interacting with live images of documents using hybrid gestures on a cell phone <sup id=fnref:12><a href=#fn:12 class=footnote-ref role=doc-noteref>12</a></sup>, and Knotty gestures, which presents subtle traces to support interactive use of paper <sup id=fnref:13><a href=#fn:13 class=footnote-ref role=doc-noteref>13</a></sup>.</p><p>Paper-augmented digital documents merge physical annotations on paper with a digital representation <sup id=fnref:14><a href=#fn:14 class=footnote-ref role=doc-noteref>14</a></sup>. Paper and digital media have also been used together to support co-located group meetings <sup id=fnref:15><a href=#fn:15 class=footnote-ref role=doc-noteref>15</a></sup>. Work has also been done on applying paper affordances to digital workspaces such as page flipping and annotations in e-readers <sup id=fnref:16><a href=#fn:16 class=footnote-ref role=doc-noteref>16</a></sup>. The Paper Windows project <sup id=fnref:17><a href=#fn:17 class=footnote-ref role=doc-noteref>17</a></sup> projected digital environments onto physical paper allowing for the affordances of both. The TextTearing tool <sup id=fnref:18><a href=#fn:18 class=footnote-ref role=doc-noteref>18</a></sup> was the inspiration for how we create space in digital documents from analog texts. In our framework, this is done automatically from a picture of an analog text taken in situ with a smartphone or tablet and not simply from an electronic version of the document.</p><h2 id=computer-vision-for-document-analysis>Computer Vision for Document Analysis</h2><p>Our tool uses several computer vision techniques to aid in the OCR from the camera of a mobile device. The following papers all describe techniques that helped us to consider the problems and solutions of in-the-wild document digitization. Digitizing historical documents is a difficult prospect and the binarization and filtering techniques presented in da Silva’s work <sup id=fnref:19><a href=#fn:19 class=footnote-ref role=doc-noteref>19</a></sup> provided guidance on working with older texts as would be often found in a humanities setting. Also, Gupta discusses OCR binarization and preprocessing specifically for historical documents <sup id=fnref:20><a href=#fn:20 class=footnote-ref role=doc-noteref>20</a></sup>. Much work has been done on document capture using digital cameras including background removal <sup id=fnref:21><a href=#fn:21 class=footnote-ref role=doc-noteref>21</a></sup>, document image analysis <sup id=fnref:22><a href=#fn:22 class=footnote-ref role=doc-noteref>22</a></sup>, and marginal noise removal <sup id=fnref:23><a href=#fn:23 class=footnote-ref role=doc-noteref>23</a></sup>. Liang et al. provided a useful survey of camera-based analysis of text and documents <sup id=fnref:24><a href=#fn:24 class=footnote-ref role=doc-noteref>24</a></sup>. Commercial applications such as the WordLens (now part of Google Translate) also integrate text processing and analysis <sup id=fnref:25><a href=#fn:25 class=footnote-ref role=doc-noteref>25</a></sup>. WordLens provides real-time translation of text detected through the mobile camera, creating an augmented reality environment. We build on this concept by matching the vision technology with information visualization and natural language processing tools to enhance document images with rich and task-specific augmentations.</p><p>Post-processing for text documents tends to be idiosyncratic to the documents themselves. For example, a document with a completely white background may not need to be gamma corrected. We built into our tool the ability to control multiple processing parameters. Influence for this decision came from work such as perspective correction <sup id=fnref:26><a href=#fn:26 class=footnote-ref role=doc-noteref>26</a></sup>, quality assessment and improvement <sup id=fnref:27><a href=#fn:27 class=footnote-ref role=doc-noteref>27</a></sup>, and image segmentation <sup id=fnref:28><a href=#fn:28 class=footnote-ref role=doc-noteref>28</a></sup>. There has also been research on tools for these kinds of post-processing tasks. Specifically, Photodoc is a program to aid in document processing acquired by digital cameras <sup id=fnref:29><a href=#fn:29 class=footnote-ref role=doc-noteref>29</a></sup>.</p><h2 id=text-visualization>Text Visualization</h2><p>Text vis is an enormous subsection of information visualization. Rather than a survey of available text visualizations, we have included a list of references that directly affected our work and design decisions. Early text visualizations such as ThemeRiver and TextArc demonstrate novel ways of visualizing text <sup id=fnref:30><a href=#fn:30 class=footnote-ref role=doc-noteref>30</a></sup> <sup id=fnref:31><a href=#fn:31 class=footnote-ref role=doc-noteref>31</a></sup>. Sparklines over tag clouds have been previously presented by Lee et al. <sup id=fnref:32><a href=#fn:32 class=footnote-ref role=doc-noteref>32</a></sup>. Projects such as SeeSoft <sup id=fnref:33><a href=#fn:33 class=footnote-ref role=doc-noteref>33</a></sup>, a text-based tool for visualizing segments of programming code, gave us an insight into how portions of a document could be visualized separately to indicate different workspaces. And TileBars <sup id=fnref:34><a href=#fn:34 class=footnote-ref role=doc-noteref>34</a></sup> showed ways of visualizing document structure at the sentence level. There has been quite a lot of work visualizing document collections, but for our purposes, two of the most relevant projects were Compus <sup id=fnref:35><a href=#fn:35 class=footnote-ref role=doc-noteref>35</a></sup> and ThemeRiver <sup id=fnref1:30><a href=#fn:30 class=footnote-ref role=doc-noteref>30</a></sup>. Compus used XML documents to visualize structure in historical manuscripts and ThemeRiver depicted thematic changes over time in document collections. For larger surveys of related work, we suggest the text visualization survey by Kucher and Kerren <sup id=fnref:36><a href=#fn:36 class=footnote-ref role=doc-noteref>36</a></sup> and a specific treatment of text analysis in the Digital Humanities by Jänicke et al. <sup id=fnref:37><a href=#fn:37 class=footnote-ref role=doc-noteref>37</a></sup>.</p><figure><img loading=lazy alt="Graphic visualization of Textension framework." src=/dhqwords/vol/13/3/000426/resources/images/figure02.jpeg sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/13/3/000426/resources/images/figure02_hue73c4e015e4e5131aa0f5b7a71890500_61809_500x0_resize_q75_box.jpeg 500w,
/dhqwords/vol/13/3/000426/resources/images/figure02_hue73c4e015e4e5131aa0f5b7a71890500_61809_800x0_resize_q75_box.jpeg 800w,/dhqwords/vol/13/3/000426/resources/images/figure02_hue73c4e015e4e5131aa0f5b7a71890500_61809_1200x0_resize_q75_box.jpeg 1200w,/dhqwords/vol/13/3/000426/resources/images/figure02.jpeg 1200w" class=landscape><figcaption><p>The Textension framework architecture: images are captured using a mobile device or webcam. The text is extracted by the OCR engine and the digitized text is processed using NLP techniques. The background of the document is detected and synthesized for the insertion of interactive elements. External resources are brought in as needed to create augmentations which combine with the document image to create the final layout on screen. Finally, any interaction queries are fed back to the layout engine and augmentations and an updated output are generated.</p></figcaption></figure><h2 id=word-scale-visualizations>Word-Scale Visualizations</h2><p>Perhaps the pioneering work in word-scale visualization was Tufte’s proposal of Sparklines <sup id=fnref:38><a href=#fn:38 class=footnote-ref role=doc-noteref>38</a></sup>, which are small line charts which reveal trends and work well in small multiples. The different types of possible word-scale visualizations have been explored by Brandes et al. <sup id=fnref:39><a href=#fn:39 class=footnote-ref role=doc-noteref>39</a></sup>, where they discuss the ideas of Gestalt psychology as it applies to word level visualization. Goffin has explored implementations and effects of word-scale visualizations in depth <sup id=fnref:40><a href=#fn:40 class=footnote-ref role=doc-noteref>40</a></sup> <sup id=fnref:41><a href=#fn:41 class=footnote-ref role=doc-noteref>41</a></sup> <sup id=fnref:42><a href=#fn:42 class=footnote-ref role=doc-noteref>42</a></sup> <sup id=fnref:43><a href=#fn:43 class=footnote-ref role=doc-noteref>43</a></sup> <sup id=fnref:44><a href=#fn:44 class=footnote-ref role=doc-noteref>44</a></sup>. These include explorations of the history, design, placement within a text, the interactive possibilities of word-scale visualizations, and their impact on reading behavior. Sparkclouds use word-scale visualizations to show trends as a text visualization <sup id=fnref1:32><a href=#fn:32 class=footnote-ref role=doc-noteref>32</a></sup> and Nacenta et al. introduced Fat Fonts, a method for encoding numerical information on the word-scale directly into the text itself <sup id=fnref:45><a href=#fn:45 class=footnote-ref role=doc-noteref>45</a></sup>. There has also been work on producing word size graphics for scientific texts <sup id=fnref:46><a href=#fn:46 class=footnote-ref role=doc-noteref>46</a></sup>, and a particularly interesting interaction study that investigates eye movement while users engage with word-scale visualizations <sup id=fnref:47><a href=#fn:47 class=footnote-ref role=doc-noteref>47</a></sup>.</p><h2 id=textension-framework>Textension Framework</h2><p>Previous studies on digital document interaction, annotations, e-readers, and marginal interactions <sup id=fnref:48><a href=#fn:48 class=footnote-ref role=doc-noteref>48</a></sup> <sup id=fnref:49><a href=#fn:49 class=footnote-ref role=doc-noteref>49</a></sup> <sup id=fnref1:40><a href=#fn:40 class=footnote-ref role=doc-noteref>40</a></sup> <sup id=fnref2:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> have identified five spaces that are interacted with on a digital document: word space, line space, margin space, occlusion space, and canvas space. Mehta discussed word space, line space, and margin space as places of interaction for analog annotation <sup id=fnref3:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> after studying literary critics working on poetry with pen and paper. Goffin et al. use word space and line space as alternatives for the placement of word-scale visualizations <sup id=fnref2:40><a href=#fn:40 class=footnote-ref role=doc-noteref>40</a></sup>. Occlusion space or the space above the document has long been an accepted interaction modality, the most famous incarnation being the everyday tooltip. Goffin et al. use this space to provide enlarged maps on text documents <sup id=fnref3:40><a href=#fn:40 class=footnote-ref role=doc-noteref>40</a></sup>. Canvas space was discussed by Cheema et al. in a paper outlining how to extend documents using external resources such as drag and drop images <sup id=fnref1:49><a href=#fn:49 class=footnote-ref role=doc-noteref>49</a></sup>. While each of these spaces represents a unique portion of a digital document where interactive elements can be placed, Textension is a modular system that allows for these elements to be created interchangeably in all document spaces. Our goal is to demonstrate a framework where analog documents can be turned into interactive objects in a short span of time by using the document spaces appropriately.</p><h2 id=the-five-interactive-spaces-of-a-digital-document>The Five Interactive Spaces of a Digital Document</h2><p>From the previous work, we have identified five different types of spaces in digital documents that can be augmented:</p><ul><li><strong>Word Space</strong> : Space inside the bounding boxes of words, lines, and paragraphs.</li><li><strong>Line Space</strong> : The space within the bounding box of the text on the page that makes up white space between the lines.</li><li><strong>Margin Space</strong> : Space that is outside of the text bounding box but within the boundaries of the document itself.</li><li><strong>Occlusion Space</strong> : Any overlay on the document whether permanent or impermanent that covers up the existing text or space.</li><li><strong>Canvas Space</strong> : Space that is created outside of the borders of the original text and can be infinitely expanded.</li></ul><p>It is important to note when discussing document spaces as part of a larger framework that they can be used alone or in concert with each other and that sometimes the lines blur between them. For example, the <strong>occlusion space</strong> bounding box of a paragraph includes the <strong>line spaces</strong> from that paragraph and the <strong>word spaces</strong> above each word. Despite this stacking, it would be possible, for example, to design annotations which are complementary at each level.</p><h2 id=defining-document-spaces>DEFINING DOCUMENT SPACES</h2><p><strong>Word Space</strong> is any portion of a document that has the printed word on it. The OCR engine we used, Tesseract, can identify and create bounding boxes around both printed and hand-written words, lines, and paragraphs. Depending on the specific implementation, word space could be considered any or all of these. In an analog book the actual printed type is an element that can be interacted with cognitively, but as we move into digital representations of that text it allows us to alter and query the text in interesting ways. The possibilities here are great, with one of the motivations being simply on-demand OCR. But when the text is digitized it opens up possibilities for text analysis, computational linguistics, and machine learning based on language. Marshall et al. reference both highlighting and annotation as a way that this space is interacted with on paper documents <sup id=fnref1:48><a href=#fn:48 class=footnote-ref role=doc-noteref>48</a></sup>.</p><p><strong>Line space</strong> is any space that exists between existing printed lines but remains inside the bounding box of the entire block of printed text on the page. Digitizing documents using our framework allows for on-demand opening and closing of these spaces. The manipulation of line space can create room for additional elements, such as ink annotations, inserted figures, and data visualizations that relate to the text. We synthesize the background of the document to avoid jarring the reader, and once this is done effectively any amount of space can be added to the document. Previous studies have found that this space is most often used for annotation, specifically in-line notes and for connectors such as arrows between words.</p><p><strong>Margin space</strong> is any area outside of the bounding box of the text but still within the bounds of the original document. This is the space commonly used for free-form note-taking. For example, when studying, editing, or conducting a close reading of a document <sup id=fnref4:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>. In addition to free-form note-taking, this space can be used for inserting new elements related to the text automatically, such as grammar trees, maps, or simply any placement of third-party or generated images.</p><p><strong>Occlusion space</strong> is a layer that covers the document. Additions in the occlusion space can obscure the text, so semi-transparent and impermanent elements are appropriate to maintain document legibility. In an analog setting, the occlusion space is often used with physical additions such as sticky notes. This space can be accessed in multiple ways, but the underlying function of the space tends to be information that is needed in the moment, but not on a continual basis. We demonstrate how to interact with this space by using tool tips that show the definitions of words on demand. Most of the techniques we demonstrate in this paper could be taken from one of the other spaces and placed into occlusion space, the question of permanence or impermanence will be a decision that rests with the designer.</p><p><strong>Canvas space</strong> is a concept that we could loosely attribute to the desk that a printed book is placed on or the space outside a page of text taped on a whiteboard. When we move to a digital representation of the book, the canvas space could be infinitely expandable allowing for the insertion of larger interactive elements. We have chosen to demonstrate how linguistic analysis could lead to automatic insertion of images, figures, and tables within the canvas space. It is important to note that external images could also be inserted in this space as Cheema et al. propose in AnnotateVis <sup id=fnref2:49><a href=#fn:49 class=footnote-ref role=doc-noteref>49</a></sup>.</p><h2 id=framework-architecture>Framework Architecture</h2><p>For this project, we imagined a tool that scholars of the humanities could use to do their required work on printed manuscripts, edited collections, and books, while still having access to digital affordances. Imagine the scenario where a literary scholar is in a rare book archive and cannot write directly on the document, or a scenario where a humanist is interested in the linguistic statistics of a text, but lacks the training to execute the digitization and processing of a text using code. The latter is one of the main problems that is ever present within the emerging field of digital humanities, the roadblock of technical knowledge needed to produce tools. We set out to build an extensible framework that would allow a humanities scholar with limited technical knowledge the ability to process, augment, and export digital versions of analog texts. To achieve this, we bring together multiple technologies including OCR, machine translation, and information visualization. By enumerating the set of spaces that can be used for these techniques and demonstrating their possibilities with examples, we hope to inspire users to add their own document augmentations to our existing framework.</p><figure><img loading=lazy alt="Annotated digitized image of a document." src=/dhqwords/vol/13/3/000426/resources/images/figure03.jpeg sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/13/3/000426/resources/images/figure03_hu5f52d5900ff4c468bff6e822793d1197_74378_500x0_resize_q75_box.jpeg 500w,
/dhqwords/vol/13/3/000426/resources/images/figure03_hu5f52d5900ff4c468bff6e822793d1197_74378_800x0_resize_q75_box.jpeg 800w,/dhqwords/vol/13/3/000426/resources/images/figure03.jpeg 720w" class=landscape><figcaption><p>The space insertion algorithm uses Hough Transform to identify the x-coordinates of vertical lines in document backgrounds [^duda1972]. <code>pixel cut height</code> and <code>pixel cut width</code> measure the height and width in pixels of the background crop used for background synthesis. These parameters can be adjusted for different documents because of localized color and lighting effects.</p></figcaption></figure><p>The Textension framework starts with a document image, processes the image to discern both the content as well as the use of space on the page, adds space to the page as needed, and creates augmentations, both static and interactive, to insert into the newly digital object. The resulting processed image is presented to the user for further exploration, annotation, and interaction. The framework architecture is illustrated in <a href=#figure02>Figure 2</a>.</p><h2 id=textension-prototype>Textension Prototype</h2><p>We provide a specific implementation of Textension in a web-based system which offers a selection of document augmentations and interactive tools, which we will describe in this section.</p><h2 id=image-capture-and-processing>Image Capture and Processing</h2><p>When a user comes to the opening screen of Textension they are presented with two input options. They can either use the camera that is built into their device (webcam, phone camera, front facing tablet camera) and take a snapshot of the document they wish to process, or they can upload an image file that has been previously prepared. Document images can be single or multiple pages and are uploaded with a drag and drop interface. The next stage of document processing begins immediately after the upload completes. The system uses image processing from the Python Image Library and image manipulation from OpenCV. We have found that a combination of binarization, grey-scaling, and image sharpening have had a noticeable effect on the results of the OCR, which is the next stage of processing.</p><h2 id=ocr-engine>OCR engine</h2><p>We used the open source Tesseract OCR engine in the Textension prototype. Smith provides an overview and a history of the development of the engine <sup id=fnref:50><a href=#fn:50 class=footnote-ref role=doc-noteref>50</a></sup> <sup id=fnref:51><a href=#fn:51 class=footnote-ref role=doc-noteref>51</a></sup>, and Patel et al. provide a case study approach for its use <sup id=fnref:52><a href=#fn:52 class=footnote-ref role=doc-noteref>52</a></sup>. Tesseract can be trained with many different languages and also with handwriting, making it a robust choice for an implementation such as this.</p><p>While great effort is being taken by many companies to digitize the world’s books, this process is expensive, hardware dependent, and time-consuming. The Tesseract OCR engine provides high-quality open source OCR in a local setting for printed and hand-written text. Often when working between analog and digital platforms scholars are forced to type passages out, for example, to extract a quote from a book for insertion into a manuscript. This is due to the fact that many digital book readers do not give you access to text that can be copy and pasted for copyright reasons, or in some cases, only images of paper documents are provided. Our domain expert has been using Textension to quickly digitize small portions of text for inclusion into working documents. This application of Textension allows for easy transfer of quotable information from analog books to digital platforms using only the OCR functionality. In addition, the OCR engine provides the content in a machine-readable form for later linguistic processing, linking, and other augmentations. Tesseract also provides bounding boxes for each word, which we use to identify document spaces.</p><h2 id=background-synthesis>Background Synthesis</h2><p>In order to augment documents with helpful annotations, or to provide space for users to make pen-based annotations, document spaces often need to be enlarged. This is not possible when working with an analog document. However, in the digital version, we can manipulate the image to provide the needed space. For example, to place a translation of text between lines, the inter-line spacing first needs to be increased. Document backgrounds can be complicated, with changing lighting conditions almost guaranteed using mobile phone and tablet cameras. To retain the original look of the document image, we created a method for inserting space by synthesizing sections of the document background which seamlessly integrate with the original. These regions can be optionally clearly noted, for example by using a different color. This may be preferable in situations where differentiating the original document from manipulations is important, for example in archival and preservation work. Background pre-processing is a computationally intensive process, so an option for low or high-resolution processing is included. Low-resolution processing is suitable for quick interactive applications, where high-resolution processing is more suitable for printing and saving the results of an analysis session.</p><figure><img loading=lazy alt="Comparison of four digitized documents." src=/dhqwords/vol/13/3/000426/resources/images/figure04.jpeg sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/13/3/000426/resources/images/figure04_hu0bcc2a0611b630520a0331f4f991649b_159578_500x0_resize_q75_box.jpeg 500w,
/dhqwords/vol/13/3/000426/resources/images/figure04_hu0bcc2a0611b630520a0331f4f991649b_159578_800x0_resize_q75_box.jpeg 800w,/dhqwords/vol/13/3/000426/resources/images/figure04.jpeg 916w" class=landscape><figcaption><p>A. Original document image. B. A single line space inserted. C. Horizontal word space and vertical line space inserted into the document. D. The whole page with all line spaces opened.</p></figcaption></figure><p>To improve image capture quality, which can affect background synthesis, we provide the user with a frame to set their image in. While it is possible to adjust skew correction and automatically crop text from images, we found from our internal testing that forcing the user to frame the image themselves resulted in much better OCR and therefore a much better experience. There is precedence for this type of interaction in commercial settings such as remote cheque deposits for online banking, where a user is forced to frame and focus the cheque before the system will accept the image. Once we have the image we use the bounding boxes provided by the OCR engine to rebuild the document in image fragments within the web platform. Each space and word is modeled separately to allow us to manipulate those elements within the browser.</p><p>The important image regions for the space insertion algorithm are illustrated in <a href=#figure03>Figure 3</a>. To expand the space between the lines (expand <strong>line space</strong> ) in high-resolution mode we first use Hough line detection to identify x-values where vertical lines exist on the page <sup id=fnref:53><a href=#fn:53 class=footnote-ref role=doc-noteref>53</a></sup>. The Hough transform is a feature extraction technique to find instances of objects within a certain class of shapes (vertical lines in our application) by a voting procedure. This allows us to maintain the edges of pages and also to recreate lighting conditions near the bound edges as we expand space within the document.</p><p>The algorithm then copies a slice of the image from between each individual line from one edge of the page to the other. The height of the slice (the <code>pixel cut height</code> ) is set to the height of the unimpeded space between the bounding boxes of the lines of text above and below. For the low-resolution processing, copies of this slice are inserted vertically to create space in the document. Depending on the complexity of the background this process is sometimes adequate. However, in most cases, this results in image streaking, which is usable for testing and exploration, but can be distracting and is not sufficient for photo realistic background additions.</p><p>As local lighting and color effects are so prevalent in scanned and photographed documents, especially historical documents, we wanted to model the backgrounds from as local a position as possible. The intuition behind our approach is that we can randomly reorder pixels in a local region to reduce streaking while retaining local lighting. From the extracted slice of the document, we select a patch of the original image that has the dimensions of <code>pixel cut height</code> by <code>pixel cut width</code> . To insert a new patch below it we simply randomize the pixels from the current patch and insert it. As we scan across the line we continue this in increments of <code>pixel cut width</code> until the line is complete. The one exception is when the patch location falls within a definable threshold of the x-values of the vertical lines found by the Hough line algorithm at the start of the process. In this case, the pixels in that patch are not randomized but rather copied, to retain the sharpness of the detected edge. This allows us to sample local lighting effects in the background of the image. For adding horizontal space between words a similar process is used. When space is added only between two words on a single line, we also add the same amount of space to all lines by distributing it across all inter-word spaces. This preserves the original justification of the document (see <a href=#figure04>Figure 4</a>).</p><p>The <code>pixel cut height</code> and <code>pixel cut width</code> parameters control the locality of the modeling. Reasonable defaults are provided, but they can be varied in the settings screen to obtain the best result.</p><p>We have also found that artifacts on the page disrupt the color balance within each individual cut and can affect the quality of the background rendering. We offer the option of removing artifacts on the input screen. To remove blemishes on the page, the cropped line is binarized, any pixels that turn black in the binarization process are not used within the color randomization that synthesizes the background color. This leaves the original artifact but does not propagate it during the space insertion process.</p><p>Background synthesis and artifact removal are pre-processed across all candidate regions of the document to a pre-set threshold of inserted space. The synthesized image data is stored for quick access and insertion during document augmentation and interaction.</p><figure><img loading=lazy alt="Screenshot of tool interface, with menu on the left." src=/dhqwords/vol/13/3/000426/resources/images/figure05.jpeg sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/13/3/000426/resources/images/figure05_huc28c140fb416574f0eb7d2b37dbe430f_225277_500x0_resize_q75_box.jpeg 500w,
/dhqwords/vol/13/3/000426/resources/images/figure05_huc28c140fb416574f0eb7d2b37dbe430f_225277_800x0_resize_q75_box.jpeg 800w,/dhqwords/vol/13/3/000426/resources/images/figure05.jpeg 815w" class=portrait><figcaption><p>The interactive interface of Textension. On the left is the menu tray with buttons that toggle on/off states for each augmentation. OCR confidence is in the word space, and a definition tool-tip is shown in the occlusion space.</p></figcaption></figure><h2 id=layout-engine>LAYOUT ENGINE</h2><p>After creating an interactive, expandable document from the captured image, augmentations can be added to provide supportive features as required for the specific task and context. For example, a learner may require word definitions, while a literary scholar may be interested in the contemporary use of the words in the document. Augmentations can take the form of inserted glyphs, images, overlays, and annotations in the document spaces, or they may replace or change the words in the document. Augmentations can be temporary or permanent, as appropriate for their purpose and the document space in which they appear. The insertion and placement of augmentations and the provision of interactivity on the document and its augmentations is provided by the layout engine (see <a href=#figure05>Figure 5</a>).</p><p>The images after upload are broken into individual word and space objects that are then recompiled in order onto an HTML canvas to reproduce the original image with the added flexibility of moving, inserting, and changing elements. Augmentations are placed on the canvas as a layer on top of the image objects. Textension has been developed to support the creation of new augmentations, which can draw on custom data processing, local datasets, or public APIs and data. Textension was built using flask, a python server back-end; bootstrap, for UI elements; jinja, a template engine for python; and jquery, for data handling.</p><h2 id=document-augmentations>DOCUMENT AUGMENTATIONS</h2><p>What we present in this section are a series of concrete implementations of document augmentations that demonstrate a subset of the possibilities of the Textension framework. The selected examples highlight the possible breadth available when considering the five document spaces and the possibilities for interaction with those spaces. We explore insertion augmentations, as well as temporary and permanent overlays. The availability of the plain text allows easy integration of natural language processing, and the fact that the digital document is built in pieces allows for easy insertion of space to accommodate for the adding of new features. In this way, we envision Textension as both a sandbox for designing interactive elements for digital documents and a way to use both digital and analog affordances simultaneously when working with texts.</p><figure><img loading=lazy alt="Demonstration of stylus tool on digitized document." src=/dhqwords/vol/13/3/000426/resources/images/figure06.jpeg sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/13/3/000426/resources/images/figure06_hu96117d095220cc4264486bb1e354b5c7_41436_500x0_resize_q75_box.jpeg 500w,
/dhqwords/vol/13/3/000426/resources/images/figure06_hu96117d095220cc4264486bb1e354b5c7_41436_800x0_resize_q75_box.jpeg 800w,/dhqwords/vol/13/3/000426/resources/images/figure06.jpeg 821w" class=landscape><figcaption><p>A demonstration of writing with a stylus in newly created line space in the document.</p></figcaption></figure><h2 id=insertion-of-space>Insertion of Space</h2><p>To allow for interaction with books off the shelf we wanted to cross over between digital and analog affordances. The ability to write notes directly on the pages of a book is one of the analog affordances that is constantly used, much to the dismay of librarians throughout the world. As seen in studies by Marshall <sup id=fnref2:48><a href=#fn:48 class=footnote-ref role=doc-noteref>48</a></sup> and Mehta <sup id=fnref5:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>, there are workflows that depend on this type of interaction. Due to space constraints on the printed page, these annotations often take the form of writing in the margins of the text or squeezing into the spaces between the printed lines. For the digital version of the text, we wanted to enhance the ability to write on the page in both the <strong>line space</strong> and the <strong>margin space</strong> . The pre-processed background is used to insert space within the text, to allow for writing notes or inserting elements such as maps.</p><p>There are two ways to insert space in Textension. The first is to simply tap and hold on a line with the stylus, or click with a mouse and that line will open up allowing space for writing. This works both vertically and horizontally; the trigger for vertical line creation is in the space between lines and for the horizontal space insertion it is in the space between words (see <a href=#figure04>Figure 4</a>). The size of the space to be added is determined in the parameter settings for the tool. By default, we have set the opening increment at 20 pixels. Inserted space is both editable as a text box by clicking on it, or by toggling the draw mode space can be written in any way the user chooses (see <a href=#figure06>Figure 6</a>). The second method is to open all spaces of a given type (e.g. line spaces) through a menu function. Spaces can also be inserted by other augmentations which require space. For example, inserting space is a precursor to inserting translations between lines. Together, these methods allow for flexible interaction that can be used for editing and annotation.</p><h2 id=drawing-and-typing-on-the-document>Drawing and Typing on the Document</h2><p>Once space has been opened up we wanted to maintain the ability to type and write on the document. Both functions have a toggle and a color picker that allows for this type of interactivity. You can draw anywhere on the document, but typing has been constrained to text boxes that have been created in the new space of the document (see <a href=#figure06>Figure 6</a>).</p><figure><img loading=lazy alt="Digitized illustrated document." src=/dhqwords/vol/13/3/000426/resources/images/figure07.jpeg sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/13/3/000426/resources/images/figure07_hu46237a1a2ae027dfca73f4f2a5eccf3e_59506_500x0_resize_q75_box.jpeg 500w,
/dhqwords/vol/13/3/000426/resources/images/figure07_hu46237a1a2ae027dfca73f4f2a5eccf3e_59506_800x0_resize_q75_box.jpeg 800w,/dhqwords/vol/13/3/000426/resources/images/figure07.jpeg 431w" class=portrait><figcaption><p>Display of recognized OCR text inserted into created line space on the first page of an illustrated copy of Alice in Wonderland.</p></figcaption></figure><h2 id=ocr-confidence>OCR Confidence</h2><p>Often when digitizing analog texts OCR confidence is very important. Textension provides a feature where users can see an overlay of how uncertain the OCR algorithm was for each word. This augmentation is displayed as an overlay in the <strong>word space</strong> . The darker the color the less confident the score. This mapping was designed to draw attention to and “obscure” those words that the system had difficulty recognizing. With the current trend in machine learning and the relative anxiety that is brought with black box algorithms, showing the inner workings of the OCR engine is a way to both help the user understand how the system is working but also where exactly work may need to be done to make for a better user experience and digital document (see <a href=#figure05>Figure 5</a>).</p><figure><img loading=lazy alt="Digitized page from English-language manuscript with inserted French translations." src=/dhqwords/vol/13/3/000426/resources/images/figure08.jpeg sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/13/3/000426/resources/images/figure08_hucefe6afa3385e941e6427c2985e4f959_63098_500x0_resize_q75_box.jpeg 500w,
/dhqwords/vol/13/3/000426/resources/images/figure08_hucefe6afa3385e941e6427c2985e4f959_63098_800x0_resize_q75_box.jpeg 800w,/dhqwords/vol/13/3/000426/resources/images/figure08.jpeg 537w" class=portrait><figcaption><p>Automatic insertion of line space and translations of the text.</p></figcaption></figure><h2 id=ocr-text>OCR Text</h2><p>While the OCR confidence overlay can allow users to understand how well their scan has been processed, we added another feature that works in concert with the previous ones to place text within the <strong>line space</strong> . The OCR text feature will spread all the lines in the document and insert the OCR text into editable text boxes (see <a href=#figure07>Figure 7</a>). The user can then correct the OCR directly on the newly built document and export the finished text into a text file. Our own project’s domain expert is already using this feature to solve the problem of not being able to cut and paste from digital books while writing Humanities essays. He has been taking pictures of quotations from physical copies of the books and exporting the OCR directly into a word processor.</p><h2 id=translation>Translation</h2><p>Once the user has tuned the OCR to their liking, they can toggle the auto-translate menu button which will then use the Google translate API and automatically insert a translation of the text in the <strong>line space</strong> (see <a href=#figure08>Figure 8</a>). This method works for all of the languages currently supported by Google and its one limitation is typographical, in that books often split words on the end of lines. Future work will address this limitation.</p><figure><img loading=lazy alt="Screenshot of Textension tool in use." src=/dhqwords/vol/13/3/000426/resources/images/figure09.jpeg sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/13/3/000426/resources/images/figure09_hu1b7a73da23163211346ae4bb92472632_275162_500x0_resize_q75_box.jpeg 500w,
/dhqwords/vol/13/3/000426/resources/images/figure09_hu1b7a73da23163211346ae4bb92472632_275162_800x0_resize_q75_box.jpeg 800w,/dhqwords/vol/13/3/000426/resources/images/figure09_hu1b7a73da23163211346ae4bb92472632_275162_1200x0_resize_q75_box.jpeg 1200w,/dhqwords/vol/13/3/000426/resources/images/figure09.jpeg 1234w" class=landscape><figcaption><p>Sparklines showing lexical usage from the Google books corpus from 1800–2012. Words that the OCR did not recognize do not have a sparkline.</p></figcaption></figure><h2 id=manual-removal-and-replacement-of-words>Manual Removal and Replacement of Words</h2><p>The manipulation of the <strong>word space</strong> on the level of the text is an option that could be used in many scenarios. This widget provides the ability to select an individual word, erase it from the document image, and substitute in a word provided by the user. New words are scaled to fit into the space of the existing word and are highlighted to show that they were additions. Possible scenarios for this type of inclusion could be manual translation, gender pronoun switching or switching between the Latin and common names of scientific organisms.</p><h2 id=location-based-maps>Location-Based Maps</h2><p>With named entity recognition, we demonstrate the power of digital affordances with photographed texts by inserting maps. During pre-processing, we detect and store place names within the text. When the map feature is toggled Textension highlights the place name in the document, and automatically inserts a map from the Google maps API directly into the document in the <strong>margin space</strong> . This feature is a demonstration of the power of combining existing technology, such as the Google maps API with automatic document space expansion. Because the document is built in pieces we can freely move interactive elements into different document spaces to see which works best for the specific implementation.</p><h2 id=sparklines>Sparklines</h2><p><strong>Word space</strong> visualizations have been showing promise as ways to augment digital texts <sup id=fnref4:40><a href=#fn:40 class=footnote-ref role=doc-noteref>40</a></sup> Textension offers the ability to automatically insert these visualizations into images of analog texts. We have chosen to implement lexical usage sparklines <sup id=fnref1:38><a href=#fn:38 class=footnote-ref role=doc-noteref>38</a></sup> directly above each word showing the usage within the Google Books corpus from 1800–2012 (see <a href=#figure09>Figure 9</a>). This technique could be used for many different types of visualizations limited only by the power of the OCR and NLP techniques available.</p><figure><img loading=lazy alt="Document image with generated context map." src=/dhqwords/vol/13/3/000426/resources/images/figure10.jpeg sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/13/3/000426/resources/images/figure10_hu906b26bbc9aba7fc91addcd8b9a2dcbe_367983_500x0_resize_q75_box.jpeg 500w,
/dhqwords/vol/13/3/000426/resources/images/figure10_hu906b26bbc9aba7fc91addcd8b9a2dcbe_367983_800x0_resize_q75_box.jpeg 800w,/dhqwords/vol/13/3/000426/resources/images/figure10_hu906b26bbc9aba7fc91addcd8b9a2dcbe_367983_1200x0_resize_q75_box.jpeg 1200w,/dhqwords/vol/13/3/000426/resources/images/figure10_hu906b26bbc9aba7fc91addcd8b9a2dcbe_367983_1500x0_resize_q75_box.jpeg 1500w,/dhqwords/vol/13/3/000426/resources/images/figure10_hu906b26bbc9aba7fc91addcd8b9a2dcbe_367983_1800x0_resize_q75_box.jpeg 1800w,/dhqwords/vol/13/3/000426/resources/images/figure10.jpeg 1884w" class=landscape><figcaption><p>Context maps generated within the canvas space around a document image.</p></figcaption></figure><h2 id=context-maps>Context Maps</h2><p>A context map lists all of the ways that a particular word or phrase has been used within a document. This digital affordance uses <strong>canvas space</strong> to build interactive concordance lines that highlight the four words before and after the word in question. The maps are built using the images patches of words in the document to maintain the document aesthetics and reduce the impact of OCR errors. This is an example of the types of things that can be done with ready access to linguistic information and expandable canvas space (see <a href=#figure10>Figure 10</a>).</p><h2 id=lexical-uniqueness-glyphs>Lexical Uniqueness Glyphs</h2><p>The second <strong>word space</strong> visualization we implemented was showing the uniqueness of each word within the language (see <a href=#figure11>Figure 11</a>). When this mode is toggled active the user is given two time sliders, one for the upper and one for the lower bounds of the time in question and small bar charts are automatically inserted for each word in the document. Each chart is a relative representation of the word’s uniqueness within the given document, meaning that as the upper and lower bounds time sliders are adjusted, each glyph will adjust relative to the other. This type of interaction could be adjusted to address specific historical questions from literary scholars and could be extended to display anything that has data relating to the text. Possible scenarios for this include showing etymological information, usage information, or using color as a visual variable to display languages of origin.</p><h2 id=word-definitions>Word Definitions</h2><p>To demonstrate the possibilities of the <strong>occlusion space</strong> we have implemented a widget that allows the user to hover on a word within the newly digitized document and get dictionary information scraped from Webster’s Online Dictionary API <sup id=fnref:54><a href=#fn:54 class=footnote-ref role=doc-noteref>54</a></sup>. This space will often be used for impermanent information that the user will need once, such as a definition, and then can disappear (see <a href=#figure05>Figure 5</a>).</p><figure><img loading=lazy alt="Screenshot of Textension word glyph tool." src=/dhqwords/vol/13/3/000426/resources/images/figure11.jpeg sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/13/3/000426/resources/images/figure11_hu9c492b1425269708826c8b672c9f7db3_130611_500x0_resize_q75_box.jpeg 500w,
/dhqwords/vol/13/3/000426/resources/images/figure11_hu9c492b1425269708826c8b672c9f7db3_130611_800x0_resize_q75_box.jpeg 800w,/dhqwords/vol/13/3/000426/resources/images/figure11.jpeg 820w" class=portrait><figcaption><p>Word-level visualization of the uniqueness of a given word within the English language. More bars on the glyph indicate the word is more unique. Missing glyphs are the result of OCR failures.</p></figcaption></figure><h2 id=print-save-and-download>Print, Save, and Download</h2><p>As the features are toggled on and off within the tool, the user is given the option to save both editable text as an external text document but also high-resolution images of the current state within the program. The user has the option to export whatever document state that they create. Many features of Textension can be used at the same time so it is possible to create and export multiple variations of a single document.</p><h2 id=conclusion>CONCLUSION</h2><p>The tension that exists between our analog pasts and our digital present can be addressed using our framework. Our prototype, Textension, leverages the power of OCR and digitally manipulates the five document spaces in near real-time. The system we present is an implementation of previous studies brought together in a way that can be extended easily for domain-specific analysis tasks. The web-based platform allows for easy integration with mobile technology and makes it possible to use Textension in a variety of locations and scenarios. We have demonstrated a breadth of possible use cases and have chosen widgets that demonstrate the utility of each of the document spaces.</p><h2 id=discussion>Discussion</h2><p>Textension can be used in situations where quick digitization is necessary and digital versions may not be available, such as within the stacks of a university library. The web-based framework allows for easy document digitization using any web-enabled camera. This could be on a mobile phone or a desktop computer with a webcam. The system also allows for the uploading of previous digitized texts. The drag and drop interface allows for the uploading of PDF’s and digital images providing a robustness of input possibilities. The provided text augmentations support humanities activities such as annotation and close reading. By bringing in linked reference resources such as maps and lexical uniqueness scores, Textension can situate an unknown text in the greater spatial and linguistic context, assisting with tasks associated with “distant reading” <sup id=fnref:55><a href=#fn:55 class=footnote-ref role=doc-noteref>55</a></sup> .</p><h2 id=future-work>Future Work</h2><p>While we designed Textension to demonstrate the usefulness and power of bringing together affordances from paper and digital documents, there are still several ways that we can expand the system. The first is by using larger canvases. Textension focuses on space within documents and provides a limited extended canvas space. An infinite canvas workspace, such as the zoomable interface of PAD++ <sup id=fnref:56><a href=#fn:56 class=footnote-ref role=doc-noteref>56</a></sup>, could allow for insertion of more than one document image in the same workspace, and the addition of larger and more sophisticated interactive visualizations.</p><p>The second addition that we envision is to apply these techniques in the opposite direction, namely to augment digital books with the same types of interactive elements. We have already seen some of these approaches within existing e-readers like Amazon’s Kindle, but there is a lot of room to experiment with that design space. Another welcome addition would be horizontal space organization to solve problems like line breaks when using the Google Translate API. Because the OCR uses lines as an organizing principle, hyphenated words often disrupt the OCR and the translation algorithm. Reconnecting hyphenated words would require reflowing the document to maintain a justified layout.</p><p>A final addition that would solve a problem in the digital humanities is to provide a way to easily create new augmentations for Textension so that users with limited programming abilities would be able to add features to the interface. The current implementation makes it easy to add new features with modest programming skills, but we would like to make that more accessible in the future.</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>Hrim Mehta, Adam James Bradley, Mark Hancock, and Christopher Collins. 2017. _Metatation: Annotation as Implicit Interaction to Bridge Close and Distant Reading. _  in (TOCHI) ACM Transactions on Computer-Human Interaction. Volume 24 Issue 5, November 2017, Article No. 35.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref2:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref3:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref4:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref5:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>EEBO - Early English Books Online. <a href=http://eebo.chadwyck.com/home>http://eebo.chadwyck.com/home.</a> (2003). Accessed: 2017-09-18.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>Google Books. <a href=https://books.google.ca/>https://books.google.ca/</a>. (2017). Accessed: 2017-09-18.&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p>Bay-Wei Chang, Jock D. Mackinlay, Polle T. Zellweger, and Takeo Igarashi. 1998. A negotiation architecture for fluid documents. In <em>Proceedings of the 11th annual ACM symposium on User interface software and technology (UIST &lsquo;98)</em> . ACM, New York, NY, USA, 123-132. DOI: <a href=http://dx.doi.org/10.1145/288392.288585>http://dx.doi.org/10.1145/288392.288585</a>&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5><p>Polle T. Zellweger, Susan Harkness Regli, Jock D. Mackinlay, and Bay-Wei Chang. 2000. The impact of fluid documents on reading and browsing: an observational study. In <em>Proceedings of the SIGCHI conference on Human Factors in Computing Systems (CHI &lsquo;00)</em> . ACM, New York, NY, USA,249-256. DOI=http://dx.doi.org/10.1145/332040.332440&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:6><p>Olha Bondarenko and Ruud Janssen. 2005. Documents at Hand: Learning from Paper to Improve Digital Technologies. In <em>Proc. SIGCHI Conf. on Human Factors in Computing Systems (CHI)</em> . ACM, 121–130. DOI: <a href=http://dx.doi.org/10.1145/1054972.1054990>http://dx.doi.org/10.1145/1054972.1054990</a>&#160;<a href=#fnref:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:7><p>Matthew Hong, Anne Marie Piper, Nadir Weibel, Simon Olberding, and James Hollan. 2012. Microanalysis of active reading behavior to inform design of interactive desktop workspaces. In <em>Proc. ACM Conf. on Interactive Tabletops and Surfaces (ITS)</em> . 215–224. DOI: <a href=http://dx.doi.org/10.1145/2396636.2396670>http://dx.doi.org/10.1145/2396636.2396670</a>&#160;<a href=#fnref:7 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:8><p>Bill N. Schilit, Gene Golovchinsky, and Morgan N. Price. 1998. Beyond Paper: Supporting Active Reading with Free Form Digital Ink Annotations. In <em>Proc. SIGCHI Conf. on Human Factors in Computing Systems (CHI)</em> . ACM, 249–256. DOI: <a href=http://dx.doi.org/10.1145/274644.274680>http://dx.doi.org/10.1145/274644.274680</a>&#160;<a href=#fnref:8 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:9><p>Nadir Weibel, Adam Fouse, Colleen Emmenegger, Whitney Friedman, Edwin Hutchins, and James Hollan. 2012. Digital Pen and Paper Practices in Observational Research. In <em>Proc. SIGCHI Conf. on Human Factors in Computing Systems (CHI)</em> . ACM, 1331–1340. DOI: <a href=http://dx.doi.org/10.1145/2207676.2208590>http://dx.doi.org/10.1145/2207676.2208590</a>&#160;<a href=#fnref:9 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:10><p>Dongwook Yoon, Nicholas Chen, François Guimbretière, and Abigail Sellen. 2014. RichReview: Blending Ink, Speech, and Gesture to Support Collaborative Document Review. In <em>Proc. ACM Symp. on User Interface Software and Technology (UIST)</em> . ACM, 481–490. DOI: <a href=http://dx.doi.org/10.1145/2642918.2647390>http://dx.doi.org/10.1145/2642918.2647390</a>&#160;<a href=#fnref:10 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:11><p>Chunyuan Liao, François Guimbretière, Ken Hinckley, and Jim Hollan. 2008. Papiercraft: A Gesture-based Command System for Interactive Paper. <em>ACM Trans. Comput.-Hum. Interact.</em> 14, 4, Article 18 (Jan. 2008), 27 pages. DOI: <a href=http://dx.doi.org/10.1145/1314683.1314686>http://dx.doi.org/10.1145/1314683.1314686</a>&#160;<a href=#fnref:11 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:12><p>Chunyuan Liao, Qiong Liu, Bee Liew, and Lynn Wilcox. 2010. Pacer: Fine-grained Interactive Paper via Camera-touch Hybrid Gestures on a Cell Phone. In <em>Proc. SIGCHI Conf. on Human Factors in Computing Systems (CHI)</em> . ACM, 2441–2450. DOI: <a href=http://dx.doi.org/10.1145/1753326.1753696>http://dx.doi.org/10.1145/1753326.1753696</a>&#160;<a href=#fnref:12 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:13><p>Theophanis Tsandilas and Wendy E. Mackay. 2010. Knotty Gestures: Subtle Traces to Support Interactive Use of Paper. In <em>Proc. Int. Conf. on Advanced Visual Interfaces (AVI)</em> . ACM, 147–154. DOI: <a href=http://dx.doi.org/10.1145/1842993.1843020>http://dx.doi.org/10.1145/1842993.1843020</a>&#160;<a href=#fnref:13 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:14><p>François Guimbretière. 2003. Paper Augmented Digital Documents. In <em>Proc. ACM Symp. on User Interface Software and Technology (UIST)</em> . ACM, 51–60. DOI: <a href=http://dx.doi.org/10.1145/964696.964702>http://dx.doi.org/10.1145/964696.964702</a>&#160;<a href=#fnref:14 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:15><p>Michael Haller, Jakob Leitner, Thomas Seifried, James R. Wallace, Stacey D. Scott, Christoph Richter, Peter Brandl, Adam Gokcezade, and Seth Hunter. 2010. The NiCE Discussion Room: integrating Paper and Digital Media to Support Co-Located Group Meetings. In <em>Proc. SIGCHI Conf. on Human Factors in Computing Systems (CHI)</em> . ACM, 609–618. DOI: <a href=http://dx.doi.org/10.1145/1753326.1753418>http://dx.doi.org/10.1145/1753326.1753418</a>&#160;<a href=#fnref:15 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:16><p>Ken Hinckley, Xiaojun Bi, Michel Pahud, and Bill Buxton. 2012. Informal Information Gathering Techniques for Active Reading. In <em>Proc. SIGCHI Conf. on Human Factors in Computing Systems (CHI)</em> . ACM, 1893–1896. DOI: <a href=http://dx.doi.org/10.1145/2207676.2208327>http://dx.doi.org/10.1145/2207676.2208327</a>&#160;<a href=#fnref:16 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:17><p>David Holman, Roel Vertegaal, Mark Altosaar, Nikolaus Troje, and Derek Johns. 2005. Paper Windows: Interaction Techniques for Digital Paper. In <em>Proc. SIGCHI Conf. on Human Factors in Computing Systems (CHI)</em> . ACM, 591–599. DOI: <a href=http://dx.doi.org/10.1145/1054972.1055054>http://dx.doi.org/10.1145/1054972.1055054</a>&#160;<a href=#fnref:17 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:18><p>Dongwook Yoon, Nicholas Chen, and Francois Guimbretière. 2013. TextTearing: opening white space for digital ink annotation. In Proceedings of the 26th annual ACM symposium on User interface software and technology (UIST &lsquo;13). ACM, New York, NY, USA,107-112. DOI: <a href=http://dx.doi.org/10.1145/2501988.2502036>http://dx.doi.org/10.1145/2501988.2502036</a>&#160;<a href=#fnref:18 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:19><p>João Marcelo M. da Silva, Rafael Dueire Lins, and Valdemar Cardoso da Rocha. 2006. Binarizing and Filtering Historical Documents with Back-to-front Interference. In <em>Proc. ACM Symp. on Applied Computing (SAC ’06)</em> . ACM, 853–858. DOI: <a href=http://dx.doi.org/10.1145/1141277.1141471>http://dx.doi.org/10.1145/1141277.1141471</a>&#160;<a href=#fnref:19 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:20><p>Maya R. Gupta, Nathaniel P. Jacobson, and Eric K. Garcia. 2007. OCR binarization and image pre-processing for searching historical documents. <em>Pattern Recognition</em> 40, 2 (2007), 389–397. DOI: <a href=http://dx.doi.org/10.1016/j.patcog.2006.04.043>http://dx.doi.org/10.1016/j.patcog.2006.04.043</a>&#160;<a href=#fnref:20 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:21><p>Gabriel de França Pereira da Silva, Rafael Dueire Lins, and André Ricardson Silva. 2013. A new algorithm for background removal of document images acquired using portable digital cameras. In <em>Proc. Int. Conf. on Image Analysis and Recognition (ICIAR)</em> . Springer, 290–298. DOI:<a href=http://dx.doi.org/10.1007/978-3-642-39094-4_33>http://dx.doi.org/10.1007/978-3-642-39094-4_33</a>&#160;<a href=#fnref:21 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:22><p>D. Doermann, Jian Liang, and Huiping Li. 2003. Progress in camera-based document image analysis. In <em>Proc. Int. Conf. on Document Analysis and Recognition (ICDAR)</em> . 606–616. DOI: <a href=http://dx.doi.org/10.1109/ICDAR.2003.1227735>http://dx.doi.org/10.1109/ICDAR.2003.1227735</a>&#160;<a href=#fnref:22 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:23><p>Kuo Chin Fan, Yuan Kai Wang, and Tsann Ran Lay. 2001. Marginal noise removal of document images. In <em>Proc. Int. Conf. on Document Analysis and Recognition (ICDAR)</em> . 317–321. DOI: <a href=http://dx.doi.org/10.1109/ICDAR.2001.953806>http://dx.doi.org/10.1109/ICDAR.2001.953806</a>&#160;<a href=#fnref:23 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:24><p>Jian Liang, David Doermann, and Huiping Li. 2005. Camera-based Analysis of Text and Documents: A Survey. <em>Int. J. Doc. Anal. Recognit.</em> 7, 2-3 (July 2005), 84–104. DOI: <a href=http://dx.doi.org/10.1007/s10032-004-0138-z>http://dx.doi.org/10.1007/s10032-004-0138-z</a>&#160;<a href=#fnref:24 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:25><p>WordLens. Software application. (2010). <a href=https://questvisual.com/>https://questvisual.com/</a> Accessed: 2017-09-08.&#160;<a href=#fnref:25 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:26><p>L Jagannathan and CV Jawahar. 2005. Perspective correction methods for camera based document analysis. In <em>Proc. Int. Workshop on Camera-based Document Analysis and Recognition</em> . 148–154.&#160;<a href=#fnref:26 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:27><p>R. Lins, G. P. e. Silva, and A. R. Gomes e Silva. 2007. Assessing and Improving the Quality of Document Images Acquired with Portable Digital Cameras. In <em>Proc. Int. Conf. on Document Analysis and Recognition (ICDAR)</em> , Vol. 2. 569–573. DOI: <a href=http://dx.doi.org/10.1109/ICDAR.2007.4376979>http://dx.doi.org/10.1109/ICDAR.2007.4376979</a>&#160;<a href=#fnref:27 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:28><p>Shijian Lu and Chew Lim Tan. 2006. The Restoration of Camera Documents Through Image Segmentation. In <em>Proc. Int. Workshop on Document Analysis Systems (DAS)</em> . Springer, 484–495. DOI: <a href=http://dx.doi.org/10.1007/11669487_43>http://dx.doi.org/10.1007/11669487_43</a>&#160;<a href=#fnref:28 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:29><p>Gabriel Pereira and Rafael Lins. 2007. PhotoDoc: A Toolbox for Processing Document Images Acquired Using Portable Digital Cameras. In <em>Camera Based Document Analysis and Recognition</em> . 107–115.&#160;<a href=#fnref:29 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:30><p>Susan Havre, Elizabeth Hetzler, Paul Whitney, and Lucy Nowell. 2002. ThemeRiver: Visualizing Thematic Changes in Large Document Collections. <em>IEEE Trans. on Visualization and Computer Graphics</em> 8, 1 (Jan. 2002), 9–20. DOI:<a href=http://dx.doi.org/10.1109/2945.981848>http://dx.doi.org/10.1109/2945.981848</a>&#160;<a href=#fnref:30 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:30 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:31><p>W. Bradford Paley. 2002. TextArc: Showing word frequency and distribution in text. In: Posters of IEEE Conf. on Information Visualization. (2002).&#160;<a href=#fnref:31 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:32><p>Bongshin Lee, Nathalie Henry Riche, Amy K. Karlson, and Sheelash Carpendale. 2010. SparkClouds: Visualizing Trends in Tag Clouds. <em>IEEE Trans. on Visualization and Computer Graphics</em> 16, 6 (Nov. 2010), 1182–1189. DOI: <a href=http://dx.doi.org/10.1109/TVCG.2010.194>http://dx.doi.org/10.1109/TVCG.2010.194</a>&#160;<a href=#fnref:32 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:32 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:33><p>S. C. Eick, J. L. Steffen, and E. E. Sumner. 1992. Seesoft-a tool for visualizing line oriented software statistics. <em>IEEE Trans. on Software Engineering</em> 18, 11 (Nov. 1992), 957–968. DOI: <a href=http://dx.doi.org/10.1109/32.177365>http://dx.doi.org/10.1109/32.177365</a>&#160;<a href=#fnref:33 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:34><p>Marti A. Hearst. 1995. TileBars: Visualization of Term Distribution Information in Full Text Information Access. In <em>Proc. SIGCHI Conf. on Human Factors in Computing Systems (CHI)</em> . ACM, 59–66. DOI: <a href=http://dx.doi.org/10.1145/223904.223912>http://dx.doi.org/10.1145/223904.223912</a>&#160;<a href=#fnref:34 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:35><p>Jean Daniel Fekete and Nicole Dufournaud. 2000. Compus: Visualization and Analysis of Structured Documents for Understanding Social Life in the 16th Century. In <em>Proc. ACM Conf. on Digital Libraries (DL)</em> . ACM, 47–55. DOI: <a href=http://dx.doi.org/10.1145/336597.336632>http://dx.doi.org/10.1145/336597.336632</a>&#160;<a href=#fnref:35 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:36><p>K. Kucher and A. Kerren. 2015. Text visualization techniques: Taxonomy, visual survey, and community insights. In <em>Proc. IEEE Pacific Visualization Symp. (PacificVis)</em> . 117–121. DOI: <a href=http://dx.doi.org/10.1109/PACIFICVIS.2015.7156366>http://dx.doi.org/10.1109/PACIFICVIS.2015.7156366</a>&#160;<a href=#fnref:36 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:37><p>S. Jänicke, G. Franzini, M. F. Cheema, and G. Scheuermann. 2017. Visual Text Analysis in Digital Humanities. <em>Computer Graphics Forum</em> 36, 6 (2017), 226–250. DOI:<a href=http://dx.doi.org/10.1111/cgf.12873>http://dx.doi.org/10.1111/cgf.12873</a>&#160;<a href=#fnref:37 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:38><p>Edward Tufte. 2004. Sparklines: Intense, simple, word-sized graphics. In <em>Beautiful Evidence</em> . 46–63.&#160;<a href=#fnref:38 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:38 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:39><p>Ulrik Brandes, Bobo Nick, Brigitte Rockstroh, and Astrid Steffen. 2013. Gestaltlines. In <em>Proc. Eurographics Conf. on Visualization (EuroVis)</em> . 171–180. DOI: <a href=http://dx.doi.org/10.1111/cgf.12104>http://dx.doi.org/10.1111/cgf.12104</a>&#160;<a href=#fnref:39 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:40><p>Pascal Goffin, Wesley Willett, Jean-Daniel Fekete, and Petra Isenberg. 2014. Exploring the placement and design of word-scale visualizations. <em>IEEE Trans. on Visualization and Computer Graphics</em> 20, 12 (2014), 2291–2300.&#160;<a href=#fnref:40 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:40 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref2:40 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref3:40 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref4:40 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:41><p>Pascal Goffin, Wesley Willett, Jean-Daniel Fekete, and Petra Isenberg. 2015. Design Considerations for Enhancing Word-Scale Visualizations with Interaction. Posters of Conf. on Information Visualization (InfoVis). (Oct. 2015). Poster.&#160;<a href=#fnref:41 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:42><p>Pascal Goffin, Wesley Willett, and Petra Isenberg. 2015a. Sharing information from personal digital notes using word-scale visualizations. In <em>Proc. IEEE VIS Workshop on Personal Visualization: Exploring Data in Everyday Life</em> . <a href=https://hal.inria.fr/hal-01216223/>https://hal.inria.fr/hal-01216223/</a>&#160;<a href=#fnref:42 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:43><p>Pascal Goffin. 2016. <em>An Exploration of Word-Scale Visualizations for Text Documents</em> . Ph.D. Dissertation. Université Paris-Saclay. <a href=http://www.theses.fr/2016SACLS256>http://www.theses.fr/2016SACLS256</a>&#160;<a href=#fnref:43 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:44><p>P. Goffin, J. Boy, W. Willett, and P. Isenberg. 2017. An Exploratory Study of Word-Scale Graphics in Data-Rich Text Documents. <em>IEEE Trans. on Visualization and Computer Graphics</em> 23, 10 (Oct 2017), 2275–2287. DOI: <a href=http://dx.doi.org/10.1109/TVCG.2016.2618797>http://dx.doi.org/10.1109/TVCG.2016.2618797</a>&#160;<a href=#fnref:44 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:45><p>Miguel Nacenta, Uta Hinrichs, and Sheelagh Carpendale. 2012. FatFonts: Combining the Symbolic and Visual Aspects of Numbers. In <em>Proc. Int. Conf. on Advanced Visual Interfaces (AVI)</em> .&#160;<a href=#fnref:45 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:46><p>F. Beck and D. Weiskopf. 2017. Word-Sized Graphics for Scientific Texts. <em>IEEE Trans. on Visualization and Computer Graphics</em> 23, 6 (June 2017), 1576–1587. DOI: <a href=http://dx.doi.org/10.1109/TVCG.2017.2674958>http://dx.doi.org/10.1109/TVCG.2017.2674958</a>&#160;<a href=#fnref:46 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:47><p>F. Beck, Y. Acurana, T. Blascheck, R. Netzel, and D. Weiskopf. 2016. An expert evaluation of word-sized visualizations for analyzing eye movement data. In <em>IEEE Workshop on Eye Tracking and Visualization (ETVIS)</em> . IEEE, 50–54. DOI: <a href=http://dx.doi.org/10.1109/ETVIS.2016.7851166>http://dx.doi.org/10.1109/ETVIS.2016.7851166</a>&#160;<a href=#fnref:47 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:48><p>Catherine C. Marshall, Annotation: From paper books to the digital library. In <em>Proc. ACM Conf. on Digital Libraries (DL)</em> . ACM. DOI:<a href=http://dx.doi.org/10.1145/263690.263806>http://dx.doi.org/10.1145/263690.263806</a>&#160;<a href=#fnref:48 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:48 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref2:48 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:49><p>Muhammad Faisal Cheema, Stefan Jänicke, and Gerik Scheuermann. 2016. AnnotateVis: Combining Traditional Close Reading with Visual Text Analysis. In <em>IEEE VIS Workshop on Visualization for the Digital Humanities</em> . 4.&#160;<a href=#fnref:49 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:49 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref2:49 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:50><p>R. Smith. 2007. An Overview of the Tesseract OCR Engine. In <em>Proc. Int. Conf. on Document Analysis and Recognition (ICDAR)</em> , Vol. 2. 629–633. DOI: <a href=http://dx.doi.org/10.1109/ICDAR.2007.4376991>http://dx.doi.org/10.1109/ICDAR.2007.4376991</a>&#160;<a href=#fnref:50 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:51><p>Ray W. Smith. 2013. History of the Tesseract OCR engine: What worked and what didn’t. <em>Proc. SPIE</em> 8658 (2013), 865802–865802–12. DOI: <a href=http://dx.doi.org/10.1117/12.2010051>http://dx.doi.org/10.1117/12.2010051</a>&#160;<a href=#fnref:51 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:52><p>Chirag Patel, Atul Patel, Dharmendra Patel, Archana A. Shinde, Hui Wu, and Jian Liang. 2012. Optical Character Recognition by Open source OCR Tool Tesseract: A Case Study. <em>Int. Journal of Computer Applications</em> 40, 10 (2012).&#160;<a href=#fnref:52 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:53><p>R. O. Duda and P. E. Hart. 1972. Use of the Hough Transformation to Detect Lines and Curves in Pictures. <em>Comm. ACM</em> 15 (Jan. 1972), 11–15.&#160;<a href=#fnref:53 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:54><p>Webster’s Online Dictionary API. <a href=http://www.dictionaryapi.com/>https://www.dictionaryapi.com/.</a> (2017). Accessed: 2017-09-18.&#160;<a href=#fnref:54 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:55><p>Franco Moretti. 2005. <em>Graphs, Maps, Trees: Abstract Models for a Literary History</em> . Verso, Brooklyn, NY.&#160;<a href=#fnref:55 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:56><p>Benjamin B. Bederson and James D. Hollan. 1994. Pad++: A Zooming Graphical Interface for Exploring Alternate Interface Physics. In <em>Proc. ACM Symp. on User Interface Software and Technology (UIST)</em> . ACM, 17–26. DOI:<a href=http://dx.doi.org/10.1145/192426.192435>http://dx.doi.org/10.1145/192426.192435</a>&#160;<a href=#fnref:56 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div></div></article></main><footer><nav aria-label="footer links"><ul><li><a class=highlight-focus href=/dhqwords/tags/>Tags</a></li><li><a class=highlight-focus href=/dhqwords/about/>About</a></li><li><a class=highlight-focus href=/dhqwords/categories/>Keywords</a></li></ul></nav><div class=icons><a class="license highlight-focus" rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons Attribution No-Derivatives 4.0 International License" src=/dhqwords/img/logos/license.svg width=120 height=42></a></div></footer></body></html>