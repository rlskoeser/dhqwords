<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=theme-color content="#3D206C"><meta http-equiv=x-ua-compatible content="IE=edge"><link rel=apple-touch-icon sizes=180x180 href=/img/favicon/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/img/favicon/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/img/favicon/favicon-16x16.png><link rel=icon type=image/png sizes=384x384 href=/img/favicon/android-chrome-384x384.png><link rel=icon type=image/png sizes=192x192 href=/img/favicon/android-chrome-192x192.png><link rel=icon type=image/png sizes=150x150 href=/img/favicon/mstile-150x150.png><link rel="shortcut icon" href=/favicon.ico><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/img/favicon/safari-pinned-tab.svg color=#3d206c><link rel=schema.dc href=http://purl.org/DC/elements/1.0/><meta name=citation_public_url content="https://rlskoeser.github.io/dhqwords/vol/14/4/000497/"><meta name=citation_title content="Exploring Digitised Moving Image Collections: The SEMIA Project, Visual Analysis and the Turn to Abstraction"><meta name=citation_date content="2020/12"><meta name=citation_author content="Eef Masson"><meta name=citation_author content="Christian Gosvig Olesen"><meta name=citation_author content="Nanne van Noord"><meta name=citation_author content="Giovanna Fossati"><meta name=citation_abstract content="_One senior curator said that some of museum staff [sic] were skeptical of the project at first. We would get an email from Wes asking, Do you have a list of green objects? Could you send us a list of everything you have that is yellow? Our data system does not have these categories. _ 1
Introduction Until late April of 2019, visitors of the Kunsthistorisches Museum in Vienna could drop in on the exhibit “Spitzmaus Mummy in a Coffin and Other Treasures” ,2 co-curated by filmmaker Wes Anderson and designer Juman Malouf."><meta name=citation_journal_title content="DHQwords"><meta name=citation_issue content="14.4"><meta name=citation_publisher content="Center for Digital Humanities, Princeton University"><meta name=DC.rights content="http://creativecommons.org/licenses/by/4.0/"><meta name=author content="Eef Masson, Christian Gosvig Olesen, Nanne van Noord, Giovanna Fossati"><meta name=generator content="Center for Digital Humanities, Princeton University"><meta name=dcterms.created content="2020-12"><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-regular.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-italic.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-regular.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-300.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-700.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-300italic.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-500italic.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-700italic.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-500.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/Source_Sans_Pro/SourceSans3VF-Roman.otf.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/Source_Sans_Pro/SourceSans3VF-Italic.otf.woff2 crossorigin><title>Exploring Digitised Moving Image Collections: The SEMIA Project, Visual Analysis and the Turn to Abstraction</title><meta name=description content="DHQwords Issue 14.4, December 2020. an experiment republishing DHQ articles with Hugo & Startwords theme. "><meta property="og:title" content="Exploring Digitised Moving Image Collections: The SEMIA Project, Visual Analysis and the Turn to Abstraction"><meta property="og:description" content="_One senior curator said that some of museum staff [sic] were skeptical of the project at first. We would get an email from Wes asking, Do you have a list of green objects? Could you send us a list of everything you have that is yellow? Our data system does not have these categories. _ 1
Introduction Until late April of 2019, visitors of the Kunsthistorisches Museum in Vienna could drop in on the exhibit “Spitzmaus Mummy in a Coffin and Other Treasures” ,2 co-curated by filmmaker Wes Anderson and designer Juman Malouf."><meta property="og:type" content="article"><meta property="og:url" content="https://rlskoeser.github.io/dhqwords/vol/14/4/000497/"><meta property="og:image" content="https://rlskoeser.github.io/dhqwords/social.png"><meta property="article:section" content="vol"><meta property="article:published_time" content="2020-12-20T00:00:00+00:00"><meta property="article:modified_time" content="2020-12-20T00:00:00+00:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://rlskoeser.github.io/dhqwords/social.png"><meta name=twitter:title content="Exploring Digitised Moving Image Collections: The SEMIA Project, Visual Analysis and the Turn to Abstraction"><meta name=twitter:description content="_One senior curator said that some of museum staff [sic] were skeptical of the project at first. We would get an email from Wes asking, Do you have a list of green objects? Could you send us a list of everything you have that is yellow? Our data system does not have these categories. _ 1
Introduction Until late April of 2019, visitors of the Kunsthistorisches Museum in Vienna could drop in on the exhibit “Spitzmaus Mummy in a Coffin and Other Treasures” ,2 co-curated by filmmaker Wes Anderson and designer Juman Malouf."><link rel=stylesheet href=/dhqwords/style.css><link rel=stylesheet href=/dhqwords/print.css media=print><script src=/dhqwords/js/polyfills.js></script><script defer src=/dhqwords/js/bundle.js></script></head><body class=article><header><nav class=main aria-label=main><ul><li class=home><a href=/dhqwords/>dhq</a></li><li class=issues><a href=/dhqwords/vol/>volumes</a></li></ul></nav></header><main><article><div class=grid><header><p class=number><a href=/dhqwords/vol/14/4/>Issue 14.4</a></p><p class=theme>Digital Humanities & Film Studies: Analyzing the Modalities of Moving Images</p><h1>Exploring Digitised Moving Image Collections: The SEMIA Project, Visual Analysis and the Turn to Abstraction</h1><p><ul class=authors><li><address>Eef Masson</address></li><li><address>Christian Gosvig Olesen</address></li><li><address>Nanne van Noord</address></li><li><address>Giovanna Fossati</address></li></ul></p><p><time class=pubdate datetime=2020-12>December 2020</time></p><ul class="categories tags"><li><span class=tag>moving images</span></li><li><span class=tag>digitization</span></li><li><span class=tag>archives</span></li><li><span class=tag>information retrieval</span></li><li><span class=tag>images</span></li></ul><ul class=tags></ul><p class=formats></p></header><section class=print-only><a class=first-page-header href=/ aria-label=Startwords><img alt=Startwords src=/pdf-logotype.svg></a>
<a class=page-header href=/ aria-label=Startwords><img alt=Startwords src=/pdf-logo.svg></a>
<a href=http://doi.org/ rel=alternate class=page-doi>doi:</a></section><div class=text-container><p>_One senior curator said that some of museum staff [sic] were skeptical of the project at first. We would get an email from Wes asking, Do you have a list of green objects? Could you send us a list of everything you have that is yellow? Our data system does not have these categories. _ <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></p><h2 id=introduction>Introduction</h2><p>Until late April of 2019, visitors of the Kunsthistorisches Museum in Vienna could drop in on the exhibit “Spitzmaus Mummy in a Coffin and Other Treasures” ,<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup> co-curated by filmmaker Wes Anderson and designer Juman Malouf.<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup> The exhibit consisted of 430 relatively obscure objects selected from a collection of more than four million, spanning over 5,000 years. In putting the exhibit together, the curators had relied heavily on the museum’s curatorial staff, who had helped them navigate the collection <sup id=fnref1:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>. Without such assistance, their task arguably would have been impossible to perform. The reason is that while most museums these days work with searchable digital catalogues (or collection management systems), the descriptions those systems contain typically neglect certain aspects of the objects represented. For example, they usually do not contain specifications of such sensory features as colour – precisely the kind which, as the epigraph to this piece suggests, Anderson and his colleague were interested in.</p><p>In a more general sense, this holds true also for most moving image archives. Oftentimes, such institutions house collections of many thousands of films or television episodes, composed in turn of millions of discrete images. Typically, the sensory characteristics of those objects barely feature in catalogue descriptions. While some entries contain information, either at the title or the fragment level, about the colour or sound systems used, this information tends to be fragmentary. Moreover, further specifics about the films’ or episodes’ visual features are usually absent.</p><p>In recent years, audiovisual heritage institutions have invested much time and resources into digitising their collections, so as to enable various kinds of reuse. Yet in spite of this, the above situation is largely unchanged. So far, attempts to improve usability have focused primarily on the searchability of collections and the retrieval of collection items through (linked) metadata. Therefore, access to digital archives is overwhelmingly governed, even today, by a logic of search – one dominant in practices of information retrieval more in general <sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup>. Search relies on the use of semantic descriptors: keywords or other labels produced either manually, or as (semi-)automatically generated metadata. Apart from being labour-intensive to produce, such descriptors are also highly selective. In the case of audiovisual materials, for instance, they are usually limited to facts about production, or about the people, events and geographic locations they feature. Arguably, they serve the needs of a rather limited range of reuse practices; for instance, the production of documentaries, or scholarship in socio-political history, media production or (to a lesser extent) certain forms of aesthetic analysis. The design of an exhibit like Anderson and Malouf’s, but also other kinds of more creative reuse, require different kinds of information.</p><p>For users, the selectiveness of catalogue descriptions poses two important problems. On the one hand, it forces them to search collections on the basis of prior interpretations, and from the perspective of those who catalogued them – rather than to more freely explore them. On the other, it prevents them from relying in the process on features that are essential to their experience of heritage objects, but inadequately captured through verbal description; for example, visual features such as colour, but also shape or movement. Such characteristics are particularly significant for historic (moving) images, as those are valued not only for the information they hold, but also for their look and feel <sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup> (cf. <sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup>).</p><p>The research project The Sensory Moving Image Archive (SEMIA): Boosting Creative Reuse for Artistic Practice and Research<sup id=fnref:7><a href=#fn:7 class=footnote-ref role=doc-noteref>7</a></sup> departs from the observation that this situation impedes the work (and play) of a range of potential users. In response to this problem, it raises the question how sensory object features can be mobilised as the driving criterion to explore – rather than search – digitised audiovisual collections. Users, in this context, are filmmakers or exhibition designers, but also scholars. It has been argued, indeed, that the work of researchers may benefit from modes of access that do not (solely) rely on search and retrieval of single items but afford a more explorative form of browsing <sup id=fnref:8><a href=#fn:8 class=footnote-ref role=doc-noteref>8</a></sup>, ideally also drawing on the sensory relations between discrete items within collections.<sup id=fnref:9><a href=#fn:9 class=footnote-ref role=doc-noteref>9</a></sup> Beneficiaries of such an approach are scholars already concerned in their work, for instance, with colour palettes, or patterns of movement in historical film – whether considered in terms of their technological preconditions (as in <a href=#yumibe2012>Yumibe [2012]</a>), their relation to film style and aesthetics <sup id=fnref:10><a href=#fn:10 class=footnote-ref role=doc-noteref>10</a></sup> <sup id=fnref:11><a href=#fn:11 class=footnote-ref role=doc-noteref>11</a></sup> <sup id=fnref:12><a href=#fn:12 class=footnote-ref role=doc-noteref>12</a></sup> <sup id=fnref:13><a href=#fn:13 class=footnote-ref role=doc-noteref>13</a></sup> <sup id=fnref:14><a href=#fn:14 class=footnote-ref role=doc-noteref>14</a></sup> or from a more experiential perspective <sup id=fnref:15><a href=#fn:15 class=footnote-ref role=doc-noteref>15</a></sup>, for instance in terms of their haptic or synaesthetic aspects <sup id=fnref:16><a href=#fn:16 class=footnote-ref role=doc-noteref>16</a></sup>. But arguably, also others can benefit, as it can help reveal previously unanticipated patterns or relations in or between widely divergent materials, that elicit novel research questions of their own.</p><p>SEMIA, a two-and-a-half year project that ran until late January 2020, was a collaboration between the University of Amsterdam (with contributions from media and audiovisual heritage scholars as well as computer scientists), the Amsterdam University of Applied Sciences (specifically, experts in the domain of data visualisation and interface design), the interaction design company Studio Louter (experienced in the development of museum presentations) and two audiovisual heritage institutions: Eye Filmmuseum (focusing on film and cinematography) and the Netherlands Institute for Sound and Vision (television).<sup id=fnref:17><a href=#fn:17 class=footnote-ref role=doc-noteref>17</a></sup> The team’s overarching aim was to establish whether, and how, repurposing software for analysing and visualising colour, shape, visual complexity and movement might enable alternative forms of accessing collections of moving images. To this end, it developed a prototype tool that invites users to explore collections on the basis of those features, rather than to search them through (verbal) descriptions resulting from prior interpretations of specific objects in discrete films or film sequences. In doing so, it not only sought to delay the moment in time when significance is assigned – that is, when the meaning of specific sensory features, or of the relations between them, is determined – but also to place this task in the users’ own hands (compare <a href=#kuhn2013>Kuhn et al. [2013]</a>). The tool was designed to deal with large numbers of heterogeneous materials (in terms of production date, genre, but also medium) so as to allow for the revelation of potentially surprising connections. The corpus used for testing was made up of fragments from the collections of Eye and Sound and Vision, as featured on the open access platform Open Images.<sup id=fnref:18><a href=#fn:18 class=footnote-ref role=doc-noteref>18</a></sup></p><p>The project consisted of two phases, whose timings partly overlapped: a first, focused on image feature extraction and analysis, and a second, concerned with the development of a “generous” interface <sup id=fnref1:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup>, visualising the relations between fragments on the basis of analysis results. The first phase, which we elaborate on in this article, involved the use of computer vision methods.</p><p>In computer vision, a subdiscipline of AI, models are developed for extracting key information – so-called visual features – from images, so that they can subsequently be cross-referenced. In the analysis process, images are transformed into descriptions that are used in turn to classify them. In the early years of the field, methods were developed that required humans to determine which operations systems had to perform in order to produce the intended analysis results. More recently, however, methods based on machine learning, whereby computers are trained with techniques for automatic feature learning, are becoming more popular.<sup id=fnref:19><a href=#fn:19 class=footnote-ref role=doc-noteref>19</a></sup></p><p>In SEMIA, we used a combination of both types of methods. In what follows, we explain why this is the case, and elaborate on how the computer scientists in our team aligned their work with our overall objective of enabling new forms of exploration. In doing so, we specifically focus on how we changed the preconditions for archival reuse (the scholarly kind in particular). We are motivated by the observation that reliance on visual features and relations in accessing collections not only opens up new avenues for research, but also helps challenge current understandings of how knowledge is produced – in media and heritage studies (traditional as well as digital) and in the digital humanities more broadly.</p><p>In our contribution, we take a funnel approach, gradually narrowing our focus to the specific extraction and analysis tasks carried out within the SEMIA project. First, we provide a broad outline, and discussion, of the landscape of visual analysis for media scholarly research, and developments in this area over time. We pay attention both to the interests and objectives of those active in the field (along with their epistemic underpinnings) and to their specific approaches or methods. The purpose of this exercise is twofold: to specify the project’s place among prior efforts, and to further elucidate our overall motivation in taking it on. Subsequently, we zoom in on what feature analysis means for SEMIA: first, by looking at the general principles behind our approach to feature extraction, and then, by discussing some analysis results. In our conclusions, we confront those results with our initial intent in exploring the affordances of computer vision for providing access to collections.</p><h2 id=visual-analysis-in-digital-scholarship-media-art-and-explorative-browsing>Visual Analysis in Digital Scholarship, Media Art and Explorative Browsing</h2><p>In developing a tool that supports a more unconstrained browsing of media archives than is currently available, we sought to complement existing approaches to, and methods for, the visual analysis of moving images. Those approaches and methods have emerged primarily in the context of stylometric research of the 1970s and on, and tend to be tailored to the detection of patterns in specific analytical units. In the interpretation of data, stylometric research usually adheres to semantic categories that have traditionally had relevance also for both archives and media historical research (in particular, the above-mentioned categories of director or creator, or production time). For the purposes of the SEMIA project, we needed to let go of the assumptions this implied about what is meaningful about collection objects.</p><p>To achieve this, we followed the line of reasoning of a recent trend in digital film and media studies scholarship that seeks to reorient visual analysis methods by drawing on artistic practices of archival moving image appropriation. Such strategies are not intent on finding patterns in preselected image units, but are geared instead towards accidental or unanticipated finds that reveal more surprising similarities – or contrasts – in audiovisual materials. Those pioneering scholars, whose work we sample below, are convinced that artistic work can inspire users <em>not</em> to approach data from the perspective of specific questions or hypotheses, but to explore them more freely, also letting go in the process of more conventional categories for interpretation.</p><p>In order to specify the epistemological underpinnings of our own approach, it is helpful to start off with a brief consideration of the foundational assumptions of stylometry. This will help us to subsequently explain how more recent projects in visual analysis in our field draw on this tradition, while also moving it in different directions. We end the section with some further elaboration on the appropriation-indebted trend in film and media studies, explaining how it was inspirational for us.</p><p>In film and media studies, the visual analysis of moving images was developed as part of the intertwining stylometric research programmes commonly referred to as statistical style analysis and cinemetrics, initiated with the pioneering work of Barry Salt and Yuri Tsivian respectively. Arguably, these programmes had their very early roots in film theory and criticism from the 1910s and 1920s, attending to the interrelations between film editing, style and perception, and gained a foothold in academic institutions in the 1970s (see <a href=#buckland2008>Buckland [2008]</a> and <a href=#olesen2017>Olesen [2017]</a> for more on those historical developments). Their objective was to discern patterns in audiovisual materials, in a way that resembles the analysis of linguistic patterns in literary computing (for instance, for the purpose of authorship attribution, for the dating of films, or for the creation of statistical profiles of directorial styles, periods or genres and their changes over time). Such research often took a deductive approach, producing data that supports stylistic analysis as a more rigorous alternative, or complement, to traditional hermeneutic approaches. In its first decades as a scholarly form of research, stylometry pursued its objectives primarily by manually annotating, coding and quantifying data on shot lengths and shot types in films and television materials, to subsequently relate the data thus obtained to known information (for instance production or release date, production company, genre or director) in an attempt to interpret significant patterns.</p><p>In recent years, as digital humanities methods have proliferated, stylometric research in media studies has become more complex in its methods, but also more varied in its interests. In the past, shot length and shot type were key parameters for analysis; more recently, however, attention is also being paid to colour, motion, (recurring) objects and aspects of visual composition. Projects such as Digital Formalism<sup id=fnref:20><a href=#fn:20 class=footnote-ref role=doc-noteref>20</a></sup> (2007-2010) and ACTION<sup id=fnref:21><a href=#fn:21 class=footnote-ref role=doc-noteref>21</a></sup> (2011-2013) are illustrative of this development. Digital Formalism (a collaboration of the University of Vienna, the Austrian Filmmuseum and the Vienna University of Technology) sought to analyse the complex formal characteristics of Soviet director Dziga Vertov’s films. To achieve this, it strongly relied on a logic of feature-learning, whereby relevant image information was extracted with the help of purpose-produced algorithms. This involved the analysis of high-level – that is, complex – semantic features, such as visual composition or motion composition <sup id=fnref:22><a href=#fn:22 class=footnote-ref role=doc-noteref>22</a></sup>. The ACTION project at Dartmouth College, resulting in an open-source toolkit, expanded the scope of authorship attribution research by facilitating not only the analysis of motion, but also colour and audio features; moreover, it focused on the films of twenty-four canonical directors, rather than a more homogeneous corpus consisting of work by a single maker.<sup id=fnref:23><a href=#fn:23 class=footnote-ref role=doc-noteref>23</a></sup> In addition, the project relied less on purpose-produced algorithms, making use instead of existing solutions, including (but not limited to) machine learning tools <sup id=fnref:24><a href=#fn:24 class=footnote-ref role=doc-noteref>24</a></sup>. This way, it also expanded stylometry’s scope in the technological sense, while it remained true to its foundational drive towards quantitative, empirical research.</p><p>In this respect, ACTION certainly paved the way for SEMIA. On the one hand, because the project relies to a considerable extent on techniques developed or used in the context of previous stylometric research. And on the other, because it likewise engages in the extraction and quantification of moving image data. In SEMIA, however, such extraction serves rather different purposes. Data analysis, in this case, is not done with the objective of authorship attribution or for the establishment of genre features dominant in a particular corpus or period. As previously explained, the project is focused rather on enabling exploratory browsing, affording (possibly incidental) discovery of similarities that do not neatly align with existing interpretative frameworks. For instance, similarities between collection items that do <em>not</em> have a maker or production time in common, or visual features that can <em>not</em> easily be understood as shared stylistic elements.<sup id=fnref:25><a href=#fn:25 class=footnote-ref role=doc-noteref>25</a></sup></p><p>To this end, the project draws inspiration from an emerging approach to visual analysis and data visualisation in digital film and media studies scholarship – an approach that is indebted in turn to media art practice and experimental filmmaking. Kevin Ferguson, a proponent of this trend, explains that there is a tradition of experimental work in media studies that “balances between [&mldr;] new media art and digital humanities scholarship” <sup id=fnref:26><a href=#fn:26 class=footnote-ref role=doc-noteref>26</a></sup>, intent on “deforming” its object of study <sup id=fnref:27><a href=#fn:27 class=footnote-ref role=doc-noteref>27</a></sup>. Arguably, such work challenges (especially early) stylometry’s version of visual analysis, in pursuit of “a digital humanities project that is more aleatory and aesthetic than it is formal and constrained” <sup id=fnref1:27><a href=#fn:27 class=footnote-ref role=doc-noteref>27</a></sup>. Instead of rigorously counting and then comparing calculation results to produce historical insights into film form and its development, it highlights the occurrence of highly complex formal systems (which select images features are always part of) that may meaningfully relate to each other in multifarious ways. In doing so, it demonstrates the need to pay attention also to similarities that may not be detected if one sticks to more carefully defined analytical registers.</p><p>As previously mentioned, film and media scholars who proceed in this way oftentimes seek inspiration in the work of artists, and specifically, those engaged in practices of archival appropriation. History has shown that these practitioners in particular have their own contributions to make to the challenging of preconceptions underpinning scholarly analysis. At times, they even use the same analytical devices for this purpose – but in methodologically less rule-bound ways. In the last few decades, this has led to productive exchanges between academics, archivists and artists – the constellation Thomas Elsaesser once dubbed the “three A&rsquo;s” <sup id=fnref:28><a href=#fn:28 class=footnote-ref role=doc-noteref>28</a></sup> – and as such, produced novel interpretations of audiovisual materials.</p><p>For instance, in the 1970s, when film historians would use projectors and editing tables to come up with statistics providing insight into developments in film style, artists would use those same devices to visually explore archival films in more idiosyncratic ways. They would focus in the process on particular image details, or dwell on and contemplate specific temporal units by stretching them. Examples of this practice are the 1970s structural films of Ken Jacobs, Al Razutis or Ernie Gehr, who repurposed films from the early 1900s. Their oftentimes rather abstract works highlighted the different formal properties of early cinema (compared to the narrative standard of later years). In bringing those to the fore, they challenged prevalent assumptions among contemporary historians, who had in fact largely neglected early cinema in their stylistic accounts to date <sup id=fnref:29><a href=#fn:29 class=footnote-ref role=doc-noteref>29</a></sup>. While scholars may not always be able to make immediate (historiographic) sense of such work – although the contemporaries of Jacobs and others ultimately did – it may invite them to look at specific visual features with fresh eyes, or from different perspectives.</p><p>Ultimately, the great merit of such artistic work is that it strips archival films of the categories and interpretive frameworks with which they have previously been associated – thus opening up the possibility of applying new ones. Film scholar Michael Pigott, in this context, has credited the practice with “inducing illegibility.” In his view, this sort of work serves “the dual purpose (and double tension) of making the image illegible (again) and then attempting to read it” <sup id=fnref:30><a href=#fn:30 class=footnote-ref role=doc-noteref>30</a></sup>. The potential for inducing illegibility is not exclusive to structural filmmaking (a common reference point for this purpose within film studies) but can also be found in contemporary media art. Currently, there is a small, but important body of artworks that critically explore moving image data, and prove inspirational also to film and media scholars; for instance, the film data visualisation work by such artists as Jim Campbell<sup id=fnref:31><a href=#fn:31 class=footnote-ref role=doc-noteref>31</a></sup> or Jason Salavon<sup id=fnref:32><a href=#fn:32 class=footnote-ref role=doc-noteref>32</a></sup> <sup id=fnref:33><a href=#fn:33 class=footnote-ref role=doc-noteref>33</a></sup> <sup id=fnref2:27><a href=#fn:27 class=footnote-ref role=doc-noteref>27</a></sup>. This work precedes contemporary digital scholarship by fifteen to twenty years, and has used different coding languages and visualisation softwares, but resulting in at times remarkably similar expressions. Likewise, artist and designer Brendan Dawes’ Cinema Redux<sup id=fnref:34><a href=#fn:34 class=footnote-ref role=doc-noteref>34</a></sup> project (2004) experimented with grid visualisations of classic films, inviting gallery visitors to contemplate film data visualisations as visual compositions in their own right, rather than to use them as an empirical basis for establishing patterns along well-known interpretive lines.</p><p>In setting up SEMIA, the project team, while familiar with the above-mentioned examples, was more directly inspired by the work of Dutch video artist Geert Mul – a long-term collaborator of heritage partner Sound and Vision. Particularly influential for the projects’ approach was <em>Match of the Day</em> (2004-2008), an early example of an artwork produced with the help of algorithms for visual analysis, made up entirely of stills from satellite television images (see <a href=#figure01>Figure 1</a>). The piece demonstrates particularly well how artists can productively exploit similarities in image features among widely heterogeneous objects, that are too fuzzy to be meaningful for the rigorous testing of hypotheses.</p><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://player.vimeo.com/video/7305683 style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 title="Geert Mul, Match of the Day (2004-2008)." webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe></div><p>To create his work, Mul used large databases of images, for which he extracted a wide variety of visual features. Those features served in turn as the basis for a matching of images at different levels of similarity. The first part of this process was conducted automatically; however, human intervention occurred when the artist selected approximate rather than identical matches to include in his work <sup id=fnref:35><a href=#fn:35 class=footnote-ref role=doc-noteref>35</a></sup>. In stylometric research, such matches would likely be considered errors, glitches or mismatches. But in the context of an exploratory browse through an archival collection, they are precisely the kinds of results that may yield unexpected connections or patterns, worth investigating further outside of conventional notions of authorship, genre or period.</p><p>The above observations informed our decision, made early on in the SEMIA project, to radically abandon those kinds of categories, as embedded in archival metadata through semantic descriptions, and to opt instead for a visual analysis approach. We did this primarily by way of experiment, and in the assumption that the explorative options it opened up would eventually prove useful primarily <em>in combination with</em> search-based approaches drawing on existing metadata. (Inducing illegibility, after all, is rarely the end of a research process, and primarily makes sense in the early, exploratory phases of study. Further on in the process, existing metadata categories may then prove productive once again.) In what follows, we discuss how we undertook this visual analysis task, paying specific attention to the choices we made in the process – conceptual as well as technical, and in light of the aforementioned principles.</p><h2 id=feature-extraction-in-semia-a-turn-towards-abstraction>Feature Extraction in SEMIA: A Turn towards Abstraction</h2><p>In addition to pursuing a different set of media scholarly objectives, the SEMIA project team also sought to engender a shift in terms of the techniques used for visual analysis. In this section, we discuss the rationale behind our choice for specific feature extraction methods, and why we chose to tweak existing ones in particular ways. The connecting links between those different choices are, first, our wish to extract features that would point to unanticipated – rather than predictable – connections among objects, and second, to do so at a higher level of abstraction than is currently considered “state of the art,” in light of the overwhelming focus in computer vision on the recognition of meaningful semantic entities.</p><p>To a greater extent than other projects so far – ACTION, for instance, or the Zürich-based FilmColors – SEMIA set out to explore the affordances of deep learning techniques for revealing similarity-based patterns in (large) collections of digitised moving images.<sup id=fnref:36><a href=#fn:36 class=footnote-ref role=doc-noteref>36</a></sup> The assumption was that those patterns would enable users to follow alternative routes through the collections, remixing them as it were, and that this would elicit new questions about the items and their mutual relations. As we previously explained, we were specifically interested in relations inspired by the material’s visual features – rather than the sort of filmographic or technical data that make up traditional metadata categories for film and video.</p><p>As it happens, such metadata, in archival collections, are often also fragmentary – and therefore, hardly reliable as a starting point for an inclusive form of collection exploration. Early on in the project, we took this as a key argument for looking into the possibilities of computer vision, and specifically deep learning techniques, for the purpose of feature extraction. This approach would help us generate large quantities of new metadata that would invite, if not a more inclusive kind of exploration, then at least one that could complement approaches to access based on search. After all, a lack of metadata in the form of semantic descriptors as encountered in an institutional catalogue may render the objects in a collection invisible, and therefore unfindable. While an approach relying on visual analysis does not solve this problem – as it can create new invisibilities, which we argue elsewhere (see <a href=#masson>Masson and Olesen [2020]</a>) – it does challenge existing hierarchies of visibility.</p><p>Initially, the choice for a deep learning approach seemed to fit neatly with the project’s intent to refrain as much as possible from determining in advance the route a computer might take in order to identify similarities between collection items. In the alternative scenario, known as feature engineering, it is humans who design task-specific algorithms, which are used to extract pre-defined features from the images in a database (so that they can subsequently be compared). Deep learning, which relies to an overwhelming extent on the use of Neural Networks (NNs, or neural nets for short), involves algorithms trained with techniques for automatic feature learning (and as such, is a particular brand of machine learning). As we mentioned in the introduction, this is a more recent approach, and it entails the learning of specific data representations rather than set analysis tasks. Like feature engineering, deep learning does to some extent rely on the intervention of humans; after all, it is people who, at the training and/or retraining stages, determine which similarities do or do not make sense (see also <a href=#masson2019>Masson and van Noord [2019]</a>; in <a href=#masson2020>Masson and Olesen [2020]</a>, we elaborate on the epistemic implications for users of our tool). However, it does not require them to decide in advance <em>how</em> the task of identifying those similarities needs to be performed (that is, on the basis of which features). In principle, this opens the door for image matches unanticipated by people, and therefore, of novel routes through a database or collection.</p><p>However, we soon decided to only partially rely on such techniques – and the abovementioned role of human knowledge is certainly one of the reasons why. As a rule, deep learning is employed for the recognition of semantic classes, and more specifically, object categories. This is hardly surprising, as the development of such techniques is oftentimes done for purposes that involve the recognition of semantic entities: vehicles, people, buildings, and so on. (One might think here of applications for transportation and traffic control, geolocation, or biometrics; see e.g. <a href=#ucar2017>Uçar et al. [2017]</a>; <a href=#arandjelovic2018>Arandjelović et al. [2018]</a>; <a href=#taigman2014>Taigman et al. [2014]</a>). Within the SEMIA context, however, the use of conventional semantic classes does not make sense, as it is the sensory aspects of collection items – rather than the meanings we may assign to images, or image sections, on the basis of specific content – that are of interest. In fact, semantic classes commonly identified by deep learning approaches partially overlap with the sorts of categories that are used in descriptive metadata for archival collections, and that are central also to practices of search and retrieve. In performing feature extraction, we had hoped to be able to work instead with more abstract visual categories, which according to computer vision logic, involves extraction at a lower (syntactic) feature level (a point we elaborate on further below).</p><p>Another reason why exclusive reliance on a deep learning approach ultimately did not make sense, is that its underlying logic clashed with the requirements we had for interfacing. If our objective was to take sensory features as the point of entry into the collections, then it was imperative that our exploration tool allowed users to also take those features as the basis for digging further into the connections between items. For this purpose, we would need to at least minimally categorise, or re-categorise, those features, from the outset. The most logical choice here was to use the same intuitive classes that had also inspired the project: features such as colour, shape and visual complexity, and, for relations across time, movement.</p><p>One way of tackling this task with deep learning methods might have been to run successive analyses, whereby each time, the focus would be on one specific set of features, while other features would be cancelled out. For example, in order to extract information about shape, we might have deactivated the neural net’s colour ‘sensitivity’ by temporarily turning all images in the database into black-and-white, so as to focus its attention in the required direction. This type of approach is generally associated with a (fairly new) line of research in computer science, focused on learning so-called “disentangled representations” (see <a href=#xiao2018>Xiao et al. [2018]</a>; <a href=#denton2017>Denton and Birodkar [2017]</a>). So far, however, it has had limited success.<sup id=fnref:37><a href=#fn:37 class=footnote-ref role=doc-noteref>37</a></sup> But even aside from the issues it currently entails, it also undermines our most fundamental rationale for working with deep learning techniques: the fact that one need not determine in advance how the particular task of similarity detection is carried out, and specifically, which types of features are used in the process. For this reason, we ultimately decided on a more diversified approach, which combined the use of deep learning with feature engineering.</p><p>A major point of attention was the need to attain a sufficient measure of abstraction in the results of the computer vision part of the project – results that were used in the development of a tool for visualising the sensory relations between the films and fragments in our database (a process we shall discuss elsewhere). We explained that our objective within SEMIA was to inspire users by revealing potentially significant relations between database items; in doing so, however, we sought to relegate the act of assigning such significance – or in Pigott’s terms: of attempting to read images made illegible, through novel relations – as much as possible to users. For example, while we may want to draw attention to the circumstance that a specific set of database objects covers very similar colour schemes, or that they feature remarkably similar shapes, we leave it to the user to figure out whether, and if so how, this might be significant (that is, what questions it raises about media and their histories, or which alternative ways of researching historical film or television materials it affords). But arguably, we also withhold interpretation at a more basic level. In the above example, for instance, we leave undetermined whether similarity in colour or shape derives from the fact that the images concerned actually feature the same things. (They might, and they often do – but it is not necessarily so.) In this respect, what we do is entirely at odds with the objectives of much machine learning practice in the field of computer vision.</p><p>Our search for abstraction is evidenced in a very concrete way by what happened exactly in the feature extraction process. First, the extraction of image information along the lines of colour, shape, visual complexity and movement was not followed in our case by an act of labelling: of placing an image or image section in a particular (semantic) class (we elaborate on this point in <a href=#masson2020>Masson and van Noord [2020]</a>). The reason, of course, is that we did not actually seek to identify objects. For the purposes of our SEMIA experiment, the information as such, and the relations it allowed us to infer, were all we were interested in. Second, our search for abstraction is also evident from our application of deep learning methods, which was limited to the extraction of information about shape. Here, we focus on what computer vision experts call lower-level features – a notion that requires some further elaboration.</p><p>In computer vision, conceptual distinctions are oftentimes made between image features at different levels. From one perspective, these are distinctions in terms of feature complexity. Levels of complexity range from descriptions relevant to smaller units (such as pixels in discrete images) to larger spatial segments (sections of such images, or entire images), whereby the former also serve as building blocks for the latter. From another, complementary perspective, the distinction can also be understood as a sliding scale from more syntactic (and abstract) to more semantic features (the latter of which serve the purpose of object identification). Taking the example of shape-related information, we might think of a range that extends from unspecified shapes, for instance defined in terms of their edges (low-level), to more defined spatial segments such as contours or silhouettes (mid-level), all the way to actual object entities (e.g. things, people, faces, etc.) or relations between such entities. In SEMIA, we made use of a neural network trained for making matches at the highest (semantic) level. However, we scraped information at a slightly lower one, which generally contains descriptions of object parts. At this level, it recognises shapes, but without relating them to the objects they are part of.<sup id=fnref:38><a href=#fn:38 class=footnote-ref role=doc-noteref>38</a></sup></p><p>Arguably, this approach helped us mitigate a broader issue that the use of computer vision methods, and machine learning in general, posed for the project: that its techniques are designed, as Adrian MacKenzie puts it, to “mediate future-oriented decisions” – but by implication, also to <em>narrow down</em> a range of options by ruling other decisions out <sup id=fnref:39><a href=#fn:39 class=footnote-ref role=doc-noteref>39</a></sup>. In machine learning, datasets are used to produce probabilistic models, learned rules or associations, that generate predictive and classificatory statements <sup id=fnref1:39><a href=#fn:39 class=footnote-ref role=doc-noteref>39</a></sup>. In the case of networks for image pattern recognition, for example, these are statements that lead to conclusions as to how much (or how little) images look alike. However, the valuation of accurate identifications at the semantic level as the highest achievable goal within machine learning also imposes limitations, in that it renders meaningless all other similarities – and importantly, dissimilarities – between objects in a database. Anna Munster, therefore, argues that prediction also “takes down potential” (quoted in <a href=#mackenzie2017>Mackenzie [2017, 7]</a>). Within the SEMIA context, we expressly tried to bring back some of this potential for the user. Sometimes this required us to deviate from what was ‘state of the art’ in the field of computer vision. Only in this way, after all, we could leave room for matches that might, within a purely semantic logic, be considered mistakes but still provide productive starting points for unrestrained explorations of patterns that perhaps no one had noticed before.</p><h2 id=extraction-results-lesson-learnt>Extraction Results: Lesson Learnt</h2><p>To round off this account, we now look at the results of our feature extraction efforts, and at what we learnt about the aptness of the approach for our goals. The classes of features the SEMIA project centred on are embedded in a rich history of computer vision research, which, as we previously explained, began with a process of manually designing features for predefined analysis tasks.<sup id=fnref:40><a href=#fn:40 class=footnote-ref role=doc-noteref>40</a></sup> We also, however, deviated from this history, in that we did not use such algorithms for the purpose they were meant to serve: the assigning of (object) labels. Instead, we only relied on the feature descriptions they produced. Those descriptions are quite general, but still specific enough to bring out the sensory aspects of image elements that we were interested in. In what follows, we very briefly touch upon our methods (further technical details can be found in the notes) and then consider the results we obtained, evaluating their usefulness in light of the project’s goals.</p><p>As mentioned earlier, we chose to focus on four broad sets of image features, commonly understood as instances of shape, colour, visual complexity and movement. Shape, we explained, is the only feature for which we extract information using a neural net. The net we chose was trained for object recognition, but is commonly repurposed for other tasks.<sup id=fnref:41><a href=#fn:41 class=footnote-ref role=doc-noteref>41</a></sup> To make it meet our demands, we selected an intermediate feature representation rather than the uppermost layer in the net (that is, the highest complexity level, where, as we explained in <a href=#section>the previous section</a>, the prediction probabilities for the semantic classes it was trained on are to be found).<sup id=fnref:42><a href=#fn:42 class=footnote-ref role=doc-noteref>42</a></sup> This way, we could use its description of object parts and general shape, rather than of specific objects. For colour extraction, we made use of histograms, a common method in image processing.<sup id=fnref:43><a href=#fn:43 class=footnote-ref role=doc-noteref>43</a></sup> Specifically, we chose histograms in CIELAB colour space (one that aligns closely with human perception) capturing the colour values used in a moving image irrespective of their spatial position. Visual complexity was understood in SEMIA as a measure of how much clutter there is in a visual scene (for instance, a highly textured or very busy scene will have a greater visual complexity than an empty scene, or one with mostly smooth surfaces). For the extraction of information of this kind, we used a method called Subband Entropy, which expresses a scene’s visual complexity as a single scalar value.<sup id=fnref:44><a href=#fn:44 class=footnote-ref role=doc-noteref>44</a></sup></p><p>The features used to describe shape, colour, and visual complexity were all extracted with techniques that are applied to still images. In order to apply them to moving image material, we extracted feature descriptions from shots taken from the films and programmes in our corpus. Specifically, we extracted the shape, colour, and visual complexity features from five frames, evenly spaced throughout the shot, and aggregated them to create the final feature descriptions. Movement, however, is a feature specific to moving images. For extracting this kind of information, we relied on an optical flow method, measuring relative motion between two subsequent frames. In each case, we applied it to the same sets of five frames.<sup id=fnref:45><a href=#fn:45 class=footnote-ref role=doc-noteref>45</a></sup></p><p>For the purpose of the project, we gathered approximately 7,000 videos, which we subsequently segmented into over 100,000 shots with the help of automatic shot boundary detection. Each of those shots was subjected to the four feature extraction algorithms. Altogether, this resulted in four different feature spaces, in which every shot constitutes a datapoint. By measuring the distance between all points, we could determine which other shots are most similar to a given one; the two closest points are known in this context as so-called nearest neighbours.<sup id=fnref:46><a href=#fn:46 class=footnote-ref role=doc-noteref>46</a></sup></p><p>In <a href=#figure02>Figure 2</a>, we show three examples with the Query shots to the left, represented here by a single still each, and the 16 shots identified as their nearest neighbours in the four different feature spaces to the right. A first possible observation concerns the diversity between the nearest neighbours for the three query shots: while all nearest neighbours share sensory aspects with their respective query image, they are considerably different from those for the other query shots. This at the very least suggests that they are not randomly selected. The second query shot, furthermore, shows a visible similarity between nearest neighbours across the four different feature spaces for each query image. This last pattern logically follows from the nature of nearest neighbours, in that shots that look similar in one sensory aspect, are likely to also look similar in others. Colours in a nature shot (such as the mushroom in the third query shot), for example, are very distinctive, making it likely that its nearest neighbours in terms of colour are also nature scenes. Similarly, the movement of leaves swaying in the wind is very distinctive, making it probable that the nearest neighbours of a shot with this element, in movement terms, also show leaf-rich scenes.</p><p>At the same time and in spite of other visual similarities, our query images also produce matches that are quite distinct, precisely, in terms of the semantic entities they feature. The movement feature space for the mushroom query image, for instance, features a standing man (presumably, one who moves from left to right or the other way around, in the same way that the mushroom does; however, it would require further inspection to ascertain this or to make sense of this pairing). In instances like these, the matching process has arguably yielded more unexpected or surprising results and variations. Moreover, such matches occur more often if we look beyond the closest of the nearest neighbours. For example, a desert scene is similar to a beach scene in terms of colour, but not in terms of movement; in contrast, a grassy plain has similar movement to a beach scene, but differs strongly in colour. Hence, by exploring similarities in multiple feature spaces, we are still able to uncover such relations that would otherwise remain hidden.</p><figure><img loading=lazy alt src=/dhqwords/vol/14/4/000497/resources/images/figure02.1.jpg sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/14/4/000497/resources/images/figure02.1_hub8662fe88092625548b385c513327acb_327675_500x0_resize_q75_box.jpg 500w,
/dhqwords/vol/14/4/000497/resources/images/figure02.1_hub8662fe88092625548b385c513327acb_327675_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/14/4/000497/resources/images/figure02.1_hub8662fe88092625548b385c513327acb_327675_1200x0_resize_q75_box.jpg 1200w,/dhqwords/vol/14/4/000497/resources/images/figure02.1_hub8662fe88092625548b385c513327acb_327675_1500x0_resize_q75_box.jpg 1500w,/dhqwords/vol/14/4/000497/resources/images/figure02.1.jpg 1772w" class=landscape><figcaption><p>Sample stills of query shots from the Open Images platform with four nearest neighbours in the shape, colour, movement, and visual complexity feature spaces</p></figcaption></figure><figure><img loading=lazy alt src=/dhqwords/vol/14/4/000497/resources/images/figure02.2.jpg sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/14/4/000497/resources/images/figure02.2_hu50763555baec2082ca98bb61d628773c_492264_500x0_resize_q75_box.jpg 500w,
/dhqwords/vol/14/4/000497/resources/images/figure02.2_hu50763555baec2082ca98bb61d628773c_492264_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/14/4/000497/resources/images/figure02.2_hu50763555baec2082ca98bb61d628773c_492264_1200x0_resize_q75_box.jpg 1200w,/dhqwords/vol/14/4/000497/resources/images/figure02.2_hu50763555baec2082ca98bb61d628773c_492264_1500x0_resize_q75_box.jpg 1500w,/dhqwords/vol/14/4/000497/resources/images/figure02.2.jpg 1772w" class=landscape><figcaption><p>Sample stills of query shots from the Open Images platform with four nearest neighbours in the shape, colour, movement, and visual complexity feature spaces</p></figcaption></figure><figure><img loading=lazy alt src=/dhqwords/vol/14/4/000497/resources/images/figure02.3.jpg sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/14/4/000497/resources/images/figure02.3_hu6fd49538f993a99cbd8b0ccc8f2d06e1_463988_500x0_resize_q75_box.jpg 500w,
/dhqwords/vol/14/4/000497/resources/images/figure02.3_hu6fd49538f993a99cbd8b0ccc8f2d06e1_463988_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/14/4/000497/resources/images/figure02.3_hu6fd49538f993a99cbd8b0ccc8f2d06e1_463988_1200x0_resize_q75_box.jpg 1200w,/dhqwords/vol/14/4/000497/resources/images/figure02.3_hu6fd49538f993a99cbd8b0ccc8f2d06e1_463988_1500x0_resize_q75_box.jpg 1500w,/dhqwords/vol/14/4/000497/resources/images/figure02.3.jpg 1772w" class=landscape><figcaption><p>Sample stills of query shots from the Open Images platform with four nearest neighbours in the shape, colour, movement, and visual complexity feature spaces</p></figcaption></figure><h2 id=conclusions>Conclusions</h2><p>In this article, we have argued for a reorientation of existing visual analysis methods, in response to a need for exploratory browsing of media archives. We explained how we took our cue from a recent line of digital scholarship inspired by artistic strategies in (new) media art, and how we also built on the tradition of exchange between film archives, media history and appropriation art. Historically, artists have used the analytical devices of scholars to different ends, thus engendering shifts in the latter’s working assumptions. In a similar vein, the SEMIA project team drew inspiration from the ways in which data artists repurpose existing visual analysis tools. We did so with the specific goal of enabling a transition from searching to browsing large-scale moving image collections. This way, we not only hoped to significantly expand the range of available metadata, but also to allow for the revaluation of the images’ sensory dimensions in the very early stages of research. Ultimately, we think, both approaches to collection access can very well complement each other.</p><p>Our goal required that for the extraction of data, we adhered to the following general guidelines. In order to reduce the system’s reliance on <em>a priori</em> interpretations, we first of all sought to avoid direct human intervention in the actual extraction process. As a matter of principle, it should be up to the algorithm to determine similar, somewhat similar, or dissimilar – even if, as we argue elsewhere, algorithms ultimately always rely on knowledge that originates in humans (see <a href=#masson2019>Masson and van Noord [2019]</a>). Furthermore, we tweaked the algorithm to partially prevent it from recognising (human-taught) semantic units. Consequently, it could focus on similarities at a more abstract level. At this stage, some human intervention is ultimately unavoidable, as it is the computer scientist who decides (ideally on the basis of sample testing results) at which feature level the extraction takes place.</p><p>One conclusion that can be drawn from our review of most similar results is that extracting data with a minimum of labelling and human intervention, while also attending to intermediate similarities, never truly cancels out the detection of semantic relations and patterns altogether. In fact, this is hardly surprising, because this relation between low-level feature representations and objects – one that frames objects in terms of its facets; for instance, in the case of an orange, its colour and rounded shape – has been commonly exploited in early work on computer vision to detect semantic relations and objects. Therefore, some feature combinations are simply too distinctive to not be detected with our chosen approach – even if we do our best to block the algorithms’ semantic impulse.<sup id=fnref:47><a href=#fn:47 class=footnote-ref role=doc-noteref>47</a></sup> Yet our examples show that the analysis of query images also produces nearest neighbour matches that initially seem more illegible, and therefore, invite further exploration. In this sense, our working method does yield surprising results, or unexpected variations. In the remainder of our project, which we report on elsewhere, our intent has been to further stimulate users in exploring those less obvious connections by extending our interface with the capacity to also browse <em>dis</em> similar results.</p><p>The next step, which we expand on in an upcoming piece, is to assess which kinds of questions and ideas exploratory browsing through the lens of sensory features ultimately yields, and to evaluate how this furthers the efforts of various user groups <sup id=fnref:48><a href=#fn:48 class=footnote-ref role=doc-noteref>48</a></sup>. Throughout our research process, we have been wondering about the potential of such browsing for the purpose of what (social) scientists, and more recently also information and media scholars, have termed “serendipitous” discoveries <sup id=fnref:49><a href=#fn:49 class=footnote-ref role=doc-noteref>49</a></sup> <sup id=fnref:50><a href=#fn:50 class=footnote-ref role=doc-noteref>50</a></sup>. The literature uses this term for chance encounters with research objects that engender new ways of explaining or thinking about problems – both known ones, and problems one was perhaps previously unaware of.</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>Brown, K. “Wes Anderson’s Offbeat Debut as a Curator Drove a Storied Museum’s Staff Crazy: The Results Are Enchanting” . <em>Artnet News</em> (2018). Available at: <a href=https://news.artnet.com/exhibitions/wes-anderson-curator-kunsthistorisches-museum-1387429>https://news.artnet.com/exhibitions/wes-anderson-curator-kunsthistorisches-museum-1387429</a> (accessed 1 March 2020).&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p><a href=https://www.khm.at/en/visit/exhibitions/2019/wesandersonandjumanmalouf2018/>https://www.khm.at/en/visit/exhibitions/2019/wesandersonandjumanmalouf2018/</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>The exhibit ran from 5 November 2018 until 28 April 2019.&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p>Whitelaw, M. “Generous Interfaces for Digital Cultural Collections” , <em>Digital Humanities Quarterly</em> , 9.1 (2015). Available at: <a href=/dhqwords/vol/9/1/000205/>http://www.digitalhumanities.org/dhq/vol/9/1/000205/000205.html</a> (accessed 29 March 2020).&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5><p>Delpeut, P. <em>Diva dolorosa: Reis naar het einde van een eeuw.</em> Meulenhoff, Amsterdam (1999).&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:6><p>Dudley, S. (ed.). <em>Museum Materialities: Objects, Engagements, Interpretations.</em> Routledge, London (2010).&#160;<a href=#fnref:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:7><p><a href=http://sensorymovingimagearchive.humanities.uva.nl/>http://sensorymovingimagearchive.humanities.uva.nl/</a>&#160;<a href=#fnref:7 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:8><p>Flanders, J. “Rethinking Collections” . In P. Longley Arthur and K. Bode (eds.), <em>Advancing Digital Humanities: Research, Methods, Theories</em> , Palgrave Macmillan, Houndmills (2014), pp. 163-174.&#160;<a href=#fnref:8 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:9><p>For further exploration of the relation between searching and browsing, and the explorative affordances of browsing, see <a href=#masson2019>Masson (2019)</a>, or <a href=#masson>Masson and Olesen (2020)</a>.&#160;<a href=#fnref:9 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:10><p>Heftberger, A., Tsivian, Y., and Lepore, M. “Man with a Movie Camera (SU 1929) under the Lens of Cinemetrics” , <em>Maske und Kothurn</em> 55.3 (2009): 31-50. DOI: 10.7767/muk.2009.55.3.61.&#160;<a href=#fnref:10 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:11><p>Street S. <em>Colour Films in Britain: The Negotiation of Innovation 1900-1955</em> . BFI/Palgrave Macmillan, London (2012).&#160;<a href=#fnref:11 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:12><p>Street, S., and Yumibe, J. “The temporalities of intermediality: Colour in cinema and the arts of the 1920s” , <em>Early Popular Visual Culture</em> 11.2 (2013): 140-57. DOI: 10.1080/17460654.2013.783149.&#160;<a href=#fnref:12 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:13><p>Flückiger, B. “A Digital Humanities Approach to Film Colors” , <em>The Moving Image</em> , 17.2 (2017): 71–93.&#160;<a href=#fnref:13 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:14><p>Heftberger, A. <em>Digital Humanities and Film Studies: Visualising Dziga Vertov’s Work</em> . Springer, Cham (2018).&#160;<a href=#fnref:14 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:15><p>Mazzanti, M. “Colours, Audiences and (Dis)Continuity in the Cinema of the Second Period ” , <em>Film History</em> 21.1 (2009): 67-93.&#160;<a href=#fnref:15 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:16><p>Catanese, R., Scotto Lavina, F. and Valente, V. (eds.). <em>From Sensation to Synaesthesia in Film and New Media.</em> Cambridge Scholars Publishing, Cambridge (2019).&#160;<a href=#fnref:16 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:17><p>Funding was obtained within the SMART Culture scheme of the Netherlands Organisation for Scientific Research (NWO).&#160;<a href=#fnref:17 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:18><p>See <a href=https://www.openbeelden.nl/>https://www.openbeelden.nl/</a>. Of course, working with digitised versions of originally analogue moving images entails that some of their potentially significant material aspects are already ‘erased’ in a process that precedes the act of engaging with a collection. In the SEMIA project, we took this to be an inevitability.&#160;<a href=#fnref:18 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:19><p>In <a href=#masson2020>Masson and van Noord (2020)</a>, we elaborate on this history.&#160;<a href=#fnref:19 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:20><p><a href=https://www.ims.tuwien.ac.at/projects/digital-formalism>https://www.ims.tuwien.ac.at/projects/digital-formalism</a>&#160;<a href=#fnref:20 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:21><p><a href=https://hcommons.org/deposits/item/hc:12153/>https://hcommons.org/deposits/item/hc:12153/</a>&#160;<a href=#fnref:21 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:22><p>Zeppelzauer, M., Mitrović, D., and Breiteneder, C. “Archive Film Material – A Novel Challenge for Automated Film Analysis” , <em>Frames Cinema Journal</em> , 1.1 (2012). Available at <a href="http://www.framescinemajournal.com/article/archive-film-material-a-novel-challenge/?format=pdf">http://www.framescinemajournal.com/article/archive-film-material-a-novel-challenge/?format=pdf</a> (accessed 29 march 2020).&#160;<a href=#fnref:22 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:23><p>ACTION is short for Audio-visual Cinematic Toolbox for Interaction, Organization, and Navigation. <a href=http://digitalhumanities.dartmouth.edu/projects/the-action-toolbox/>http://digitalhumanities.dartmouth.edu/projects/the-action-toolbox/</a>&#160;<a href=#fnref:23 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:24><p>Casey, M., and Williams, M. “ACTION (Audio-visual Cinematic Toolbox for Interaction, Organization, and Navigation): an open-source Python platform” white paper, report ID 104081 (2013). Available at: <a href=https://hcommons.org/deposits/item/hc:12153/>https://hcommons.org/deposits/item/hc:12153/</a> (accessed 29 March 2020).&#160;<a href=#fnref:24 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:25><p>In this respect, the SEMIA project also differs in its intent from other initiatives that have been reported on since the writing of this piece; for instance, projects by the Distant Viewing Lab (<a href=https://www.distantviewing.org/>https://www.distantviewing.org/</a>) at the University of Richmond, reported on in <a href=#arnold2019>Arnold and Tilton (2019)</a> (focusing on narrative patterns and patterns in photographic style) or at the National Library of the Netherlands, by researchers-in-residence Melvin Wevers and Thomas Smits (on stylistic trends in newspaper visuals) <sup id=fnref:51><a href=#fn:51 class=footnote-ref role=doc-noteref>51</a></sup>.&#160;<a href=#fnref:25 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:26><p>Ferguson, K. L. “The Slices of Cinema: Digital Surrealism as Research Strategy” . In C. R. Acland and E. Hoyt (eds.), <em>The Arclight Guidebook to Media History and Digital Humanities</em> , REFRAME Books, Sussex (2016), pp. 270-299.&#160;<a href=#fnref:26 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:27><p>Ferguson, K. L. “Digital Surrealism: Visualizing Walt Disney Animation Studios” , <em>Digital Humanities Quarterly</em> , 11.1 (2017). Available at: <a href=/dhqwords/vol/11/1/000276/>http://www.digitalhumanities.org/dhq/vol/11/1/000276/000276.html</a> (accessed 29 March 2020).&#160;<a href=#fnref:27 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:27 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref2:27 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:28><p>Elsaesser, T. “Archives and Archaeology: The Place of Non-Fiction Film in Contemporary Media” . In V. Hediger and P. Vondereau (eds.), <em>Films That Work: Industrial Film and the Productivity of Media</em> , Amsterdam University Press, Amsterdam (2009), pp. 19-34.&#160;<a href=#fnref:28 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:29><p>Testa, B. <em>Back and Forth: Early Cinema and the Avant-Garde</em> . Art Gallery of Ontario, Ontario (1992).&#160;<a href=#fnref:29 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:30><p>Pigott, M. <em>Joseph Cornell Versus Cinema</em> . Bloomsbury Academic, London (2015).&#160;<a href=#fnref:30 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:31><p><a href=http://www.jimcampbell.tv/portfolio/still_image_works/illuminated_averages/index.html>http://www.jimcampbell.tv/portfolio/still_image_works/illuminated_averages/index.html</a>&#160;<a href=#fnref:31 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:32><p><a href=http://www.salavon.com/work/Top25/>http://www.salavon.com/work/Top25/</a>&#160;<a href=#fnref:32 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:33><p>Habib, A. <em>La Main gauche de Jean-Pierre Léaud</em> . Les Éditions du Boréal, Montréal (2015).&#160;<a href=#fnref:33 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:34><p><a href=http://www.brendandawes.com/projects/cinemaredux>http://www.brendandawes.com/projects/cinemaredux</a>&#160;<a href=#fnref:34 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:35><p>Mul, G., and Masson, E. “Data-Based Art, Algorithmic Poetry: Geert Mul in Conversation with Eef Masson” , <em>TMG – Journal for Media History</em> , 21.2 (2018). Available at: <a href=https://www.tmgonline.nl/articles/10.18146/2213-7653.2018.375/>https://www.tmgonline.nl/articles/10.18146/2213-7653.2018.375/</a> (accessed 29 March 2020).&#160;<a href=#fnref:35 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:36><p>The project’s full title is: <em>FilmColors: Bridging the Gap Between Technology and Aesthetics</em> . It runs until August of 2020. <a href=https://filmcolors.org/2015/06/15/erc/>https://filmcolors.org/2015/06/15/erc/</a>&#160;<a href=#fnref:36 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:37><p>Hitherto, it has primarily been successful when applied to restricted domain and toy problems.&#160;<a href=#fnref:37 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:38><p>Many thanks to Matthias Zeppelzauer (St. Poelten University of Applied Sciences) for helping us gain a better understanding of these conceptual distinctions. For more on how neural nets specifically understand images, see also <a href=#olah2017>Olah et al. (2017)</a>.&#160;<a href=#fnref:38 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:39><p>MacKenzie, A. <em>Machine Learners: Archaeology of a Data Practice</em> . MIT Press, Cambridge, MA (2017).&#160;<a href=#fnref:39 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:39 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:40><p>For example, Swain and Ballard, in the early 1990s, used colour information to identify and localise the position of objects <sup id=fnref:52><a href=#fn:52 class=footnote-ref role=doc-noteref>52</a></sup>.&#160;<a href=#fnref:40 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:41><p>Specifically, we used ResNet-101; for more information on its repurposing, see <a href=#he2016>He et al. (2016)</a>.&#160;<a href=#fnref:41 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:42><p>The layer we selected was the one located just below the fully connected layers, of 2048 dimensions.&#160;<a href=#fnref:42 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:43><p>With this approach, each colour dimension is described by 16 bins, resulting in a feature representation of 4096 dimensions.&#160;<a href=#fnref:43 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:44><p>For more information, see <a href=#rosenholtz2007>Rosenholtz et al. (2007)</a>.&#160;<a href=#fnref:44 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:45><p>This involved constructing a histogram, for which we separately binned the angle and magnitude for a three by three grid of non-overlapping spatial regions – an approach akin to the HOFM approach described in <a href=#colque2017>Colque et al. (2017)</a>.&#160;<a href=#fnref:45 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:46><p>The concept of nearest neighbour is also key to the <em>Neural Neighbours: Pictorial Tropes in the Meserve-Kunhardt Collection</em> project (<a href=https://dhlab.yale.edu/projects/neural-neighbors/>https://dhlab.yale.edu/projects/neural-neighbors/</a>) conducted by the Yale Digital Humanities Lab at the Yale Beinecke Rare Book & Manuscript Library. So far, however, this project has focused specifically on (originally) still images.&#160;<a href=#fnref:46 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:47><p>Exact matches rarely occur, because for the purposes of the project, the detection settings are tweaked in such a way that matches between images from the same videos are ruled out. (Therefore, only duplicate videos in the database can generate such results.)&#160;<a href=#fnref:47 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:48><p>Masson, E., and Olesen, C.G. “Digital Access as Archival Reconstitution: Algorithmic Sampling, Visualization, and the Production of Meaning in Large Moving Image Repositories” . <em>Signata: Annales des sémiotiques/Annals of Semiotics</em> , 12 (2020).&#160;<a href=#fnref:48 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:49><p>Van Andel, P. “Anatomy of the Unsought Finding. Serendipity: Origin, History, Domains, Traditions, Appearances, Patterns and Programmability” , <em>The British Journal for the Philosophy of Science</em> , 45.2 (1994): 631-48.&#160;<a href=#fnref:49 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:50><p>Foster, A. E., and Ellis, D. “Serendipity and its study” , <em>Journal of Documentation</em> , 70.6 (2014): 1015-38. DOI: 10.1108/00220410310472518.&#160;<a href=#fnref:50 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:51><p>Wevers, M. and Smits, T. “The visual digital turn: Using neural networks to study historical images” , <em>Digital Scholarship in the Humanities</em> , 35.1 (2020): 194-207. DOI: 10.1093/llc/fqy085.&#160;<a href=#fnref:51 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:52><p>Swain, M. J., and Ballard, D. H. “Color Indexing” , <em>International Journal of Computer Vision</em> , 7.1 (1991): 11–32. DOI: 10.1007/BF00130487.&#160;<a href=#fnref:52 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div></div></article></main><footer><nav aria-label="footer links"><ul><li><a class=highlight-focus href=/dhqwords/tags/>Tags</a></li><li><a class=highlight-focus href=/dhqwords/about/>About</a></li><li><a class=highlight-focus href=/dhqwords/categories/>Keywords</a></li></ul></nav><div class=icons><a class="license highlight-focus" rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons Attribution No-Derivatives 4.0 International License" src=/dhqwords/img/logos/license.svg width=120 height=42></a></div></footer></body></html>