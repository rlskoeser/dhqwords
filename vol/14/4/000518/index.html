<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=theme-color content="#3D206C"><meta http-equiv=x-ua-compatible content="IE=edge"><link rel=apple-touch-icon sizes=180x180 href=/img/favicon/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/img/favicon/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/img/favicon/favicon-16x16.png><link rel=icon type=image/png sizes=384x384 href=/img/favicon/android-chrome-384x384.png><link rel=icon type=image/png sizes=192x192 href=/img/favicon/android-chrome-192x192.png><link rel=icon type=image/png sizes=150x150 href=/img/favicon/mstile-150x150.png><link rel="shortcut icon" href=/favicon.ico><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/img/favicon/safari-pinned-tab.svg color=#3d206c><link rel=schema.dc href=http://purl.org/DC/elements/1.0/><meta name=citation_public_url content="https://rlskoeser.github.io/dhqwords/vol/14/4/000518/"><meta name=citation_title content="Automated Visual Content Analysis for Film Studies: Current Status and Challenges"><meta name=citation_date content="2020/12"><meta name=citation_author content="Kader Pustu-Iren"><meta name=citation_author content="Julian Sittel"><meta name=citation_author content="Roman Mauer"><meta name=citation_author content="Oksana Bulgakowa"><meta name=citation_author content="Ralph Ewerth"><meta name=citation_abstract content="1 Introduction In contrast to the field of computer-assisted research in the arts that has been established for several years 1 2 3 4, there is a need to catch up in scientific approaches to film (represented in the fields of New Film History and Stylometry). An important reason is the lack of practical software solutions available to date and the incompatibility of quantitative research designs with existing methodologies for film studies."><meta name=citation_journal_title content="DHQwords"><meta name=citation_issue content="14.4"><meta name=citation_publisher content="Center for Digital Humanities, Princeton University"><meta name=DC.rights content="http://creativecommons.org/licenses/by/4.0/"><meta name=author content="Kader Pustu-Iren, Julian Sittel, Roman Mauer, Oksana Bulgakowa, Ralph Ewerth"><meta name=generator content="Center for Digital Humanities, Princeton University"><meta name=dcterms.created content="2020-12"><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-regular.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-italic.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-regular.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-300.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-700.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-300italic.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-500italic.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-700italic.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-500.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/Source_Sans_Pro/SourceSans3VF-Roman.otf.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/Source_Sans_Pro/SourceSans3VF-Italic.otf.woff2 crossorigin><title>Automated Visual Content Analysis for Film Studies: Current Status and Challenges</title><meta name=description content="DHQwords Issue 14.4, December 2020. an experiment republishing DHQ articles with Hugo & Startwords theme. "><meta property="og:title" content="Automated Visual Content Analysis for Film Studies: Current Status and Challenges"><meta property="og:description" content="1 Introduction In contrast to the field of computer-assisted research in the arts that has been established for several years 1 2 3 4, there is a need to catch up in scientific approaches to film (represented in the fields of New Film History and Stylometry). An important reason is the lack of practical software solutions available to date and the incompatibility of quantitative research designs with existing methodologies for film studies."><meta property="og:type" content="article"><meta property="og:url" content="https://rlskoeser.github.io/dhqwords/vol/14/4/000518/"><meta property="og:image" content="https://rlskoeser.github.io/dhqwords/social.png"><meta property="article:section" content="vol"><meta property="article:published_time" content="2020-12-20T00:00:00+00:00"><meta property="article:modified_time" content="2020-12-20T00:00:00+00:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://rlskoeser.github.io/dhqwords/social.png"><meta name=twitter:title content="Automated Visual Content Analysis for Film Studies: Current Status and Challenges"><meta name=twitter:description content="1 Introduction In contrast to the field of computer-assisted research in the arts that has been established for several years 1 2 3 4, there is a need to catch up in scientific approaches to film (represented in the fields of New Film History and Stylometry). An important reason is the lack of practical software solutions available to date and the incompatibility of quantitative research designs with existing methodologies for film studies."><link rel=stylesheet href=/dhqwords/style.css><link rel=stylesheet href=/dhqwords/print.css media=print><script src=/dhqwords/js/polyfills.js></script><script defer src=/dhqwords/js/bundle.js></script></head><body class=article><header><nav class=main aria-label=main><ul><li class=home><a href=/dhqwords/>dhq</a></li><li class=issues><a href=/dhqwords/vol/>volumes</a></li></ul></nav></header><main><article><div class=grid><header><p class=number><a href=/dhqwords/vol/14/4/>Issue 14.4</a></p><p class=theme>Digital Humanities & Film Studies: Analyzing the Modalities of Moving Images</p><h1>Automated Visual Content Analysis for Film Studies: Current Status and Challenges</h1><p><ul class=authors><li><address>Kader Pustu-Iren</address></li><li><address>Julian Sittel</address></li><li><address>Roman Mauer</address></li><li><address>Oksana Bulgakowa</address></li><li><address>Ralph Ewerth</address></li></ul></p><p><time class=pubdate datetime=2020-12>December 2020</time></p><ul class="categories tags"><li><span class=tag>moving images</span></li><li><span class=tag>content analysis</span></li><li><span class=tag>tools</span></li><li><span class=tag>annotation</span></li></ul><ul class=tags></ul><p class=formats></p></header><section class=print-only><a class=first-page-header href=/ aria-label=Startwords><img alt=Startwords src=/pdf-logotype.svg></a>
<a class=page-header href=/ aria-label=Startwords><img alt=Startwords src=/pdf-logo.svg></a>
<a href=http://doi.org/ rel=alternate class=page-doi>doi:</a></section><div class=text-container><h2 id=1-introduction>1 Introduction</h2><p>In contrast to the field of computer-assisted research in the arts that has been established for several years <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> <sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup> <sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup> <sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup>, there is a need to catch up in scientific approaches to film (represented in the fields of New Film History and Stylometry). An important reason is the lack of practical software solutions available to date and the incompatibility of quantitative research designs with existing methodologies for film studies. In this context, some researchers criticise above all the appropriation of a technicistic, unrelated mission statement, which advocates of digital humanities apply to their own subject following other principles <sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup> <sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup>. However, more recent research <sup id=fnref:7><a href=#fn:7 class=footnote-ref role=doc-noteref>7</a></sup> <sup id=fnref:8><a href=#fn:8 class=footnote-ref role=doc-noteref>8</a></sup> has shown that qualitative and quantitative analysis are by no means mutually exclusive, but can be integrated in order to enrich film studies with new impulses.</p><p>The statistical film analysis developed by the physicist Salt thus holds the potential of a methodological guideline for quantifying filmic characteristics <sup id=fnref:9><a href=#fn:9 class=footnote-ref role=doc-noteref>9</a></sup> <sup id=fnref:10><a href=#fn:10 class=footnote-ref role=doc-noteref>10</a></sup>. This methodology focuses on quantifiable factors in the formal structure of a film such as camera shot length, which is considered an objective unit because it can be measured over time. The various forms of camera usage and movement (such as tracking shots, pans and tilts, camera distance, i.e., shot size) as well as other techniques (such as zoom-in and -out or the use of a camera crane) are also relevant for quantification. Casting this set of techniques as measuring instruments, it is possible to obtain data that scientists can relate to verifiable criteria in terms of film production history and to formulate hypotheses that allow conclusions to be drawn about the formal stylistic development of the selected films. To this end, Salt&rsquo;s concept allows for complete traceability of the measurement results and thus also of the numerical values to a theory set.</p><p>The concept was criticized for its reductionism <sup id=fnref:11><a href=#fn:11 class=footnote-ref role=doc-noteref>11</a></sup>, which prevents it from being connected to the qualitative research methods that dominate film studies. However, research in digital humanities has shown that quantitative parameters such as shot length are a suitable foundation for various analytical tools when it comes to the qualitative investigation of data (<a href=#tsivian2008>Tsivian 2008</a>; <a href=#buckland2009>Buckland 2009</a>; many others). In this way, quantitative research according to Salt makes it possible to validate stylistic changes in the work of emigrated European directors due to technical opportunities of the American studio system, or even to collect the average shot lengths of numerous productions from the beginning of film history to the present. Such research allows researchers to draw conclusions about the progressive acceleration of editing and thus provides information about the development of film technology and changes in our viewing habits. These questions concerning stylistic research in film studies cannot be examined without a corresponding quantitative research design, although Salt&rsquo;s concept remains too inflexible for broader application.</p><p>In this context, <a href=#korte2010>Korte&rsquo;s work (2010)</a> is regarded as pioneering (especially in German-speaking countries) in transferring quantitative methods into the framework of a qualitative analysis immanent in the work. An example for this is Rodenberg&rsquo;s analysis of Zabriskie Point (directed by Michelangelo Antonioni, 1970) in Korte&rsquo;s introduction to systematic film analysis (2010), which graphically depicts the stylistic structure of the film in an innovative way. Thus, for a detailed analysis Rodenberg visualises the adaption and alignment of editing rhythms to the characterisations of the persons in the narrative, and makes the alignment of music and narration comprehensible using diagrams. <a href=#heftberger2016>Heftberger (2016)</a>, for example, combines the analysis of historical sources such as editing diagrams and tables by the Russian director Vertov with computer-aided representations of digital humanities in order to provide insights into the filmmaker&rsquo;s working methods. Heftberger starts with copies of Vertov&rsquo;s films, which were analysed for structural features using the annotation software ANVIL <sup id=fnref:12><a href=#fn:12 class=footnote-ref role=doc-noteref>12</a></sup>, but also for dissolves or condition-related factors (e.g. markings in the film rolls or damage to the film). Within the framework of single and overall analyses, the data serve to elucidate the director’s intentions.</p><figure><img loading=lazy alt src=/dhqwords/vol/14/4/000518/resources/images/figure01.png sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/14/4/000518/resources/images/figure01_hue1a917cbb89061fe2982a498d4439d42_30703_500x0_resize_box_3.png 500w,
/dhqwords/vol/14/4/000518/resources/images/figure01_hue1a917cbb89061fe2982a498d4439d42_30703_800x0_resize_box_3.png 800w,/dhqwords/vol/14/4/000518/resources/images/figure01.png 788w" class=landscape><figcaption><p>Increase and decrease of shot lengths in Antonioni’s Blow Up (1966) using Videana.</p></figcaption></figure><p><a href=#sittel2016>Sittel (2016)</a> investigated the principle of increase and decrease of shot lengths in Michelangelo Antonioni’s <em>Blow Up</em> (1966). The visualization in Figure 1 was created during this study with the video analysis software Videana <sup id=fnref:13><a href=#fn:13 class=footnote-ref role=doc-noteref>13</a></sup> and gives an insight into this pattern. This technique, which is increasingly used in film editing, represents a structuring feature of the second half of the film and can be interpreted as a message with regard to the film content.</p><figure><img loading=lazy alt src=/dhqwords/vol/14/4/000518/resources/images/figure02.png sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/14/4/000518/resources/images/figure02_hu26151d25560a9ce7b0c3530e35377b84_148495_500x0_resize_box_3.png 500w,
/dhqwords/vol/14/4/000518/resources/images/figure02_hu26151d25560a9ce7b0c3530e35377b84_148495_800x0_resize_box_3.png 800w,/dhqwords/vol/14/4/000518/resources/images/figure02_hu26151d25560a9ce7b0c3530e35377b84_148495_1200x0_resize_box_3.png 1200w,/dhqwords/vol/14/4/000518/resources/images/figure02.png 1462w" class=landscape><figcaption><p>Average shot length of all Antonioni films in chronological order.</p></figcaption></figure><p>Using Salt&rsquo;s paradigm as a guideline, an analysis of all Antonioni films makes it possible to identify a clear change in style based on shot sizes and camera movements, which extends existing, non-qualitative approaches in a differentiated way. Figure 2 shows that the average shot length becomes shorter across nearly all films. To this end, a principle of systematic camera movement (longer shots) and thus an abandonment of montage gradually gives way to the increasing use of editing.</p><p>Research efforts like this benefit from automated computer-based methods that measure basic features of filmic structure. Similar to former work <sup id=fnref:14><a href=#fn:14 class=footnote-ref role=doc-noteref>14</a></sup>, we present a comprehensive survey of related software tools for video annotation, but particularly focus on methods for visual content analysis for film studies. First, we examine major software tools for video analysis with a focus on automated analysis algorithms and discuss their advantages and drawbacks. In addition, related work that applies automated video analysis to film studies is discussed. Moreover, we summarise current progress in computer vision and visual content analysis with a focus on deep learning methods. Besides, a comparison of machine vs. human performance in annotation tasks that are relevant for video content analysis is provided. Finally, we discuss future desirable functionalities for automated analysis in software tools for film studies.</p><p>The remainder of the paper is structured as follows. <a href=#section02>Section 2</a> reviews existing software tools and algorithms for quantitative approaches in film analysis. <a href=#section03>Section 3</a> discusses recent advancements in the field of video analysis and computer vision. A comparison of human and machine performance in different annotation tasks is provided in <a href=#section04>Section 4</a>. <a href=#section05>Section 5</a> describes requirements for software tools in scholarly film studies and outlines areas for future work.</p><h2 id=2-software-tools-and-algorithms-for-quantitative-film-studies>2 Software Tools and Algorithms for Quantitative Film Studies</h2><p>Researchers who want to utilise quantitative strategies to analyse film as presented in Figure 1 and 2 usually have to evaluate larger films or video corpora. However, existing software tools so far, with a few exceptions, require a high degree of manual annotation. As a consequence, many current film productions are difficult to evaluate due to ever shorter shot lengths. This section provides an overview of software solutions for film studies and their degree of automation in terms of capturing basic filmic features. While there are numerous existing annotation and video analysis tools, the focus lies on ready-to-use software applications most suitable for quantitative film studies. An overview of the functionalities provided by the selected applications and their current status of availability is presented in Table 1. We also distinguish between application areas in Table 1, since not all of the tools were originally proposed for scholarly film studies as targeted here.</p><p>Table 1. Overview of software applications for quantitative approaches in film analysis, characterised by their degree of automation regarding video content analysis tasks. While m denotes manual annotation, a refers to automated annotation.<br>Tool Application Area Availability Shot change detection Camera motion Video OCR Face detection Colour analysis Annotation level Visualisations Advene<br>Film studies/</p><p>teaching platform</p><p>Desktop App</p><p>(free)<br>a ROI<br>Timelines for shots &</p><p>annotation<br>ANVIL<br>Psycholinguistics/</p><p>social sciences</p><p>Desktop App</p><p>(free)<br>m Shot<br>Timelines for annotation tiers &</p><p>speech track<br>Cinemetrics<br>Film studies<br>Web-based crowd-sourcing platform m Shot Cut frequency diagram ELAN<br>Psycholinguistics/</p><p>social sciences</p><p>Desktop App</p><p>(free)<br>m Shot<br>Timelines for annotation tiers &</p><p>speech track segments<br>Ligne de Temps Film studies<br>Desktop App</p><p>(free)<br>a m Shot Timeline for cuts Media-thread Teaching platform<br>Web App</p><p>(source code available)<br>ROI Hyper video annotations VIAN<br>Film studies</p><p>(colour analysis)</p><p>Desktop App<sup id=fnref:15><a href=#fn:15 class=footnote-ref role=doc-noteref>15</a></sup></p><p>(not publicly available yet)<br>a a¹ a ROI Timelines for shots & annotations, colour schemes view, screenshot manager Videana<br>Media/</p><p>film studies</p><p>Desktop App</p><p>(on request until 2012)<br>a a a a a ROI<br>Timelines of detections annotations & cuts,</p><p>cut frequency diagram, shot list</p><h2 id=21-cinemetrics>2.1 Cinemetrics</h2><p>In the Cinemetrics project <sup id=fnref:16><a href=#fn:16 class=footnote-ref role=doc-noteref>16</a></sup>, Yuri and Gunnar Tsivian took up Salt’s methodology and used it for the first time as conceptual guidelines for the implementation of digital analytical instruments, which are freely available as a Web-based platform since 2005.<sup id=fnref:17><a href=#fn:17 class=footnote-ref role=doc-noteref>17</a></sup> The tool allows the user to synchronously view and annotate video material, or to systematically comb through AVI video files. In the advanced mode, customized annotation criteria can be defined in addition to basic features (e.g. frequency and length of shots). The data sets obtained are then collected within an online database according to the principle of crowdsourcing. With metadata for more than 50,000 films to date, it acts as a comprehensive research data archive that documents the development and history of film style. Cinemetrics is the only platform based on Web 2.0 principles that consistently aggregates film data and makes it publicly accessible. However, the analysis of film data is not systematic. Moreover, Cinemetrics relies exclusively on the manual acquisition of data such as shot changes, camera distances, or camera movements. The accurate evaluation of video material such as feature films therefore requires an effort of several hours and is unsuitable for broader studies. Nonetheless, the platform enjoys a high level of visitor traffic. Between 2010 and 2013, the number of users almost doubled (2010: 4,500 clicks per day, 2013: 8,326). The program is regularly used in seminars at the Universities of Chicago, Amsterdam, Taiwan and at the Johannes Gutenberg University of Mainz.</p><h2 id=22-anvil-elan>2.2 ANVIL, ELAN</h2><p>ANVIL <sup id=fnref:18><a href=#fn:18 class=footnote-ref role=doc-noteref>18</a></sup> <sup id=fnref1:12><a href=#fn:12 class=footnote-ref role=doc-noteref>12</a></sup> and ELAN (EUDICO Linguistic Annotator)<sup id=fnref:19><a href=#fn:19 class=footnote-ref role=doc-noteref>19</a></sup> <sup id=fnref:20><a href=#fn:20 class=footnote-ref role=doc-noteref>20</a></sup> are visual annotation tools, which were originally designed for psycholinguistics and gesture research. They are suitable for the differentiated graphical representation of editing rhythms or for the annotation of previously defined structural features at the shot and sequence level, though it usually requires the export of the data to another statistical software or Microsoft Excel. In contrast to Cinemetrics, ANVIL and ELAN allow the user to directly interact with the video material. They offer a much greater methodological scope, especially through the option of adding several feature dimensions to the video material in the form of tracks. Both systems work according to a similar principle, to which the video segments or the collected shots can be viewed and annotated with metadata. Annotations can also be arranged hierarchically by means of multiple layers which are called tiers. Thus, annotations can be cross-referenced to other annotations or to corresponding shots or video segments, making the programs particularly suitable for a fine-grained analysis of the structure of individual works. However, if several parameters are to be recorded during an evaluation run, repeated viewing of a film is required in order to label the individual tracks with the respective characteristics and features.</p><figure><img loading=lazy alt src=/dhqwords/vol/14/4/000518/resources/images/figure03.png sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/14/4/000518/resources/images/figure03_huf595d6ac9cd6a0cdf2a22783ef1743f9_114309_500x0_resize_box_3.png 500w,
/dhqwords/vol/14/4/000518/resources/images/figure03_huf595d6ac9cd6a0cdf2a22783ef1743f9_114309_800x0_resize_box_3.png 800w,/dhqwords/vol/14/4/000518/resources/images/figure03.png 430w" class=landscape><figcaption><p>Screenshot of ANVIL software.</p></figcaption></figure><figure><img loading=lazy alt src=/dhqwords/vol/14/4/000518/resources/images/figure04.png sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/14/4/000518/resources/images/figure04_hu06e6e2064539e1a85ddd3ad07ea04b42_268959_500x0_resize_box_3.png 500w,
/dhqwords/vol/14/4/000518/resources/images/figure04_hu06e6e2064539e1a85ddd3ad07ea04b42_268959_800x0_resize_box_3.png 800w,/dhqwords/vol/14/4/000518/resources/images/figure04.png 869w" class=landscape><figcaption><p>Screenshot of ELAN software.</p></figcaption></figure><h2 id=23-ligne-de-temps>2.3 Ligne de temps</h2><p>The analysis tool Ligne de temps<sup id=fnref:21><a href=#fn:21 class=footnote-ref role=doc-noteref>21</a></sup> was developed between 2007 and 2011 by the Institut de recherche et d’innovation du Centre Pompidou. The tool provides a graphical timeline-based representation of the material and allows for selecting temporal segments in order to annotate different types of modality (video, audio, text) of the corresponding sequence in the movie, or add information in the form of images or external links. Moreover, it is possible to generate colour-coded annotations aligned with the shots by choosing from a range of available RGB colour values. This function can be implicitly used for colour analysis. However, a single colour cannot represent a holistic image or even an entire shot. Ligne de temps enables the automated detection of shot boundaries in the video, as only few of the presented tools do. But there is no information available about the algorithm used and its performance on benchmark data sets.</p><figure><img loading=lazy alt src=/dhqwords/vol/14/4/000518/resources/images/figure05.png sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/14/4/000518/resources/images/figure05_hu89afbebbdca6d2181624038cb14f1ffd_191527_500x0_resize_box_3.png 500w,
/dhqwords/vol/14/4/000518/resources/images/figure05_hu89afbebbdca6d2181624038cb14f1ffd_191527_800x0_resize_box_3.png 800w,/dhqwords/vol/14/4/000518/resources/images/figure05.png 995w" class=landscape><figcaption><p>Screenshot of Ligne de Temps.</p></figcaption></figure><h2 id=24-advene-mediathread>2.4 Advene, Mediathread</h2><p>Advene (Annotate Digital Video, Exchange on the NEt) <sup id=fnref:22><a href=#fn:22 class=footnote-ref role=doc-noteref>22</a></sup> is an ongoing project for the annotation of digital videos that also provides a format to share annotations. The tool has been developed since 2002 and is freely available as a cross-platform desktop application.<sup id=fnref:23><a href=#fn:23 class=footnote-ref role=doc-noteref>23</a></sup> It provides a broader number of functionalities compared with the former tools. In particular, the tool enables textual as well as graphical annotations to augment the video and also provides automatically generated thumbnails for each shot. Moreover, it is possible to edit and visualise hypervideos consisting of both the annotations and the video. Target groups are film scholars, teachers and students who want to exchange multimedia comments and analyses about videos such as movies, conferences, or courses. The tool has also been used for reflexive interviews of museum visitors, or with regard to on-demand video providers and movie-related social networks. However, the automatic analysis options that Advene provides are restricted to shot boundary detection and temporal segmentation of audio tracks.</p><p>Mediathread<sup id=fnref:24><a href=#fn:24 class=footnote-ref role=doc-noteref>24</a></sup> was developed by Columbia University’s Center for Teaching and Learning (CTL)<sup id=fnref:25><a href=#fn:25 class=footnote-ref role=doc-noteref>25</a></sup> and first launched in 2010. It is a web application for multimedia annotations enabling collaboration on video and image analysis. Similar to Advene, Mediathread primarily serves as a platform for collaboratively working and sharing annotations for multimedia and is therefore actively used in classroom environments at various universities, also including courses on film studies. However, it does not provide automated analysis capabilities such as shot detection. In addition, film studies researches wanting to use the web application need a certain degree of expertise to individually deploy the openly available source code.</p><figure><img loading=lazy alt src=/dhqwords/vol/14/4/000518/resources/images/figure06.png sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/14/4/000518/resources/images/figure06_hu05bd5eb5983d6a57804af69b01322ef6_343195_500x0_resize_box_3.png 500w,
/dhqwords/vol/14/4/000518/resources/images/figure06_hu05bd5eb5983d6a57804af69b01322ef6_343195_800x0_resize_box_3.png 800w,/dhqwords/vol/14/4/000518/resources/images/figure06.png 907w" class=landscape><figcaption><p>Screenshot of Advene.</p></figcaption></figure><h2 id=25-videana>2.5 Videana</h2><p>The video analysis software Videana was developed at the University of Marburg <sup id=fnref1:13><a href=#fn:13 class=footnote-ref role=doc-noteref>13</a></sup>. Videana is one of the few software tools to offer more than simple analysis functions such as the detection of shot changes <sup id=fnref:26><a href=#fn:26 class=footnote-ref role=doc-noteref>26</a></sup> <sup id=fnref:27><a href=#fn:27 class=footnote-ref role=doc-noteref>27</a></sup>. For example, the software integrates algorithms for text detection <sup id=fnref:28><a href=#fn:28 class=footnote-ref role=doc-noteref>28</a></sup>, video OCR <sup id=fnref:29><a href=#fn:29 class=footnote-ref role=doc-noteref>29</a></sup>, estimation of camera motion <sup id=fnref:30><a href=#fn:30 class=footnote-ref role=doc-noteref>30</a></sup> and of object motion <sup id=fnref:31><a href=#fn:31 class=footnote-ref role=doc-noteref>31</a></sup>, and face detection <sup id=fnref:32><a href=#fn:32 class=footnote-ref role=doc-noteref>32</a></sup>. Further functionalities, which are however not part of the standard version, are the recognition of dominant colour values, person recognition and indexing <sup id=fnref:33><a href=#fn:33 class=footnote-ref role=doc-noteref>33</a></sup>, or the temporal segmentation of the soundtrack. The many available features and the possibility to combine them allow for the flexible formulation of complex research hypotheses and their empirical verification. For example, Videana was used for a media study to investigate user behaviour in Google Earth Tours <sup id=fnref:34><a href=#fn:34 class=footnote-ref role=doc-noteref>34</a></sup> <sup id=fnref:35><a href=#fn:35 class=footnote-ref role=doc-noteref>35</a></sup>. However, the software has not been updated since 2012 and therefore does not rely on current state-of-the-art methods in video analysis. Another drawback of the tool is the lack of enhanced visualizations that go beyond simple cut frequency diagrams and event timelines. Finally, the software is not usable through a Web browser, but only available as a desktop software.</p><figure><img loading=lazy alt src=/dhqwords/vol/14/4/000518/resources/images/figure07.png sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/14/4/000518/resources/images/figure07_hu9772a8830d82037af5a1a9bd1298498e_668527_500x0_resize_box_3.png 500w,
/dhqwords/vol/14/4/000518/resources/images/figure07_hu9772a8830d82037af5a1a9bd1298498e_668527_800x0_resize_box_3.png 800w,/dhqwords/vol/14/4/000518/resources/images/figure07.png 808w" class=landscape><figcaption><p>Screenshot of the main window of Videana (provided in <a href=#ewerth2009>Ewerth et al. 2009</a>).</p></figcaption></figure><h2 id=26-vian>2.6 VIAN</h2><p>Within the project “Film Colors - Bridging the Gap Between Technology and Aesthetics” <sup id=fnref:36><a href=#fn:36 class=footnote-ref role=doc-noteref>36</a></sup> at the University of Zurich, the tool VIAN <sup id=fnref:37><a href=#fn:37 class=footnote-ref role=doc-noteref>37</a></sup> for video analysis and annotation is currently being developed with a focus on film colour patterns. In comparison to general-purpose annotation tools like ELAN, VIAN particularly addresses aesthetic analyses of full feature films. The tool is also planned to allow a variety of (semi)-automatic tools for the analysis and visualisation of film colours based on computer vision and deep learning methods such as figure-ground separation and extraction of the corresponding colour schemes. Although the tool is not released yet (announced to be open source), VIAN seems to be a very promising tool with regard to state-of-the-art visual content analysis methods.</p><p>Figure 8. Screenshot of VIAN temporal segmentation and screenshot manager.</p><h2 id=27-other-tools-and-approaches>2.7 Other Tools and Approaches</h2><p>There are some other tools that offer video and film analysis and (to a lesser extent) provide functions similar to the previously introduced applications for automatic film analysis. The Semantic Annotation Tool (SAT) is launched in the context of the Media Ecology Project (MEP) (<a href=http://mediaecology.dartmouth.edu/sat/>http://mediaecology.dartmouth.edu/sat/</a>). It allows for the annotation and sharing of videos on the Web in free-text form, by a controlled set of tags, or polygonal regions in a frame. It targets classroom as well as research environments. The tool itself does not provide (integrated) quantitative measures. However, it can be fed with external machine-generated metadata (by automated video analysis methods). The Distant Viewing Toolkit<sup id=fnref:38><a href=#fn:38 class=footnote-ref role=doc-noteref>38</a></sup> is a python package that provides computational analysis and visualisation methods for video collections. The software extracts and visualises semantic metadata from videos using standard computer vision methods as well as exploring more high-level patterns such as screen time per character. Another project is eFilms<sup id=fnref:39><a href=#fn:39 class=footnote-ref role=doc-noteref>39</a></sup> that provides a web-based film player that is supplemented with contextual meta information such as date, geolocation or visual events for the footage. Its main use case is the contextualization of historical footage of the Nazi era. The project also provides a film annotation editor to insert the context annotations. However, as for the aforementioned projects as well, the source code needs to be deployed first and thus is no off-the-shelf and ready-to-use solution for film scholars. Furthermore, the SAT and eFilms tools only offer manual annotation.</p><p>Many other works exist that deal with analysis and visualisation of certain filmic aspects. Some early works by Adams et al. transfer concepts of film grammar to computational measures for automated film analysis. <a href=#adams2000>Adams and Dorai (2000)</a> introduce a computational measure of movie tempo based on its filmic definition and use it to automatically extract dramatic sections and events from film. Furthermore, <a href=#adams2001>Adams et al. (2001)</a> present a computational model for extracting film rhythm by deriving classes for different motion characteristics in order to identify narrative structures and dramatic progression. Another work deals with the extraction of narrative act boundaries, in particular the 3-act-story telling paradigm using a probabilistic model <sup id=fnref:40><a href=#fn:40 class=footnote-ref role=doc-noteref>40</a></sup>.</p><p><a href=#pause2016>Pause and Walkowski (2016)</a> address the characterization of dominant colours in film and discuss the limitations of the k-means clustering approach. Furthermore, they propose to proceed according to Itten&rsquo;s method of seven colour contrasts <a href=#pause1961>(1961)</a> and outline how it can be implemented algorithmically. <a href=#burghardt2016>Burghardt et al. (2016)</a> present a system that automatically extracts colour and language information using k-means clustering as well as subtitles and propose an interactive visualisation. <a href=#hoyt2014>Hoyt et al. (2014)</a> propose a tool to visualise the relationships between characters in a movie. <a href=#john2017>John et al. (2017)</a> present a visual analytics approach (for the analysis of single or a set of videos) that combines automatic data analysis and visualisation. The interface supports the representation of so-called semantic frames, simple keywords, hierarchical annotations, as well as keywords for categories and similarities. Hohman et al (2017) propose a method to explore individual videos (entertainment, series) on the basis of colour information (dominant colour, etc.) as well as texts from dialogues and evaluate the approach on the basis of two use cases for the series <em>Game of Thrones</em> .</p><p><a href=#tseng2013b>Tseng (2013b)</a> provides an analysis of plot cohesion in film by tracking film elements such as characters, objects, settings, and character action. Furthermore, <a href=#tseng2013a>Tseng (2013a)</a> distinguishes basic narrative types in visual images by interconnecting salient people, objects and settings within single and across sequential images. <a href=#bateman2014>Bateman (2014)</a> reviews empirical, quantitative approaches to the analysis of films and, moreover, suggests employing discourse semantics for more systematic interpretative analysis in order to overcome the difficulty of relating particular technical film features to qualitative interpretations.</p><h2 id=3-current-developments-in-computer-vision-and-video-analysis>3 Current Developments in Computer Vision and Video Analysis</h2><p>Apart from analysing film style (shot and scene segmentation, use of camera motion and colours, etc.), film studies are also concerned with question(s) such as: Who (or what) did what, when, and where?. To answer such questions, algorithms for recognising persons, location and time of shots, etc. are required. There are some state-of-the-art approaches that basically target these questions (e.g., visual concept detection, geolocation and date estimation of images) and might be applicable to film style and content analysis – at least in future work.</p><p>A central question in computer vision approaches is the choice of a feature representation for the visual data. Classical approaches rely on hand-crafted feature descriptors. While global descriptors like Histogram of Oriented Gradients (HOG) are considered to represent an image holistically, descriptors based on Scale Invariant Feature Transform (SIFT) <sup id=fnref:41><a href=#fn:41 class=footnote-ref role=doc-noteref>41</a></sup> or Speeded Up Robust Features (SURF) <sup id=fnref:42><a href=#fn:42 class=footnote-ref role=doc-noteref>42</a></sup> have proven to be particularly suitable for local features since they are invariant to coordinate transformations, and robust to noise as well as to illumination changes. With the emergence of deep learning, however, feature representations based on convolutional neural networks (CNNs) largely replaced hand-crafted low-level features. Nowadays, CNNs and deep features are the state-of-the-art for many computer vision tasks <sup id=fnref:43><a href=#fn:43 class=footnote-ref role=doc-noteref>43</a></sup> <sup id=fnref:44><a href=#fn:44 class=footnote-ref role=doc-noteref>44</a></sup> <sup id=fnref:45><a href=#fn:45 class=footnote-ref role=doc-noteref>45</a></sup>.</p><h2 id=31-shot-boundary-detection>3.1 Shot Boundary Detection</h2><p>Shot boundary detection (SBD) is an essential prerequisite for video processing and for video content analysis tasks. Typical techniques in SBD <sup id=fnref:46><a href=#fn:46 class=footnote-ref role=doc-noteref>46</a></sup> <sup id=fnref:47><a href=#fn:47 class=footnote-ref role=doc-noteref>47</a></sup> <sup id=fnref:48><a href=#fn:48 class=footnote-ref role=doc-noteref>48</a></sup> rely on low-level features, consisting of global or local frame features that are used to measure the distance between consecutive frames. One notable approach <sup id=fnref:49><a href=#fn:49 class=footnote-ref role=doc-noteref>49</a></sup> utilises both colour histograms and SURF descriptors <sup id=fnref1:42><a href=#fn:42 class=footnote-ref role=doc-noteref>42</a></sup> along with GPU acceleration to identify abrupt and gradual transitions in real time. This approach achieves a F1 accuracy score of 0.902 on a collection of 15 videos gathered from different video archives while being 3x faster than real-time processing on GPU. In order to enhance detection, especially for more subtle gradual transitions, several CNN-based proposals have been introduced. They extract and employ representative deep features for frames <sup id=fnref:50><a href=#fn:50 class=footnote-ref role=doc-noteref>50</a></sup> or train networks that detect shot boundaries directly <sup id=fnref:51><a href=#fn:51 class=footnote-ref role=doc-noteref>51</a></sup>. Xu et al. conduct experiments on TRECVID 2001 test data and achieve F1 scores of 0.988 and 0.968 for cut and gradual transitions, respectively. <a href=#gygli2018>Gygli (2018)</a> reports a F1 score of 0.88 on the RAI dataset <sup id=fnref:52><a href=#fn:52 class=footnote-ref role=doc-noteref>52</a></sup> outperforming previous work, while being extremely fast (more than 120x real-time on GPU).</p><h2 id=32-scene-detection>3.2 Scene Detection</h2><p>Given the shots, it is often desirable to segment a broadcast video into higher level scenes. A scene is considered as a sequence of shots, which are related in a spatio-temporal manner. For this purpose, <a href=#baraldi2015b>Baraldi et al. (2015b)</a> detect superordinate scenes describing shots by means of colour histograms and subsequently apply a hierarchical clustering approach. The approach is applied to a collection of ten randomly selected broadcasting videos from the RAI Scuola video archive constituting the RAI dataset. The method achieves a F1 score of 0.70 at 7.7x real-time on CPU. <a href=#sidiropoulos2011>Sidiropoulos et al. (2011)</a> suggest an alternative approach, where shots constitute nodes in a graph representation and edges between shots are weighted by shot similarity. Exploiting this representation, scenes can be determined by partitioning the graph. On a test set of six movies as well as on a set of 15 documentary films, the approach obtains a F1 accuracy of 0.869 and 0.890 (F1 score on RAI dataset of <a href=#baraldi2015b>Baraldi et al. 2015b</a>: 0.54), respectively. Another solution <sup id=fnref:53><a href=#fn:53 class=footnote-ref role=doc-noteref>53</a></sup> is to apply a multimodal deep neural network to learn a metric for rating the difference between pairs of shots utilizing both visual and textual features from the transcript. Similarity scores of shots are used to segment the video into scenes. Beraldi et al. evaluate the approach on 11 episodes from the BBC educational TV series <em>Planet Earth</em> and report a F1 score of 0.62.</p><h2 id=33-camera-motion-estimation>3.3 Camera Motion Estimation</h2><p>Camera motion is considered as a significant element in film production. Thus, estimating the types of camera motion can be helpful in breaking down a video sequence into shots or for motion analysis of objects. Some techniques for camera motion estimation perform direct optical flow computation <sup id=fnref:54><a href=#fn:54 class=footnote-ref role=doc-noteref>54</a></sup>, while others consider motion vectors that are available in compressed video files <sup id=fnref1:30><a href=#fn:30 class=footnote-ref role=doc-noteref>30</a></sup>. On four short movie sequences Nguyen et al. individually obtain 94.04% to 98.26% precision (percentage of correct detections). The latter approach is evaluated on a video test set consisting of 32 video sequences including all kinds of motion types. It detects zooming with 99%, tilting (vertical camera movement) with 93% and panning (horizontal camera movement) with 79% precision among other motion types. This approach achieved the best results in the task of zoom detection at TRECVID 2005. More recent works in this field couple the camera motion problem with similar tasks in order to train neural networks in a joint unsupervised framework <sup id=fnref:55><a href=#fn:55 class=footnote-ref role=doc-noteref>55</a></sup> <sup id=fnref:56><a href=#fn:56 class=footnote-ref role=doc-noteref>56</a></sup> <sup id=fnref:57><a href=#fn:57 class=footnote-ref role=doc-noteref>57</a></sup>.</p><h2 id=34-object-detection-and-visual-concept-classification>3.4 Object Detection and Visual Concept Classification</h2><p>Object detection is the task of localising and classifying objects in an image. Motivated by the first application of CNNs to object classification <sup id=fnref:58><a href=#fn:58 class=footnote-ref role=doc-noteref>58</a></sup>, regions with CNN features (R-CNN) were introduced in order to (localize and) classify objects based on region proposals <sup id=fnref:59><a href=#fn:59 class=footnote-ref role=doc-noteref>59</a></sup>. However, since this approach is computationally very expensive, several improvements have been proposed. Fast-R-CNNs <sup id=fnref:60><a href=#fn:60 class=footnote-ref role=doc-noteref>60</a></sup> were designed to jointly train feature extraction, classification and bounding box regression in an unified network. Additionally integrating a region proposal subnetwork enabled Faster-R-CNNs <sup id=fnref:61><a href=#fn:61 class=footnote-ref role=doc-noteref>61</a></sup> to significantly speed up the formerly separate process of generating regions of interest. Thus, Faster-R-CNNs achieve an accuracy of 42.7 mAP (mean Average Precision) at a frame rate of 5 fps (frames per second) on the challenging MS COCO detection dataset <sup id=fnref:62><a href=#fn:62 class=footnote-ref role=doc-noteref>62</a></sup> (MSCOCO: Microsoft Common Objects in Context). Furthermore, mask R-CNNs <sup id=fnref:63><a href=#fn:63 class=footnote-ref role=doc-noteref>63</a></sup>extend the Faster-R-CNN approach for pixel-level segmentation of object instances by predicting object masks. Running at 5 fps, this approach predicts object boxes as well as segments with an accuracy of 60.3 mAP and 58.0 mAP on the COCO dataset. In contrast to region proposal based methods, <a href=#redmon2016>Redmon et al. (2016)</a> introduced YOLO,  a single shot object detector that predicts bounding boxes and associated object classes based on a fixed-grid regression. While YOLO is very fast in terms of inference time, further extensions <sup id=fnref:64><a href=#fn:64 class=footnote-ref role=doc-noteref>64</a></sup> <sup id=fnref:65><a href=#fn:65 class=footnote-ref role=doc-noteref>65</a></sup> employ anchor boxes and make several improvements on network design also boosting the overall detection accuracy to 57.9 mAP at 20 fps on the COCO dataset. In order to detect a wide variety of over 9000 object categories, <a href=#redmon2017>Redmon and Farhadi (2017)</a> also introduced the real-time system YOLO9000, which was simultaneously trained on the COCO detection dataset as well as the ImageNet classification dataset <sup id=fnref:66><a href=#fn:66 class=footnote-ref role=doc-noteref>66</a></sup>. Apart from detecting objects, there were also many approaches and more importantly datasets introduced for classifying images into concepts. The SUN database <sup id=fnref:67><a href=#fn:67 class=footnote-ref role=doc-noteref>67</a></sup> provides up to 5400 categories of objects and scenes. Current image concept classification approaches typically rely on deep models trained with state-of-the-art architectures <sup id=fnref:68><a href=#fn:68 class=footnote-ref role=doc-noteref>68</a></sup> <sup id=fnref:69><a href=#fn:69 class=footnote-ref role=doc-noteref>69</a></sup> <sup id=fnref:70><a href=#fn:70 class=footnote-ref role=doc-noteref>70</a></sup>.</p><h2 id=35-face-recognition-and-person-identification>3.5 Face Recognition and Person Identification</h2><p>Motivated by the significant progress in object classification, deep learning methods have also been applied to face recognition. In this context, DeepFace <sup id=fnref:71><a href=#fn:71 class=footnote-ref role=doc-noteref>71</a></sup> is one of the first approaches that is trained to obtain deep features for face verification and, moreover, enhances face alignment based on explicit 3D modelling of faces. This approach achieves an accuracy of 97.35% on the prominent as well as challenging Labeled Faces in the Wild (LFW) benchmark set. While DeepFace uses a cross-entropy loss (cost function) for feature learning, <a href=#schroff2015>Schroff et al. (2015)</a> introduced with FaceNet a novel and more sophisticated loss based on training with triplets of roughly aligned matching and nonmatching face patches. On the LFW benchmark, FaceNet obtains an accuracy of 99.63%.</p><p>In the context of broadcast videos, the task of detecting faces and clustering them for person indexing of frames or shots has been widely studied (e.g., <a href=#ewerth2007b>Ewerth et al. 2007b</a>). While <a href=#muller2016>Müller et al. (2016)</a> present a semi-supervised system for automatically naming characters in TV broadcasts by extending and correcting weakly labelled training data, <a href=#jin2017>Jin et al. (2017)</a> both detect faces and cluster them by identity in full-length movies. For content-based video retrieval in broadcast videos face recognition as well as concept detection based on deep learning have also been proven to be effective <sup id=fnref:72><a href=#fn:72 class=footnote-ref role=doc-noteref>72</a></sup>.</p><h2 id=36-recognition-of-places-and-geolocation>3.6 Recognition of Places and Geolocation</h2><p>Recognising a place in a frame or shot might yield also useful information for film studies. The Places database contains over 400 unique place categories for scene recognition. Along with the dataset, <a href=#zhou2014>Zhou et al. (2014;</a> <a href=#zhou2018>2018</a>) provide CNN models trained with various architectures on the Places365 dataset. Using the ResNet architecture <sup id=fnref1:68><a href=#fn:68 class=footnote-ref role=doc-noteref>68</a></sup>, for example, a top-5 accuracy of 85.1% can be obtained on the Places365 validation set. <a href=#mallya2018>Mallya and Lazebnik (2018)</a> introduce PackNet for training multiple tasks in a single model by pruning redundant parameters. Thus, the network is trained on classes of the ImageNet as well as the Places365 dataset. Being trained on multiple tasks, the model yields a top-5 classification error of 15.6% for the Places365 classes on the validation set, while the individually trained network by <a href=#zhou2018>Zhou et al. (2018)</a> shows a top-5 error rate of 16.1%. <a href=#hu2018>Hu et al. (2018)</a> introduce a novel CNN architecture unit called SE block, which enables a network to use global information to selectively emphasise informative features and suppress less useful ones by performing dynamic channel-wise feature recalibration. This approach was trained on the Places365 dataset as well and shows a top-5 error rate of 11.0% on the corresponding validation set.</p><p>For the task of earth scale photo geolocation two major directions have been taken so far. Im2GPS, one fundamental proposal for photo geolocation estimation, infers GPS coordinates by matching the query image against a reference database of geotagged images <sup id=fnref:73><a href=#fn:73 class=footnote-ref role=doc-noteref>73</a></sup> <sup id=fnref:74><a href=#fn:74 class=footnote-ref role=doc-noteref>74</a></sup> and was recently enhanced by incorporating deep learning features <sup id=fnref:75><a href=#fn:75 class=footnote-ref role=doc-noteref>75</a></sup>. In this context, the Im2GPS test set consisting of 237 challenging photos (only 5% are depicting touristic sites) was introduced. The latest deep feature based Im2GPS version <sup id=fnref1:75><a href=#fn:75 class=footnote-ref role=doc-noteref>75</a></sup> achieves an accuracy of 47.7% at region scale (location error less than 200 km). Other major proposals cast the task as a CNN-based classification approach by partitioning the earth into geographical cells <sup id=fnref:76><a href=#fn:76 class=footnote-ref role=doc-noteref>76</a></sup> and considering combinatorial partitioning of maps, which facilitate more accurate and robust class predictions <sup id=fnref:77><a href=#fn:77 class=footnote-ref role=doc-noteref>77</a></sup>. These frameworks called PlaNet and CPlaNet achieve 37.6% and 42.6% accuracy at region scale on the benchmark, respectively. Current state-of-the-art results (51.9% accuracy at region scale) are achieved by similarly combining hierarchical map partitions and additionally distinguishing three different settings (indoor, urban, and rural) through automatic scene recognition <sup id=fnref:78><a href=#fn:78 class=footnote-ref role=doc-noteref>78</a></sup>.</p><h2 id=37-image-date-estimation>3.7 Image Date Estimation</h2><p>While unrestricted photo geolocation estimation is well covered by several studies, the problem of estimating the date of arbitrary (historical) photos was addressed less frequently in the past. The first unrestricted work in this context <sup id=fnref:79><a href=#fn:79 class=footnote-ref role=doc-noteref>79</a></sup> dates historical colour images from 1930 to 1980 utilizing colour descriptors that model the evolution of colour imaging processes over time. Thus, Palermo et al. report an overall accuracy of 45.7% on a set of 1375 Flickr images which are uniformly distributed across the considered decades. A recent deep learning approach <sup id=fnref:80><a href=#fn:80 class=footnote-ref role=doc-noteref>80</a></sup> dates images from 1930 to 1999 considering the task either as a classification or a regression problem. Müller et al. introduce the Date Estimation in the Wild test set consisting of 1120 Flickr images, which cover every year uniformly, and report a mean estimation error of less than 8 years for both the classification and regression models.</p><h2 id=4-human-and-machine-performance-in-annotation-tasks>4 Human and Machine Performance in Annotation Tasks</h2><p>When applying computer vision approaches to film studies, the question arises whether their accuracy is sufficiently high. In this respect, we provide a comparison of human and machine performance based on own previous work <sup id=fnref:81><a href=#fn:81 class=footnote-ref role=doc-noteref>81</a></sup> for some specific visual recognition tasks: face recognition, geolocation estimation of photos, date estimation of photos, as well as visual object and concept detection (<a href=#table02>Table 2</a>).</p><p>A major field where human and machine performance has been compared frequently is visual concept classification. For a long time human annotations were (significantly) superior to machine-generated ones, as demonstrated by many studies <sup id=fnref:82><a href=#fn:82 class=footnote-ref role=doc-noteref>82</a></sup> <sup id=fnref:83><a href=#fn:83 class=footnote-ref role=doc-noteref>83</a></sup> <sup id=fnref:84><a href=#fn:84 class=footnote-ref role=doc-noteref>84</a></sup> on datasets like PASCAL VOC or SUN. However, the accuracy of machine annotations could be significantly raised by deep convolutional neural networks. The error rate of 6.7% reported with GoogLeNet <sup id=fnref:85><a href=#fn:85 class=footnote-ref role=doc-noteref>85</a></sup> on the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2014 was already close to that of humans (5.1 %), until in 2015 the error rate of neural networks <sup id=fnref:86><a href=#fn:86 class=footnote-ref role=doc-noteref>86</a></sup> was slightly lower (error rate: 4.94%) than human error on the ImageNet challenge. However, the human error rate is only based on a single annotator. Hence, no information about human inter-annotator agreement is available.<br>Comparison of human and machine performance in visual recognition tasks. Approach Challenge/test set Error Metric Human Performance Machine Performance Visual Concept Classification<br>GoogLeNet</p><p><sup id=fnref1:85><a href=#fn:85 class=footnote-ref role=doc-noteref>85</a></sup><br>ILSVRC’14 Top-5 test error 5.1% 6.66% He et al. 2015 ImageNet’12 dataset Top-5 test error 5.1% 4.94% Face Verification<br>DeepFace</p><p><sup id=fnref1:71><a href=#fn:71 class=footnote-ref role=doc-noteref>71</a></sup><br>LFW Accuracy 97.53% 97.35%<br>FaceNet</p><p><sup id=fnref:87><a href=#fn:87 class=footnote-ref role=doc-noteref>87</a></sup><br>LFW Accuracy 97.53% 99.63% Geolocation Estimation<br>PlaNet</p><p><sup id=fnref1:76><a href=#fn:76 class=footnote-ref role=doc-noteref>76</a></sup><br>Im2GPS test set<br>Accuracy at</p><p>region/continent scale<br>3.8% / 39.3% 37.6% / 71.3%<br>Im2GPS</p><p><sup id=fnref2:75><a href=#fn:75 class=footnote-ref role=doc-noteref>75</a></sup><br>Im2GPS test set<br>Accuracy at</p><p>region/continent scale<br>3.8% / 39.3% 47.7% / 73.4%<br>CPlaNet</p><p><sup id=fnref1:77><a href=#fn:77 class=footnote-ref role=doc-noteref>77</a></sup><br>Im2GPS test set<br>Accuracy at</p><p>region/continent scale<br>3.8% / 39.3% 46.4% / 78.5% <sup id=fnref1:78><a href=#fn:78 class=footnote-ref role=doc-noteref>78</a></sup> Im2GPS test set<br>Accuracy at</p><p>region/continent scale<br>3.8% / 39.3% 51.9% / 80.2% Date Estimation <sup id=fnref1:79><a href=#fn:79 class=footnote-ref role=doc-noteref>79</a></sup> Test set crawled from Flickr<br>Classification accuracy</p><p>by decade<br>26.0% 45.7% <sup id=fnref1:80><a href=#fn:80 class=footnote-ref role=doc-noteref>80</a></sup> _Date Estimation in the Wild _ test set<br>Absolute mean error</p><p>(in years)<br>10.9% 7.3%<br>A comparison of face identification capabilities of humans and machines was made for the first time in a Face Recognition Vendor Test (FRVT) study in 2006 <sup id=fnref:88><a href=#fn:88 class=footnote-ref role=doc-noteref>88</a></sup>. Thus, it has been shown - interestingly, already nearly 15 years ago - that industry-strength methods can compete with human performance in identifying unfamiliar faces under illumination changes. On the more recent as well as more challenging Labeled Faces in the Wild (LFW) benchmark, human performance has also been reached by several approaches. Among several others, DeepFace <sup id=fnref2:71><a href=#fn:71 class=footnote-ref role=doc-noteref>71</a></sup> and FaceNet <sup id=fnref1:87><a href=#fn:87 class=footnote-ref role=doc-noteref>87</a></sup> are prominent deep learning approaches reporting human-level (97.53%) results of 97.35% and 99.63% accuracy on the benchmark, practically solving the LFW dataset.</p><p>While humans are relatively good at recognising persons, estimating or guessing the location and date of an image imposes a more difficult task for subtle spatial as well as temporal differences. Various systems have been proposed for earth scale photo geolocation that outperform human performance in this task. The latest of these <sup id=fnref2:77><a href=#fn:77 class=footnote-ref role=doc-noteref>77</a></sup> <sup id=fnref3:75><a href=#fn:75 class=footnote-ref role=doc-noteref>75</a></sup> <sup id=fnref2:76><a href=#fn:76 class=footnote-ref role=doc-noteref>76</a></sup> employ deep learning methods consuming up to 90 million training images for an imposed classification task of cellular earth regions and/or rely on an extensive retrieval database. Current state-of-the-art results are reached by considering hierarchical as well as scene information of photos within the established classification approach, while using only 5 million training images <sup id=fnref2:78><a href=#fn:78 class=footnote-ref role=doc-noteref>78</a></sup>. Since human performance reported on the established Im2GPS benchmark <sup id=fnref4:75><a href=#fn:75 class=footnote-ref role=doc-noteref>75</a></sup> is relatively poor in this task, the system clearly surpasses human ability to guess geolocation by 48.1% accuracy within a tolerated distance level of 200 km (predictions at region level).</p><p>The creation date of (historical) photos is very hard to judge for periods of time that are quite similar. Therefore, a fundamental work <sup id=fnref2:79><a href=#fn:79 class=footnote-ref role=doc-noteref>79</a></sup> predicts the decade of historical colour photos with an accuracy of 45.7% exceeding that of untrained humans (26.0% accuracy), <a href=#muller2017>Müller et al. (2017)</a> suggest a deep learning system that infers the capturing date of images taken between 1930 and 1999 in terms of 5-year periods. When comparing human and machine annotations by means of absolute mean error in years, the deep learning system achieves better results nearly at all periods and improves the overall mean error by more than three years.</p><p>In general, the promising performance of the computer vision approaches compared to humans in Table 2 makes a strong case for their use in film studies. Although these methods are still prone to errors (to a lesser or greater extent), they can highly raise the exploration of media sources and help in finding patterns. In spite of the impressive performance of the selected approaches, computer vision approaches still have some shortcomings. Such approaches are optimized for specific visual content analysis tasks and rely on custom training data. Therefore, they have limited flexibility and cannot adapt to arbitrary images across different genres. While they perform well in basic computer vision tasks, humans are by far superior in grasping and interpreting images in their context, for example, in identifying gradual transitions between shots or in captioning images/videos.</p><h2 id=5-conclusions-and-future-prospects-for-software-tools-in-film-studies>5 Conclusions and Future Prospects for Software Tools in Film Studies</h2><p>In this paper, we have reviewed off-the-shelf software solutions for film studies. Since quantitative approaches to film content analysis are a handy way to effectively explore basic filmic features (see <a href=#section01>Section 1</a>), we have put a focus on offered functionalities regarding automated film style and content analysis. In this respect, only the tools Videana and VIAN offer a wider range of automated video analysis methods. However, with Videana not being developed anymore and the most recent tool VIAN focusing on film colour patterns, the field still lacks available tools that provide powerful state-of-the-art methods for visual content analysis. We discuss needed functionality in detail in the latter part of this section. Furthermore, we have outlined recent advances in the (automated) analysis of film style (shot and scene segmentation, use of camera motion or colours) as well as current developments and progress in the field of computer vision. As also discussed in the beginning of this paper, quantitative approaches are partially criticised for being incompatible with qualitative film analysis. To showcase the chances of basic computer vision approaches, we have compared machine and human performance in visual annotation tasks. We have shown that machine annotations are of similar quality to those of humans for some basic tasks like object classification, face recognition or geolocation estimation. Even when these methods do not reach human abilities, they can build a valid basis for exploring media sources for further manual inspection.</p><p>What kind of basic and extended functionality for automated analysis and visualisation should a software tool have in order to support research in film studies? Previous practical experience with the basic functions of Videana has shown that such software is basically suitable for effectively supporting film research and teaching. However, film scholars often need more advanced functions tailored to their particular domain that allow the automatic detection of complex stylistic film elements such as the shot-reverse-shot technique. This requires, for example, detecting a series of alternating close-ups with a dialogue component and can be detected through the syntactic interaction of different factors. Starting from the smallest discontinuity in film, the cut, it is also possible to detect changing colour values or certain structural semantic properties such as the position of an object within consecutive shots. These could also allow drawing conclusions about changes within the storyline. Such forms of automatic segmentation and creating individual segments of events can be relevant in the context of narrative analyses. Researchers could be offered an effective structuring and orientation aid, which can undergo further manual differentiation based on content-related or motivic aspects. Therefore we envision a program that also offers a high degree of flexibility with regard to manual annotation opportunities. For a specific film or film corpus, individual survey parameters must be generated in order to judge their correlation with other parameters - depending on which hypotheses are to be applied to the object of investigation or can be formulated on the basis of the film material.</p><p>Considering, for example, a film as a system of information mediation as in quantitative suspense research <sup id=fnref:89><a href=#fn:89 class=footnote-ref role=doc-noteref>89</a></sup> <sup id=fnref:90><a href=#fn:90 class=footnote-ref role=doc-noteref>90</a></sup> <sup id=fnref:91><a href=#fn:91 class=footnote-ref role=doc-noteref>91</a></sup>, individual shots and sequences could be detected using automatic methods and manually be supplemented with further content parameters. Here, classifications such as the degree of information shared between the film character and the viewer (if the film viewer knows more or less than the character), the degree of correspondence between narrative time (duration of the film) and narrated time (time period covered by the filmic narrative), or the degree of narrative relevance (is the event only of relevance within the sequence or does it affect the central conflict of the narrative) are considered in order to draw conclusions about the suspense potential of a sequence. In the outlined framework, these factors favour the exploitation of the viewer&rsquo;s anticipation of damage - in particular their emotional connection to a film character - through a principle of delay, as primarily applied in battle sequences typical of action films. Due to the principle of delaying the resolution of a conflict, they have a low information gain in view of the entire narrative. The principles of this parameterisation can now be used to check which dramaturgical context is characterised by which formal characteristics in comparison with the editing parameters of Salt. This concept of data acquisition can be extended by further parameters such as the application of digital visual effects in individual sequences, which provides a differentiated insight into the internal dynamics and proportions of filmic representation systems. Especially with regard to computer-generated imagery, which is subject to development processes spanning decades, it is possible to examine on a longitudinal scale how this process has had a concrete effect on production practices. However, such highly complex interwoven data structures require sophisticated statistical models with which these hypotheses can be tested.</p><p>Finally, a software tool adapted to the research object should be developed in a productive dialogue between computer science and film science. Such software for film studies could be based on experiences with software such as Videana or VIAN that allows for the evaluation of larger film corpora on a differentiated and reliable data basis, which cannot be generated with previous analysis instruments. Furthermore, it is desirable to include specific forms of information visualisations tailored to the needs of film scholars. Cinemetrics or Ligne de temps, as software designed for film studies, are also limited to a graphical representation that cannot take into account the requirements of narrative questions. Since an all-in-one approach will not fit to all analysis and visualisation requirements, such a tool should also provide an interface for plugins that offer additional functionality. Using these approaches, a large collection of research data can be gathered and exported for further processing, but the lack of a working digital environment for media science continues to be a problem. Statistical software or Microsoft Excel were not designed for the visualization of editing rhythms or narrative structures, which makes it difficult to process the corresponding data. An interdisciplinary cooperation could foster research in designing an optimal solution for scholarly film studies that allows direct interaction with the automatically determined parameters as well as their method-dependent annotation and graphical processing.</p><p>In summary, the development of a comprehensive software solution for scientists who systematically carry out film or video analysis would be desirable. This group includes media and film scientists, but also scientists from other disciplines (e.g., applications in journalism and media education, analysis of propaganda videos and political video works, image and educational films, television programmes). Also, empirical studies of media psychology in the field of event indexing require the annotation and analysis of structural properties of audiovisual research data. Factors such as the duration of an action segment as well as temporal, spatial, figure-related or action-related changes within the sequences (four dimension models), but also shot lengths are integrated into the research design and are prerequisites for the formulation of hypotheses and their empirical validation <sup id=fnref:92><a href=#fn:92 class=footnote-ref role=doc-noteref>92</a></sup>.</p><p>An interdisciplinary all-in-one software tool of this kind should be openly available on a web-based platform, intuitive, easy to use and rely on state-of-the-art algorithms and technology. On the one hand, by providing automatic methods for the quantitative analysis of film material, large film and video corpora could become the object of investigation and hypotheses could, for example, be statistically tested; on the other hand, the interpretation would be simplified by different possibilities of visualisation. Last but not least, legal questions regarding the storage, processing and use of moving image material should be clarified and taken into account in the technical implementation.</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>Anitha A., Brasoveanu, A., Duarte M., Hughes, S., Daubechies, I., Dik, J., Janssens, K., & Alfeld, M. “Restoration of X-ray fluorescence images of hidden paintings.” <em>Signal Processing</em> , 93(3) (2013): 592-604.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>Johnson, C. R., Hendriks, E., Berezhnoy, I. J., Brevdo, E., Hughes, S. M., Daubechies, I., & Wang, J. Z. “Image processing for artist identification.” <em>IEEE Signal Processing Magazine</em> , 25(4) (2008): 37-48.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>Klein C., Betz J., Hirschbuehl M., Fuchs C., Schmiedtová B., Engelbrecht M., Mueller-Paul, J., & Rosenberg, R. “Describing Art – An Interdisciplinary Approach to the Effects of Speaking on Gaze Movements during the Beholding of Paintings.” <em>PLoS ONE</em> 9(12) (2014).&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p>Resig, J. “Using computer vision to increase the research potential of photo archives.” <em>Journal of Digital Humanities</em> , 3(2) (2014): 33.&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5><p>Liu, A. “Where Is Cultural Criticism in the Digital Humanities?” [Online] (2012). Available at: <a href=http://dhdebates.gc.cuny.edu/debates/text/20>http://dhdebates.gc.cuny.edu/debates/text/20</a> (Accessed: 19 December 2018)&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:6><p>Missomelius, P. “Medienbildung und Digital Humanities: Die Medienvergessenheit technisierter Geisteswissenschaften.” In: Ortner, H., Pfurtscheller, D., Rizzolli, M, & Wiesinger, A. (Hg.): <em>Datenflut und Informationskanäle</em> . Innsbruck: Innsbruck UP (2014), 101-112.&#160;<a href=#fnref:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:7><p>Heftberger, A. <em>Kollision der Kader: Dziga Vertovs Filme, die Visualisierung ihrer Strukturen und die Digital Humanities</em> , München: edition text+kritik (2016).&#160;<a href=#fnref:7 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:8><p>Sittel, J. “Digital Humanities in der Filmwissenschaft.” In: <em>ZfM 4</em> (2017), 472-489.&#160;<a href=#fnref:8 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:9><p>Salt, B . <em>Moving into Pictures</em> . London: Starword (2006).&#160;<a href=#fnref:9 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:10><p>Salt, B. <em>Film Style and Technology: History and Analysis</em> . London: Starword (2009).&#160;<a href=#fnref:10 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:11><p>Flückiger, B. “Die Vermessung ästhetischer Erscheinungen.” <em>ZfM</em> 5 (2011): 44-60.&#160;<a href=#fnref:11 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:12><p>Kipp, M. (2001). “Anvil - A Generic Annotation Tool for Multimodal Dialogue.” In: <em>Proceedings of the 7th European Conference on Speech Communication and Technology</em> (Eurospeech) (2001), pp. 1367-1370.&#160;<a href=#fnref:12 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:12 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:13><p>Ewerth, R., Mühling, M., Stadelmann, T., Gllavata, J., Grauer, M., & Freisleben, B. “Videana: A Software Toolkit for Scientific Film Studies.” <em>Digital Tools in Media Studies – Analysis and Research. An Overview.</em> Transcript Verlag, Bielefeld, Germany (2009): 101-116.&#160;<a href=#fnref:13 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:13 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:14><p>Estrada, L. M, Hielscher, E. Koolen, M., Olesen, C. G., Noordegraaf, J. & Jaap Blom, J. “Film Analysis as Annotation: Exploring Current Tools.” <em>The Moving Image - Special Issue on Digital Humanities and/in Film Archives</em> (Vol 17, no 2) (2017): 40-70.&#160;<a href=#fnref:14 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:15><p>background-character/figure segmentation&#160;<a href=#fnref:15 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:16><p>Tsivian, Y. “Cinemetrics, Part of the Humanities’ Cyberinfrastructure.” In: Michael Ross, Manfred Grauer, Bernd Freisleben (eds.), <em>Digital Tools in Media Studies</em> 9, Bielefeld: Transcript Verlag (2009): 93-100.&#160;<a href=#fnref:16 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:17><p><a href=http://www.cinemetrics.lv/>http://www.cinemetrics.lv/</a>&#160;<a href=#fnref:17 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:18><p>Available at <a href=http://www.anvil-software.org/>http://www.anvil-software.org/</a>&#160;<a href=#fnref:18 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:19><p>Available at <a href=https://tla.mpi.nl/tools/tla-tools/elan/>https://tla.mpi.nl/tools/tla-tools/elan/</a>&#160;<a href=#fnref:19 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:20><p>Sloetjes, H., & Wittenburg, P. “Annotation by category – ELAN and ISO DCR.” In: <em>Proceedings of the 6th International Conference on Language Resources and Evaluation</em> (2008).&#160;<a href=#fnref:20 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:21><p><a href=http://www.iri.centrepompidou.fr/outils/lignes-de-temps-2/>http://www.iri.centrepompidou.fr/outils/lignes-de-temps-2/</a>&#160;<a href=#fnref:21 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:22><p>Aubert, O. & Prié, Y. “Advene: Active reading through hypervideo.” In: <em>Proceedings of ACM Hypertext &lsquo;05</em> (2005), pp. 235-244.&#160;<a href=#fnref:22 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:23><p><a href=https://www.advene.org>www.advene.org</a>&#160;<a href=#fnref:23 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:24><p><a href=https://mediathread.info>https://mediathread.info</a>&#160;<a href=#fnref:24 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:25><p><a href=https://ctl.columbia.edu/>https://ctl.columbia.edu/</a>&#160;<a href=#fnref:25 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:26><p>Ewerth, R. & Freisleben, B. “Video Cut Detection without Thresholds.” In: <em>Proc. of 11th Workshop on Signals, Systems and Image Processing</em> , Poznan, Poland (2004), pp. 227-230.&#160;<a href=#fnref:26 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:27><p>Ewerth, R. & Freisleben, B. “Unsupervised Detection of Gradual Shot Changes with Motion-Based False Alarm Removal.” In: <em>Proceedings of 8th International Conference on Advanced Concepts for Intelligent Vision Systems (ACIVS)</em> , Bordeaux, France, Springer (2009), pp. 253-264.&#160;<a href=#fnref:27 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:28><p>Gllavata, J., Ewerth, R., & Freisleben, B. “Text Detection in Images Based on Unsupervised Classification of High-Frequency Wavelet Coefficients.” <em>ICPR</em> (2004), pp. 425-428.&#160;<a href=#fnref:28 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:29><p>Gllavata, J., Ewerth, R., & Freisleben, B. “Tracking text in MPEG videos.” In: <em>ACM Multimedia</em> (2004), pp. 240-243.&#160;<a href=#fnref:29 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:30><p>Ewerth, R., Schwalb, M., Tessmann, P., & Freisleben, B. “Estimation of Arbitrary Camera Motion in MPEG Videos.” In: <em>Proceedings of 17th Int. Conference on Pattern Recognition</em> , (2004), pp. 512–515.&#160;<a href=#fnref:30 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:30 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:31><p>Ewerth, R., Schwalb, M., Tessmann, P., & Freisleben, B. “Segmenting Moving Objects in the Presence of Camera Motion.” In: <em>Proc. of 14th Int. Conference on Image Analysis and Processing</em> , Modena, Italy (2007), pp. 819-824.&#160;<a href=#fnref:31 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:32><p>Viola, P. & Jones, M. “Robust Real-Time Face Detection.” <em>Int. Journal of Computer Vision</em> , 57(2) (2004): 137–154.&#160;<a href=#fnref:32 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:33><p>Ewerth, R., Mühling, M., & Freisleben, B. “Self-Supervised Learning of Face Appearances in TV Casts and Movies.” Invited Paper (Best papers from IEEE International Symposium on Multimedia ‘06): <em>International Journal on Semantic Computing, World Scientific</em> (2007), pp. 185-204.&#160;<a href=#fnref:33 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:34><p>Abend, P., Thielmann, T.,  Ewerth, R., Seiler, D., Mühling, M., Döring, J., Grauer, M., & Freisleben, B. “Geobrowsing the Globe: A Geovisual Analysis of Google Earth Usage.” In: <em>Proc. of Linking GeoVisualization with Spatial Analysis and Modeling</em> (GeoViz), Hamburg, (2011).&#160;<a href=#fnref:34 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:35><p>Abend, P., Thielmann, T., Ewerth, R., Seiler, D., Mühling, M., Döring, J., Grauer, M., & Freisleben, B. “Geobrowsing Behaviour in Google Earth: A Semantic Video Content Analysis of On-Screen Navigation.” In: <em>Proc. of Geoinformatics Forum</em> , Salzburg, Österreich, (2012), pp. 2-13.&#160;<a href=#fnref:35 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:36><p><a href=https://filmcolors.org/>https://filmcolors.org/</a>&#160;<a href=#fnref:36 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:37><p>Flückiger, B.,  Evirgen, N., Paredes, E. G., Ballester-Ripoll, R.,  & Pajarola, R. “Deep Learning Tools for Foreground-Aware Analysis of Film Colors.” In: <em>Computer Vision in Digital Humanities</em> , Digital Humanities Conference, Montreal (2017).&#160;<a href=#fnref:37 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:38><p><a href=https://www.distantviewing.org/labs/>https://www.distantviewing.org/labs/</a>&#160;<a href=#fnref:38 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:39><p><a href=http://efilms.ushmm.org>http://efilms.ushmm.org</a>&#160;<a href=#fnref:39 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:40><p>Adams, B., Venkatesh, S., Bui, H. H. & Dorai, C. “A Probabilistic Framework for Extracting Narrative Act Boundaries and Semantics in Motion Pictures.”   <em>Multimedia Tools Appl.</em>  27(2) (2005): 195-213.&#160;<a href=#fnref:40 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:41><p>Lowe, D. G. “Distinctive Image Features from Scale-Invariant Keypoints.” <em>International Journal of Computer Vision</em> , 60(2) (2004): 91–110.  &#160;<a href=#fnref:41 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:42><p>Bay, H., Ess, A., Tuytelaars, T., & Van Gool, L. “Speeded-Up Robust Features (SURF).” <em>Computer Vision and Image Understanding</em> , 110(3) (2008): 346-359.&#160;<a href=#fnref:42 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:42 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:43><p>Brejcha, J. & Cadík, M. “State-of-the-art in visual geo-localization.” <em>Pattern Analysis and Applications</em> , 20(3) (2017): 613-637.&#160;<a href=#fnref:43 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:44><p>Rawat, W. & Wang, Z. “Deep Convolutional Neural Networks for Image Classification: A Comprehensive Review.” <em>Neural Computation</em> 29(9) (2017): 2352-2449.&#160;<a href=#fnref:44 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:45><p>Wang, M. & Deng, W. “Deep Face Recognition: A Survey.” CoRR abs/1804.06655 (2018).&#160;<a href=#fnref:45 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:46><p>Baber, J., Afzulpurkar, N. V., Dailey, M. N., & Bakhtyar, M. “Shot boundary detection from videos using entropy and local descriptor.” In: <em>Proceedings of the 17th International Conference on Digital Signal Processing</em> , Corfu, Greece (2011), pp. 1-6.&#160;<a href=#fnref:46 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:47><p>Lankinen, J., & Kämäräinen, J. “Video Shot Boundary Detection Using Visual Bag-of-Words.” In: <em>Proceedings of the International Conference on Computer Vision Theory and Applications</em> (1), Barcelona, Spain (2013), pp. 788-791.&#160;<a href=#fnref:47 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:48><p>Li, J., Ding, Y., Shi, Y., & Li, W. “A divide-and-rule scheme for shot boundary detection based on sift.” <em>Journal of Digital Content Technology and its Applications</em> (2010): 202–214.&#160;<a href=#fnref:48 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:49><p>Apostolidis, E. & Mezaris, V. “Fast shot segmentation combining global and local visual descriptors.” In: <em>International Conference on Acoustics, Speech and Signal Processing</em> , Florence, Italy (2014), pp. 6583-6587.&#160;<a href=#fnref:49 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:50><p>Xu, J., Song, L., & Xie, R. “Shot boundary detection using convolutional neural networks.” In: <em>Proceedings of the International Conference on Visual Communications and Image Processing</em> (2016), pp. 1-4.&#160;<a href=#fnref:50 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:51><p>Gygli, M. “Ridiculously Fast Shot Boundary Detection with Fully Convolutional Neural Networks.” In: <em>International Conference on Content-Based Multimedia Indexing</em> , La Rochelle, France (2018), pp. 1-4.&#160;<a href=#fnref:51 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:52><p>Baraldi, L., Grana,  C., & Cucchiara, R. “Shot and scene detection via hierarchical clustering for re-using broadcast video.” In: <em>International Conference on Computer Analysis of Images and Patterns</em> (2015), pp. 801-811.&#160;<a href=#fnref:52 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:53><p>Baraldi, L., Grana, C., & Cucchiara, R. A “Deep Siamese Network for Scene Detection in Broadcast Videos.” In <em>Proceedings of the 23rd Annual ACM Conference on Multimedia Conference</em> , Brisbane, Australia (2015), pp. 1199-1202.&#160;<a href=#fnref:53 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:54><p>Nguyen, B. T., Laurendeau, D., & Albu, A. B. “A robust method for camera motion estimation in movies based on optical flow.” <em>IJISTA</em> , 9(3/4) (2010): 228-238.&#160;<a href=#fnref:54 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:55><p>Zhou, T., Brown, B., Snavely, N. & Lowe, D. G. “Unsupervised learning of depth and ego-motion from video.” In: <em>Conference on Computer Vision and Pattern Recognition</em> (2017), pp. 6612-6619.&#160;<a href=#fnref:55 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:56><p>Yin, Z. & Shi, J. “GeoNet: Unsupervised Learning of Dense Depth, Optical Flow and Camera Pose.” In: <em>Conference on Computer Vision and Pattern Recognition</em> (2018), pp. 1983-1992.&#160;<a href=#fnref:56 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:57><p>Ranjan, A., Jampani, V., Balles, L., Kim, K., Sun, D., Wulff, J. & Black, M. J. “Competitive Collaboration: Joint Unsupervised Learning of Depth, Camera Motion, Optical Flow and Motion Segmentation.” In: <em>Proceedings of the Conference on Computer Vision and Pattern Recognition</em> (2019), pp.12240-12249.&#160;<a href=#fnref:57 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:58><p>Krizhevsky, A., Sutskever, I., & Hinton, G. E. “ImageNet Classification with Deep Convolutional Neural Networks.” In: <em>Proc. of 26th Conf. on Neural Information Processing Systems 2012</em> . Lake Tahoe, Nevada, United States (2012), pp. 1106–1114.&#160;<a href=#fnref:58 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:59><p>Girshick R. B., Donahue, J., Darrell, T., & Malik, J. “Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation.” In <em>Proceedings of the Conference on Computer Vision and Pattern Recognition</em> , Columbus, OH, USA (2014), pp. 580-587.&#160;<a href=#fnref:59 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:60><p>Girshick, R. B. “Fast R-CNN.” In <em>Proceedings of the International Conference on Computer Vision</em> , Santiago, Chile (2015), pp. 1440-1448.&#160;<a href=#fnref:60 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:61><p>Ren, S., He, K., Girshick, R., B., & Sun, J. “Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks.” <em>Transactions on Pattern Analysis and Machine Intelligence</em> , 39(6) (2017): 1137-1149.&#160;<a href=#fnref:61 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:62><p>Lin, T., Maire, M., Belongie, S. J., Hays, H., Perona, P., Ramanan, D., Dollár, P., & Zitnick, L. “Microsoft COCO: Common Objects in Context.” In: <em>Proceedings of ECCV</em> (2014), pp. 740-755.&#160;<a href=#fnref:62 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:63><p>He, K., Gkioxari, G., Dollár, P., & Girshick, R. B. “Mask R-CNN.” In <em>International Conference on Computer Vision</em> , Venice, Italy (2018), pp. 2980-2988.&#160;<a href=#fnref:63 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:64><p>Redmon, J. & Farhadi, A. “YOLO9000: Better, Faster, Stronger.” In: <em>Proceedings of the Conference on Computer Vision and Pattern Recognition</em> , Honolulu, HI, USA (2017), pp. 6517-6525.&#160;<a href=#fnref:64 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:65><p>Redmon, J. & Farhadi, A. “YOLOv3: An Incremental Improvement.” CoRR abs/1804.02767 (2018).&#160;<a href=#fnref:65 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:66><p>Deng, J., Dong, W., Socher, R., Li, L., Li, K., & Fei-Fei, L. “ImageNet: A large-scale hierarchical image database.” In: <em>Proceedings of the Conference on Computer Vision and Pattern Recognition</em> (2009), pp. 248–255.&#160;<a href=#fnref:66 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:67><p>Zhou, B., Lapedriza, A., Xiao, J.,  Torralba, A., & Oliva, A. “Learning Deep Features for Scene Recognition using Places Database.” In: <em>NIPS Proceedings</em> , Montreal, Quebec, Canada (2014), pp. 487-495.&#160;<a href=#fnref:67 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:68><p>He, K., Zhang, X.,  Ren, S., & Sun, J. “Deep Residual Learning for Image Recognition.” In <em>Proceedings of CVPR</em> (2016), pp. 770-778.&#160;<a href=#fnref:68 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:68 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:69><p>Liu, C., Zoph, B., Shlens, J., Hua, W., Li, L., Fei-Fei, L., Yuille, A., Huang, J., & Murphy, K. <em>Progressive Neural Architecture Search</em> (2017).&#160;<a href=#fnref:69 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:70><p>Szegedy, C., Ioffe, S., & Vanhoucke, V. “Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning.” CoRR abs/1602.07261 (2016).&#160;<a href=#fnref:70 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:71><p>Taigman, Y., Yang, M., Ranzato, M., & Wolf, L. “DeepFace: Closing the Gap to Human-Level Performance in Face Verification.” In: <em>Proceedings of the Conference on Computer Vision and Pattern Recognition</em> , Columbus, OH, USA (2014), pp. 1701-1708.&#160;<a href=#fnref:71 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:71 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref2:71 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:72><p>Mühling, M., Korfhage, N., Müller, E., Otto, C., Springstein, M., Langelage, T., Veith, U., Ewerth, R., & Freisleben, B. “Deep learning for content-based video retrieval in film and television production.” <em>Multimedia Tools Appl.</em> 76(21) (2017): 22169-22194.&#160;<a href=#fnref:72 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:73><p>Hays, J. & Efros, A. A. “IM2GPS: estimating geographic information from a single image.” In <em>Proceedings of the Conference on Computer Vision and Pattern Recognition</em> , Anchorage, Alaska, USA (2008).&#160;<a href=#fnref:73 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:74><p>Hays, J. & Efros, A. A. “Large-Scale Image Geolocalization.” <em>Multimodal Location Estimation of Videos and Images</em> (2015): 41-62.&#160;<a href=#fnref:74 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:75><p>Vo, N., Jacobs, N., Hays, J. “Revisiting IM2GPS in the Deep Learning Era.” In: <em>International Conference on Computer Vision</em> (2017), pp. 2640-2649.&#160;<a href=#fnref:75 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:75 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref2:75 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref3:75 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref4:75 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:76><p>Weyand, T., Kostrikov, I., & Philbin, J. “PlaNet - Photo Geolocation with Convolutional Neural Networks.” In: <em>Proceedings of the European Conference on Computer Vision</em> , Amsterdam, The Netherlands (2016), pp. 37-55.&#160;<a href=#fnref:76 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:76 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref2:76 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:77><p>Seo, P. H., Weyand, T., Sim, J., & Han, B. “CPlaNet: Enhancing Image Geolocalization by Combinatorial Partitioning of Maps.” In: <em>Proceedings of the European Conference on Computer Vision</em> , Munich, Germany (2018), pp. 544-560.&#160;<a href=#fnref:77 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:77 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref2:77 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:78><p>Müller-Budack, E., Pustu-Iren, K., & Ewerth, R. “Geolocation Estimation of Photos Using a Hierarchical Model and Scene Classification.” In: <em>Proceedings of the European Conference on Computer Vision</em> , Munich, Germany (2018), pp. 575-592.&#160;<a href=#fnref:78 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:78 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref2:78 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:79><p>Palermo, F., Hays, J., & Efros, A. A. “Dating Historical Color Images.” In: <em>Proceedings of the European Conference on Computer Vision</em> , Florence, Italy (2012), pp. 499-512.&#160;<a href=#fnref:79 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:79 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref2:79 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:80><p>Müller, E., Springstein, M., & Ewerth, R. “ When Was This Picture Taken? - Image Date Estimation in the Wild.” In: <em>Proceedings of the European Conference on IR Research</em> , Aberdeen, UK (2017), pp. 619-625.&#160;<a href=#fnref:80 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:80 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:81><p>Ewerth, R., Springstein, M., Phan-Vogtmann, L. A., & Schütze, J. “ Are Machines Better in Image Tagging? – A User Study Adds to the Puzzle.” In:   <em>Proceedings of 39th European Conference on Information Retrieval (ECIR)</em> , Aberdeen, UK (2017), pp. 186-198&#160;<a href=#fnref:81 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:82><p>Jiang, Y. G., Ye, G., Chang, S. F., Ellis, D., & Loui, A. C. “Consumer video understanding: a benchmark database and an evaluation of human and machine performance.” In: <em>Proceedings of the International Conference on Multimedia Retrieval</em> (2011), p. 29.&#160;<a href=#fnref:82 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:83><p>Parikh, D. & Zitnick, C. L. “The role of features, algorithms and data in visual recognition.” In: <em>Conference on Computer Vision and Pattern Recognition</em> (2010), pp. 2328–2335.&#160;<a href=#fnref:83 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:84><p>Xiao, J., Hays, J., Ehinger, K. A., Oliva, A., & Torralba, A. “SUN database: large-scale scene recognition from abbey to zoo.” In: <em>Conference on Computer Vision and Pattern Recognition</em> (2010), pp. 3485–3492.&#160;<a href=#fnref:84 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:85><p>Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, & V., Rabinovich, A. “Going deeper with convolutions.” In <em>Proceedings of the Conference on Computer Vision and Pattern Recognition</em> (2015), pp. 1–9.&#160;<a href=#fnref:85 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:85 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:86><p>He, K., Zhang, X., Ren, S., & Sun, J. “Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification.” In <em>Proceedings of ICCV</em> (2015), pp. 1026-1034.&#160;<a href=#fnref:86 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:87><p>Schroff, F., Kalenichenko, D., & Philbin, J. “FaceNet: A unified embedding for face recognition and clustering.” In: <em>Conference on Computer Vision and Pattern Recognition</em> , Boston, MA, USA (2015), pp. 815-823.&#160;<a href=#fnref:87 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:87 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:88><p>Phillips, P. J., Scruggs, W. T., O’Toole, A. J., Flynn, P. J., Bowyer, K. W., Schott, C. L., & Sharpe, M. FRVT 2006 and ICE 2006 Large-Scale Results (2006).&#160;<a href=#fnref:88 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:89><p>Junkerjürgen, R. <em>Spannung – narrative Verfahrenweisen der Leseraktivierung: eine Studie am Beispiel der Reiseromane von Jules Verne.</em> Frankfurt am Main; Berlin; Bern; Bruxelles; New York; Oxford; Wien: Lang (2001).&#160;<a href=#fnref:89 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:90><p>Weibel, A. <em>Spannung bei Hitchcock. Zur Funktionsweise auktorialer Suspense</em> . Würzburg: Königshausen & Neumann (2008).&#160;<a href=#fnref:90 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:91><p>Weibel, A. <em>Suspense im Animationsfilm Band I Methodik: Grundlagen der quantitativen Spannungsanalyse. Studienbeisipiel Ice Age 3</em> . Norderstedt: Books on Demand (2017).&#160;<a href=#fnref:91 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:92><p>Huff, M., Meitz, T., & Papenmeier, F. “Changes in Situation Models Modulate Processes of Event Perception in Audiovisual Narratives.” <em>Journal of Experimental Psychology - Learning, Memory, and Cognition</em> , 40(5) (2014): 1377-1388.&#160;<a href=#fnref:92 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div></div></article></main><footer><nav aria-label="footer links"><ul><li><a class=highlight-focus href=/dhqwords/tags/>Tags</a></li><li><a class=highlight-focus href=/dhqwords/about/>About</a></li><li><a class=highlight-focus href=/dhqwords/categories/>Keywords</a></li></ul></nav><div class=icons><a class="license highlight-focus" rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons Attribution No-Derivatives 4.0 International License" src=/dhqwords/img/logos/license.svg width=120 height=42></a></div></footer></body></html>