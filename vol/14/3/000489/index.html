<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=theme-color content="#3D206C"><meta http-equiv=x-ua-compatible content="IE=edge"><link rel=apple-touch-icon sizes=180x180 href=/img/favicon/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/img/favicon/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/img/favicon/favicon-16x16.png><link rel=icon type=image/png sizes=384x384 href=/img/favicon/android-chrome-384x384.png><link rel=icon type=image/png sizes=192x192 href=/img/favicon/android-chrome-192x192.png><link rel=icon type=image/png sizes=150x150 href=/img/favicon/mstile-150x150.png><link rel="shortcut icon" href=/favicon.ico><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/img/favicon/safari-pinned-tab.svg color=#3d206c><link rel=schema.dc href=http://purl.org/DC/elements/1.0/><meta name=citation_public_url content="https://startwords.cdh.princeton.edu/vol/14/3/000489/"><meta name=citation_title content="Reassessing the locus of normalization in machine-assisted collation"><meta name=citation_date content="2020/09"><meta name=citation_author content="David J. Birnbaum"><meta name=citation_author content="Elena Spadini"><meta name=citation_abstract content="Introduction In this essay we explore the process of textual normalization in the context of machine-assisted collation, which is a common operation in digital textual scholarship. Collation, that is, the alignment of versions of (traditionally calledwitnesses to) the same work, is used by scholars for studying the textual transmission or the genetic process of a work, often as a step in the preparation of a scholarly edition of that work. Machine-assisted collation refers to the use of computers for performing, in full or in part, this alignment."><meta name=citation_journal_title content="DHQwords"><meta name=citation_issn content="2694-2658"><meta name=citation_issue content="14.3"><meta name=citation_publisher content="Center for Digital Humanities, Princeton University"><meta name=DC.rights content="http://creativecommons.org/licenses/by/4.0/"><meta name=author content="David J. Birnbaum, Elena Spadini"><meta name=generator content="Center for Digital Humanities, Princeton University"><meta name=dcterms.created content="2020-09"><link rel=preload as=font type=font/woff2 href=/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-regular.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-italic.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-regular.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-300.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-700.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-300italic.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-500italic.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-700italic.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-500.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/fonts/Source_Sans_Pro/source-sans-pro-v13-latin-regular.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/fonts/Source_Sans_Pro/source-sans-pro-v13-latin-900.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/fonts/Source_Sans_Pro/source-sans-pro-v13-latin-300.woff2 crossorigin><title>Reassessing the locus of normalization in machine-assisted collation</title><meta name=description content="DHQwords Issue 14.3, September 2020. A research periodical irregularly published by the Center for Digital Humanities at Princeton. "><meta property="og:title" content="Reassessing the locus of normalization in machine-assisted collation"><meta property="og:description" content="Introduction In this essay we explore the process of textual normalization in the context of machine-assisted collation, which is a common operation in digital textual scholarship. Collation, that is, the alignment of versions of (traditionally calledwitnesses to) the same work, is used by scholars for studying the textual transmission or the genetic process of a work, often as a step in the preparation of a scholarly edition of that work. Machine-assisted collation refers to the use of computers for performing, in full or in part, this alignment."><meta property="og:type" content="article"><meta property="og:url" content="https://startwords.cdh.princeton.edu/vol/14/3/000489/"><meta property="og:image" content="https://startwords.cdh.princeton.edu/social.png"><meta property="article:section" content="vol"><meta property="article:published_time" content="2020-09-25T00:00:00+00:00"><meta property="article:modified_time" content="2023-08-04T13:14:15-04:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://startwords.cdh.princeton.edu/social.png"><meta name=twitter:title content="Reassessing the locus of normalization in machine-assisted collation"><meta name=twitter:description content="Introduction In this essay we explore the process of textual normalization in the context of machine-assisted collation, which is a common operation in digital textual scholarship. Collation, that is, the alignment of versions of (traditionally calledwitnesses to) the same work, is used by scholars for studying the textual transmission or the genetic process of a work, often as a step in the preparation of a scholarly edition of that work. Machine-assisted collation refers to the use of computers for performing, in full or in part, this alignment."><link rel=stylesheet href=/style.css><link rel=stylesheet href=/print.css media=print><script src=/js/polyfills.js></script><script defer src=/js/bundle.js></script></head><body class=article><header><nav class=main aria-label=main><ul><li class=home><a href=/>dhq</a></li><li class=issues><a href=/vol/>volumes</a></li></ul></nav></header><main><article><div class=grid><header><p class=number><a href=/vol/14/3/>Issue 14.3</a></p><h1>Reassessing the locus of normalization in machine-assisted collation</h1><p><ul class=authors><li><address>David J. Birnbaum</address></li><li><address>Elena Spadini</address></li></ul></p><p><time class=pubdate datetime=2020-09>September 2020</time></p><ul class="categories tags"></ul><ul class=tags></ul><p class=formats></p></header><section class=print-only><a class=first-page-header href=/ aria-label=Startwords><img alt=Startwords src=/pdf-logotype.svg></a>
<a class=page-header href=/ aria-label=Startwords><img alt=Startwords src=/pdf-logo.svg></a>
<a href=http://doi.org/ rel=alternate class=page-doi>doi:</a></section><div class=text-container><h2 id=introduction>Introduction</h2><p>In this essay we explore the process of textual normalization in the context of machine-assisted collation, which is a common operation in digital textual scholarship. Collation, that is, the alignment of versions of (traditionally calledwitnesses to) the same work, is used by scholars for studying the textual transmission or the genetic process of a work, often as a step in the preparation of a scholarly edition of that work. Machine-assisted collation refers to the use of computers for performing, in full or in part, this alignment. As such, it has a long history, as long as that of the Digital Humanities<a class=footnote-ref href=#nury2020> [nury2020] </a>: over the past sixty years, not only have the machines at our disposal changed, but so have scholars’ understanding of the collation process, and the most refined computational model of collation available today is the one devised in 2009 in Gothenburg by the developers of the collation software Juxta and CollateX and by the textual scholars of the COST Action Open Scholarly Communities on the Web and of the program Interedition<a class=footnote-ref href=#dekker2011> [dekker2011] </a><a class=footnote-ref href=#dekker2015>[dekker2015] </a><a class=footnote-ref href=#bleeker2017>[bleeker2017] </a><a class=footnote-ref href=#interedition>[interedition] </a>. The Gothenburg modular architecture for computer-assisted collation is a five-stage computational pipeline, in which <strong>Normalization</strong> constitutes the optional second stage, where it contributes to improving the alignment.<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></p><p>In this essay we argue that normalization contributes not only to the alignment, but also to the interpretation of the texts. Furthermore, it occurs at several moments of the collation process, between transcription and rendering, and should not be regarded as happening all at once and only in one location. Our point is not to question the modeling of collation as a computational pipeline, which has led to impressive, productive, and influential results, but to explore the complexity that is elided in the simpler description of <strong>Normalization</strong> in the Gothenburg model, as well as the consequences of that complexity for conceptualizing and implementing a collation project. In fact, for reasons that we explore in detail below, an awareness of how pervasive normalization might be in machine-assisted collation is fundamental to devising a computational workflow that corresponds appropriately to the scholarly requirements of each project. The relationship between our understanding of normalization and our workflow is valid not only for researchers relying on (software that is organized according to) the Gothenburg model, but, more generally, as something to be taken into account by those dealing with machine-assisted collation. That is, the Gothenburg model offers a foundation for discussion, but our argument about the importance and ubiquity of normalization is not dependent on it. Furthermore, as often happens with the computational counterpart of a scholarly practice, machine-assisted collation tends to make explicit the assumptions implicit in manual collation: the study proposed here is largely applicable to collation in general, whether some operations are performed mentally or computationally. For this reason, the essay is addressed not only to textual scholars, but also to developers, who can find here a thorough analysis of the complex procedures implied in machine-assisted collation, with a particular focus on the different kinds of normalization that happen throughout the process.</p><p>The essay first lays out preliminary remarks about what normalization is, its manifold forms and purposes, and provides an overview of the Gothenburg model. That model supplies the architecture for what follows: for each stage of the model, the potential role of normalization is discussed and illustrated with examples. In the conclusions, we summarize how this study contributes to a reconsideration of both the role and significance of normalization in collation and digital textual scholarship and, more narrowly, of the Gothenburg model of machine-assisted collation.</p><h2 id=normalization>Normalization</h2><p>In the context of machine-assisted collation as formalized by the Gothenburg model, described below, <strong>Normalization</strong> refers to the creation and use of shadow copies of word tokens when performing alignment. A common and simple type of normalization is case-folding. For example, althoughcatandCatare not string-equal, collation processing might create a lower-cased shadow copy of each word in the text before performing the comparisons needed for alignment. As a result, although the output might retain the original casing, the alignment engine would use the shadow copies for comparison, and would therefore recognize that the difference betweencatandCatis not significant for alignment purposes.</p><p>In this essay we use <strong>Normalization</strong> (capitalized and embolded) to refer to the second, optional, stage of the Gothenburg model, in which the differences in witness tokens that should be ignored for alignment purposes are neutralized, ornormalized,prior to alignment. We use normalization (without special capitalization or other formatting) to refer to normalization as a general concept, present in many forms distributed over several stages of the pipeline, and affecting not only Stage 2 ( <strong>Normalization</strong> ), but also, in particular, Stage 0 ( <strong>Transcription</strong> ), Stage 1 ( <strong>Tokenization</strong> ), and Stage 4 ( <strong>Analysis</strong> ). The distinction is instrumental for the main argument of this essay, anticipated above, which is that normalization does not occur only at the <strong>Normalization</strong> stage, and it is, instead, pervasive in machine-assisted collation.</p><p>Normalization can be performed at different textual levels and it affects many types of textual variation. For example, in addition to the case-folding described above, researchers might wish to neutralize the <strong>graphemic</strong> distinction between Latin alphabet regulars(U+0073 LATIN SMALL LETTER S) and longſ(U+017F LATIN SMALL LETTER LONG S). <strong>Orthographic</strong> variation may also transcend the graphemic level, as in the orthographic distinction between US Englishcolorand Britishcolour,or the logographic replacement, rooted in homonymy, of the English prepositiontoby the digit2in a telegraphic communicative style popular on social networks. <strong>Morphological</strong> variation includes differences in categories like tense, gender, number, and others; for example, a collation operation might want to recognize that different inflected forms of the same lexeme (such as Englishisandare,which have no characters in common) may be textual variants, and are therefore potential candidates for alignment. <strong>Lexical</strong> variants might be identified by semantic features; for example, if one manuscript witness readsbooks and magazinesand another readsjournals,we might want to alignjournalswithmagazines,rather than withbooks,becausejournalsis semantically closer tomagazinesthan it is tobooks.For multilingual alignment of witnesses in languages with greatly different orthographies (such as Greek [Greek script] and Old Church Slavonic [Glagolitic or Cyrillic script]), normalization might take the form of recording only the part of speech in the shadow copy, since in this research context, part of speech is a better predictor of the alignment of variants than any modification of the orthographic strings<a class=footnote-ref href=#birnbaum2018> [birnbaum2018]</a></p><p>Normalization, in a nutshell, makes it possible to identify phenomena on multiple orthographic and linguistic levels and use them to create surrogates for the literal word tokens that then shape and interpret the results of collation, tacitly neutralizing and ignoring other distinctions that are present in the literal tokens. During the process of normalization, an original form is replaced (or, rather, shadowed, since the original form is typically retained and is available for use in the eventual rendered output) by a normalized form. The point of this deliberate neutralization is that different original forms that are normalized (or, in computational terms, thathash) to the same value are deemed to be the same at a certain point of the process of machine-assisted collation.</p><h2 id=machine-assisted-collation-the-gothenburg-model>Machine-assisted collation: the Gothenburg model</h2><p>Within a computational pipeline, the output of one process becomes the input to the next, much as water may flow through plumbing that is constructed by concatenating small pieces of piping<a class=footnote-ref href=#mcilroy1964> [mcilroy1964] </a>.<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup> Under the Gothenburg model, the digitized text of each witness is piped first through <strong>Tokenization</strong> (Stage 1) and then through <strong>Normalization</strong> (Stage 2), at which point it is joined with all other witnesses to constitute the complex input into <strong>Alignment</strong> (Stage 3). The single output of <strong>Alignment</strong> is then piped through <strong>Analysis</strong> (Stage 4) and <strong>Visualization</strong> (Stage 5) to generate the eventual output of the collation operation. This is illustrated in Figure 1, below, where three witnesses are collated, and Stage 2 ( <strong>Normalization</strong> ) is highlighted in green.<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup></p><figure><img loading=lazy alt src sizes="(max-width: 768px) 100vw, 80vw" srcset="500w,
800w, w" class=landscape><figcaption><p>Gothenburg model.</p></figcaption></figure><p>The key contribution of the Gothenburg model to our understanding of collation is its modularization of the process in a way that facilitates customization<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup> and is agreed upon by a community of users.<sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup> Instead of a monolithic black box that ingests witness input and extrudes a collation as a single process (that is, a degenerate pipeline with a single step), a collation system built according to the Gothenburg model, such as CollateX, can expose hooks that support developer intervention at any stage of the process without a need to interact explicitly with the other stages. For example, a user can replace the default CollateX <strong>Normalization</strong> with a customized alternative (see below) without having to touch (or, for the most part, even know about) the implementation of the other Gothenburg stages. As long as the input and output format of each stage is documented, replacing a step in the pipeline is an autonomous operation, that is, one that does not require accommodation elsewhere in the system. In this report we rely for illustrative examples on CollateX, which may be considered the reference implementation of the Gothenburg model.<sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup></p><h2 id=discussion>Discussion</h2><p>This essay challenges the characterization of normalization in the Gothenburg model as something that happens in a single location in the chain of processes (at Stage 2, called <strong>Normalization</strong> ). Instead, as we illustrate below, normalization may be performed in four different stages within the overall collation process, not only to improve the alignment, but also to annotate the sources with a layer of information that may be vital for subsequent analysis. In the following description of the five stages of the Gothenburg model (preceded by <strong>Transcription</strong> , which we call Stage 0), the notation(n)after the section heading means that normalization plays a role in the stage.</p><h2 id=stage-0-transcription-for-the-purpose-of-collation-n>Stage 0. Transcription for the purpose of collation (n)</h2><p>Except in the case of born-digital character-based data, witnesses to be input into a collation engine must first be digitized, that is, transcribed, whether manually or with machine assistance (e.g., OCR). In the case of handwritten documents, this transcription entails the conversion of an analog original to a digital character-stream surrogate. This transcription necessarily is not a one-to-one mapping because the handwritten analog original is infinitely variable, while the distinct character types in the digital result are limited by the character inventory. In practice, the level of detail in the transcription (that is, the extent to which it isdiplomatic) depends on the goals and purposes of the edition, and for that reason, “transcripts are best judged on how useful they will be &mldr;, rather than as an attempt to achieve a definitive transcription” <a class=footnote-ref href=#robinson1993>[robinson1993] </a>. Furthermore, Robinson and Solopova continue, because the complete character representation of all features on a handwritten original is both impossible and undesirable, transcriptions “must be seen as fundamentally incomplete and fundamentally interpretative” <a class=footnote-ref href=#robinson1993>[robinson1993] </a>.</p><p>The conflation of multiple physically different written signs into the same single digital characters is an instance of normalization, that is, of determining, sometimes subconsciously, that certain objective differences may (or, perhaps, should) be ignored because they are not relevant informationally.<sup id=fnref:7><a href=#fn:7 class=footnote-ref role=doc-noteref>7</a></sup> The risk of normalization during manual transcription, though, is that information that is discarded during transcription is not available at later stages of collation, or during subsequent use of the collation output in research. Researchers are nonetheless comfortable with this type of normalization not only because it is inevitable, for the reasons described above, but also because they are confident that the data that they are excluding is not informational. The alternative, that is, trying to transcribe with as little normalization as possible, not only cannot be perfect, but also comes at a cost, because the greater the number of graphic distinctions the researcher tries to preserve during transcription, the greater the price in terms of both efficiency (because the researcher must be alert to more details) and accuracy (because there is more opportunity for distraction, error, and inconsistency)<a class=footnote-ref href=#robinson1993> [robinson1993] </a>. A sensible compromise, especially in situations where photographic facsimiles are available, and therefore reduce the documentary value of a <em>hyperdiplomatic</em> transcription, is that the digital transcription should preserve differences in the original orthography that might be needed for adequate rendering (as determined by the goals of the project) or serve as eventual input into computational analysis (which might include alignment within the collation process, subsequent analysis of patterns of variation, or orthographic or linguistic analysis that is not connected explicitly to collation).<sup id=fnref:8><a href=#fn:8 class=footnote-ref role=doc-noteref>8</a></sup></p><p>Normalization during transcription need not be entirely silent. For example, as a way of accommodating both diplomatic and normalized representations during transcription, the Text Encoding Initiative (TEI) makes it possible to couple the two formally where that is sensible and practical, as in the following example from the TEI P5 Guidelines:<sup id=fnref:9><a href=#fn:9 class=footnote-ref role=doc-noteref>9</a></sup></p><blockquote></blockquote><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-xml data-lang=xml><span style=display:flex><span><span style=color:#f92672>&lt;l&gt;</span>But this will be a <span style=color:#f92672>&lt;choice&gt;</span> <span style=color:#f92672>&lt;orig&gt;</span>meere<span style=color:#f92672>&lt;/orig&gt;</span> <span style=color:#f92672>&lt;reg&gt;</span>mere<span style=color:#f92672>&lt;/reg&gt;</span> <span style=color:#f92672>&lt;/choice&gt;</span> confusion<span style=color:#f92672>&lt;/l&gt;</span> <span style=color:#f92672>&lt;l&gt;</span>And hardly shallter we all be <span style=color:#f92672>&lt;choice&gt;</span> <span style=color:#f92672>&lt;orig&gt;</span>vnderstoode<span style=color:#f92672>&lt;/orig&gt;</span> <span style=color:#f92672>&lt;reg&gt;</span>understood<span style=color:#f92672>&lt;/reg&gt;</span> <span style=color:#f92672>&lt;/choice&gt;</span> <span style=color:#f92672>&lt;/l&gt;</span>
</span></span></code></pre></div><p>This strategy makes it possible, outside a collation context, to index or search for words according to their normalized forms while rendering them according to the variant orthography that appears in the source. It is nonetheless the case that even the <code>&lt;orig></code> reading necessarily undergoes some degree of normalization as part of the transcription process.</p><h2 id=stage-1-tokenization-n>Stage 1. Tokenization (n)</h2><p>The alignment of segments of text for collation presupposes the division of a single continuous stream of characters into tokens, typically words (however we define them) and punctuation marks, although nothing precludes other forms of tokenization.<sup id=fnref:10><a href=#fn:10 class=footnote-ref role=doc-noteref>10</a></sup> CollateX, for example, incorporates a default tokenization function that splits the input into sequences of word characters (regex <code>\w+</code> ) plus any trailing whitespace (regex <code>\s+</code> ), and further breaks off punctuation marks together with any trailing whitespace into their own tokens (regex <code>\W+</code> ). In this way, for example, a word followed by a period at the end of a sentence will be recognized as string-equal to the same word without the period in a different witness.</p><p>How to manage whitespace, which is one of the common issues that must be resolved during tokenization, might also entail forms of normalizations. Although tokenization on whitespace in some other programming contexts (such as the XPath <code>tokenize()</code> function or the Python <code>str.split()</code> method with a nullseparatorvalue) may regard whitespace as a separator between tokens that should be discarded, default tokenization in CollateX, for example, keeps the whitespace as the trailing part of the preceding token, so that nothing is discarded during tokenization.<sup id=fnref:11><a href=#fn:11 class=footnote-ref role=doc-noteref>11</a></sup> This whitespace is subsequently removed in CollateX by the default <strong>Normalization</strong> stage unless that is overridden (see the following section), thus preserving the strict Gothenburg model distinction between <strong>Tokenization</strong> and <strong>Normalization</strong> . Yet regarding all whitespace characters as equivalent for tokenizing the input, and regarding sequences of multiple whitespace characters as equivalent to a single space, both of which are part of the default implementation of <strong>Tokenization</strong> in CollateX, are forms of normalization, that is, situations where forms that are not string-equal are nonetheless deemed to be equivalent for a particular purpose.</p><p>Thanks to the modular architecture of the Gothenburg model adopted by CollateX, which provides hooks into the <strong>Tokenization</strong> stage, users can replace the default <strong>Tokenization</strong> with custom code without needing to know about or otherwise touch the code that manages the other stages of processing.<sup id=fnref:12><a href=#fn:12 class=footnote-ref role=doc-noteref>12</a></sup> For example, where punctuation is not intended to be used in alignment but must nonetheless remain available in the final output, it is possible to tokenize only on sequences of whitespace characters, regarding punctuation not as a separate token, but as part of the word token that precedes or follows it, where it can then be ignored (during Stage 2) for the alignment (during Stage 3).</p><h2 id=stage-2-normalization-n>Stage 2. Normalization (n)</h2><p>The Gothenburg model regards <strong>Normalization</strong> primarily as a way of improving Alignment by recognizing that tokens that are not string-equal should nonetheless be deemed to be equivalent for the purpose of <strong>Alignment</strong> :</p><blockquote><p>It might suffice to normalize the tokens’ textual content such that an exact matching of the normalized content yields the desired equivalence relation. For instance, in many cases all tokens of the text versions are normalized to their lower-case equivalent before being compared, thereby making their comparison case insensitive. Other examples would be the removal of punctuation, the rule-based normalization of orthographic differences or the stemming of words.<br><a class=footnote-ref href=#collatex>[collatex] </a>As implemented within CollateX, a token is a complex object that contains at least two properties: a t (text) property, which represents the string value of the token after <strong>Tokenization</strong> , and an n (normalized) property, which represents the result of applying <strong>Normalization</strong> to the t value. The software uses agreement among the n properties to recognize tokens that should be aligned. The t and n values are created either by the built-in default <strong>Tokenization</strong> and default <strong>Normalization</strong> operations,<sup id=fnref:13><a href=#fn:13 class=footnote-ref role=doc-noteref>13</a></sup> or by custom operations implemented by the researcher to replace the defaults. The fact that each token has the t and n properties means that CollateX exposes the output of <strong>Tokenization</strong> alongside the output of <strong>Normalization</strong> , providing hooks for customization that make the software useful for scholars with a wide variety of editorial requirements.</p></blockquote><p><strong>Normalization</strong> at Stage 2 in the Gothenburg process serves a specific purpose: it is performed so that variation that the editor considers unimportant for alignment will not influence Stage 3, when <strong>Alignment</strong> is performed.<sup id=fnref:14><a href=#fn:14 class=footnote-ref role=doc-noteref>14</a></sup> This is different from the purpose of normalization during <strong>Transcription</strong> (which we called Stage 0, above) or during <strong>Analysis</strong> (Stage 4, below), and it has different consequences. If the editor normalizes the text during <strong>Transcription</strong> and does not record the non-normalized forms, those forms become irretrievable. <strong>Normalization</strong> at Stage 2 of the Gothenburg model, however, is non-destructive, since the normalized form is created as a shadow copy of the original, and not as a replacement for it. As Bleeker explains:</p><blockquote><p>normalizationin the context of automated collation is not equivalent to normalization that happens in transcription. For example, editors can transcribe orthographic variation because they consider it important to be preserved in both the transcription and the collation output. However, in the collation process itself, they may want to normalize orthographic variation because they do not want it to influence the alignment. In that case they need to normalize their tokens before inputting them in the collation software.<br><a class=footnote-ref href=#bleeker2017>[bleeker2017] </a>Common types of normalization, some of which were mentioned in the introduction, above, are discussed individually in the subsections below. Many of these examples might have been aligned correctly even without normalization because of forced matches, near-matching (see below), or — in situations where the software can decide only arbitrarily among alternatives — by chance.<sup id=fnref:15><a href=#fn:15 class=footnote-ref role=doc-noteref>15</a></sup> But because of the computational complexity of alignment, normalization that leads to the identification of more exact matches (of the normalized shadows of the tokens) can nonetheless improve both the accuracy and the efficiency of the overall performance.</p></blockquote><h2 id=case-folding-orthographic>Case-folding (orthographic)</h2><p>In the following example from the <em>Frankenstein variorum</em> Project, the case distinction betweenSAVILLEandSavillein the third token column is not regarded as significant for <strong>Alignment</strong> purposes:<sup id=fnref:16><a href=#fn:16 class=footnote-ref role=doc-noteref>16</a></sup> Case distinction in the <em>Frankenstein variorum</em> ProjectWitnessTokens1818ToMrs.SAVILLE,England.1823ToMrs.SAVILLE,England.ThomasToMrs.SAVILLE,England.1831ToMrs.Saville,England.These tokens would have been aligned correctly in any case because they constitute a forced match betweenMrs.andEngland.But the information stored and processed during the <strong>Normalization</strong> stage is available not only for <strong>Alignment</strong> , but also, for example, for distinguishing, during subsequent <strong>Analysis</strong> , forced matches that are not deemed equivalent from those that are.</p><h2 id=graphemic-orthographic>Graphemic (orthographic)</h2><p>In early Cyrillic writing,з(U+437 CYRILLIC SMALL LETTER ZE) andѕ(U+0455 CYRILLIC SMALL LETTER DZE) had different pronunciations and distribution in Old Church Slavonic, but came to be used largely interchangeably in early East Slavic writing, including in the <em>Rus′ primary chronicle</em> .<sup id=fnref:17><a href=#fn:17 class=footnote-ref role=doc-noteref>17</a></sup> In the last column in the following example from that work (4,8), the Xle witness uses one variant and other witnesses agree on the other, and the editors neutralize the distinction at the <strong>Normalization</strong> stage, so that the forms will be deemed equivalent for purposes of <strong>Alignment</strong> :<sup id=fnref:18><a href=#fn:18 class=footnote-ref role=doc-noteref>18</a></sup><br>Graphemic variation in the <em>Rus′ primary chronicle</em> WitnessTokensLavпосемужеморюсѣдѧтьварѧ|зиTroпосемужеморюсѣдятьварязиRadпо |семоужеморюприседѧтьварѧзиAkaпосемѹжеморюприседѧтъварѧзи.Ipaпосемужеморюсѣдѧтьва|рѧзиXleпосемоужеморюсѣдѧтварѧѕи.ByčпосемужеморюсѣдятьВарязиŠaxпосемужеморюсѣдятьВарязиLixПосему&amp;#xao;жеморюсѣдятьварязиOstПосемужеморюсѣдятьВарязи</p><h2 id=spelling-orthographic>Spelling (orthographic)</h2><p>The 1818 and 1823 editions of Mary Shelley’s <em>Frankenstein</em> regularly use the US-associated spelling of the verbal suffix-ize,while the 1831 edition regularly uses the British-associated spelling-ise.In the <em>Frankenstein variorum</em> Project, this distinction can be neutralized before performing <strong>Alignment</strong> , as in the case oftranquillizeandtranquillisein the seventh token column in the following example.<br>Spelling variation in the <em>Frankenstein variorum</em> ProjectWitnessTokens1818fornothingcontributessomuchtotranquillizethemind1823fornothingcontributessomuchtotranquillizethemind1831fornothingcontributessomuchtotranquillisethemind<br>The same phenomenon can be observed in the following example, from the <em>Lancelot</em> manuscript transmission<a class=footnote-ref href=#spadini2016> [spadini2016] </a>, where Witness A preserves an archaic spelling and Witnesses B and C reflect the modern one:<br>Spelling variation in <em>Lancelot en prose</em> WitnessTokensAge-teconoisBartujeteconoisCartusjeteconois<br>When CollateX fails to find an exact string match during Alignment and there are multiple options for placing a token (in this case,gemay be placed in the first or second token column), the software arbitrarily defaults to the leftmost position, which, in the example above, is philologically incorrect.<sup id=fnref:19><a href=#fn:19 class=footnote-ref role=doc-noteref>19</a></sup> But by normalizing the two different spellings of the pronoun,gein Witness A is aligned correctly:<br>Correct alignment of spelling variants in <em>Lancelot en prose</em> WitnessTokensA-geteconoisBartujeteconoisCartusjeteconois<br>The <em>Rus′ primary chronicle</em> manuscripts illustrated above vary orthographically for reasons that are both properly orthographic (that is, concerned with scribal conventions for writing correctly) and underlyingly phonetic (insofar as the manuscripts were written at different times, they do not all represent the same state of the language). Identifying all individual neutralizations of letter differences for normalization is not practical because of the complexity of the systems and their relationships to one another. It is, however, possible to overcome this limitation with a normalization scheme that retains only the parts of the tokens that have a high information load with respect to grouping readings into variant sets. For this project we implemented a normalization scheme based on a modification of Soundex<a class=footnote-ref href=#soundex> [soundex] </a>, recognizing that, in these writing systems, the beginning of the word has a higher information load than the end, consonants have a higher information load than vowels, and some phonetic distinctive features have a higher information load than others. For details and examples see<a href=#birnbaum2015>Birnbaum 2015</a>.</p><h2 id=digits-orthographic>Digits (orthographic)</h2><p>The distinction between numbers spelled as digits and those spelled as words may be neutralized for alignment purposes. The following example is from the <em>Rus′ primary chronicle</em> (1,2), where, in the fourth token column, Ost uses an Arabic numeral; Rad, Aka, and Ipa use a Cyrillic letter to represent a numerical value, followed by a grammatical ending; and the other witnesses spell out the entire number as a word (a different number in Lav than in the others).<br>Number rendering in the <em>Rusʹ primary chronicle</em> WitnessTokensLavпопотопѣ.первиеснвеноеви.раздѣлишаTroпопотопѣтриесыновеноевираздѣлишаRadпопотопѣ.г҃есн҃веноеви.разделишаAkaпопотопе.г҃.есн҃веноеви.разделишаIpaпопотопѣбо.г҃.е.с҃нвеноевироздѣлишаXleПѡпотопѣоуботрїес҃новеноеви.раздѣлишаByčПопотопѣтриесыновеНоевираздѣлишаŠaxПопотопѣуботриесыновеНоевираздѣлишаLixПопотопѣтриесыновеНоевираздѣлишаOstПопотопѣ3-есыновеНоевираздѣлиша</p><h2 id=punctuation-orthographic>Punctuation (orthographic)</h2><p>If what is important to the researcher is to distinguish simply the presence or absence of punctuation, but not the form it takes, different punctuation tokens may be neutralized as a single, generic punctuation token. If punctuation is treated during tokenization as part of the word that precedes it, the punctuation marks may be compared literally, as they appear in the manuscripts, in which case, for example, a word followed by a comma will not match the same word followed by a dot. Alternatively, trailing punctuation can be normalized so as to be ignored during alignment, as in the example below from the <em>Rus′ primary chronicle</em> . Here in the fourth token column the Rad and Xle readings differ not only in the second letter, but also in the trailing punctuation, which is a dot in Rad and a comma in Xle:<br>Variation in punctuation in the <em>Rusʹ primary chronicle</em> WitnessTokensLavинд&lt;иꙗ>|поефратърѣку.вавилонъ.кордуна.TroиндияпоефратърекувавилонъкордунаRadмидиꙗ.|иефратрека.ивавилон.кордоуна.Akaмидиа.иефратърека.|ивавилонъ.кордѹна.Ipaмидиа.и |ефратърѣку.ивавилонъ.|кордуна.Xleмедїаиефратъ |рѣка,ивавѵлѡн.кордоуна.иByčМидияпоЕфратърѣку,Вавилонъ,Кордуна,ŠaxМидияиЕфратърѣкаиВавилонъ,Кордуна,LixМидияпоЕфратърѣку,Вавилонъ,Кордуна,αМидияиЕфратърѣкуиВавилонъ,Кордуна,</p><h2 id=morphological-linguistic>Morphological (linguistic)</h2><p>Morphological variation (e.g., inflectional endings that express number, gender, case, tense, and other linguistic categories) may be neutralized before <strong>Alignment</strong> , similarly to what in corpus linguistics is calledlemmatizationorstemming. In the following example the present (Witness A) and future (Witnesses B and C) tense of the verbchevaucher(to ride) might be normalized in order to obtain an optimal alignment.<br>Morphological variation in <em>Lancelot en prose</em> WitnessTokensAquegechevauchoie-atotmonpoirBquejechevalcheroie-atotmonpooirCsi-chevalcheroielaatotmonpooir</p><h2 id=lexical-linguistic>Lexical (linguistic)</h2><p>In Old French,pasandmieare negative particles that are used together with the negation adverbne. They have no textual characters in common, but their meaning and syntactic pattern of use is the same, making them candidates for alignment. In the following example, the alignment would be sub-optimal without a normalization of the two forms, becausemiewould be placed into the fourth token column, aligned withonin Witness B, instead of in the fifth, aligned withpas.<sup id=fnref:20><a href=#fn:20 class=footnote-ref role=doc-noteref>20</a></sup> If normalization is performed, the alignment is correct, as reflected in the table below:<br>Lexical variation in <em>Lancelot en prose</em> WitnessTokensAne-doit-mieatornerBnelidoitonpasatornerCneildoit-pasatorner</p><h2 id=syntactic-role-linguistic>Syntactic role (linguistic)</h2><p>Consider the following hypothetical alignment example from the CollateX development test suite:<br>Example of texts to be alignedWitnessUntokenized textAI bought this glass because it matches those plates.BI bought those glasses.<br>Alignment according to exact orthographic matching would align the two instances ofthose,producing:<br>Problematic alignment of the example in Table 10WitnessTokensAIboughtthisglassbecauseitmatchesthoseplates.BIboughtthoseglasses.<br>Some editors, though, might prefer to align the direct objects, that is, to assign greater weight to the syntactic role than to string matching. This would produce:<br>Preferable alignment of the example in Table 10WitnessTokensAIboughtthisglassbecauseitmatchesthoseplates.BIboughtthoseglasses.</p><h2 id=language-linguistic>Language (linguistic)</h2><p>In this section a segment of text from the Old Church Slavonic (OCS) <em>Codex Suprasliensis</em> is aligned with a reconstructed Greek parallel text. Because the two languages use different scripts, no type of orthographic normalization would improve string matching, but when a part of speech identifier is used as the shadow normalization property of the tokens, it allows for quite accurate alignment. In the following example, the second gap in the OCS is aligned with a Greek definite article because Greek has articles and OCS does not. The first gap reflects the presence of a personal pronoun in the Greek that happens not to be in the OCS. The tokens that are aligned in the two witnesses belong, pairwise, to the same parts of speech, which is how CollateX knew where to position the gaps<a class=footnote-ref href=#birnbaum2018> [birnbaum2018] </a>.<br>Multilingual alignment in the <em>Codex Suprasliensis</em> projectWitnessTokensOCSпосълалъктебѣи҅скоуситио̑усрь҆диѥGreekἀπέσταλκέμεπρὸςσέδοκιμάσαιτὴνπρόθεσίν</p><h2 id=stage-3-alignment>Stage 3. Alignment</h2><p>No normalization is performed during the <strong>Alignment</strong> stage of the Gothenburg model, but the alignment engine has access to the results of the normalization performed in the <strong>Normalization</strong> stage, as well as the less explicit normalization, described above, that happens during <strong>Transcription</strong> and <strong>Tokenization</strong> . And, as is explained below, because the output of <strong>Analysis</strong> (Stage 4) can be cycled back into another iteration of <strong>Alignment</strong> , <strong>Alignment</strong> also may have access to normalization performed during <strong>Analysis</strong> .</p><p>The input into the <strong>Alignment</strong> stage of CollateX is a set of token sequences, one sequence for each witness, where, as described above, each token has a t property, which represents a transcription of its reading according to the witness, and an n property, which represents a normalized version of the t property.<sup id=fnref:21><a href=#fn:21 class=footnote-ref role=doc-noteref>21</a></sup> As described above, Normalization is not limited to orthography; the researcher may create, as the n property, any surrogate for the token that will facilitate alignment. The alignment engine then uses only the n property to try to find the best alignment of the tokens, that is, the alignment that corresponds most closely to what a human philologist would produce.<sup id=fnref:22><a href=#fn:22 class=footnote-ref role=doc-noteref>22</a></sup></p><p>Alignment is, to be sure, a more complicated process than simply aligning the tokens with matching n values. On the one hand, in a text of any meaningful length, word tokens are likely to repeat, which means that alignment must associate the correct instances of each value, evaluating and making decisions about many-to-one or many-to-many correspondences. For example, in the following alignment table:<br>Correct alignment of repeated word tokenlaWitnessTokensAlaouilconquistparsachevalerieBouilconquistlagranthautesceClaouilconquistlagranthatece<br>“la” (token columns 1 and 5) occurs once each in witnesses A and B (in different locations) and twice in witness C. The alignment engine matches these instances correctly because, although it operates at a certain level with individual tokens, it also knows about sequences of vertically alignedblocks.This means that it has access to the context when deciding which instances of repeated tokens in one witness to align with which instances of matching tokens in the others.</p><p>Repetition (that is, the occurrence of multiple instances of the same token) is a well-known challenge for collation, and Andrews (2020) proposes an unusual normalization strategy as a way of meeting that challenge. Andrews observes that when a computational process aligns very common words because they are string-equal, they often do not belong to corresponding textual units, which both produces misalignments and increases the computation complexity of the collation process:<sup id=fnref:23><a href=#fn:23 class=footnote-ref role=doc-noteref>23</a></sup></p><blockquote><p>Common readings keep being treated as equivalent, just because they are identical! This often throws off the collation of more substantive words, because the collator doesn’t know how to tell what is substantive or not. In a long text with many witnesses, this can throw off a collation [&mldr; and &mldr;] the collation algorithm was taking longer and longer.<br><a class=footnote-ref href=#andrews2020>[andrews2020] </a>To avoid the spurious alignment of tokens that, although string-equal, are so common that their correspondence is more likely to be accidental than philologically meaningful, Andrews replaces these common tokens, as well as punctuation tokens, with random strings during <strong>Normalization</strong> . As a consequence of this random replacement, the normalized surrogates do not agree with anything during <strong>Alignment</strong> , and therefore do not complicate or confuse that process. This enables substantive agreements to govern the alignment, improving both the quality of the output and the speed of the operation (Andrews reports that it “made the process run 4–5 times faster” ).</p></blockquote><p>On the other hand, the alignment engine may also choose to align non-matching tokens that fall in the same position, as in the fourth position in the example below.<br>Forced match in the alignmentWitnessTokensAlorsabatladestremanicledesonhaubercBlorsabatlasenestremanicledesonhaubercClorsabatlasenestremanicledesonhauberc<br>The token alignment in seven of the eight columns reflects exact string equality across all witnesses, butdestre(right) andsenestre(left), in the fourth token column, are not an exact match. This alignment is forced by the context: given that thelatokens before and themanicletokens afterwards are aligned as exact matches in all witnesses, the tokens between them fall into alignment by default, a situation we call aforced match.<sup id=fnref:24><a href=#fn:24 class=footnote-ref role=doc-noteref>24</a></sup> <strong>Alignment</strong> is further complicated by token transposition, where tokens in one order in one witness may need to be aligned with tokens in a different order in another witness, while nonetheless retaining each witness’s token order.</p><p>Finally, the algorithms most commonly used to implement alignment routines, such as Needleman-Wunsch<a class=footnote-ref href=#needleman1970> [needleman1970] </a>or the Dekker<a class=footnote-ref href=#dekker2011> [dekker2011] </a>algorithm in CollateX, reduce the computational complexity of the alignment process only at a cost to the philological integrity of the output:</p><blockquote><p>These two algorithms follow the principle ofprogressive alignment,which means that they do not compare all witnesses at the same time. Instead, they first compare two witnesses, store the result of that comparison in a so-called variant graph and then progressively compare another witness against that graph, at every turn merging the result of that comparison into the graph until all witnesses are merged. This progressive alignment method reflects the idea ofdynamic programming,which takes a complicated problem and breaks it down in smaller sub-problems. In this case, the complicated task of multi-witness alignment is broken down into a repetition of the relatively easier task of two-witness alignment. A downside of progressive alignment is that, apparently, the order in which the witnesses are compared influences the final result. That is, the final alignment of three witnesses A, B, and C may differ if Witness C is compared against the variant graph of Witness A and B, or if Witness B is compared against the variant graph of Witness A and C.<br><a class=footnote-ref href=#bleeker2017>[bleeker2017] </a>The preceding means that progressive alignment cannot be considered an optimal strategy for multiple witness collation, since the philologically optimal alignment of the witnesses obviously cannot logically depend on the order in which the philologist (or algorithm) looks at them<a class=footnote-ref href=#spadini2017> [spadini2017] </a>.<sup id=fnref:25><a href=#fn:25 class=footnote-ref role=doc-noteref>25</a></sup> At the same time, the computational complexity of comparing all witnesses to all other witnesses simultaneously means that optimal order-independent multiple-witness alignment is currently an unsolved problem in computer science,<sup id=fnref:26><a href=#fn:26 class=footnote-ref role=doc-noteref>26</a></sup> and progressive alignment algorithms, despite their limitations, represent the current state of the art.</p></blockquote><p>Alignment algorithms typically test only for exact matches between tokens, because looking for the closest but inexact match is prohibitively expensive computationally. For that reason, the CollateX implementation of near matching, which we describe below, is performed in the <strong>Analysis</strong> stage and then recycled into another instance of <strong>Alignment</strong> . What is important in this discussion of Stage 3 is that, to the extent that near matching can be said to entail a type of normalization, it happens not in Stage 2, but, rather, in Stage 4, after an initial alignment has already been constructed.</p><h2 id=stage-4-analysis-and-interpretation-n>Stage 4. Analysis and interpretation (n)</h2><p>The 2009 meeting that led to the elaboration of the Gothenburg method did not produce a final report or white paper, and in the absence of an authoritative definition, the type of analysis that occurs at Stage 4 has been interpreted variously as both computational postprocessing and manual, human intervention between the output of <strong>Alignment</strong> (Stage 3) and the input into <strong>Visualization</strong> (Stage 5). In situations where the alignment is sub-optimal and cannot be repaired algorithmically, human intervention becomes necessary. The cost of this intervention with respect to the workflow, though, is that it introduces changes into a generated interim artifact, rather than into thebase view, that is, the transcribed witness files that serve as the initial input into the processing pipeline. Any changes introduced manually beyond the base view mean that if the collation operation must be re-run from the beginning (for example, if a new witness is added, or if editorial decisions about word division change), the manual effort is lost, and must be repeated. In other words, manual intervention creates a new, derived base view, one that can no longer be generated from the original base view entirely by the computational pipeline.</p><p>The purpose of the <strong>Analysis</strong> in Stage 4 in the Gothenburg model, whether automated or implemented manually, is described clearly in the CollateX documentation. In the case of sub-optimal output from the <strong>Alignment</strong> stage:</p><blockquote><p>[a]n additional [&mldr;] analysis of the alignment result [&mldr;] may alleviate that deficiency by introducing the possibility of a feedback cycle, in which users edit the alignment and feed their knowledge back into the alignment process for another run delivering enhanced results.<sup id=fnref:27><a href=#fn:27 class=footnote-ref role=doc-noteref>27</a></sup><br>Postprocessing performed at Stage 4, in addition to possibly improving the eventual alignment, may also be used to infer knowledge from the information added to the original texts through the first three Stages of the collation pipeline. That is, the end products of a full collation pipeline, from <strong>Tokenization</strong> through <strong>Visualization</strong> , may include not only a critical edition with variation, but also, for example, summary analytic reports about the tradition, whether textual (e.g., in statistical tables) or graphic (e.g., in charts or diagrams).</p></blockquote><p>In this section we identify two types of computational analysis, both involving normalization, that are located at Stage 4 of the Gothenburg model. These are 1) near matching, 2) the analysis of patterns of agreement within alignment sets. Near matching is intended to improve the alignment, which is to say that the output of this Stage 4 process, after modification for near matching, is fed back into Stage 3 for realignment. The analysis of patterns of agreement is intended to enrich and customize the collation output, and is fed into Stage 5 ( <strong>Visualization</strong> ). Especially in light of the absence of clear guidance in the literature about the Gothenburg model concerning the <strong>Analysis</strong> stage, the two types of computational analysis discussed here should be regarded as only some of the possibilities available at this stage of the collation pipeline.</p><h2 id=near-matching>Near matching</h2><p>The termnear matching, sometimes calledapproximateorfuzzymatching, refers to the identification of the closest match for alignment purposes, specifically in situations where there is no exact match.<sup id=fnref:28><a href=#fn:28 class=footnote-ref role=doc-noteref>28</a></sup> As was noted above, when CollateX compares the n properties of tokens to create an initial alignment, it looks only for exact string matches. That is, to align a token it asks notwhat is the closest match?,but, rather,is there an exact match?The computational complexity of checking for the closest match is sufficiently greater than the complexity of checking for an exact match that it would not be realistic to perform an entire alignment operation by computing all closest matches in all alignment positions. Furthermore, as noted earlier in the context of forced matches, as long as the alignment process finds a sufficient number of exact matches, a large number of inexact matches are likely to be forced into proper alignment anyway.</p><p>A situation that is susceptible to incorrect alignment involves the following two features:</p><p>One witness has fewer tokens than another, which means that there will be a gap in the alignment, where a token in the longer witness does not have a corresponding token in the shorter one.There is a token in the shorter witness that is adjacent to the gap and that does not have an exact match in any of the alignment positions where it could be placed.</p><p>When both of the preceding conditions are met, an alignment engine that relies on exact matching is unable to decide where to position the gap, that is, whether to push a token with two or more possible inexact adjacent alignment points to the left or to the right. It is at this stage that near matching can be used to resolve the uncertainties, and because the number of comparisons for this type of limited, strategically targeted near matching is exponentially less demanding computationally than what would be required to perform near matching on all tokens during the initial alignment, it does not impose an unacceptable delay.</p><p>As an example, if we need to alignThe gray koalawithThe grey koala(note the different spellings of the color adjective) and we have not normalized one of the spellings to the other, the color words will nonetheless wind up aligned correctly because they represent a forced match between the perfect matches ofTheto the left andkoalato the right. But suppose we have to alignThe gray koalawithThe fluffy grey koala,that is, suppose we have to decide whether to aligngrayin the first witness withfluffyor withgreyin the second. Without near matching, CollateX has to guess, and its arbitrary strategy is to push the token in question to the left, which will produce an incorrect result:<br>Problematic alignment of similar word tokensWitnessTokensAThegraykoalaBThefluffygreykoala<br>The alignment a scholar would prefer is:<br>Correct alignment of similar word tokensWitnessTokensAThegraykoalaBThefluffygreykoala<br>With near matching, however, the <strong>Analysis</strong> stage could determine thatgrayis more similar, in terms of its string value, togreythan it is tofluffy,and recycle this information into a second, targeted, <strong>Alignment</strong> process that would position the token accordingly. This is how near matching works in CollateX.<sup id=fnref:29><a href=#fn:29 class=footnote-ref role=doc-noteref>29</a></sup></p><p>The following example of the use of near matching in the <strong>Analysis</strong> stage to correct misalignments in the <strong>Alignment</strong> stage is from the <em>Rus′ primary chronicle</em> (3,5). In the table below, which presents the output of CollateX without near matching, the last token of Tro is misaligned. It is not string-equal to any token in either of the last two columns (note the fourth letter of the word, which does not match the fourth letter of the words in the last column of the other witnesses), so CollateX arbitrarily pushes it to the left, even though it is a closer match with the tokens to the right.<br>Incorrect alignment of similar word tokens in the <em>Rus′ primary chronicle</em> WitnessTokensLavгаръматитавріани.сируфьꙗ.фраци.TroгаръматитаврианискуфиафракиRadсарматитаврилнискоуфиаифрациAkaсармати.таврїанискѹфїа.ифрациIpaсармати.тавриани.скуфиꙗ.фраци.Xleсармати.таврїани.скѵфїафраци.BychСаръмати,Тавриани,Скуфиа,Фраци,ShakhСармати,Тавриани,Скуфия,Фраци,LikhСаръмати,Тавриани,Скуфиа,Фраци,OstСармати,Тавриани,Скуфия,Фраци,<br>Near matching is optional in CollateX, and without it we get the output above. If we turn on near matching, though, the last token in Tro is moved to the right because it is a closer match to tokens in the right column than to any in the left.</p><p>Near matching in CollateX uses the python-Levenshtein package to calculate similarity, which means that it does not have access to variation information beyond the character level, such as lexical substitution. A small Levenshtein distance can identify situations where scribes might have misread or miswritten individual letters and introduced a small, local corruption. But scribes might also intervene consciously to replace one entire word with another, and in such situations, the Levenshtein distance between the words is not necessarily a useful measure of <em>editorial</em> distance, that therefore also not necessarily a useful predictor of the optimal alignment. In such cases, a comparison metric that did not rely solely on Levenshtein character edit distance, and that also had access to other properties, might achieve a philologically preferable alignment.</p><h2 id=patterns-of-agreement>Patterns of agreement</h2><p>Normalization is ultimately a decision about when objectively different written forms should be deemed equivalent <em>for a specific purpose</em> . At the <strong>Alignment</strong> stage (whether initially or with recycled input after near matching, as described above), that specific purpose is to determine which tokens should be considered part of the same variant set, whatever the category of variation. For subsequent philological research, however, it is common to distinguish betweenformalandsubstantive(sometimes calledinsignificantandsignificant) variants, where only the latter are traditionally regarded as useful for stemmatic purposes:</p><blockquote><p>philological judgement is deployed to distinguishsignificantfrominsignificanttextual variation — that is, to select those variants that are more or less likely to betray information about the exemplar from which a given text was copied.<sup id=fnref:30><a href=#fn:30 class=footnote-ref role=doc-noteref>30</a></sup><br><a class=footnote-ref href=#andrews2016>[andrews2016]</a></p></blockquote><p>As described above, <strong>Normalization</strong> for <strong>Alignment</strong> purposes (Stage 2, feeding into Stage 3) in CollateX, by default, writes a shadow value to be used during alignment into an n property on the token. A user-designed replacement for the default normalization routine built into Stage 2 could customize the modifications employed to create the n property, and could also add other properties to the tokens — for example, in addition to orthographic normalization in the n property, it might identify the lemma, the part of speech, and the morphology, and write those into l, p, and m properties. The built-in <strong>Alignment</strong> stage of CollateX ignores these other properties, but it passes them along the pipeline, which means that they are accessible at later stages. Recalling that normalization is “a decision about when objectively different written forms should be deemed equivalent <em>for a specific purpose</em> ,” we might perform a different type of normalization (indeed, a form of interpretation) during <strong>Analysis</strong> in Stage 4, comparing tokens that have already been aligned with one another to determine whether their l, p, and m properties (or some subset of those) agree. This type of analysis would enable us to distinguish, within a variant set, which readings agree completely (all properties), which agree in traditionally significant properties but differ in insignificant ones (e.g., agree in l, p, and m, but not n), and which differ in significant properties (e.g., disagree in l, p, or m).<sup id=fnref:31><a href=#fn:31 class=footnote-ref role=doc-noteref>31</a></sup></p><p>In the following example, the tokensgeandjeboth have the same (hypothetical) substantive properties: l (lemma) =je,p (part of speech) =personal pronoun,and m (morphology) =first person singular.But because the original reading (t property) is different, we deduce that the variant is orthographic or phonological, which we consider formal, rather than substantive. Toward the end of the sentence,conoisis aligned withfes. In this case, their p and m properties are equal (p = verb, m = present first person singular), but the l properties carry different values, which means that the variant is lexical.<br>Categorized variantsWitnessTokensAgeteconoismielzquetuneconoismoiBArtujeteconoismielsquetunefesmoiVariant typeform.form.lex.<br>This information could be passed along to the <strong>Visualization</strong> stage for different purposes. For example, the output of the <strong>Visualization</strong> stage might be a critical apparatus where variants are grouped according to whether they agree in substantive properties, regardless of whether their n values coincide. Independently of the way variants might be presented in a text intended for critical reading, tables or graphic representations of the pairwise agreement in different types of significant features among witnesses might let us hypothesize about textual filiation and transmission. From this perspective, the output in the <strong>Visualization</strong> stage might include a report about textual relationships among the witnesses, or a dendrogram representation of the result of agglomerative clustering of the witnesses, as in a phylogenetic tree. Such an analytic product is no less a visualization of textual relations than a reading view that reproduces the full text with apparatus, even though it is an abstraction of the relationships, and not an organized reproduction of the primary data. Most importantly, as long as the pipeline is completely automated, different scripts can be run or re-run in order to obtain different, but complementary visualizations.</p><h2 id=stage-5-visualization>Stage 5. Visualization</h2><p>CollateX supports several output views of the variant graph, including an alignment table (in plain text or HTML, as well as CSV and TSV), a graph (in SVG), XML (both TEI and a generic XML that is suitable for subsequent transformation with XSLT), and a JSON document. Of these built-in output views, only the JSON output renders both the n property and all custom properties assigned during earlier processing, which means that users can post-process the JSON output in order to create custom visualizations. Some other built-in output formats are also able to render more than one property value for a token. For example, two types of SVG output are supported, one with just the n property value and the other with the n value and, for each node in the variant graph, all associated t values and the identifiers of the witnesses in which those readings are found, as in the figure below (Fig. 2). In this graphic, the n value is made of the part of speech and the lemma, and is written in bold in the top left cell for each node; the corresponding t values are listed below, together with the identifiers of the witnesses in which they can be found.</p><figure><img loading=lazy alt src sizes="(max-width: 768px) 100vw, 80vw" srcset="500w,
800w, w" class=landscape><figcaption><p>CollateX output SVG graph.</p></figcaption></figure><h2 id=conclusion>Conclusion</h2><h2 id=revisiting-the-gothenburg-model-and-its-collatex-implementation>Revisiting the Gothenburg model and its CollateX implementation</h2><p>The Gothenburg model has advanced our understanding of the machine-assisted analysis of textual variation in both conceptual and practical ways. The modular nature of the model recognizes both the substantial independence of the five stages and, at the same time, the extent to which they interact within a processing pipeline. This modular conceptualization, in turn, has enabled modular implementation, where an application such as CollateX incorporates hooks that permit the user to modify one stage of the pipeline without having to interact with the others except by continuing to observe the input and output specifications built into the API.</p><p>Meanwhile, with more than ten years of experience of the Gothenburg model behind us, we now also recognize aspects of the model, and of its implementations, that could benefit from further consideration. For example, <strong>Visualization</strong> (Stage 5) might more transparently be calledOutputorRendering.The implementation of the model in CollateX is a good example of this, since the richest output format supported, JSON, is intended not for human visual consumption, but for automated post-processing. Additionally, in CollateX the modular stages might have been implemented with even greater mutual independence. For example, the output format (text table, HTML, SVG, JSON, etc.) is specified as an argument to the function that performs the <strong>Alignment</strong> , which means that generating multiple output formats requires performing the same <strong>Alignment</strong> operation anew for each of them.<sup id=fnref:32><a href=#fn:32 class=footnote-ref role=doc-noteref>32</a></sup> The clearer understanding of the model that comes from a decade of experience with the CollateX implementation suggests directions for further development that could continue to enhance the benefits that are already visible in both the modular nature of the Gothenburg model and its implementation in the software.</p><h2 id=revisiting-normalization>Revisiting normalization</h2><p>A uniquely broad and consequential insight in this retrospective context is that <strong>Normalization</strong> (Stage 2) and normalization (with a lower-casen) are different, and normalization (smalln) is broadly and correctly distributed over several stages of the pipeline. <strong>Normalization</strong> (Stage 2) means identifying string-level differences in witness tokens that should be ignored for alignment purposes, but the point of this report has been to explore the many ways in which normalization (smalln) is pervasive, affecting not only Stage 2, but also Stage 0 ( <strong>Transcription</strong> ), Stage 1 ( <strong>Tokenization</strong> ), and Stage 4 ( <strong>Analysis</strong> ). The type of normalization that is applied at these different stages emerges from a variety of considerations. For example, the character-based transcription of handwritten sources in Stage 0 necessarily entails normalization because handwriting is infinitely variable, and infinite variation cannot be represented usefully in character data. And the normalization applied in Stage 4 to support near-matching in CollateX is an accommodation to the intractable computational complexity of implementing near-matching globally as part of the <strong>Alignment</strong> stage. In other words, despite the overall clarity, accuracy, and utility of the modularization of the Gothenburg model, both scholars and developers benefit from an awareness that normalization as part of a collation pipeline neither begins nor ends with Stage 2, and also that normalization may be used in analysis and reporting in situations that are independent of alignment.</p><p>Figure 3, below, reproduces Figure 1, the earlier plot of the five stages of the Gothenburg model of textual collation, with the addition of an explicit Transcription (or digitization) stage between the Input (the documentary artifact) and <strong>Tokenization</strong> , which serves as Stage 1 of the original model. Additionally, in this revised figure <em>all</em> of the stages that involve normalization are highlighted in green:</p><figure><img loading=lazy alt src sizes="(max-width: 768px) 100vw, 80vw" srcset="500w,
800w, w" class=landscape><figcaption><p>Gothenburg model (with Transcription)</p></figcaption></figure><p>This figure represents a pipeline, corresponding to an editorial workflow. Despite our identification in this report of a broad distribution of normalization operations beyond the titular <strong>Normalization</strong> Stage, the modular pipeline architecture, including that stage, remains fundamental both as a way of modeling the collation process and as a guide for implementation. Indeed, the existence of an official <strong>Normalization</strong> Stage between <strong>Tokenization</strong> and <strong>Alignment</strong> , where differences among witness tokens are neutralized in order to enable their comparison for alignment purposes, provides a context for recognizing and understanding the aspects of normalization that necessarily take place elsewhere.</p><blockquote><p>an additional analytical step in which the alignment results are augmented (and optionally fed back as preconditions into the collator) appears essential to us in order to bridge the methodologicalimpedancebetween a plain computational approach and the established hermeneuticalbest-practiceapproach to the problem.</p></blockquote><blockquote><p>The unforeseeable nature of original research means that a research tool cannot be extended in a predictable way to meet the demands of research. What is needed are research tools whose every aspect can [be] modified and extended through easily added modules. In other words, to be truly extensible, a tool should be capable of having any component replaced.</p></blockquote><ul><li id=andrews2016>Andrews, Tara L. 2016. “Analysis of variation significance in artificial traditions using Stemmaweb” . _Digital scholarship in the humanities_ 31, no. 3 (2016): 523–39.<a href=https://doi.org/10.1093/llc/fqu072>https://doi.org/10.1093/llc/fqu072</a>.</li><li id=andrews2020>Andrews, Tara L. 2020. “Abusing the concept of normalization for better collation results (and profit)” .<a href=https://hcommons.org/deposits/item/hc:31925>https://hcommons.org/deposits/item/hc:31925</a>.</li><li id=bleeker2017>Bleeker, Elli. 2017. “Mapping invention in writing: digital Infrastructure and the role of the editor” . PhD diss., University of Antwerp.<a href=https://repository.uantwerpen.be/docman/irua/e959d6/155676.pdf>https://repository.uantwerpen.be/docman/irua/e959d6/155676.pdf</a>.</li><li id=birnbaum2015>Birnbaum, David J. 2015. “CollateX normalization” . Presented at the “Computer-supported collation with CollateX” workshop, DH2015, Sydney.<a href=https://github.com/DiXiT-eu/collatex-tutorial/blob/master/unit7/soundex-normalization.pdf>https://github.com/DiXiT-eu/collatex-tutorial/blob/master/unit7/soundex-normalization.pdf</a>.</li><li id=birnbaum2018>Birnbaum, David J. and Hanne Martine Eckhoff. 2018. “Machine-assisted multilingual alignment of the _Codex Suprasliensis_ ” , in Stephen M. Dickey and Mark Richard Lauersdorf, eds, _V zeleni drželi zeleni breg. Studies in honor of Marc L. Greenberg_ , 1–14. Bloomington, IN: Slavica.</li><li id=camps2019>Camps, Jean-Baptiste, Lucence Ing, and Elena Spadini. 2019. “Collating medieval vernacular texts. Aligning witnesses, classifying variants” . In _DH2019 Digital humanities conference 2019_ . Utrecht, Netherlands.<a href=https://hal.archives-ouvertes.fr/hal-02268348>https://hal.archives-ouvertes.fr/hal-02268348</a>.</li><li id=collatex>CollateX — software for collating textual sources. Documentation.<a href=https://collatex.net/doc/>https://collatex.net/doc/</a>(Java version) and<a href=https://github.com/interedition/collatex/blob/master/docs/pythonport.md>https://github.com/interedition/collatex/blob/master/docs/pythonport.md</a>(Python version).</li><li id=fv>_Frankenstein variorum._ <a href=https://frankensteinvariorum.github.io/viewer/>https://frankensteinvariorum.github.io/viewer/</a>.</li><li id=froger1968>Froger, Jacques. 1968. _La critique des textes et son automatisation._ . Coll. “Initiation aux nouveautés de la science” n° 7. Paris, Dunod.</li><li id=gilbert1973>Gilbert, Penny. 1973. “Automatic collation: a technique for medieval texts” . _Computers and the humanities_ 7: 139–47.<a href=https://www.jstor.org/stable/30199534>https://www.jstor.org/stable/30199534</a>.</li><li id=gilbert1979>Gilbert, Penny. 1979. “The preparation of prose-text editions with the ‘Collate’ System” . In _La Pratique des ordinateurs dans la critique des textes_ , 245–54. Paris: Ed. du C. N. R. S.</li><li id=dekker2011>Haentjens Dekker, Ronald and Gregor Middell. 2011. “Computer-supported collation with CollateX. Managing textual variance in an environment with varying requirements” . Paper presented at the meeting of Supporting Digital Humanities 2011, Copenhagen. In Bente Maegaard, ed., _Supporting digital humanities, Copenhagen 17–18 November 2011: conference proceedings._</li><li id=dekker2015>Haentjens Dekker, Ronald, Dirk van Hulle, Gregor Middell, Vincent Neyt, and Joris van Zundert. 2015. “Computer-supported collation of modern manuscripts: CollateX and the Beckett Digital Manuscript Project” . _Literary and linguistic computing_ , 30, no. 3 (1 September 2015): 452–70,<a href=https://doi-org.pitt.idm.oclc.org/10.1093/llc/fqu007>https://doi-org.pitt.idm.oclc.org/10.1093/llc/fqu007</a>.</li><li id=interedition>Interedition.<a href=http://www.interedition.eu/>http://www.interedition.eu/</a>.</li><li id=mcilroy1964>McIlroy, Douglas. 1964. “Summary — what's most important” .<a href=http://doc.cat-v.org/unix/pipes/>http://doc.cat-v.org/unix/pipes/</a>.</li><li id=needleman1970>Needleman, Saul B. and Christian D. Wunsch. 1970. “A general method applicable to the search for similarities in the amino acid sequence of two proteins” . _Journal of molecular biology_ 48 (3), 443–53.<a href=https://doi.org/10.1016/0022-2836(70)90057-4>https://doi.org/10.1016/0022-2836(70)90057-4</a>.</li><li id=nury2020>Nury, Elisa, and Elena Spadini. 2020. “From giant despair to a new heaven: the early years of automatic collation” . _It - Information Technology_ 62 (2): 61–73.<a href=https://doi.org/10.1515/itit-2019-0047>https://doi.org/10.1515/itit-2019-0047</a>.</li><li id=pvl>_PVL. Povestʹ vremennyx let._ <a href=http://pvl.obdurodon.org/>http://pvl.obdurodon.org/</a>.</li><li id=robinson1993>Robinson, Peter and Elizabeth Solopova. 1993. “Guidelines for transcription of the manuscripts of _The Wife of Bath’s prologue_ ” , in Norman Blake and Peter Robinson, eds, _The Canterbury Tales Project occasional papers_ I, 19–51. Oxford: Office for Humanities Communication.</li><li id=robinson1989>Robinson, P. M. W. 1989. “The collation and textual criticism of Icelandic manuscripts (1): collation” . _Literary and linguistic computing_ 4 (2): 99–105.<a href=https://doi.org/10.1093/llc/4.2.99>https://doi.org/10.1093/llc/4.2.99</a>.</li><li id=rockwell1998>Rockwell, Geoffrey and John Bradley. 1998. “Eye-ConTact: towards a new design for text-analysis tools” . _Digital studies/Le Champ numérique_ , February.<a href=https://doi.org/10.16995/dscn.232>https://doi.org/10.16995/dscn.232</a>.</li><li id=secretlabs2001>Secret Labs. 2001. “Secret Labs’ regular expression engine” .<a href=https://github.com/python/cpython/blob/3.6/Lib/re.py>https://github.com/python/cpython/blob/3.6/Lib/re.py</a>.</li><li id=segre1976>Segre, Cesare. 1976. “Critique textuelle, théorie des ensembles et diasystème” . _Bulletin de la classe des lettres et des sciences morales et politiques de l’Académie Royale de Belgique_ 62 (1976): 279–92.<a href=https://www.persee.fr/doc/barb_0001-4133_1976_num_62_1_55259>https://www.persee.fr/doc/barb_0001-4133_1976_num_62_1_55259</a>.</li><li id=sels2015>Sels, Lara and David J. Birnbaum. 2015. “Editing the _Bdinski sbornik_ as a multilayered reality” . In _Агиославика. Проблеми и подходи в изследването на Станиславовия чети-миней: доклади от едноименната конференция - 21 май 2013 г. (Hagioslavica. Issues and approaches in the study of the Stanislav Reading Menaion: presentations from the conference of May 21, 2013.)_ , ed. Diana Atanasova. Sofia: Kliment Oxridski University, 2015 (appeared in May 2016), 184–99.</li><li id=silva1969>Silva, Georgette, and Harold Love. 1969. “The identification of text variants by Ccmputer” . _Information Storage and Retrieval_ 5 (3): 89–108.<a href=https://doi.org/10.1016/0020-0271(69)90014-X>https://doi.org/10.1016/0020-0271(69)90014-X</a>.</li><li id=soundex>Soundex.<a href=https://en.wikipedia.org/wiki/Soundex>https://en.wikipedia.org/wiki/Soundex</a>.</li><li id=spadini2016>Spadini, Elena. 2016. “Studi sul Lancelot en prose. ” PhD diss., Sapienza Università di Roma.<a href=http://hdl.handle.net/11573/1307347>http://hdl.handle.net/11573/1307347</a>.</li><li id=spadini2017>Spadini, Elena. 2017. “The role of the base manuscript in the collation of medieval texts” , in _Advances in digital scholarly editing. Papers presented at the DiXiT conferences in the Hague, Cologne, and Antwerp,_ eds. Peter Boot, Anna Cappellotto, Wout Dillen, Franz Fischer, Aodhán Kelly, Andreas, Mertgens, Anna-Maria Sichani, Elena Spadini, and Dirk van Hulle. Leiden: Sidestone Press, pp. 345–49.<a href=https://www.sidestone.com/books/advances-in-digital-scholarly-editing>https://www.sidestone.com/books/advances-in-digital-scholarly-editing</a>.</li><li id=unicode2020>Unicode Consortium. 2020. The Unicode standard version 13.0 – core specification. Chapter 23, “Special areas and format characters” , 881–916 (esp. 885-86).<a href=https://www.unicode.org/versions/Unicode13.0.0/ch23.pdf>https://www.unicode.org/versions/Unicode13.0.0/ch23.pdf</a>.</li></ul><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>We capitalize and embold <strong>Normalization</strong> when we refer to the second step in the Gothenburg model. When we refer to the general concept of normalization (that is, of removing what the researcher considers insignificant variation during the editorial process), independently of the Gothenburg model, we render the word without special capitalization or other formatting.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>In the most common case, the entire output of one process becomes the entire input into the next, although Unix piping also supports tee connections, which send the same output to two destinations, typically stdout and a specified file or variable.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>The dotted line indicates that Stage 4 ( <strong>Analysis</strong> ) may pipe its output into another Stage 3 ( <strong>Alignment</strong> ) process, representing a situation where <strong>Analysis</strong> may require a new <strong>Alignment</strong> , which may or may not be subjected to additional <strong>Analysis</strong> (or additional cycles of <strong>Analysis</strong> and <strong>Alignment</strong> ) before visualization. As<a href=#dekker2011>Haentjens Dekker and Middell (2011)</a>write:&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p>The adoption of the design principle calledseparation of concernsin the field of automatic collation dates back to the 1970s. See, e.g.,<a href=#gilbert1973>Gilbert (1973), 144</a>: “The program is modular in design, that is, it consists of several steps each doing a simple task.” The importance of this modularity in tool design in general, and not only for collation, is highlighted also in<a href=#rockwell1999>Rockwell and Bradley (1999)</a>, section 1.1:&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5><p>The Gothenburg model originated in the context of Interedition, a European-funded collaborative research project whose aim was “to encourage the creators of tools for textual scholarship to make their functionality available to others, and to promote communication between scholars so that we can raise awareness of innovative working methods” <a class=footnote-ref href=#interedition>[interedition] </a>&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:6><p>Our examples are drawn from the Python version of the software, which is available at<a href=https://pypi.python.org/pypi/collatex>https://pypi.python.org/pypi/collatex</a>; the Java version is available at<a href=https://collatex.net/>https://collatex.net</a>. See also the discussion of the implementation of the Gothenburg model in Juxta at<a href=http://juxtacommons.org/tech_info>http://juxtacommons.org/tech_info</a>.&#160;<a href=#fnref:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:7><p>Especially in the early days of computers, hardware and software constraints limited ways of representing digital texts; see, e.g.,<a href=#froger1968>Froger (1968), 230–32</a>. Conventions were quickly adopted to overcome these limitations, such as the use of the $ sign as a diacritic to indicate a capital letter.&#160;<a href=#fnref:7 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:8><p>A manuscript is typically a multilayered reality<a class=footnote-ref href=#sels2015> [sels2015] </a>, where various systems of language and meaning coexist<a class=footnote-ref href=#segre1976> [segre1976] </a>. Useful tokenization and normalization demands careful attention to distinguishing orthographic and linguistic features of the text from those of the manuscript.&#160;<a href=#fnref:8 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:9><p><a href=http://www.tei-c.org/release/doc/tei-p5-doc/en/html/ref-orig.html>http://www.tei-c.org/release/doc/tei-p5-doc/en/html/ref-orig.html</a>&#160;<a href=#fnref:9 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:10><p>Tokenization and normalization are well-known operations in computational and corpus linguistics, and their use in CollateX is similar to the analogous linguistic operations. Cf. tokenization in computer science, sometimes calledlexingorlexical analysis, which shares with linguistic tokenization the goal of dividing a continuous stream of characters into meaningful substrings for subsequent processing. In computer science, the subsequent processing is typicallyparsing, that is, interpreting the tokens as expressions according to the syntax of a programming language.&#160;<a href=#fnref:10 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:11><p>In Python regular expressions, <code>\s</code> “[m]atches any whitespace character; equivalent to <code>[ \t\n\r\f\v]</code> in bytes patterns or string patterns with the ASCII flag. In string patterns without the ASCII flag, it will match the whole range of Unicode whitespace characters” <a class=footnote-ref href=#secretlabs2001>[secretlabs2001] </a>. See<a href=#unicode2020>Unicode Consortium (2020)</a>for an inventory and discussion of Unicode whitespace characters.&#160;<a href=#fnref:11 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:12><p>The default implementation of Tokenization in the Python version of CollateX is performed by the <code>tokenize()</code> method of the WordPunctuationTokenizer class inside <code>core_classes.py: re.findall(r'\w+\s*|\W+', contents)</code> . This defines a token as 1) either a sequence of word characters followed optionally by a sequence of whitespace characters, or 2) a sequence of non-word characters, which is typically punctuation, also followed optionally by a sequence of whitespace characters. This means that a token may consist entirely of whitespace only when it falls at the beginning of a text (where it will match <code>\W+</code> ), since any whitespace not at the beginning of the text will form the trailing part of the token that begins with whatever word or punctuation characters precede it.&#160;<a href=#fnref:12 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:13><p>Default tokenization in CollateX is described above. Default normalization is limited to stripping trailing whitespace in the Python version, and includes both that and case folding in the Java version. In the Python version, the normalization is created for objects of the Witness class (defined inside <code>core_classes.py</code> ) with: <code>Token({'t': token_string, 'n': re.sub(r'\s+$', '', token_string)})</code> . The regular expression <code>\s+$</code> matches one or more whitespace characters at the end of the token; the <code>re.sub()</code> function replaces them with the empty string, that is, removes them from the token before writing the modified form as the value of the n property.&#160;<a href=#fnref:13 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:14><p>Neutralizing phenomena that are not considered relevant for the purpose of alignment so that they would not add noise to the output was a concern also in the early days of automatic collation; see<a href=#nury2020>Nury and Spadini (2020)</a>,<a href=#silva1969>Silva and Love (1969), 93</a>;<a href=#gilbert1979>Gilbert (1979), 247</a>;<a href=#robinson1989>Robinson (1989), 100–01</a>.&#160;<a href=#fnref:14 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:15><p>We use the termforced matchto designate a situation where a single token is sandwiched between unambiguous matches of all witnesses on either side, as in the case-folding example immediately below. The match is forced because, with the neighbors fully aligned, the tokens between them, whether the same or different, are forced into alignment.&#160;<a href=#fnref:15 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:16><p>See <em>Frankenstein variorum</em> . In the source files used in the project, this heading also includes presentational markup, which could be recruited to override case distinctions in the character data as part of a strategy for controlling normalization.&#160;<a href=#fnref:16 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:17><p>See<a href=#pvl>PVL</a>. Cyrillic used letters of the alphabet as numerical digits, and these two letters continued to be distinguished in early Cyrillic when they represented numerical values even after any distinction between them had largely ceased to be significant in words.&#160;<a href=#fnref:17 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:18><p>Normalization in support of alignment in this edition also implements case folding, neutralizes several other character distinctions (includingѧvsяin this word), and ignores line breaks (represented by a vertical bar), punctuation, and superscription. The examples in this article simplify the actual CollateX output by representing characters tagged with <code>&lt;sup></code> tags as superscript characters and removing the <code>&lt;sup></code> tags and all others (principally <code>&lt;lb></code> and <code>&lt;pb></code> ). This markup is removed from the n properties when they are created during Stage 2 <strong>Normalization</strong> , and therefore is not involved in Alignment decisions.&#160;<a href=#fnref:18 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:19><p><strong>Alignment</strong> in CollateX by default distinguishes only exact matches and non-matches, and it has no understanding ofnear matching, that is, of finding the closest inexact match. In this case that means that the <strong>Alignment</strong> stage alone cannot recognize thatgeis more similar tojethan it is toartu(s).&#160;<a href=#fnref:19 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:20><p>This would happen because, as noted above, in case of multiple alternatives regarded as equivalent by the software, CollateX pushes the token to the left.&#160;<a href=#fnref:20 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:21><p>The researcher may also attach other properties, in addition to t and n, to tokens, and these can be returned as part of the output. Properties other than n are not used by the default alignment engine inside CollateX, but, in keeping with the modularity of the Gothenburg model, a researcher could replace the default CollateX alignment routine with an alternative that performs a more complex evaluation. For example, it is theoretically possible to perform several types of normalization, assign their results to different token properties, and perform the alignment in a way that assigns different weights to different degrees of matching in different token properties.&#160;<a href=#fnref:21 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:22><p>Philologists may disagree about which of two possible alignments to prefer, and some such decisions have no clearly defined resolution. As a simplified example, givenvery interestingandvery, very interesting, there is no clear way to decide whether to align the single instance ofveryin the first witness with the first matching instance in the second witness, or with the second matching instance.&#160;<a href=#fnref:22 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:23><p>Repetition, when the same token appears multiple times in the witnesses, makes it difficult to determine which instances in one witness to align with which instances in another. The greater the extent of the repetition, the greater the risk of incorrect alignment.&#160;<a href=#fnref:23 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:24><p>A forced match and an alignment based on shared n property values are represented differently in the variant graph that CollateX produces. That difference is not translated into the alignment table output, but it is part of the CollateX SVG rendering of the graph.&#160;<a href=#fnref:24 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:25><p>In situations where researchers are able to hypothesize about the relationships among the witnesses before performing machine-assisted collation, they can exploit order dependencies of the algorithms by comparing the two witnesses that are assumed to be most closely related first. This approach will fail, though, in situations where different witnesses may be closest in different portions in the work. Additionally, and more generally, confidence about which witnesses are most closely related is proportional to the extent to which the collation has already been completed, a circular workflow that effectively amounts to requiring collation as a precursor to deciding how to implement the collation. CollateX has limited control over the order in which witnesses are incorporated into a collation. It can add witnesses one by one in an order specified by the researcher, but it cannot compare, for example, witnesses A and B and, separately, C and D, and then merge the two (A+B, C+D) interim variant graphs. That is, after comparing the first two witnesses and producing an initial variant graph, CollateX always aligns exactly one new witness at a time against the variant graph, and it cannot align a new witness directly against another new witness or an interim variant graph directly against another interim variant graph.&#160;<a href=#fnref:25 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:26><p>For summary descriptions of the prevalent algorithms see<a href=https://en.wikipedia.org/wiki/Multiple_sequence_alignment>https://en.wikipedia.org/wiki/Multiple_sequence_alignment</a>.&#160;<a href=#fnref:26 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:27><p>CollateX documentation is available at<a href=https://collatex.net/doc/>https://collatex.net/doc/#analysis-feedback</a>.&#160;<a href=#fnref:27 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:28><p>For a previous implementation of near matching, see<a href=#robinson1989>Robinson (1989), 103</a>.&#160;<a href=#fnref:28 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:29><p>CollateX performs near matching not by revising the alignment table directly, but by adjusting the rank of the token in the variant graph, which, among other things, informs the layout of the alignment table. This makes the adjusted alignment information available at a lower level, and therefore also in output structures that do not use the alignment table, such as graph or SVG output.&#160;<a href=#fnref:29 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:30><p>Another way to regard this issue emerges from the diasystemic nature of manuscript evidence, where (to simplify) features of the text are transmitted through the filter of features of the manuscript (that is, scribal practice)<a class=footnote-ref href=#segre1976> [segre1976] </a>. During this transmission, the features traditionally considered significant (e.g., lexical) are those that are not affected subconsciously by scribal practice or habit as easily as those traditionally considered insignificant (e.g., orthography, although this, too, is a simplification). From this perspective, insofar as scribal norms or habits may be independent of the content of the text being copied, focusing on significant variation may be considered a strategy for prioritizing features of the text over those of the manuscript. Andrews 2016 challenges the traditional division of features into significant and insignificant, finding that, in the artificial traditions she examined, “human judgement was not significantly better than random selection for choosing the variant readings that fit the stemma in a text-genealogical pattern.” <a class=footnote-ref href=#andrews2016>[andrews2016] </a>&#160;<a href=#fnref:30 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:31><p>This approach is explored in Camps et al. 2019, for which see also<a href=https://github.com/CondorCompPhil/falcon>https://github.com/CondorCompPhil/falcon</a>.&#160;<a href=#fnref:31 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:32><p>More strictly, performing the <strong>Alignment</strong> and generating the output are already separate pipeline steps within CollateX, but the API does not expose them individually.## Bibliography&#160;<a href=#fnref:32 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div></div></article></main><footer><nav aria-label="footer links"><ul><li><a class=highlight-focus href=/tags/>Tags</a></li><li><a class=highlight-focus href=/about/>About</a></li><li><a class=highlight-focus href=/categories/>Keywords</a></li></ul></nav><div class=icons><a class="license highlight-focus" rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons Attribution No-Derivatives 4.0 International License" src=/img/logos/license.svg width=120 height=42></a></div></footer></body></html>