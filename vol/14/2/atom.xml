<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://gohugo.io/" version="0.116.0">Hugo</generator><link href="https://rlskoeser.github.io/dhqwords/vol/14/2/" rel="alternate" type="text/html" title="html"/><link href="https://rlskoeser.github.io/dhqwords/vol/14/2/index.xml" rel="alternate" type="application/rss+xml" title="rss"/><link href="https://rlskoeser.github.io/dhqwords/vol/14/2/atom.xml" rel="self" type="application/atom+xml" title="Atom"/><updated>2023-08-06T19:55:32+00:00</updated><rights>This work is licensed under a Creative Commons Attribution-NoDerivatives 4.0 International License.</rights><id>https://rlskoeser.github.io/dhqwords/vol/14/2/</id><entry><title type="html">Remembering Stéfan Sinclair</title><link href="https://rlskoeser.github.io/dhqwords/vol/14/2/000493/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://rlskoeser.github.io/dhqwords/vol/14/2/000493/</id><author><name>DHQ editorial team</name></author><published>2020-08-15T00:00:00+00:00</published><updated>2023-08-06T15:53:44-04:00</updated><content type="html"><![CDATA[<h2 id="remembering-stéfan-sinclair">Remembering Stéfan Sinclair</h2>
<p>With great sadness and affection, the DHQ team remembers Stéfan Sinclair, who passed away on August 6, 2020 after a long illness. Stéfan was a treasured colleague and friend, and his many contributions to the digital humanities community include key tools like<a href="https://voyant-tools.org">Voyant</a>and<a href="https://bonpatron.com">BonPatron</a>, infrastructure like<a href="http://tapor.ca/home">TAPoR</a>, long-time professional service to the<a href="https://ach.org">Association for Computers and the Humanities</a>and the<a href="https://adho.org">Alliance of Digital Humanities Organizations</a>, and good ideas that were never realized, such as a journal devoted to critical tool reviews. A full obituary can be found at the<a href="https://csdh-schn.org/stefan-sinclair-in-memoriam/">CSDH-SCHN</a>site.</p>




























<figure ><img loading="lazy" alt="A photograph of Stéfan Sinclair." src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Stéfan at the Digital Humanities Summer Institute, University of Victoria, 2010. © 2010 Syd Bauman, available via CC BY-SA 4.0
        </p>
    </figcaption>
</figure>
<p>Stéfan was a member of DHQ’s founding editorial team in 2005, and remained an energetic, thoughtful, and generous contributor to the journal until his death. A review of the journal’s early development discussions shows him by turns supportive, enthusiastic, engaged, ingenious, diplomatic, and deeply hands-on with things as varied as setting up a wiki space, planning out a DHQ blog, providing useful advice on server arrangements, and advising on journal indexing services. He was passionate about multilingualism and encouraged us to be ambitious even long before we had the capacity to act on his ideas. And he was deeply supportive of early career scholars, and eager to engage new voices in DHQ activities. His deeply collaborative scholarship is evident in his various DHQ publications, all of them co-authored: “<a href="http://www.digitalhumanities.org/dhq/vol/11/4/000313/000313.html">Introducing DREaM (Distant Reading Early Modernity)</a>” ; “<a href="http://www.digitalhumanities.org/dhq/vol/7/3/000166/000166.html">Visualizing Theatrical Text: From Watching the Script to the Simulated Environment for Theatre (SET)</a>” ; “<a href="http://www.digitalhumanities.org/dhq/vol/3/3/000067/000067.html">Designing Data Mining Droplets: New Interface Objects for the Humanities Scholar</a>” ; “<a href="http://www.digitalhumanities.org/dhq/vol/14/3/000456/000456.html">Tremendous Mechanical Labor: Father Busa’s Algorithm</a>” ; and “Anatomy of Tools” , forthcoming in the DHQ special issue on Tools Criticism.</p>
<p>On the whimsical side, Stéfan was a<a href="https://twitter.com/sgsinclair/status/450949585708797952">key contributor</a>to DHQ’s<a href="http://lists.digitalhumanities.org/pipermail/humanist/2014-April/011859.html">April Fool’s Day prank</a>in 2014, in which we substituted<a href="http://theoreti.ca/?p=5192">Voyant word cloud visualizations</a>for the text of all DHQ articles, and posted an announcement that “DHQ will no longer publish scholarly articles in verbal form. Instead, articles will be processed through Voyant Tools and summarized as a set of visualizations which will be published as a surrogate for the article.” Characteristically, he also anticipated the need to support serious use of DHQ, even on April 1, and devised a clever way to stage the joke and also offer access to the full article text so that no one was inconvenienced by our foolishness.</p>




























<figure ><img loading="lazy" alt="A screenshot image of a tweet by Stéfan Sinclair, circulating DHQ&#39;s April Fool&#39;s Day joke on April 1, 2014." src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Stéfan&rsquo;s<a href="https://twitter.com/sgsinclair/status/450949585708797952">tweet</a>on April 1, 2014
        </p>
    </figcaption>
</figure>
<p>Responses from the digital humanities community were satisfyingly irate, and then appreciative. And the experiment also showed how well and how sociably Stéfan had built Voyant:<a href="https://stefansinclair.name/custom-cirrus/">it was wonderfully simple to retrieve the word clouds from the Voyant API</a>and embed them in the DHQ interface.</p>
<p>We will remember Stéfan with deep fondness in all of our future work on DHQ.</p>
<h2 id="bibliography">Bibliography</h2>
<ul>
<li id="">
</li>
</ul>
]]></content></entry><entry><title type="html">A Prosopography as Linked Open Data: Some Implications from DPRR</title><link href="https://rlskoeser.github.io/dhqwords/vol/14/2/000475/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://rlskoeser.github.io/dhqwords/vol/14/2/000475/</id><author><name>John Douglas Bradley</name></author><published>2020-07-29T00:00:00+00:00</published><updated>2023-08-06T15:53:44-04:00</updated><content type="html"><![CDATA[<h1 id="heading"></h1>
<p>In a TED conference in 2009 Tim Berners-Lee gave a presentation entitled <em>The Year open data went worldwide</em> <a class="footnote-ref" href="#bernerslee2010"> [bernerslee2010] </a>. In it he gave some examples of how open data fromgovernments, scientists and institutionscould be used to make significant statements about the state of affairs in society. He then asked governments, scientists and institutions to support this kind of work by making more of their data freely available in a form where it could be further processed rather than just looked at. This was a part of what he called the “Linked Data” Initiative, and which has more recently often been called “Linked Open Data” (LOD).</p>
<p>Berners-Lee has a scientific background, so perhaps it was not surprising that he didn&rsquo;t seem to think about LOD data from the humanities. Nonetheless, there is no technical reason why those Digital Humanists who have suitable data should not be be making their material openly available too. Indeed, the desirability of Linked Open Data from and for the humanities has been expressed by others in the digital humanities community for some time. One can, for example, see a similar motivation in the premise behind the workshop <em>Linked Data for Digital Humanities</em> that was held in the 2016 DHOxSS Summer School<a class="footnote-ref" href="#nurmikkofuller2016"> [nurmikkofuller2016] </a>, James Smith&rsquo;s <em>RDF and Linked Open Data</em> <a class="footnote-ref" href="#smith2017"> [smith2017] </a>at University of Victoria&rsquo;s Digital Humanities Summer Institute, and in other similar workshops that explore the ideas of applying Linked Open Data technologies to humanities-oriented materials.</p>
<p>This paper introduces recent project work done at King&rsquo;s College London which makes one of its many online web resources available as Linked Open Data. The project is the recently completed <em>Digital Prosopography of the Roman Republic</em> (DPRR), and this part of its work was a response to Berners-Lee&rsquo;s TED talk challenge mentioned above in which he asked people to deliver their data as LOD. This paper is also a part of this response. Here we will consider why DPRR was the first humanities project by KCL to have its full set of data published as LOD data, and what in DPRR&rsquo;s characteristics made it particularly suitable for this. The paper will then explore what DPRR&rsquo;s LOD server looks like to a user, what kind of interactions with the data it makes possible, and how this connects with Berners-Lee&rsquo;s view of how LOD should be expressed. Having done this, it will then consider what might come from allowing anyone to get at this historical material directly as pure data rather than exclusively through a browser oriented front end which, as we will see below, acts as a focusing filter between the material-as-data and the user&rsquo;s browser. Does this direct access truly empower people to explore our material in the way Berners-Lee and other people who have taken up the LOD cause intend? Does it allow for new kinds of analysis and research to be carried out, hopefully revealing new insights for the materials that are not visible through even our rather sophisticated browser-oriented front end? Some part of this issue arises out of research about the nature of querying that has been carried out in the context of the Semantic Web, and we will briefly describe this here; contrasting the original AI-related vision of the Semantic Web in the late 1990s, with the more pragmatic Linked Data vision that emerged a few years later. And finally, how does DPRR&rsquo;s RDF server fit with one of the major interests from the Digital Humanities that have come out of LOD thinking: an interest in adding links from digital resources to standard authority lists such as <em>VIAF</em> <a class="footnote-ref" href="#viaf2010"> [viaf2010] </a>or, say, <em>Pelagios</em> <a class="footnote-ref" href="#pelagios"> [pelagios] </a>to aid in the aggregation of data between different data sets?</p>
<h2 id="why-have-dprr-as-linked-open-data">Why have DPRR as Linked Open Data?</h2>
<p>I started work at King&rsquo;s College London (KCL) in 1997, first at its Centre for Computing in the Humanities which was subsequently renamed the Department of Digital Humanities (CCH/DDH). Most recently I have also become associated with at a new unit at KCL called the King&rsquo;s Digital Lab (KDL). During this period CCH, DDH and KDL have built many academic resources in collaboration with humanities academic partners. In all these projects we have championed the concept of openness and accessibility. As a consequence, since the late 1990s we have, as conscious policy, made sophisticated online digital resources available for free over the WWW.</p>
<p>One of the challenges for the work in which I particularly was involved arose out of the fact that almost all my collaborative academic projects took a strong “data” perspective to their materials. This is an approach which can seem very foreign to the text-orientation of much of the humanities and the digital humanities too. Having created a highly structured and complex set of data as a product of the scholarship, how could it be made accessible to the rather non-technical scholarly audience? Out of this issue came much work on how to present this complex data adequately through web applications. Indeed, the development of these delivery apps became a large part of CCH/DDH&rsquo;s standard project practice, and the focus was always (indeed, had to be, due to the intended audience) on making that access as non-technical as possible by creating a web application that mapped each project&rsquo;s data into dynamic web pages that could be displayed by any browser. Thus, although almost all browser-mediated resources created at KCL have been open and freely available, they have not really been conceived as providing direct access to the data <em>behind</em> the web application in the way that Berners-Lee meant for LOD in his TED talk.</p>
<p>Recently, however, the <em>Digital Prosopography of the Roman Republic</em> <a class="footnote-ref" href="#mouritsenetal2017"> [mouritsenetal2017] </a>was completed, and it presented us with an opportunity to publish the same material in two different forms. First, like all pre-existing data-oriented resources that we had created, DPRR has its web application that made access possible for nontechnical users. Second, however, and at very little additional development cost, DPRR&rsquo;s data has also been made available as pure data, in a form suitable for LOD.</p>
<p>Why was DPRR the target for this work? From a fully pragmatic perspective, DPRR came to be expressed as LOD because in its AHRC funded research proposal we actually proposed offering direct data access, in the spirit of LOD, as one of the project&rsquo;s outcomes. Furthermore, RDF and related technologies had been in the mix within the<a href="https://ancientwisdoms.ac.uk/"> <em>Sharing Ancient Wisdoms project</em> </a>(SAWS) project which had been carried out with DDH as a partner, so there was some significant experience of RDF to draw on in previous work. However, we did not do the work of expressing DPRR materials in LOD-compatible ways only because we had promised it in the proposal, or because of our experience with the SAWS project, but because we believed that DPRR connected in particularly useful ways to the three components of the idea of LOD: <em>openness</em> , <em>linked</em> , and <em>data</em> , and we thought it plausible that by opening up DPRR in this way we would allow others to explore more richly what DPRR contains than what our conventional browser-oriented mechanisms, as sophisticated as they are, would enable.</p>
<p>First, <em>Openness</em> : DPRR is a published prosopography, and we believe that, as such, it offers a highly suitable source for open data. A published prosopography is consciously intended by its creators for a global audience and for this reason it is ideally an open publication and compatible to many of the ideas of open data. This is particularly true for a prosopography which is free to all online, as DPRR is.</p>
<p><em>Linked</em> : DPRR is also a good example of scholarship that invokes the essential spirit and principles of linked data. Of course, DPRR hopes that modern Roman Republic scholars will explicitly link to it by referencing the historical entities — presumably primarily historical people — that it defines. If this happens, DPRR will become integrated into the global scholarship around the Roman Republic. However, DPRR has more significance to linking than just this. DPRR, like any prosopography, establishes formal identities for their historical persons out of the appearance of them in a range of sources, and it thus links these sources together through their shared historical people. However, DPRR takes a different approach to its prosopography than the other, factoid-based (defined in<a class="footnote-ref" href="#bradley2017a"> [bradley2017a] </a>), digital prosopographies in which DDH/CCH has been involved. Unlike these other prosopographies, such as <em>the People of Medieval Scotland</em> <a class="footnote-ref" href="#poms2014"> [poms2014] </a>, or <em>Prosopography of Anglo-Saxon England</em> <a class="footnote-ref" href="#pase2016"> [pase2016] </a>, which draw almost exclusively on their projects&rsquo; interpretation of their primary sources, DPRR has assembled and aligned work done by a range of already existing nineteenth, twentieth and twenty-first century prosopographies, and could thus be said, in itself, to represent a multi-source “global graph” (to use RDF terminology) of recent Roman Republic prosopographical scholarship. It, and many of the works upon which it draws, has been built on the work of T. Robert Broughton&rsquo;s study of office-holders<a class="footnote-ref" href="#broughton1951"> [broughton1951] </a>which remains to this day a standard reference work. Furthermore, underpinning all these other prosopographies, including Broughton, is the monumental 83 volume nineteenth century <em>Real-Encyclopaedie der classischen Altertumswissenschaft</em> <a class="footnote-ref" href="#pauly1893"> [pauly1893] </a>— referred to as RE and once called by a DPRR project member the &ldquo;grandfather&rdquo; of all DPRR&rsquo;s prosopographical sources. RE continues to provide the basis against which historical identity of individuals is argued even today. A full list of sources that provided data for DPRR can be found on their Bibiography page.<a class="footnote-ref" href="#mouritsenetal2017"> [mouritsenetal2017] </a>at web page<a href="http://www.romanrepublic.ac.uk/bibliography/">Bibliography</a>.</p>
<p><em>Data</em> : Finally, DPRR is like DDH/CCH&rsquo;s many other prosopographical projects in that it is data-oriented rather than being, as traditional published prosopography has been, article oriented. Like PoMS or PASE, DPRR represents its materials in the form of highly structured data, and, like DDH/CCH&rsquo;s other structured prosopographies, is built on top of that quintessential highly structured paradigm: the relational database. Thus, DPRR&rsquo;s historical research work has been expressed in terms of the semantic concepts of “entities” , “attributes” and “relationships” as they are thought of in the relational data model.</p>
<p>Overall, then, DPRR can be thought of as an ideal candidate for all three aspects of the LOD model: linked, open, and data oriented.</p>
<p>To take DPRR&rsquo;s materials in its relational database and to turn it into LOD presented in the forms apparently meant by Tim Berners-Lee requires taking up technologies developed for LOD. Thus, we followed the thinking of the original developers of the Linked Data (LD) concept, and of the rather broader Semantic Web too, in adapting the Resource Description Framework (RDF)<a class="footnote-ref" href="#rdf2014"> [rdf2014] </a>and its related components as fundamental technologies for expressing DPRR as LOD. RDF links have been described asthe glue of the data web<a class="footnote-ref" href="#bizer2008"> [bizer2008] </a>, and RDF has been given by LD&rsquo;s original thinkers as a key part of Berners-Lee&rsquo;s “four rules” to allow published data to become “part of a single global data space” <a class="footnote-ref" href="#bizeretal2009"> [bizeretal2009] </a>. Furthermore, relational data structures (the paradigm used for organising data in DPRR) generally map particularly well onto RDF. As Berners-Lee remarks about RDF and the Semantic Web:[O]ne of the main driving forces for the Semantic web has always been the expression, on the Web, of the vast amount of relational database information<a class="footnote-ref" href="#bernerslee1998"> [bernerslee1998] </a>. Indeed, exactly because of this thinking within the fundamental design of RDF, the task of mapping DPRR&rsquo;s materials into RDF &ndash; turned out to be conceptually relatively straightforward.</p>
<p>In the online descriptive material I have provided about the DPRR RDF server<a class="footnote-ref" href="#bradley2017b"> [bradley2017b] </a>, I describe how I used the <em>d2rq</em> tool<a class="footnote-ref" href="#d2rqnd"> [d2rqnd] </a>to map DPRR&rsquo;s database structures into RDF, how I created a basic semantic web ontology to supplement the DPRR data, and how I created an RDF server as a mostly stripped down, but in a couple of areas somewhat extended, version of the <em>rdf4j</em> <a class="footnote-ref" href="#rdf4j2017"> [rdf4j2017] </a>workbench. I based much of the RDF server on rdfj4&rsquo;s workbench because I believed that it produced quite an elegant thin HTML-based wrapping around the RDF data that allowed a browser user to explore and better understand the data without having its HTML wrapping mask or hide the nature of the RDF. The server&rsquo;s functions are documented at<a class="footnote-ref" href="#bradley2017b"> [bradley2017b] </a>on web page<a href="http://romanrepublic.ac.uk/rdf/doc/using.html">Using DPRR&rsquo;s RDF server</a>.</p>
<h2 id="exploring-dprrs-rdf-server">Exploring DPRR&rsquo;s RDF Server</h2>
<p>Where on the WWW, then, does one find DPRR&rsquo;s LOD data representation? One finds its RDF server at<a href="http://romanrepublic.ac.uk/rdf/">http://romanrepublic.ac.uk/rdf/</a>. All URIs and URLs that start in this way are delivered to DPRR&rsquo;s RDF server, and processed by it.</p>
<p>We have built the server to provide support for what we believed to be the main characteristics of RDF-oriented Linked Open Data. What are these characteristics?</p>
<p>The server meets the Linked Data requirements outlined in<a class="footnote-ref" href="#bizeretal2009"> [bizeretal2009] </a>. In particular, it is designed so that, first, all of DPRR&rsquo;s data are given public URIs (although DPRR is a prosopography, not just historical Persons are formally identified with URIs), and second that if any of these URIs is given to the WWW, they will find their way to DPRR&rsquo;s RDF server, which will deliver data it has which is connected to this entity.Furthermore, the server supports querying via RDF&rsquo;s standard query language SPARQL (see SPARQL 2013).Materials can be fetched as pure RDF data, suitable for further processing, or filtered through a light-weight browser oriented HTML presentation to facilitate human browsing of the data.The interconnections between the different entity types that makes up the DPRR data is made evident through the provision of a basic OWL ontology.</p>
<p>The attentive reader may have noticed that I claim here the RDF server is capable of delivering the DPRR RDF data in a browser-friendly manner, and may have remembered that DPRR&rsquo;s <em>other</em> , more conventional, browser-friendly interface <em>also</em> delivers DPRR data in a browser-friendly manner. What, then, is the difference between the two?</p>
<p>Although both DPRR&rsquo;s RDF server and browser-oriented search engine interact with the same data, they present quite a different face to their users. As a point of comparison,<a href="#figure1">Figure 1</a>shows the top of the front “Person Search” page of DPRR&rsquo;s conventional browser-oriented site:</p>




























<figure ><img loading="lazy" alt="" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>DPRR&rsquo;s bowser-oriented site: the Person Search
        </p>
    </figcaption>
</figure>
<p>This figure shows what someone sees if they enter “Cicero” as the Cognomen for a person. We can see there that there are 9 records (persons) who have <em>Cicero</em> as their cognomen (and they are actually listed on the page, but below the displayed area in this figure). If one focuses for a moment on the form area in the bottom half of the figure, one sees a good number of labelled boxes that can be filled in to filter the selection of persons. Note that the boxes and their labels immediately tell the user what kind of data the DPRR dataset holds that can be used for filtering (and there are even more filtering items off the bottom of this screen shot that are also available).</p>
<p>This web page uses a user interface strategy called “facetted search” (see Wikipedia&rsquo;s<a href="https://en.wikipedia.org/wiki/Faceted_search"> “faceted search” </a>entry) to steer its user towards materials relevant to them in the dataset. This facetted search approach implements interface strategies used in other commonly used sites such as Amazon&rsquo;s, and is designed to help users with a limited knowledge of a field to find things that they want. Thus, the facetted approach for this DPRR selection page is intended to help novice users (although, of course, perhaps not so much novices to the study of Roman Republic society, because they are expected to know, for example, what a “Praenomen” is) to find things that will interest them. The intent of the design is to allow historians of the Roman Republic to use this page effectively with what is only now-a-days conventional web-access skills.</p>
<p>Contrast this with the front screen one sees (shown in<a href="#figure02">Figure 2</a>) when one fetches the front page of the DPRR RDF server. It allows the user to explore and select DPRR&rsquo;s data using RDF&rsquo;s SPARQL query language (which one provides in the large text box labelled “query” .</p>




























<figure ><img loading="lazy" alt="" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>DPRR&rsquo;s RDF Server&rsquo;s front screen
        </p>
    </figcaption>
</figure>
<p>Of course, there is (not surprisingly) more to the RDF server&rsquo;s web-oriented interface than this page alone, so only so much can be learned by examining it critically by itself. Nonetheless, even though we are only looking here at one of the pages that the RDF Server can show to us, one can quickly see that the RDF&rsquo;s server&rsquo;s web interface is built under a very different set of assumptions about the kind of user who will be working with it. Indeed, although there is a banner at the top of the screen that identifies it with DPRR, the DPRR RDF server&rsquo;s public interface is not specific to DPRR in the way that the facetted search browser presented earlier is, but represents instead a general kind of interface that could be used with any collection of RDF data on any subject. The web-browser interface for DPRR we looked at a moment ago has been tailored specifically to make front and centre how DPRR&rsquo;s materials are organised and to show under what semantic issues they operate. Here, in contrast, other than that obvious DPRR banner, this RDF server could look virtually the same if it was giving access to an entirely different set of RDF data.</p>
<p>In fact, this screen is part of <em>rdf4j</em> &rsquo;s RDF workbench interface which has been specifically designed by the <em>rdf4j</em> &rsquo;s developers to work usefully with <em>any</em> kind of RDF data. Indeed, DPRR&rsquo;s RDF server&rsquo;s browser interface focuses primarily on providing an interface that fits with RDF and related technologies like SPARQL rather than being an interface that is tailored specifically to express DPRR&rsquo;s concepts and materials. For someone to use the RDF server they need to know not only about DPRR&rsquo;s data and how it is represented in RDF, but also how RDF works and (for this particular web page) how to express queries in the SPARQL language that will be able to fetch data for the user&rsquo;s particular needs. We&rsquo;ll see examples of SPARQL being used in this way later in this article. The important point at this moment is that this browser interface says something about its intended audience: to use it one needs to have a solid technical familiarity with RDF and its technologies, and to be capable of exploiting materials presented in this way. This article will look briefly at some of the other parts of its interface that is derived from the <em>rdf4j</em> workbench later.</p>
<p>I have chosen to include in this article HTML links and forms that actually invoke the DPRR RDF Server, based on the principle that by actually sending readers to the server, they will be better enabled to explore for themselves what the server is doing. Therefore, I recommend that you, the reader, click on the provided links and thus directly engage with the server yourself. The links are set up to cause your browser to open them in a new tab or window. Thus, to return to this article, you can simple close the display the link or form created when you are done with it. Furthermore, if for some reason you are unable to make the links work you can instead find screen captures in the appendix which show what appears in my Firefox browser when I click on the links. Each figure in the article is linked to the the spot in the article where it is needed.</p>
<p>Now let us turn our attention to how DPRR&rsquo;s RDF server addresses the basic requirement of Christian Bizer, Tom Heath and Tim Berners-Lee&rsquo;s conception of Linked Open Data as they describe it in their 2009 article that was mentioned earlier:<a class="footnote-ref" href="#bizeretal2009"> [bizeretal2009] </a>.</p>
<p>The first point to notice is that the server supports the fundamental principle stated by Berners-Lee and others about open data: that all entities in the data have globally defined URIs for them, and if one gives the URI for any one of these entities to the web as a URL, one gets data back from the server about it. Thus, all of DPRR&rsquo;s data (as we will see shortly, not just DPRR persons) are globally accessible in this way, since all entities in the DPRR dataset are assigned global URIs and can be directly referenced by anyone with web access who wishes to do so.</p>
<p>For example,<a href="http://romanrepublic.ac.uk/rdf/entity/Person/2072">http://romanrepublic.ac.uk/rdf/entity/Person/2072</a>refers to one of the historical persons in the dataset: in this case the famous Roman author Cicero. (A screen shot showing what my browser gives me in response is given in the appendix as<a href="#figure03">Figure 3</a>.) If you give your browser this URI (and if you are reading this article online you can readily do this by simply clicking on the URI-as link showing here) it will find its way to the DPRR RDF server. There, the server will fetch the data about the person identified by this URI (Cicero) and will return to your browser all the data it has about him, delivering it to you through the <em>rdf4j</em> workbench “wrapper” which presents all these RDF statements wrapped in lightweight HTML so your browser can effectively display them. The tabular part of the display shows the RDF statements that reference DPRR&rsquo;s URI for Cicero. RDF statements have three parts in the order “<subject> <predicate> <object>” . Thus, one of the triples part way down the list in the table can be readThe entity with URI <code>http://romanrepublic.ac.uk/rdf/entity/Person/2072</code> has Cognomen &lsquo;Cicero&rsquo;. Cicero&rsquo;s URI is likely to appear as Subject or Object part of the RDF statements (and is allowed as a Predicate, although because of the way DPRR&rsquo;s RDF works, Cicero&rsquo;s URI does not in fact occur there), and this display shows all the RDF statements that reference Cicero&rsquo;s URI for all three possible types of reference.</p>
<p>It is important to grasp the fact that DPRR&rsquo;s other non-RDF “browser oriented” web interface can <em>also</em> present similar data about Cicero, and this function is invoked through a URL that looks somewhat similar to the RDF URI used to identify Cicero:<a href="http://www.romanrepublic.ac.uk/person/2072/">http://www.romanrepublic.ac.uk/person/2072/</a>(A screen shot is shown in the appendix as<a href="#figure04">Figure 4</a>. The data about the same historical person, Cicero, is all included in the web page returned to the browser too, but it is wrapped in rather more complex HTML which has been tailored specifically to represent DPRR Person data, and which is designed to present visually well in a conventional browser for a human reader. Although both the “browser friendly” URL and the RDF-oriented URI for Cicero are based on the same underlying data and return similar results the differences between them are similar to the differences described earlier about the two front pages: Cicero&rsquo;s RDF URI is presented in terms of its RDF representation, whereas his browser-oriented URL immediately presents its material in terms focused on how DPRR data about Cicero is organised in a format which is calculated to be more immediately accessible to a less technical reader.</p>
<p>Although the RDF URI for Cicero&rsquo;s data caused the RDF server to respond with the RDF statements it holds still wrapped in a little presentation HTML you can in fact ask the RDF server to deliver its result in pure RDF — immediately suitable for further machine processing. There are two ways to do this. One can use the mechanisms recommended in the W3C&rsquo;s specification for RDF servers<a class="footnote-ref" href="#speicheretal2015"> [speicheretal2015] </a>: to ask the server to create the result in a particular RDF format by identifying the type you want with a suitable RDF mime-type (such as “application/rdf+xml” , which requests RDF expressed in XML) in the <em>HTTP request header</em> . This approach can be relatively readily done if you use the http support in most programming languages such as Python or Java. However, if you are trying to use a web browser to fetch data as simple RDF it is difficult to follow these W3C guidelines and to control the mime-type the browser will specify in the HTTP request it generates for you. So, for browser users who actually want the plain RDF rather than an HTML representation of the RDF this W3C recommended method is difficult to carry out. For this reason, DPRR&rsquo;s RDF server has been extended beyond the W3C specification to support a parameter “format” . Specifying one of the standard mime-types for RDF (or more simply “rdf” ) with it will cause the DPRR server to deliver the RDF data directly in the corresponding standard representations of RDF:<a href="http://romanrepublic.ac.uk/rdf/entity/Person/2072?format=rdf">http://romanrepublic.ac.uk/rdf/entity/Person/2072?format=rdf</a>(A screen shot of what a browser shows for this is shown in the appendix as<a href="#figure05">Figure 5</a>.)</p>
<p>This pure RDF is perhaps even more difficult for a human reader to read (especially those not familiar with RDF), but it presents the data in RDF&rsquo;s standard Turtle format<a class="footnote-ref" href="#beckettetal2014"> [beckettetal2014] </a>that can be readily processed by RDF software in programming languages like Python or Java.</p>
<p>We have now seen the data DPRR has about Cicero in both the browser-friendly and RDF data-oriented views. The packaged presentation of DPRR&rsquo;s reader-friendly view is clearly more straightforward for a non-technical DPRR user to understand: that is the intent of its design. Furthermore, the DPRR development team worked to combine together data from various related parts of the DPRR dataset to create a unified and concise presentation that appears assembled together on a single web page. In contrast, to get all the data shown on this one screen through the RDF display requires the user to, themselves, follow links given as URIs in the RDF statements and thus to look at other related parts of the DPRR RDF dataset. Since, as we have seen, the browser-oriented interface delivers information about Cicero in a way that is more user-friendly, who would want to use the RDF Server&rsquo;s representation when arguably the browser-oriented presentation is easier for us to read?</p>
<p>This question takes us to the point of the Semantic Web and Linked Open Data too: that it expresses its materials in a highly structured form (RDF) that is suitable for <em>further</em> processing rather than just human viewing. Whereas arguably the browser-oriented presentation is easier for a person to interpret, it is not as straightforward to use when the purpose is to gather data from it for further processing. Techniques called “screen scraping” or, more specifically, “web scraping” (see Wikipedia&rsquo;s definition of<a href="https://en.wikipedia.org/wiki/Web_scraping"> “web scraping” </a>for a good introduction) have been developed to get data out of human-oriented web pages such as DPRR&rsquo;s reader-oriented presentation — but screen scraping techniques are notoriously unreliable for getting at the underlying data which is presented for human eyes through the web page. In contrast, RDF has straightforward and consistent structures that are easy to process in a programming language such as Python or Java. If your aim is to further process the DPRR materials you fetch, using the RDF formats as the delivery mechanisms from DPRR are most definitely the better bet. Furthermore, as we shall see when we look at the server&rsquo;s query (SPARQL) mechanisms, the data there can also be delivered both in formats not only suitable for further processing in Python or Java, but also in spreadsheet-friendly formats such as Comma-separated values (CSV) (See Wikipedia&rsquo;s definition of<a href="https://en.wikipedia.org/wiki/Comma-separated_values"> “Comma-separated values” </a>for a brief introduction).</p>
<p>We have now seen how the RDF server delivers LOD data about persons held in DPRR. However, as mentioned earlier, one of the important characteristics of the RDF server is that <em>all</em> kinds of DPRR data — not just persons — have open and public URIs assigned to them, so that a user can fetch DPRR&rsquo;s data not only about persons but also about any other kind of information that DPRR holds.</p>
<p>For example, Cicero is recorded in DPRR as having held a post of consul in the year 66 BCE. This kind of assertion is what in DPRR is called a “Post Assertion” and the one about Cicero being a consul is one of the many Post Assertions recorded in DPRR. This particular Post Assertion is expressed as a set of RDF statements, and has its own global URI:<a href="http://romanrepublic.ac.uk/rdf/entity/PostAssertion/5439">http://romanrepublic.ac.uk/rdf/entity/PostAssertion/5439</a>(A screen shot of what a browser shows for this is shown in the appendix as<a href="#figure06">Figure 6</a>.) Giving this URI directly to the WWW will fetch the RDF statements that are associated with this particular Post Assertion about Cicero&rsquo;s consulship. Indeed, we can continue in the same line and, following the principle that <em>all</em> DPRR data has a public URI attached to it, note that the concept of consulship (which is referenced in this Post Assertion) also has its own global URI:<a href="http://romanrepublic.ac.uk/rdf/entity/Office/3">http://romanrepublic.ac.uk/rdf/entity/Office/3</a>(A screen shot of what a browser shows for this is shown in the appendix as<a href="#figure07">Figure 7</a>.) and all the data linked to the office of Consul, as identified by this URI will be returned — including all the Post Assertions that state that someone was a consul since they will all refer to this “Consulship” URI through their “hasOffice” predicate.</p>
<p>Why does having kinds of data other than just persons directly addressable via the WWW matter in what is, after all, a prosopography? Because, as we discuss later in this article, being able to start anywhere (from any kind of data) rather than just one or two kinds of “entry points” (such as, for a prosopography, “historical person” ) is a key reason why structured, interconnected, data (such as that represented using the relational paradigm or by graph representations such as RDF) is likely to be most useful. Being able to enter DPRR&rsquo;s data structures in any number of different ways makes possible fresh ways of looking at the data, something that would difficult to achieve if you could only enter the data through persons.</p>
<p>In order to make good use of the different kinds of interconnected data that the DPRR RDF server makes available beyond persons, one needs to know in some detail what is there and how it is organised. This is a place where an <em>rdf4j</em> workbench mechanism available in the RDF server comes in to be useful. The workbench&rsquo;s “types” display shows all the types of data identified in the DPRR RDF collection, and is a useful starting point for browsing DPRR&rsquo;s RDF statements. Generally, one can navigate to the types display from the browser pages presented by the server via the menu of options on the left side. Here is a direct link to it:<a href="http://romanrepublic.ac.uk/rdf/repositories/dprr/types">http://romanrepublic.ac.uk/rdf/repositories/dprr/types</a>(A screen shot of what a browser shows for this is shown in the appendix as<a href="#figure08">Figure 8</a>.)</p>
<p>Some of these items that are then displayed (the ones that begin with the prefix “owl:” , “rdf:” and “rdfs:” ) are types of data that are generic to RDF and are therefore perhaps less useful for a data investigation about DPRR. However, the ones that begin “vocab:” are the names for types of data that are specific to DPRR; “vocab:Source” , for instance, asserts that there is a type of data called “Source” in DPRR. Looking through the list of types specific to DPRR which are identified by the “vocab:” prefix one finds other types that are immediately identifiable: “vocab:Person” , of course, but also “vocab:SecondarySource” , “vocab:Praenomen” and perhaps “vocab:RelationshipAssertion” given what has already been said about “vocab:PostAssertion” .</p>
<p>Clicking on, say, “vocab:SecondarySource” causes the server to list all RDF statements that make reference to it. One can see quite a range of different kinds of statements about “vocab:SecondarySource” , including a comment associated with it, which tells us that “vocab:SecondarySource” isA modern source. DPRR is primarily built by harvesting data from 19th, 20th and 21st century scholarship.A little below this assertion is the list of Entities that are asserted to be Secondary Sources. Only their URIs are given here so one cannot immediately tell what secondary sources they represent, but all URIs in this display are clickable, so by choosing, say,<a href="http://romanrepublic.ac.uk/rdf/repositories/dprr/explore?resource=%3Chttp%3A%2F%2Fromanrepublic.ac.uk%2Frdf%2Fentity%2FSecondarySource%2F1%3E&amp;limit_explore=100&amp;show-datatypes=show-dataypes">http://romanrepublic.ac.uk/rdf/entity/SecondarySource/1</a>(A screen shot of what a browser shows for this is shown in the appendix as<a href="#figure09">Figure 9</a>.) one can see that Secondary Source 1 is <em>Broughton MRR 1</em> ; later shown to be <em>The Magistrates of the Roman Republic, Vol. I</em></p>
<p>This kind of browsing through RDF data is typical of one of the main uses of the <em>rdf4j</em> workbench displays that have been incorporated into the DPRR RDF server. They allow one to develop a feel for the meaning of the data simply by browsing through the data itself. However, not all the types of data are immediately understandable in this way, and their relationship between each other can still be difficult to grasp. Thus, DPRR data also has what is called an <em>ontology</em> : a formal description (written in OWL<a class="footnote-ref" href="#owl2012"> [owl2012] </a>, another RDF-related technology) of the types of data in DPRR (called “Classes” in OWL) and their relationships to one another. DPRR&rsquo;s ontology is described in<a class="footnote-ref" href="#bradley2017b"> [bradley2017b] </a>, web page<a href="http://romanrepublic.ac.uk/rdf/doc/ontology.html"> “The DPRR Ontology” </a>and presents all the kinds of entities in DPRR and the relationships between them.</p>
<h2 id="two-perspectives-on-uses-for-rdf-and-lod">Two Perspectives on Uses for RDF and LOD</h2>
<p>We have now briefly introduced several of the mechanisms the DPRR RDF server makes available to the world (the query-oriented SPARQL mechanism will be introduced later). It is time, therefore, to step away from its specifics to think about what this approach — providing a data-oriented historical site like DPRR as Linked Open Data (in the sense that Tim Berners-Lee conceives of it) — might mean for a humanities scholarly community.</p>
<p>Most of those people in the digital humanities who are currently working on the challenges of LOD focus on work that is often described asenriching the global graph: making explicit the links between <em>different</em> internet-accessible data collections. We can see work of this kind in projects like <em>Pelagios</em> <a class="footnote-ref" href="#pelagios"> [pelagios] </a>and, perhaps more particularly, <em>SNAP-DRGN</em> <a class="footnote-ref" href="#snapdrgn"> [snapdrgn] </a>. DPRR/RDF, however, does not look like recent historically oriented LOD initiatives such as these. So what is its connection, in and of itself, to the LOD perspective? In spite of the different connection that DPRR has to LOD than the “enriching the global graph” initiatives have, I believe that DPRR&rsquo;s RDF should still be interesting to the humanist LOD community. One needs to start by thinking more about the two different kinds of engagement with LOD materials by web users which appear at different points in time in Berners-Lee&rsquo;s conception of Linked Data.</p>
<p>Berners-Lee&rsquo;s first conception of the Semantic Web was described in the early 2001 <em>Scientific American</em> article “The Semantic Web” <a class="footnote-ref" href="#bernersleeetal2001"> [bernersleeetal2001] </a>. Here we see the authors proposing a data and semantically-rich extension to the already existing document-oriented web in a way that would allow ordinary folk without formal training in digital semantics to exploit this semantic richness. The authors give a number of imagined examples of agent-based software that could automatically exploit formal semantic data across different sources. One example (see page 36) tells us of a user who sends her agent software off to make an appointment with a medical specialist for her mom. To do this requires the agent to find specialists that fit with mom&rsquo;s prescribed treatment, then match up the appointment calendars for mom and those specialists. The software agent also needs to take into account other parameters such as distance to the appointment, and the need for physical therapists. Allowing a user&rsquo;s software agent to perform this kind of complex task reliably requires that the material it works with must be highly structured and have appropriate software-accessible semantics formally available so that the software agent can, on its own without human intervention, connect it together correctly and exploit it. In the ideal Semantic Web described by Berners-Lee et al in 2001 a human user would be able to safely delegate this task to their agent software and wouldn&rsquo;t need to worry about the details of how the agent did the job, although if she was interested she could ask the system how it went about carrying out the task and, since the computation would be based on structures that semantically mirror parts of our human understanding of the world, receive an answer that could be understood.</p>
<p><a class="footnote-ref" href="#bernersleeetal2001"> [bernersleeetal2001] </a>&rsquo;s 2001 agent-oriented vision has proven to be quite ambitious. As a consequence there has been work in Computer Science to explore the somewhat simpler task of trying to make semantic web data help ordinary, non-technical users better search for things in which they are interested in the vast global internet-wide data graph. Some of this work involves trying to find ways to enrich google-like searching (which is centred primarily on very sophisticated Natural Language retrieval principles (NLP) applied to the WWW&rsquo;s text-oriented documents) with semantically-structured material expressed in RDF and its associated technologies. When researchers tried to build systems that could jointly exploit RDF-like structured data as well as the text in Web pages they found it to be a real challenge. One of the issues was that independent but semantically related data collections were likely to have differing internal structures and might well use different vocabulary in their formal structure for what were the same or similar concepts: a condition called “Heterogeneous Datasets” by some researchers. A good summary of some of the thinking in this area from a few years ago can be found in<a class="footnote-ref" href="#freitasetal2012"> [freitasetal2012] </a>. It is not quite clearly spelled out in this article, but an important assumption seems to be that the tools that they were interested in would ideally support querying that could be characterised as coming from what I am calling here an “intuitive user” .</p>
<p>Consider Google as an example of an existing service which is also conceived of as serving an “intuitive user” . Most Google users are not familiar with the range of material that the web possesses when they start a Google search, and they phrase their question without knowing the structure or vocabulary applied to materials on the web. In this sense, their querying is intuitive. Similarly, some of the engines that Freitas <em>et al</em> describe are meant to allow users to ask questions in a natural language without knowing much about the domains the data represents. These engines use a combination of NLP techniques combined with a sophisticated understanding of relevant RDF data with their ontologies that describe them, to provide a better query result than NLP could deliver on its own. The aim is to allow users to come with what are intuitive text-oriented questions and get richer, more trustworthy, results than they would get from the NLP approaches against text-oriented documents alone. There is a good summary of more recent thinking in this area in<a class="footnote-ref" href="#noyetal2019"> [noyetal2019] </a>.</p>
<p>Of course, recent work by Google and others has shown that text-oriented big data strategies can achieve remarkable things with only vast amounts of almost-raw text as data without needing large amounts of hand crafted formal semantic data at all. Thus, it would seem that if the 2001 Semantic Web vision is ever going to be achieved, the emergence of platforms that have rich, widely available, semantic data expressed in RDF and its associated technologies, combined with AI software of the kind envisioned here that can make use of it, are still something for the future.</p>
<p>Perhaps as the challenges of implementation of the ideas in the 2001 article became clearer, Berners-Lee began to think about the benefits of having the data without the sophisticated AI-like framework that would be needed to make the more sophisticated ideas of the 2001 article work. This is the situation we find in Berners-Lee&rsquo;s 2010 TED talk that I mentioned earlier. Here, the users of Berners-Lee&rsquo;s global data are not the kind of intuitive user with their natural language query that I have just described; a user which would need to be supported by substantial Artificial Intelligence-like methods hidden from him/her. Instead, Berners-Lee gives examples of people exploiting the power of formally structured data through “mashups” which explicitly join together bits of previously disconnected global data to gain new insights into the material. In one of Berners-Lee&rsquo;s illustrations we see a person joining together data about what streets a new municipal water pumping station served with demographic data about those streets, and then being able to show how this town&rsquo;s new station was disproportionally serving the wealthier parts of the town. This kind of working with disparate data from different sources requires something quite different from its user than the intuitive engagement of the Google-like NLP+Semantic-data approach that Freitas <em>et al</em> and Noy <em>et al</em> are exploring. If someone wishes to join up data from different sources like this they cannot be an intuitive user and take an intuitive approach based on only a limited understanding of the data one is querying. Instead, to join them together they need to understand in some detail the semantic structure and significance of their data sources, and know how to formally join them correctly.</p>
<p>The important point for us here is that the Berners-Lee TED talk&rsquo;s researcher&rsquo;s discovery of the link between the new water plant and the people it served was made not with the aid of an intuitive google-like query, but by the deliberate bringing together of two sources of structured data in a way that no one else had done. To achieve this, the data analyst needed, in some way, to be the opposite of intuitive. Instead, s/he could only create new information when s/he thoroughly understood the semantics of the pieces of data s/he is working with and understood how they connected together. Furthermore, only in this way could the strength of the argument that arises from this water plant example come out of the semantic juxtaposition of the materials.</p>
<p>Freitas <em>et al</em> characterise this kind of interaction with data and the type ofstructured querythat can be expressed against it ascrispand seems to equate “crispness” of response with “precise answers” of the kind given by database formal queries in languages like SQL<a class="footnote-ref" href="#freitasetal2012"> [freitasetal2012] </a>. These interactions with data are not like queries that are conceived of as Google-like semi-natural language expressions, where one cannot actually be sure either that the result one gets matches a natural human understanding of the query or that one gets all the material that a human would consider relevant to the question asked. Instead, these crisp structured queries have a kind of processing model that, to the degree that the data being queried can be considered to be an accurate representation of its material (admittedly, an important qualification) and inasmuch as one can express what one is interested in in the formal nature of the query language, allows one to be sure of the completeness and accuracy of the result. Although Freitas <em>et al</em> don&rsquo;t explain what they mean bycrispandprecisein their article, it is, I expect, in this area that their sense of these terms resides.</p>
<p>Does DPRR&rsquo;s RDF server allow for this kind of engagement with its data? Can a classical scholar engage with the formally based mechanisms of DPRR with an intention that is similar to Berners-Lee&rsquo;s water plant mashup example? Certainly formal “crisp” queries of the form and spirit that Berners-Lee&rsquo;s 2010 examples require are available through the DPRR RDF Server&rsquo;s support of the SPARQL<a class="footnote-ref" href="#sparql2013"> [sparql2013] </a>query language.</p>
<h2 id="dprr-and-sparql">DPRR and SPARQL</h2>
<p>What is SPARQL? Wikipedia starts its article about SPARQL by sayingSPARQL allows users to write queries against [&hellip;] data that follow the RDF specification of the W3C. It works by allowing the SPARQL query creator to specify a pattern to look for in the RDF graph, and to display parts of the selected bits that match the pattern as results. This certainly is not the place to provide a tutorial on SPARQL, but here is an example of a query in it:<br>
PREFIX vocab: <a href="http://romanrepublic.ac.uk/rdf/ontology#">http://romanrepublic.ac.uk/rdf/ontology#</a> select ?name ?officeName where { ?person a vocab:Person; vocab:hasName ?name; vocab:isSex <a href="http://romanrepublic.ac.uk/rdf/entity/Sex/Female">http://romanrepublic.ac.uk/rdf/entity/Sex/Female</a>. ?assertion a vocab:PostAssertion; vocab:isAboutPerson ?person; vocab:hasOffice ?office. ?office vocab:hasName ?officeName }<br>
(A screen shot of what a browser shows when this is submitted is shown in the appendix as<a href="#figure10">Figure 10</a>.)</p>
<p>The query looks for graph patterns in the DPRR RDF data that show women who are also recorded has holding offices, and displays the woman&rsquo;s name and the name of the office. It is expressed in the SPARQL language, and the reader can doubtless see that it is not a trivial matter to learn to create queries of this kind, particularly for those without knowledge of related query languages such as XQuery for XML<a class="footnote-ref" href="#xquery2018"> [xquery2018] </a>, or SQL for relational databases<a class="footnote-ref" href="#sql2018"> [sql2018] </a>. However, once it has been learned, it provides a powerful way to explore a complex set of RDF data, such as that found in DPRR.</p>
<p>The SPARQL query presented here in this article is given in the context of an HTML form that allows one to directly send the query to the RDF Server and receive the result. To do so, push the “Execute” button. Soon thereafter you should receive a response from the Server showing, in a table, the names and offices of all women recorded as holding offices in the DPRR dataset (or, click<a href="#figure10">here</a>to see a screen image in the appendix of the beginning of the server&rsquo;s response to this query). You might recall that our first view of material from the DPRR RDF Server was of its SPARQL Query screen. And indeed, the query text shown here could be copied and pasted into that screen and run from there, and would have produced essentially the same result as what one gets from the above form.</p>
<p>The form above causes the RDF Server to return its result embedded in a light wrapping of HTML that makes it more suitable for human browsing. However, the query can also be run so that it returns results in a structured form more suitable for further processing. Here is the same query set up in a form that causes the result to be returned in JSON — a format suitable for further processing by platforms such as Python or Java (if you are curious about JSON, a good starting point is Wikipedia&rsquo;s<a href="https://en.wikipedia.org/wiki/JSON">definition</a>). Results can also be returned in CSV format which can be opened as a spreadsheet, although this is not shown in this example.<br>
PREFIX vocab: <a href="http://romanrepublic.ac.uk/rdf/ontology#">http://romanrepublic.ac.uk/rdf/ontology#</a> select ?name ?officeName where { ?person a vocab:Person; vocab:hasName ?name; vocab:isSex <a href="http://romanrepublic.ac.uk/rdf/entity/Sex/Female">http://romanrepublic.ac.uk/rdf/entity/Sex/Female</a>. ?assertion a vocab:PostAssertion; vocab:isAboutPerson ?person; vocab:hasOffice ?office. ?office vocab:hasName ?officeName }<br>
(A screen capture of the beginning of the display generated by the query is shown in the appendix as<a href="#figure11">Figure 11</a>. How your browser displays JSON data may be different from how Firefox showed it to me.)</p>
<h2 id="can-sparql-queries-further-study-of-the-roman-republic">Can SPARQL Queries Further Study of the Roman Republic?</h2>
<p>Having now briefly seen SPARQL as a querying mechanism against the DPRR dataset, perhaps the reader will still not find it obvious how such a thing could be relevant to the furtherance of the study of the Roman Republic. I can see three possible concerns:</p>
<p>Both Berners-Lee&rsquo;s mashup builder and DPRR&rsquo;s SPARQL query engine require a complex set of technical skills that one would think does not match well with the normal skill-set profile of someone interested in DPRR.Whereas Berners-Lees examples draw data from disparate sources and joins them together to make their point, DPRR is, by itself, a single source. Berners-Lees is making the point that the strength of LOD as a new way to look at data arises precisely from the way that it allows sources that have not been brought together before to be joined. What do LOD approaches have to offer for a single source like DPRR?Finally, whereas Berners-Lee&rsquo;s examples use the connecting together of data to make political points, there are likely to be few, if any, political arguments of the kind that Berners-Lee is interested in that could come out of a study of DPRR.</p>
<h2 id="point-1-technical-skills">Point 1: Technical skills</h2>
<p>In order to interact with the DPRR RDF server and get the benefits that it holds one needs to understand<br>
first, formal data modelling principles,then understand RDF,then how to query RDF datasets with RDF&rsquo;s query language SPARQL,and finally how to assemble data selected from the server for further processing, perhaps to turn the data into, say, useful displays with a spreadsheet, or with, say, Python and something like Google Graphs.</p>
<p>This is a tall technical order.</p>
<p>Of course, the complex technological requirements needed to interact with RDF data, plus the assumption that DPRR&rsquo;s users are unlikely to have the technical skills needed to interact with the data directly, is <em>exactly</em> the reason why DPRR (like CCH/DDH&rsquo;s other digital resources) have as its main public point of access a web-oriented user-friendly front end to its complex, formally structured, relational dataset. Why, then, is the fuller functionality similar to what direct interaction with RDF enables through the DPRR RDF Server not made available from DPRR or PoMS&rsquo;s more “user friendly” web front-ends? There are two reasons.<br>
One is User Experience (UX) based. The assumption behind much UX work is that the user is going to be an intuitive user, and needs to get useful results from simple interactions that require little effort to understand the database and its semantic structures. The design tries to, as much as possible, follow the user interface principle[d]on&rsquo;t make me think<a class="footnote-ref" href="#krug2014"> [krug2014] </a>as put forward a few years ago by Steve Krug, the UX guru. As a consequence of this UX thinking, if we expected our web users to understand that was going on with only minimal intellectual effort we had to restrict the investigation paths for our users to relatively straightforward ones. However, to get all the “semantic juice” out of DPRR, PoMS or any other relational database requires more understanding of the formal principles of the relational model and the structures of a particular database than what matches the UX understanding of a user community.The second reason is that the results have to also be presented in ways that suit the user and his/her browser rather than as formally-structured data that can then be readily reprocessed by software for further analysis — as a web page rather than as structured data which could be further processed — since in this day and age web pages are both accessible and, in a general sense, understandable to pretty well anyone likely to be interested in DPRR, including otherwise non-technical users.</p>
<p>How could a suitably trained person take advantage of the facilities the DPRR server offers? To show how DPRR&rsquo;s RDF server can be exploited I have created a modest “timeline” example and made it available with the server which plots the holders of the office of <em>consul</em> by their tribe. It shows how the technologies of RDF and Python can be engaged to get materials out of DPRR&rsquo;s data that would be difficult to do with the more “user-friendly” UX-designed web front-end. It is based on a SPARQL query which is formulated to fetch the tribe name associated with each consul holder. You can see the query that fetches the relevant data in the form below, and by pushing “Execute” you can run it for yourself.<br>
PREFIX rdf: <a href="http://www.w3.org/1999/02/22-rdf-syntax-ns#">http://www.w3.org/1999/02/22-rdf-syntax-ns#</a> PREFIX vocab: <a href="http://romanrepublic.ac.uk/rdf/ontology#">http://romanrepublic.ac.uk/rdf/ontology#</a> select ?startDate ?endDate ?tname ?aid ?pid ?pname where { ?person a vocab:Person; vocab:hasID ?pid; rdfs:label ?pname. ?tribeassert a vocab:TribeAssertion; vocab:isAboutPerson ?person; vocab:hasTribe ?tribe. ?postAssert a vocab:PostAssertion; vocab:hasID ?aid; vocab:isAboutPerson ?person; vocab:hasOffice <a href="http://romanrepublic.ac.uk/rdf/entity/Office/3">http://romanrepublic.ac.uk/rdf/entity/Office/3</a>; vocab:hasDateStart ?startDate; vocab:hasDateEnd ?endDate. ?tribe a vocab:Tribe; vocab:hasName ?tname. }<br>
(A screen capture of the beginning of the display generated by this query is shown in the appendix as<a href="#figure12">Figure 12</a>.)</p>
<p>Having created the SPARQL query which fetched the data needed to plot the tribe of consuls over time, the query was then embedded in a Python script which ran it directly, took the results it generated (in JSON), and used the plotting services of Google Graph to generate an HTML page that plotted the year vs tribe. The overall result can be seen<a href="http://romanrepublic.ac.uk/rdf/timeline/consuls.html">here</a>. The <em>timeline</em> materials, including the Python script, are available from<a href="http://romanrepublic.ac.uk/rdf/timeline/">http://romanrepublic.ac.uk/rdf/timeline/</a>. I built the script in a couple of hours and, having done one, could probably do another one for a different question more quickly.</p>
<p>This is all well and good, I hear you say; but, of course, although I have the skills needed to create something like this, I am not the right person to decide whether the result that the <em>timeline</em> example provides is actually useful to the study of Roman Republic history. Only Roman Republic historians themselves can do so, since they understand whether or not any connection between a person&rsquo;s tribe and the offices they held could be historically interesting. It is thus historians rather than someone like me that need to be directing the engagement with DPRR&rsquo;s RDF data. Is it possible, then, to expect historians to be able to interact in this way with the dataset: to have both the understanding of an historian and the technical skills that enable one to fetch data using SPARQL and plot it in something like Google Graph?</p>
<p>My own experiencew of teaching Python for a number of years (in a one term MA taught module) and structured data, including RDF and SPARQL (in another) in DDH&rsquo;s Digital Humanities MA programme has led me to believe that it is possible for students with a conventional humanities education, but with a real commitment to engage with the potential of DH methods, to learn sufficient technical foundations to be able to engage with data-oriented materials such as DPRR&rsquo;s RDF representation effectively. These students, with a humanities orientation in their background, were able to bring these humanities-oriented interests and curiosity to bear on their new-found technical abilities, to conceive of and perhaps construct something like DPRR&rsquo;s <em>timeline</em> example. Indeed, more than one student who has attended these two modules has directly told me that they believe that they came away from these modules with the beginnings of exactly this kind of ability.</p>
<p>I also have received news recently of an example of DPRR&rsquo;s RDF data working in exactly this way. Although the RDF server has only been available for a few months at the time this article was being written, a researcher working in an independent research project run by Professor Chris Johanson (UCLA’s Department of Classics) which is entirely external to DPRR reported to me that the RDF data has made an important contribution to their work. The project team was interested in trying to generate visualizations of Rome&rsquo;s Rostra during funerals of important people in the Roman Republic and to show the different types of togas that would be worn by the actors playing the part of the deceased&rsquo;s ancestors. They found the DPRR RDF server useful because they could use their own queries to get information about how different people were related and what offices they had held (and thus, which toga would be used to represent them). To that end the team started from the results of relevant queries to the DPRR RDF server to generate a directed graph (nodes=people, edges=paternal relationships) which they could then traverse to determine the set of togas to depict. As they say in one of the emails they have sent me, they were able to use the server to fetch readily the materials they needed themselves much more directly than they could have done with either the user-friendly DPRR web front end, or, of course, if DPRR data had not been available to them at all. There is more information at<a class="footnote-ref" href="#johansonetal2019"> [johansonetal2019] </a>, including specifically the visualisations at<a href="http://hvwc.etc.ucla.edu/funerals-rostra">http://hvwc.etc.ucla.edu/funerals-rostra</a>. Professor Johanson was interested enough in the RDF server to then spend some time introducing some of his students to it. He asked them to explore the data and to see what visualisations they could produce with its help. The result was<a href="https://dh-199-the-shape-of-roman-history.github.io/">the Shape of Roman History</a>project, which contains something like 30 charts, all of which draw their data from the DPRR RDF Server.</p>
<h2 id="point-2-single-source">Point 2: Single Source</h2>
<p>Berners-Lee&rsquo;s examples of exploiting Linked Data are often classified as what are called <em>mashups</em> : the joining together of more than one data source to enable a new representation that any one data source, by itself, could not achieve. Speaking strictly, then, the <em>timeline</em> example is not a mashup because it draws all its materials from the single DPRR data server. Thus, some might argue that it does not fit well with the assumptions in the LOD movement: that it is in the bringing together of data from <em>multiple</em> sources that new insights can come. However, it is important to understand that Berners-Lee&rsquo;s examples in his TED talk require multiple different sources of data because each online source is relatively small and structurally straightforward piece of data; the kind of thing that can be comfortably represented in, say, a spreadsheet. This is not the situation, however, with DPRR. DPRR&rsquo;s original relational model already supports quite a rich kind of interaction between many different kinds of objects, and DPRR&rsquo;s set of RDF — which is, after all, simply an expression of DPRR&rsquo;s complex database — has in fact 39 different entity types, related together by 30 types of relationship and 53 kinds of data properties spread across those 39 entity types. Thus, the DPRR RDF graph is already by itself a complex interconnected graph of data of which only a handful of all the implied relationships between these objects has ever been explored. Thus it is reasonable to expect that queries which can, relatively straightforwardly, draw on the complex interconnections in DPRR&rsquo;s RDF alone can expose connections that have never before been considered. Many more visualisations like the DPRR <em>timeline</em> demonstration are possible without needing to go outside of DPRR&rsquo;s internal web of data, and some of these might well make new insights possible about the Roman Republic.</p>
<h2 id="point-3-political-points">Point 3: Political Points</h2>
<p>Finally, Berners-Lee&rsquo;s TED talk shows that, as in his water plant example, it is possible to see that much of the work enabled by LOD that draws on contemporary data could have contemporary <em>political</em> significance. Is something as potentially significant possible in the humanities? It is true that there are unlikely to be contemporary political issues that could be usefully explored by looking at the Roman Republic and DPRR. However, there <em>is</em> some evidence around that suggests that significant original ideas can be explored though data like DPRR&rsquo;s that have, hitherto, been unavailable or difficult to work with, and that perhaps some of these might represent truly original research that presents ideas that no one else has noted before. As<a class="footnote-ref" href="#guetzkowetal2004"> [guetzkowetal2004] </a>describe in their article entitled <em>What is Originality in the Humanities and the Social Sciences</em> , new ideas &ndash; including even radically new ones that might be read as political within the humanities itself — are often valued by the humanities community. They write:</p>
<blockquote>
<p>In interviews, we found that panellists described originality, for example, in terms of the novelty of the overall approach used by the researcher (who is &lsquo;bringing a fresh perspective&rsquo;) in terms of the data being used (she is &lsquo;drawing on new sources of information&rsquo;), and in terms of the topic chosen (he is &lsquo;going outside canonized authors&rsquo;). These statements point toward a much broader definition of originality than that posited by the available literature on originality.<br>
<a class="footnote-ref" href="#guetzkowetal2004"> [guetzkowetal2004] </a><br>
The representation of historical materials as data (rather than text) and the drawing of historical conclusions from it has been controversial within history: one recalls the debates about the use (and some would say the misuse) of statistics in the <em>Time on the Cross</em> studies<a class="footnote-ref" href="#fogelandengerman1974"> [fogelandengerman1974] </a>. See Thomas Weiss&rsquo;s 2001 review<a class="footnote-ref" href="#wiess2001"> [wiess2001] </a>for a sense of the debates that arose from this work. Nonetheless, in spite of the debates it has spurred, it certainly has been, as Weiss says,a book that has not been ignored. Could some kind of data analysis, perhaps statistical, that is enabled by DPRR&rsquo;s RDF representation cause similar stimulation and consternation within the community that studies the Roman Republic? Of course, it is too early to say much about the DPRR RDF Server in this regard. However, the enthusiastic informal reports we see from users of DDH&rsquo;s many data-oriented historical resources such as <em>People of Medieval Scotland</em> (PoMS) and the <em>Prosopography of Anglo-Saxon England</em> (PASE) (mentioned earlier) suggests that research product presented as data rather than prose can also be useful to historians.</p>
</blockquote>
<p>Furthermore, recent work with data in a sister data-oriented prosopography project PoMS (which is a factoid prosopography and was captured, like DPRR, in highly structured data) suggests that significant new approaches to historical materials, when available as complex structured data and explored from new perspectives, are both possible and reveal significant potential for new insights. Starting several years ago, thanks to a grant from the Leverhulme Trust, the data <em>behind</em> PoMS&rsquo;s public website has been used as a base for historical Social Network Analysis (SNA) experiments. This SNA analysis work on PoMS has been initially extensively reported on in<a class="footnote-ref" href="#hammondandjackson2017"> [hammondandjackson2017] </a>which is an e-book of over 500 pages. As it is pointed out in the preface for this book, all the SNA analysis was enabled by PoMS&rsquo;s data-oriented interpretation of its sources and came from the relationships recorded directly and indirectly in the PoMS database. The data needed for the SNA work was explicitly provided by the database, and yet could not have been carried out with either PoMS&rsquo;s public web interface or with google-like intuitive queries that might have selected PoMS materials. To perform it effectively required access to the information behind PoMS&rsquo;s public interface. Since PoMS&rsquo;s data was organised in a relational database, the process that was used to fetch data for SNA analysis used queries expressed in the relational database&rsquo;s standard query language SQL<a class="footnote-ref" href="#sql2018"> [sql2018] </a>that were quite different from those SQL queries used behind the scenes to drive PoMS&rsquo;s public interface with its particular user perspective.</p>
<p>The resulting SNA analysis showed that PoMS data could be exploited in ways that were quite different from what one could achieve through its public interface. Although the work was still in its early days when the Leverhulme grant was over, the team was even then beginning to see that this novel SNA perspective was pointing the way to possible new insights into Medieval Scottish society. As a result, work has continued exploring this SNA approach after the grant completed, and has resulted in particpation in and leading of a number of workshops and demonstrations to the growing Digital Historian SNA community. Although the fetching of data that fed the SNA analysis of PoMS was done in the formal language of SQL rather than with LOD technologies, this technical work was quite similar to what RDF, SPAQRL, and related technologies would have enabled. If PoMS data had been available as RDF through a PoMS RDF Server this same work could have been carried out by anyone with internet access using Semantic Web technologies such as SPARQL. Furthermore, although one of the members of the SNA team, Dr Matthew Hammond, is trained as a historian, rather than as a computer scientist, he was able to master the formal language SQL well enough to, on his own, get the data in the forms that he could use for his SNA work. If PoMS data had been made available through an RDF Server like DPRR&rsquo;s, he could certainly have done the same work in SPARQL instead.</p>
<h2 id="dprr-and-enriching-the-global-graph-a-third-kind-of-user">DPRR and Enriching the Global Graph: A Third Kind of User</h2>
<p>As this article has shown, DPRR is a complex and interconnected collection of RDF statements which both (i) forms, within itself, a complex and disciplined graph of information and (ii) thus offers many possible routes for exploration. However, other than the standard references to RDF vocabularies such as RDFS and OWL, DPRR&rsquo;s RDF does not point out of itself into materials created and held elsewhere. Since the linking together of data across the entire “global” rather than DPRR&rsquo;s “local graph” is part of the vision of linked data, one needs to also think about what needs to be done within DPRR to bring it more into alignment with this aspect of the global graph vision.</p>
<p>As mentioned earlier, most of those people in the digital humanities who are currently working on the challenges of LOD are interested in what is often described asenriching the global graph— making explicit the links between <em>different</em> internet-accessible data collections. This work has sometimes been categorised as “aggregation” , and is often done by making a block of “SameAs” assertions using the <code>owl:sameAs</code> predicate or something like it. For instance,<a class="footnote-ref" href="#viaf2010"> [viaf2010] </a>describes itself as theVirtual International Authority File. It is a resource maintained by OCLC as a service to libraries which aims tolower the cost and increase the utility of library authority files by matching and linking widely-used authority files and making that information available on the Web. Thus, VIAF has the URI<a href="https://viaf.org/viaf/78769600/">https://viaf.org/viaf/78769600/</a>for Cicero and when it is invoked one gets a web page that shows how major world libraries have identified him. Thus, this VIAF URI can be considered as VIAF&rsquo;s identifier for the historical person Cicero. One can then make an <code>owl:sameAs</code> assertion via an RDF triple that asserts that the person associated with DPRR&rsquo;s URI (<a href="http://romanrepublic.ac.uk/rdf/entity/Person/2072">http://romanrepublic.ac.uk/rdf/entity/Person/2072</a>) for Cicero is the same person as the person VIAF identifies with their URI. This kind of work, when done with as many of DPRR&rsquo;s persons as VIAF has also identified, is arguably the first step in aligning DPRR&rsquo;s data with the larger digital world of data, at least as it exists in the context of libraries. Similar work could be done with world wide resources such as, say, WorldCat<a class="footnote-ref" href="#worldcat"> [worldcat] </a>.</p>
<p>Establishing <code>owl:sameAs</code> links between entities in different datasets to show how they connect to each other seems to be obviously a good idea that enriches the interlinking in global data, especially if one of the links is to a recognised authority, such as VIAF. Of course, DPRR is a published prosopography. The identification of people is the point of the work it represents, and hence DPRR has some reason to claim to be an authority for Roman Republic persons in its own right. Perhaps, then, in the same way as in the past many different independent researchers working on the Roman Republic often used Pauly&rsquo;s RE as an authority and identified people using the person identity scheme used in it, people in the future could use DPRR&rsquo;s URIs to identify which historical Roman person they were referring to.</p>
<p>Much of the work done in the DH that involves linking to authorities like VIAF has been carried out in the context of identifying people who appear in texts — as a reference from a spot in a digital edition of a text, say, or perhaps from a reference in a piece of research being written up as an article or a monograph — and is undertaken in the context of textual markup. This linking of a spot in a text to an authority such as VIAF (or DPRR itself) is a useful enriching process. However, the benefits are perhaps obviously greater when the links are not from a <em>text</em> (even one marked up using TEI) to RDF data such as DPRR, but between separate datasets both of which can both be queried by SPARQL, since SPAQRL&rsquo;s Federated Query mechanisms<a class="footnote-ref" href="#seaborneetal2013"> [seaborneetal2013] </a>allows a single query to span across more than one dataset. If, for example, a dataset (let us call it “A” here) outside of DPRR had kinds of information that does not appear in DPRR about Roman persons, and if both DPRR and “A” &rsquo;s dataset&rsquo;s associated persons could be connected through references to common VIAF URIs, it would be possible to query data that crossed both DPRR and “A” , taking advantage of the data strengths of each of them.</p>
<p>In some ways this linking work fits with the spirit of what DPRR was already doing: bringing together hitherto separate Roman Republic prosopographies; although DPRR&rsquo;s work was based more on establishing collections between what had been separate primarily <em>print</em> prosopographies. However, although DPRR did indeed assemble materials from these various prominent, independently produced, specialist prosopographies into a single large collection they were not able to take up the further task of linking their people to a world-wide resource such as VIAF. There simply was not the time and funds available. As it turns out, this may be the place for a <em>third</em> kind of person to engage in DPRR&rsquo;s LoD data: someone who might be called an “aggregator” . This third kind of user arises from the fact that it is in the nature of LOD that, now that DPRR data is open and freely available through DPRR&rsquo;s RDF server, someone <em>else</em> with an interest in historical people from the Roman Republic that appear in VIAF or WorldCat can choose to create RDF triples independently of DPRR&rsquo;s research team that assert the connections between the people identified in these resources, and those identified through DPRR&rsquo;s person URIs and then make their collection of “sameAs” RDF triples that assert the connections available over the web. Indeed, by hosting these triples that connect DPRR entities to VIAF or WorldCat outside of DPRR itself one avoids the possible confusion by users of who did what: it will be clear that the links between DPRR and VIAF or WorldCat were done as a separate project outside of DPRR. In fact, this is one of the benefits of the conception of LOD as data distributed worldwide when it is based on the RDF technologies.</p>
<p>So far in this section we have focused on DPRR&rsquo;s historic persons as the centre of a linking initiative, and DPRR is, after all, a prosopography, and thus exploiting the URIs for its people through links seem like the most obvious thing to do. However, in the RDF context all of DPRR&rsquo;s data is open and globably available. Thus, there are URIs in DPRR that represent things other than persons, and linking these other non-person objects in DPRR to authorities elsewhere could also be useful to do. For example, DPRR has what the Romans called <em>provinces</em> as entities associated with offices. Not all the Roman provinces were geographically based, but many of them were. Thus, perhaps a sameAs link could be establised between these geographically based provices identified in DPRR and those same geographic provinces as they are identified in geographic authority sites such as <em>Pelagios</em> <a class="footnote-ref" href="#pelagios"> [pelagios] </a>. Then, if other RDF sites also used Pelagios URI identifiers in their data, these Pelagios URIs could be used as linking mechanisms to allow these two datasets to be joined together in a federated SPARQL query. If, for example, there was a set of RDF data that associated climate conditions with Pelagios places, federated queries could be used to explore if there was any evidence that climate had any effect on who got postings associated with these provinces.</p>
<h2 id="a-call-to-action">A Call to Action</h2>
<p>The development of the DPRR RDF server has shown that the materials developed by a data-oriented project such as DPRR can be certainly expressed as RDF, and can be served online in this way and meet the criteria proposed for Linked Open Data by Tim Berners-Lee and others. Only time will tell, of course, how useful academics who are interested in the Roman Republic will find such an expression of this kind of research, but the fact that very soon after its launch, the UCLA project interested in Roman Republic funerals found it useful is at least encouraging.</p>
<p>Now that DPRR&rsquo;s data has been made available directly as LOD, perhaps it is time for other data-oriented sources to be made available in this form as well. Over the years King&rsquo;s DDH department, in collaboration with historians and other colleagues in the arts and humanities as well as cultural heritage sector, produced a significant number of web sites that are driven by data that could readily be mapped to and delivered as RDF in the same way that DPRR&rsquo;s has been. So, now that DPRR&rsquo;s data has been made available directly as LoD, provided that appropriate resources are in place to support this work (see, for instance,<a href="https://ahrc.ukri.org/funding/apply-for-funding/current-opportunities/towards-a-national-collection-opening-uk-heritage-to-the-world-pre-call-announcement/">this UKRI announcement</a>), perhaps it is time for other data-oriented sources to be made available in this form as well. Indeed, at the time that this article was being prepared for publication work was just finishing up which published another of them: the People of Medieval Scotland (PoMS) data through its own RDF server in essentially the same way. You can find its RDF Server<a href="https://www.poms.ac.uk/rdf">here</a>.</p>
<p>King&rsquo;s Digital Lab (KDL) is now the unit at King&rsquo;s responsible for hosting most of the resources that were started by DDH such as DPRR and PoMS, and has kindly agreed to take up the responsibility for hosting and maintaining their RDF Servers as well. The development of RDF Servers for these projects fits well with one of KDL&rsquo;s current initiatives which is centered on the idea of data exposure and publication becoming a key element in the approach to a project&rsquo;s development: see<a href="https://www.kdl.kcl.ac.uk/our-work/archiving-sustainability/">this KDL web page</a>and<a class="footnote-ref" href="#smithiesetal2019"> [smithiesetal2019] </a>. With respect to legacy projects, one of the options KDL offers to project partners is dataset deposit and the preparation of associated metadata cataloguing it. As a consequence one of the solutions KDL has developed is a CKAN instance hosted within KDL&rsquo;s infrastructure (<a href="https://data.kdl.kcl.ac.uk/)">https://data.kdl.kcl.ac.uk/)</a>. Nonetheless, KDL has also seen that the DPRR RDF Server&rsquo;s more dynamic approach to direct data access also has the potential to fit with this part of their vision. Rather than being mediated through a web application front end, these projects&rsquo; raw data might already have an important role to play to further new humanities research in their own right. As a consequence, perhaps, like DPRR, research data for other of these web resources might well also deserve to be set free for those in the humanities who are equipped to take advantage of them.</p>
<h2 id="appendix-screen-captures">Appendix: Screen Captures</h2>




























<figure ><img loading="lazy" alt="A screen capture of the response to URI for Cicero." src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>The DPRR RDF Server responds to <a href="http://romanrepublic.ac.uk/rdf/entity/Person/2072">http://romanrepublic.ac.uk/rdf/entity/Person/2072</a> (Cicero)
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="A screen capture of the browser-oriented web app&#39;s response to its URL for Cicero." src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>The DPRR browser app&rsquo;s response to <a href="http://www.romanrepublic.ac.uk/person/2072/">http://www.romanrepublic.ac.uk/person/2072/</a>
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="A screen capture of the RDF data (Turtle format) delivered by the server for Cicero&#39;s URI." src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>The DPRR RDF Server responds to <a href="http://romanrepublic.ac.uk/rdf/entity/Person/2072?format=rdf">http://romanrepublic.ac.uk/rdf/entity/Person/2072?format=rdf</a> (Cicero)
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="A screen capture of the response to a URI for a Post Assertion entity." src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>The DPRR RDF Server responds to <a href="http://romanrepublic.ac.uk/rdf/entity/PostAssertion/5439">http://romanrepublic.ac.uk/rdf/entity/PostAssertion/5439</a>
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="A screen capture of the response to the URI for the office of Consul." src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>The DPRR RDF Server responds to <a href="http://romanrepublic.ac.uk/rdf/entity/Office/3">http://romanrepublic.ac.uk/rdf/entity/Office/3</a> (Office of Consul)
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="A screen capture of the response to a request to see the types defined in the DPRR RDF dataset." src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>The DPRR RDF Server displays the list of types in the DPRR RDF dataset
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="A screen capture of the response to the URI for Broughton Vol 1" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>The DPRR RDF Server responds to <a href="http://romanrepublic.ac.uk/rdf/entity/SecondarySource/1">http://romanrepublic.ac.uk/rdf/entity/SecondarySource/1</a> (Broughton Vol 1)
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="A screen capture of the response to a SPARQL query." src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>The DPRR RDF Server responds to a SPARQL query (list of women who held offices)
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="A screen capture of browser displaying JSON data as the response to a SPARQL query." src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>The DPRR RDF Server responds to a SPARQL query with JSON data
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="A screen capture of browser displaying the result of a SPARQL query." src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>The DPRR RDF Server responds to a SPARQL query about tribes of consuls over time
        </p>
    </figcaption>
</figure>
<h2 id="acknowledgements">Acknowledgements</h2>
<p>DPRR, like all similar DH projects in which I have been involved, involves a range of organisations and people. There are, thus, several organisations and people to acknowledge here. First, one must thank the UK&rsquo;s Art&rsquo;s and Humanities Research Council, who funded the academic and technical work involved in the creation of the DPRR data and website. Second, I would like to thank the academic colleagues in the DPRR project with whom we worked for several years to create the data. Third, I must thank my colleagues at King&rsquo;s Digital Lab group (KDL), who have made it possible for the RDF Server described here to be publicly hosted on their servers. And finally, I should thank colleagues at DHQ who not only provided their stellar editorial support for this article, but agreed to extend their technical infrastructure to allow this article&rsquo;s unusual integrated interactive components to be accommodated by their digital publishing system.</p>
<h2 id="bibliography">Bibliography</h2>
<ul>
<li id="beckettetal2014">Beckett, David, Tim Berners-Lee, Eric Prud'hommeaux and Gavin Carothers, _RDF 1.1 Turtle: Terse RDF Triple Language_ . W3C documentation website.<a href="https://www.w3.org/TR/turtle/">https://www.w3.org/TR/turtle/</a>
</li>
<li id="bernerslee1998">Berners-Lee, Tim, _Relational Databases on the Semantic Web_ .<a href="https://www.w3.org/DesignIssues/RDB-RDF.html">https://www.w3.org/DesignIssues/RDB-RDF.html</a>.
</li>
<li id="bernerslee2010">Berners-Lee, Tim (2010). _The year open data went worldwide_ . TED Talk.<a href="https://www.ted.com/talks/tim_berners_lee_the_year_open_data_went_worldwide">https://www.ted.com/talks/tim_berners_lee_the_year_open_data_went_worldwide</a>
</li>
<li id="bernersleeetal2001">Berners-Lee, Tim, James Hendler and Ora Lassila, “The Semantic Web: A new form of Web content” . In _Scientific American_ Vol 284 No 5 (May 2001), pp 35-43
</li>
<li id="bizer2008">Bizer, Christian, Tom Heath, Kingsley Idehen and Tim Berners-Lee. “Linked Data on the Web (LDOW2008)” . Workshop at _WWW 2008_ , April 2008, Beijing, China.
</li>
<li id="bizeretal2009">Bizer, Christian, Tom Heath and Tim Berners-Lee. “Linked Data: the Story So Far” . In _International Journal on Semantic Web and Information Systems_ . 5 (3): 1-22. doi:10.4018/jswis.2009081901. ISSN 1552-6283.<a href="http://tomheath.com/papers/bizer-heath-berners-lee-ijswis-linked-data.pdf">http://tomheath.com/papers/bizer-heath-berners-lee-ijswis-linked-data.pdf</a>.
</li>
<li id="bradley2017a">Bradley, John, _Factoids: A site that introduces Factoid Prosopography_ .<a href="http://factoid-dighum.kcl.ac.uk/">http://factoid-dighum.kcl.ac.uk/</a>.
</li>
<li id="bradley2017b">Bradley, John (2017b). _DPRR RDF: Documentation_ website.<a href="http://www.romanrepublic.ac.uk/rdf/doc">http://www.romanrepublic.ac.uk/rdf/doc</a>.
</li>
<li id="broughton1951">Broughton, T. Robert S, _The Magistrates of the Roman Republic_ . In series De Lacy, Phillip H. (ed) _Philological Monographs_ . Atlanta: Scholars Press edition.
</li>
<li id="d2rqnd"> _D2RQ: Accessing Relational Databases as Virtual RDF Graphs_ .<a href="http://d2rq.org/">http://d2rq.org/</a>.
</li>
<li id="fogelandengerman1974">Fogel, Robert William and Stanley L. Engerman, _Time on the Cross: The Economics of American Negro Slavery_ . Boston: Little, Brown and Company, 1974. xviii + 286 pp.
</li>
<li id="freitasetal2012">Freitas, Andre, Edward Currey, JG Oliveira and S O'Riain, “Querying Hetrogeneous Datasets on the Linked Data Web: Challenges, Approaches, and Trends” . In _IEEE Internet Computing_ , Vol 16, No 1. Jan-Feb 2012. pp. 24-33.
</li>
<li id="guetzkowetal2004">Guetzkow, Joshua, Michele Lamond and Gregoire Mallard, “What is Originality in the Humanities and the Social Sciences?” . In _American Sociological Review_ . Vol 69 No 2 (Apr. 2004). pp 190-212. Online access from JSTOR:<a href="http://www.jstor/org/stable/3593084">http://www.jstor/org/stable/3593084</a>.
</li>
<li id="hammondandjackson2017">Hammond, Matthew and Jackson, Cornell, _Social Network Analysis and the People of Medieval Scotland 1093-1286 (PoMS) Database_ .<a href="https://www.poms.ac.uk/information/e-books/social-network-analysis-and-the-people-of-medieval-scotland-1093-1286-poms-database/">https://www.poms.ac.uk/information/e-books/social-network-analysis-and-the-people-of-medieval-scotland-1093-1286-poms-database/</a>.
</li>
<li id="johansonetal2019">Johanson, Christopher, Marie Saldana and Benjamin Niedzielski, _RomeLab_ . DOI 10.17605/OSF.IO/VGKT4.<a href="https://osf.io/vgkt4/">https://osf.io/vgkt4/</a>and<a href="http://hvwc.etc.ucla.edu/">http://hvwc.etc.ucla.edu/</a>.
</li>
<li id="krug2014">Krug, Steve, _Don't Make Me Think, Revisited: A Common Sense Approach to Web Usability_ . Amazon (3rd ed.). New Riders. ASIN 0321965515.
</li>
<li id="mouritsenetal2017">Mouritsen, Henrik, Dominic Rathbone, Maggie Robb, John Bradley, _Digital Prosopography of the Roman Republic_ .<a href="http://www.romanrepublic.ac.uk/">http://www.romanrepublic.ac.uk/</a>.
</li>
<li id="noyetal2019">Noy, Natasha, Yuqing Gao, Anshu Jain, Anant Narayanan, Alan Patterson and Jamie Taylor, “Industry-scale Knowledge Graphs: Lessons and Challenges” . In _Communication of the ACM_ . Vol 62 No 8. pp. 36-43.
</li>
<li id="nurmikkofuller2016">Nurmikko-Fuller, Terhi, “Linked Data for Digital Humanities: Publishing, Querying and Linking on the Semantic Web” . Announced on the Oxford Summer School website.<a href="http://digital.humanities.ox.ac.uk/dhoxss/2016/workshops/LD4DH">http://digital.humanities.ox.ac.uk/dhoxss/2016/workshops/LD4DH</a>.
</li>
<li id="owl2012"> _OWL: Web Ontology Language_ . W3C documentation website.<a href="https://www.w3.org/OWL">https://www.w3.org/OWL</a>.
</li>
<li id="pauly1893">Pauly, August, Georg Wissowa, Wilhelm Kroll, Kurt Witte, Karl Mittelhaus, Konrat Ziegler, (eds), _Real-Encyclopaedie der classischen Altertumswissenschaft_ . Stuttgart: J. B. Metzler, 1893-1980.
</li>
<li id="pelagios"> _Pelagios Commons: Linking the places of our past_ .<a href="http://commons.pelagios.org/">http://commons.pelagios.org/</a>.
</li>
<li id="poms2014"> _People of Medieval Scotland_ . Most recently updated in 2019.<a href="http://www.poms.ac.uk">http://www.poms.ac.uk</a>.
</li>
<li id="pase2016"> _Prosopography of Anglo-Saxon England_ . Most recently updated with results of work in the _Profile of the Doomed Elite Project_ (Stephen Baxter).<a href="http://www.pase.ac.uk">http://www.pase.ac.uk</a>.
</li>
<li id="rdf2014"> _RDF: Resource Description Framework_ . W3C documentation website.<a href="https://www.w3.org/RDF/">https://www.w3.org/RDF/</a>.
</li>
<li id="rdf4j2017"> _@rdf4j_ website.<a href="http://rdf4j.org/">http://rdf4j.org/</a>.
</li>
<li id="seaborneetal2013">Seaborne, Andy, Axel Polleres, Lee Feigenbaum and Gregory Todd Williamms, _SPARQL 1.1 Federated Query_ . A W3C Recommendation at<a href="https://www.w3.org/TR/sparql11-federated-query/">https://www.w3.org/TR/sparql11-federated-query/</a>.
</li>
<li id="smith2017">Smith, James, Workshop description for _RDF and Linked Open Data_ .<a href="http://dhsi.org/courses.php">http://dhsi.org/courses.php</a>.
</li>
<li id="smithiesetal2019">Smithies, James, Carina Westling, Anna-Maria Sichani, Pam Mellen and Arianna Ciula, “Managing 100 Digital Humanities Projects: Digital Scholarship & Archiving in Kings Digital Lab” . In _Digital Humanities Quarterly_ , Vol. 13 No. 1.<a href="http://www.digitalhumanities.org/dhq/vol/13/1/000411/000411.html">http://www.digitalhumanities.org/dhq/vol/13/1/000411/000411.html</a>.
</li>
<li id="snapdrgn"> _Standards for Networking Ancient Prosopographies_ .<a href="https://snapdrgn.net/">https://snapdrgn.net/</a>.
</li>
<li id="sparql2013"> _SPARQL 1.1 Overview_ . W3C website at<a href="https://www.w3.org/TR/sparql11-overview/">https://www.w3.org/TR/sparql11-overview/</a>.
</li>
<li id="speicheretal2015">Speicher, Steve, John Arwe and Ashok Malhotra, _Linked Data Platform 1.0: W3C Recommendation 26 February 2015_ .<a href="https://www.w3.org/TR/2015/REC-ldp-20150226/">https://www.w3.org/TR/2015/REC-ldp-20150226/</a>.
</li>
<li id="sql2018"> _SQL_ Wikipedia article.<a href="https://en.wikipedia.org/wiki/SQL">https://en.wikipedia.org/wiki/SQL</a>.
</li>
<li id="viaf2010"> _VIAF: The Virtual International Authority File._ <a href="https://viaf.org/">https://viaf.org/</a>.
</li>
<li id="wiess2001">Wiess, Thomas, “Review of Time on the Cross: The Economics of American Negro Slavery” . In _Economic History Association website EH.net_ .<a href="https://eh.net/book_reviews/time-on-the-cross-the-economics-of-american-negro-slavery/">https://eh.net/book_reviews/time-on-the-cross-the-economics-of-american-negro-slavery/</a>.
</li>
<li id="worldcat"> _WorldCat: The World's The World's Largest Library Catalog_ .<a href="http://www.worldcat.org/">http://www.worldcat.org/</a>.
</li>
<li id="xquery2018"> _XQuery_ . Wikipedia article.<a href="https://en.wikipedia.org/wiki/XQuery">https://en.wikipedia.org/wiki/XQuery</a>.
</li>
</ul>
]]></content></entry><entry><title type="html">Apresentação - Edição especial da DHQ em português</title><link href="https://rlskoeser.github.io/dhqwords/vol/14/2/000460/?utm_source=atom_feed" rel="alternate" type="text/html"/><link href="https://rlskoeser.github.io/dhqwords/pt/vol/14/2/000460/?utm_source=atom_feed" rel="alternate" type="text/html" hreflang="pt"/><id>https://rlskoeser.github.io/dhqwords/vol/14/2/000460/</id><author><name>Luis Ferla</name></author><author><name>Cecily Raynor</name></author><published>2020-06-19T00:00:00+00:00</published><updated>2023-08-06T15:53:44-04:00</updated><content type="html"><![CDATA[<h1 id="introduction--digital-humanities-quarterly-special-edition-in-portuguese">Introduction- Digital Humanities Quarterly, Special Edition in Portuguese</h1>
<p>Following the Spanish and French editions, we present the special edition of Digital Humanities Quarterly in Portuguese. This initiative forms part of a broader effort by the Alliance of Digital Humanities Organizations (ADHO), the entity which publishes DHQ, to bring greater diversity and multilingualism to the debates surrounding Digital Humanities, typically recognized to be centered in the Anglo-Saxon world (see<a href="#gil2016">Gil, 2016</a>, and<a href="#odonnell2016">O’Donnell et. al., 2016</a>). Within this organization, actions taken by Global Outlook::Digital Humanities (GO::DH) are ostensibly aimed at “(…) helping to break down barriers to communication and collaboration between researchers and students in the areas of Digital Arts, Humanities and Cultural Heritage in high, middle and low income economies.” Thus the “perspectives of the Global South are vital to shaping the future of digital humanities.” The premise is that the digital humanities can only fully assume its desired identity, namely that of the valuation of knowledge sharing and the freedom to produce and circulate that knowledge, if they effectively question the current geopolitics of the academic and scientific world which dictate the practices of the communities they encompass.</p>
<p>However, the breakdown of cultural hierarchies is more complex than simply the supposed recognition of good intentions or beautiful ideas. Technological infrastructures, for example, are not easily and readily transformed to better conform to notions and mindsets that are more egalitarian and democratic. The tensions resulting from these incompatibilities can be productive if they contribute to the transformation of said infrastructures in emancipatory directions or can be negative if they limit the viability of progress. NASA’s recent postponement of the first all-women spacewalk in history due to the lack of female astronautic attire is both an anecdotal and metaphorical example of this phenomenon.<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> The explanation of the incident exposes the gap between egalitarian intention and technological infrastructure: “Spacesuit design has long been biased toward men’s physiques, both due to technological constraints and the fact that NASA preferred male astronauts throughout most of its lifetime.” In the preparation of this special edition, setbacks have emerged that have illustrated the same substantive issue. In December of 2018, the Open Journal System (OJS), a valuable initiative in itself given that it makes possible editing and circulating open access journals, upgraded its platform to a supposedly more modern and functional version. However, since the migration the platform no longer recognizes thespecial charactersof the Portuguese language, such as those present inglobalização(globalization) andcontradições(contradictions). Thus, the articles, opinions and correspondences of our team and authors became functionally illegible. Many were affected by this issue, and a topic about this was opened in the discussion forum of OJS/PKP (<a href="https://forum.pkp.sfu.ca/search?q=special%20characters">https://forum.pkp.sfu.ca/search?q=special%20characters</a>, accessed on the 8th of December 2019). However, the problem of coding texts has not been solved thus far, and the difficulty of reading languages that make use ofspecial characterssuch as Portuguese persists. In the digital humanities environment, this topic is not new and has been tackled for quite some time. As Fiormonte et al (2015) affirm, “(…) apparently neutral , technical decisions, as can be observed in Unicode, TEI or other organisations, tend to oversimplify and standardise the complex diversity of languages and cultural artefacts.”</p>
<p>Taking this into consideration and after many unsuccessful attempts, we decided to continue with the editing workflow in a parallel system, resorting to email exchanges and storing files in digital repositories external to the OJS platform. In the end, the Portuguese edition reached its programmed destination, albeit late, but with all of the accents and cedillas intact.</p>
<p>The texts of this special edition can be separated into two groups: three articles rooted in the discussion of research projects identified with digital humanities that then develop more general reflections; and three others that analyze what surrounds and conditions the research and work of the digital humanist, addressing both theoretical and methodological issues, as well as thematic strongpoints and institutional trends. In the first group, it is noteworthy that the three studies use geotechnology as their primary instrumental support, thus affirming the general perception of an increasing appreciation of the spatial dimension in humanities research<a class="footnote-ref" href="#bodenhamer2010"> [bodenhamer2010] </a>. Two of these projects deal with history: Patricia Ferreira Lopes utilizes Geographic Information System (GIS) to study road networks in the 16th century Iberian Peninsula, and Maria João Ferreira dos Santos employs this same technology to reconstruct the history of natural conservation efforts in California, covering the period between 1850 and 2010. Although they stem from the fields of architecture and biology respectively, Patrícia Lopes and Maria dos Santos end up corroborating the impression that past researchers preferred GIS technologies<a class="footnote-ref" href="#gregory2007"> [gregory2007] </a>. Sarita Albagli, Hesley Py and Allan Yu Iwama compose the third team of researchers whose article is dedicated to geotechnology, in this case addressing an experience of social intervention on geographical territory. The project in question discusses the development of a “platform prototype for open access geospatial data [LindaGeo Platform] as part of an open science action-based research project conducted by the Municipality of Ubatuba on the Northern Coast of the State of São Paulo, Brazil.” Here we also reaffirm some of the main identities that are typically associated with the digital humanities, focused on the ethics of collaborative production and the free circulation of knowledge<a class="footnote-ref" href="#greenspan2016"> [greenspan2016] </a><a class="footnote-ref" href="#spiro2012"> [spiro2012] </a>.</p>
<p>In the second block of articles, Luís Corujo, Jorge Revez and Carlos Guardado da Silva explore a topic whose importance has yet to be sufficiently recognized: digital curation and its costs. The authors advocate not only for greater diligence in data management planning, but also the dissemination of more transparent and reproducible practices in order to increase access to the results of scientific production. In the second article, Claudio José Silva Ribeiro, Suemi Higuchi and Luis Ferla attempt to give a panoramic examination of digital humanities in Brazil, paying particular homage to the experience of the First International Congress on Digital Humanities in Rio de Janeiro in April of 2018. To close the special edition, Maria Paixão de Sousa provides a reflection on the radical resignification of writing and reading in the technologized environment of the present day, which, in her perspective, “profoundly alters the traditional work of the humanities” and establishes “a new discursive conformation for the field.”</p>
<p>In closing, the editors would like to thank everyone who has made this special edition of DHQ possible, starting with the authors themselves as well as the invited reviewers, whose intellectual capacity, scientific knowledge and stubborn patience were indispensable to this issue. To Alex Gil, for the initial invitation and his support since then. And to Rhian Lewis and Kate Bundy, consecutive assistants throughout the process, both of whom were tireless in their efforts and who met hardship with great fortitude and ease. They are the truespecial charactersof this edition.</p>
<ul>
<li id="bodenhamer2010">BODENHAMER, David J; CORRIGAN, John; HARRIS, Trevor M. (Ed.). _The spatial humanities:_ GIS and the future of humanities scholarship. Bloomington: Indiana University Press, 2010.
</li>
<li id="fiormonte2015">FIORMONTE, Domenico; SCHMIDT, Desmond; SCHMIDT , Paolo; SORDI, Paolo. “The Politics of code. How digital representations and languages shape culture” . _Proceedings of ISIS Summit Vienna 2015 — The Information Society at the Crossroads_ , 2015.
</li>
<li id="gil2016">GIL, Alex. “Interview with Ernesto Oroza; e Fiormonte, Domenico. Toward a Cultural Critique of Digital Humanities” . In: GOLD, M.; KLEIN, L. (eds). _Debates in the Digital Humanities._ Minneapolis: University of Minnesota Press, 2016.
</li>
<li id="greenspan2016">GREENSPAN, Brian. “Are Digital Humanists Utopian?” In: GOLD, M.; KLEIN, L. (eds). _Debates in the Digital Humanities._ Minneapolis: University of Minnesota Press. p. 393-409. 2016.
</li>
<li id="gregory2007">GREGORY, Ian; ELL, Paul. _Historical GIS: Technologies, methodologies and scholarship_ . Cambridge: Cambridge University Press, 2007.
</li>
<li id="odonnell2016">O’DONNELL, D. P.; et al. “Only Connect: The Globalization of the Digital Humanities” . In: SCHREIBMAN, S.; SIEMENS, R.; UNSWORTH, J. (eds). _A new companion to Digital Humanities_ . Malden: Blackwell, 2016.
</li>
<li id="spiro2012">SPIRO, Lisa. “‘This is why we fight’: defining the values of the digital humanities” . In: Gold, Matthew K. (editor). _Debates in the Digital Humanities._ Minneapolis: University of Minnesota Press. p. 16-35. 2012.
</li>
</ul>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>The walk, originally scheduled for March, ended up happening in October 2019. (<a href="https://www.nytimes.com/2019/10/05/science/NASA-female-spacewalk.html">https://www.nytimes.com/2019/10/05/science/NASA-female-spacewalk.html</a>, accessed on the 6th of December 2019).## Bibliography&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry><entry><title type="html">Aproximações ao cenário das humanidades digitais no Brasil</title><link href="https://rlskoeser.github.io/dhqwords/vol/14/2/000453/?utm_source=atom_feed" rel="alternate" type="text/html"/><link href="https://rlskoeser.github.io/dhqwords/pt/vol/14/2/000453/?utm_source=atom_feed" rel="alternate" type="text/html" hreflang="pt"/><id>https://rlskoeser.github.io/dhqwords/vol/14/2/000453/</id><author><name>Cláudio José Silva Ribeiro</name></author><author><name>Suemi Higuchi</name></author><author><name>Luis Antonio Coelho Ferla</name></author><published>2020-06-19T00:00:00+00:00</published><updated>2023-08-06T15:53:44-04:00</updated><content type="html"><![CDATA[<h2 id="note-on-translation">Note on Translation</h2>
<p>For articles in languages other than English, DHQ provides an English-language abstract to support searching and discovery, and to enable those not fluent in the article&rsquo;s original language to get a basic understanding of its contents. In many cases, machine translation may be helpful for those seeking more detailed access. While DHQ does not typically have the resources to translate articles in full, we welcome contributions of effort from readers. If you are interested in translating any article into another language, please contact us at <a href="mailto:editors@digitalhumanities.org">editors@digitalhumanities.org</a> and we will be happy to work with you.</p>
<h2 id="bibliography">Bibliography</h2>
<ul>
<li id="aboukhalil2014">ABOUKHALIL, Robert. “The rising trend in authorship” , _The Winnower 2:e141832.26907_ , 2014.
</li>
<li id="albagli2015">ALBAGLI, Sarita; MACIEL, Maria Lucia; ABDO, Alexandre Hannud (orgs). _Ciência aberta, questões abertas_ . Brasília: IBICT; Rio de Janeiro: Unirio, 2015.
</li>
<li id="alves2016">ALVES, Daniel. “As Humanidades Digitais como uma comunidade de práticas dentro do formalismo académico: dos exemplos internacionais ao caso português” . _Ler História_ [Online], 69 | 2016. Disponível em<a href="http://journals.openedition.org/lerhistoria/2496">http://journals.openedition.org/lerhistoria/2496</a>. Acesso em 21 de fevereiro de 2018.
</li>
<li id="beiguelman2014">BEIGUELMAN, Giselle; MAGALHÃES, Ana Gonçalves. _Futuros Possíveis: arte, museus e arquivos digitais_ . Ed.USP/Editora Petrópolis – São Paulo. 2014.
</li>
<li id="berners-lee1999">BERNERS-LEE, Tim. _Weaving the Web: The Original Design and Ultimate Destiny of the World Wide Web by Its Inventor_ . San Francisco: Harper, 1999.
</li>
<li id="burdick2012">BURDICK, A, Drucker, J, Lunenfeld, P, Presner, T, Schnapp, J. _A short guide to the Digital Humanities_ . MIT Press, 2012.
</li>
<li id="carlotto2011">CARLOTTO, Maria Caramez; ORTELLADO, Pablo. “Activist-driven innovation: uma história interpretativa do software livre” . _Revista Brasileira de Ciências Sociais_ , v. 26, n. 76, 2011.
</li>
<li id="cooper2011">COOPER, D; GREGORY, Ian. “Mapping the English Lake District: a literary GIS” . _Transactions of the Institute of British Geographers_ , v. 36, n. 1, 2011.<a href="https://doi.org/10.1111/j.1475-5661.2010.00405.x">https://doi.org/10.1111/j.1475-5661.2010.00405.x</a>.
</li>
<li id="desouza2011">DE SOUZA, Maria Clara Paixão. _Humanidades Digitais: um breve panorama_ . Site HumanidadesDigitais.org, setembro de 2011. Disponível em<a href="https://humanidadesdigitais.org/breve-panorama/">https://humanidadesdigitais.org/breve-panorama/</a>. Acesso em 19 de abril de 2018.
</li>
<li id="dhmanifesto2010"> _Manifeste des Digital Humanities_ . 2010. Disponível em<a href="http://tcp.hypotheses.org/443">http://tcp.hypotheses.org/443</a>. Acesso em 19 de abril de 2018.
</li>
<li id="dobson2015">DOBSON, James E. “Can an Algorithm be Disturbed? Machine Learning, Intrinsic Criticism, and the Digital Humanities” . In _College Literature_ . 42 (4): 543–564, 2015.
</li>
<li id="frank2016">FRANK, Zephyr. _Reading Rio de Janeiro: Literature and Society in the Nineteenth Century_ . Palo Alto: Stanford University Press, 2016.
</li>
<li id="gold2016">GOLD, M. K.; KLEIN, L. “Digital Humanities: The Expanded Field” . In GOLD, M. K.; KLEIN, L. (orgs). _Debates in the digital humanities 2016_ . University of Minnesota Press, 2016.
</li>
<li id="gold2019">GOLD, M. K.; KLEIN, L. “A DH That Matters” . In GOLD, M. K.; KLEIN, L. (orgs). _Debates in the digital humanities 2019_ . University of Minnesota Press, 2019.
</li>
<li id="kirsch2014">KIRSCH, Adam. “Technology Is Taking Over English Departments” . _The New Republic_ , 2014. Disponível em<a href="https://newrepublic.com/article/117428/limits-digital-humanities-adam-kirsch">https://newrepublic.com/article/117428/limits-digital-humanities-adam-kirsch</a>. Acesso em 7 de julho de 2017.
</li>
<li id="kirschenbaum2012">KIRSCHENBAUM, Matthew. “What is Digital Humanities and what's it doing in English Departments?” In: GOLD, Matthew K. (editor). _Debates in the Digital Humanities_ , Minneapolis: University of Minnesota Press, 2012.
</li>
<li id="marques2017">MARQUES, Fabricio. “A realidade que emerge da avalanche de dados” . In: _Revista Pesquisa Fapesp 255_ , maio de 2017. Disponível em:<a href="http://revistapesquisa.fapesp.br/2017/05/23/a-realidade-que-emerge-da-avalanche-de-dados">http://revistapesquisa.fapesp.br/2017/05/23/a-realidade-que-emerge-da-avalanche-de-dados</a>. Acesso em 3 de junho de 2017.
</li>
<li id="nyhan2014">NYHAN, Julianne; DUKE-WILLIAMS, Oliver. “Joint and multi-authored publication patterns in the Digital Humanities” . _Literary and Linguistic Computing_ , Volume 29, Issue 3, 1 September 2014, Pages 387–399,
</li>
<li id="priani2014">PRIANI, Ernesto; SPENCE, Paul; GALINA, Isabel; GONZÁLEZ-BLANCO, Elena; ALVES, Daniel; BARRÓN, José Francisco; GODÍNEZ, Marco Antonio; SOUZA, Maria Clara Paixão de. “Las humanidades digitales en español y portugués. Un estudio de caso: DíaHD/DiaHD” . _Literary and Linguistic Computing_ , Volume 29, Issue 3, 1 September 2014.
</li>
<li id="ramsey2012">RAMSEY, Stephen; ROCKWELL, Geoffrey. “Developing things: notes toward an epistemology on building in the digital humanities” . In: GOLD, Matthew K. (editor). _Debates in the Digital Humanities_ , Minneapolis: University of Minnesota Press, 2012.
</li>
<li id="svensson2012">SVENSSON, Patrik. “Beyond the big tent” . In: GOLD, Matthew K. (editor). _Debates in the Digital Humanities_ , Minneapolis: University of Minnesota Press, 2012.
</li>
</ul>
]]></content></entry><entry><title type="html">Avanços no estudo das redes de itinerários da Península Ibérica no século XVI. Aplicando os SIGH para estudar a história da arquitetura</title><link href="https://rlskoeser.github.io/dhqwords/vol/14/2/000458/?utm_source=atom_feed" rel="alternate" type="text/html"/><link href="https://rlskoeser.github.io/dhqwords/pt/vol/14/2/000458/?utm_source=atom_feed" rel="alternate" type="text/html" hreflang="pt"/><id>https://rlskoeser.github.io/dhqwords/vol/14/2/000458/</id><author><name>Patricia Ferreira-Lopes</name></author><published>2020-06-19T00:00:00+00:00</published><updated>2023-08-06T15:53:44-04:00</updated><content type="html"><![CDATA[<h2 id="note-on-translation">Note on Translation</h2>
<p>For articles in languages other than English, DHQ provides an English-language abstract to support searching and discovery, and to enable those not fluent in the article&rsquo;s original language to get a basic understanding of its contents. In many cases, machine translation may be helpful for those seeking more detailed access. While DHQ does not typically have the resources to translate articles in full, we welcome contributions of effort from readers. If you are interested in translating any article into another language, please contact us at <a href="mailto:editors@digitalhumanities.org">editors@digitalhumanities.org</a> and we will be happy to work with you.</p>
<h2 id="bibliography">Bibliography</h2>
<ul>
<li id="abrate2013">Abrate, M., Bacciu, C., Hast, A., Marchetti, A., Minutoli, S. e Tesconi, M. “GeoMemories-A Platform for Visualizing Historical, Environmental and Geospatial Changes in the Italian Landscape” . _ISPRS International Journal of Geo-Information,_ 2, (2013): 432-455.
</li>
<li id="alcázarmolina1953">Alcázar Molina, C., “Las comunicaciones en la época de los Reyes Católicos” . _Curso de conferencias sobre la política africana de los Reyes Católicos_ , V, (1953): 55-70.
</li>
<li id="alonsoruiz2003">Alonso Ruiz, B. _Arquitectura tardogótica en Castilla: los Rasines_ . Santander: Universidad de Cantabria (2003).
</li>
<li id="alonsoruiz2010">Alonso Ruiz, B. eds. _Los últimos arquitectos del gótico_ . Madrid: Ministerio de Ciencia e Innovación (2010).
</li>
<li id="alonsoruiz2009">Alonso Ruiz, B. e Jiménez Martín, A. _La traça de la Iglesia de Sevilla_ . Sevilla: Cabildo Metropolitano (2009).
</li>
<li id="azkarate2009">Azkarate, A., Barreiro, D., Criado, F., García Camino, I., Gutiérrez Lloret, S., Quirós, J.A. e Salvatierra, V., “La Arqueología hoy” . Em Ortiz de Landaluze, A. L., eds. _Medio siglo de arqueología en el cantábrico oriental y su entorno_ . Vitoria-Gasteiz Espanha (2009): 599-615.
</li>
<li id="bermudéz2004">Bermúdez Sánchez, J., “Creación de Rutinas o Macros con el Programa IDRISI: el Cálculo Acumulado de Visibilidades y Rutas Óptimas” . Em Martín de La Cruz, J. C. e Lucena Martín, A. Mª., eds. _Actas del I Encuentro Internacional. Informática aplicada a la investigación y la gestión arqueológicas, Córdoba, Espanha_ (2004): 407-418.
</li>
<li id="box1999">Box, P. _GIS and Cultural Resource Management: A manual for Heritage Managers, ed. Suki Dixon_ . Bangkok: UNESCO (1999).
</li>
<li id="cassatella2013">Cassatella C. e Carlone G. “GIS-based visual analysis for planning and designing historic urban landscapes: The case of Turin” . _Digital Heritage International Congress (DigitalHeritage),_ 1, (2013): 45-52.
</li>
<li id="castells1995">Castells, M. _La ciudad Informacional: tecnologías de la información, reestructuración económica y el proceso urbano-regional_ . Madrid: Alianza Editorial (1995).
</li>
<li id="chaunu1983">Chaunu, P. _Sevilla y América siglos XVI y XVII_ . Utrera: Secretariado de publicaciones de la Universidad de Sevilla (1983).
</li>
<li id="cresposolana2014">Crespo Solana eds. _Spatio-Temporal Narratives. Historical GIS and the Study of Global Tranding Networks (1500-1800)_ . Newcastle: Cambridge Scholars Publishing (2014).
</li>
<li id="demontis2012">De Montis, A. e Caschili, S. “Nuraghes and landscape planning: Coupling viewshed with complex network analysis” . _Landscape and Urban Planning_ , 105(3), (2012): 315-324.
</li>
<li id="delbosquegonzález2012">Del Bosque González, I., Fernández Freire, C., Martín-Forero Morente, L., e Pérez Asensio, E. _Los sistemas de información geográfica y la investigación en ciencias humanas y sociales_ . Madrid: Confederación Española de Centros de Estudios Locales (2012).
</li>
<li id="fairén2004">Fairén Jiménez, S. “¿Se hace camino al andar? Influencia de las variables medioambientales y culturales en el cálculo de caminos óptimos mediante SIG” . _Trabajos de Prehistoria_ 61 (2), (2004): 25-40.
</li>
<li id="ferreiralopes2018a">Ferreira Lopes, P. e Molina Rozalem, J.F. “Historical SDI, thematic maps and analysis of a complex network of medieval towers (13th-15th century) in the Moorish Strip” . _Int. Arch. Photogramm. Remote Sens. Spatial Inf. Sci._ , XLII-4, (2018): 177-183.
</li>
<li id="ferreiralopes2018b">Ferreira Lopes, P. e Pinto Puerto, F. “GIS and Graph models for social, temporal and spatial digital analysis in heritage: The case-study of Ancient Kingdom of Seville Late Gothic Production” . _Digital Application in Archaeology and Cultural Heritage_ , 9, (2018): 1-14.
</li>
<li id="ferreiralopes2016">Ferreira Lopes, P., Pinto Puerto, F., Jimenez Mavillard, A. e Suarez, J. “Seeing Andalucia’s Late Gothic heritage through GIS and Graphs” . Em Digital Humanities 2016: Conference Abstracts. Kraków: Jagiellonian University & Pedagogical University, (2016): 501-504.
</li>
<li id="ferreiralopes2018c">Ferreira Lopes, P. “La transformación del proceso de investigación en Historia de la Arquitectura con el uso de las tecnologías digitales” . _Artnodes_ , 22 (2018): 61-72. 
</li>
<li id="fisher1997">Fisher, P. F., Farrelly, C., Maddocks, A. e Ruggles, C. L. N. “Spatial analysis of visible areas from the Bronze Age cairns of Mull” . _Journal of Archaeological Science,_ 24 (1997): 581–592.
</li>
<li id="foresman1997">Foresman, T. W., eds., _The History of Geographic Information Systems: Perspectives from Pioneers_ . Prentice Hal (1997).
</li>
<li id="freire2014">Freire, P. e Shor, I. _Miedo y osadía la cotidianidad del docente que se arriesga a practicar una pedagogía transformadora_ . Buenos Aires: Siglo XXI Editores Argentina (2014).
</li>
<li id="gaffney1995">Gaffney, V. e van Leusen, M. “Postscript-GIS, environmental determinism and archaeology: a parallel text” . Em Lock, G. e Stancic, Z., eds. _Archaeology and geographical information systems: a European perspective_ . London: Taylor and Francis (1995): 367-382.
</li>
<li id="gestoso1899">Gestoso e Pérez, J. _Ensayo de un diccionario de los artífices que florecieron en esta ciudad de Sevilla desde el siglo XIII hasta el XVIII. T. I._ Sevilha (1899).
</li>
<li id="gregoryell2007">Gregory, I. N. e Ell, Paul S. _Historical GIS technologies, Methodologies, and Scholarship_ . Cambridge: Cambridge University Press (2007).
</li>
<li id="gregoryhealey2007">Gregory, I. N. e Healey, R. G. “Historical GIS: structuring, mapping and analysing geographies of the past” . _Progress in Human Geography 31 (5)_ , (2007): 638-653.
</li>
<li id="gregory2005">Gregory, I. N. “The Great Britain Historical GIS” . _Historical Geography 33_ , (2005):132-134.
</li>
<li id="gregory2001">Gregory, I. N., Kemp, K. K., e Mostern, R. “Geographical Information and historical research: current progress and future directions” . _History and Computing 13_ (1), (2001): 7-23.
</li>
<li id="hasensatb1983">Hasensatb, R.J. _A preliminar cultural resource sensitivity analysis for flood control facilities construction in the Paissaic River basin of New Jersey_ . Marietta: US Army Corps of Engineers (1983).
</li>
<li id="hernando2009">Hernando, M.D. e Ladero Quesada, M.A. “Caminos y ciudades en España de la Edad Media al siglo XVIII” . _La España Medieval_ , 32. (2009): 347-382.
</li>
<li id="herzog2013">Herzog, I. “Theory and practice of cost functions” . Em F. Contreras Cortés, M. Farjas e F.J. Melero, eds. _CAA2010: fusion of cultures: Proceedings of the 38th Annual Conference on Computer Applications and Quantitative Methods in Archaeology, Granada, Espanha, Abril 2010_ . Oxford: Archaeopress (2013).
</li>
<li id="judge1988">Judge, J.W. e Sebastian, L. eds. _Quantifying the present and predicting the past: theory, method and application of archaeological predictive modelling_ . Denver: U.S. Department of Interior, Bureau of Land Management (1988).
</li>
<li id="knowles2008">Knowles, A. K. e Hillier, A. eds., _Placing History: how maps, spatial data, and GIS are changing historical schorlarship_ . Redlands: ESRI Press (2008).
</li>
<li id="knowles2002">Knowles, A. K. eds., _Past Time, Past Place: GIS for History._ Redlands: ESRI Press (2002).
</li>
<li id="kvamme1983">Kvamme, K.L. _A manual for predictive site location models: examples from the Grand Junction District Colorado_ . Bureau of Land Management: Grand Junction District (1983).
</li>
<li id="kvamme1990">Kvamme, K. L. “GIS algorithms and their effects on regional archaeological analyses” . Em Allen, K. M. S., Green, S.W. y Zubrow, E. B.W., eds., _Interpreting Space: GIS and Archaeology_ , London: Taylor & Francis (1990): 112–125.
</li>
<li id="lyster2016">Lyster, C. _Learning from logistics. How Networks Change Our Cities_ . Basel: Birkhauser (2016).
</li>
<li id="mccormick2013">McCormick et al. _Roman Road Networ_ k. Harvard University (2013).<a href="https://darmc.harvard.edu/data-availability">https://darmc.harvard.edu/data-availability</a>.
</li>
<li id="murrieta-flores2017">Murrieta-Flores, P., Donaldson, C. E., e Gregory, I. N.   “GIS and literary history: advancing digital humanities research through the spatial analysis of historical travel writing and topographical literature” . Digital Humanities Quarterly, 11(1), (2017).
</li>
<li id="murrieta-flores2012">Murrieta-Flores, P.  “Understanding human movement through spatial technologies. The role of natural areas of transit in the Late Prehistory of South-western Iberia” .  _Trabajos de Prehistoria_ , 69(1), (2012)103-122. DOI: 10.3989/tp.2012.12082
</li>
<li id="pidal1951">Pidal, G. M. _Repertorio de todos los caminos de España (hasta agora nunca visto)_ . Real Academia de História, Coleção Departamento de Cartografia e Artes Gráficas, assinatura: C-030-030, R: 01101 (1951).
</li>
<li id="rodríguezestévez1996">Rodríguez Estévez, J.C. “Los Canteros de la Obra Gótica de la Catedral de Sevilla (1433-1528)” .  _Laboratorio de Arte. Revista del Departamento de História del Arte_ , 9 (1), (1996): 49-71.
</li>
<li id="rumeudearmas1974">Rumeu de Armas, A. _Itinerario de los Reyes Católicos, 1474-1516_ . Madrid: Consejo Superior de Investigaciones Científicas (1974).
</li>
<li id="rumsey2012">Rumsey, D. y Williams, M. “Historical Maps in GIS” . Em Knowles, A. K. eds. _Past Time, Past Place: GIS for History_ , Redlands, ESRI Press (2012): 1-18.
</li>
<li id="salvador2011">Salvador I. e Vitti A. “Survey, representation and analysis of a world war I complex system of surface and underground fortifications in the Gresta Valley – Italy” . _International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives,_ 38(5W16), (2011): 319-325.
</li>
<li id="schlögel2007">Schlögel, K. e Arántegui, J. L. _En el espacio leemos el tiempo: sobre historia de la civilización y Geopolítica_ . Madrid: Siruela (2007).
</li>
<li id="serradesfilis2011">Serra Desfilis, A. “La arquitectura del tardogótico en la Corona de Aragón Intercambios y trayectorias” . Em Alonso Ruiz, B. eds. _La arquitectura tardogótica castellana entre Europa y América_ . Madrid: Silex, (2011): 459-490.
</li>
<li id="serradesfilis2016">Serra Desfilis, A. “La logia abierta transferencias y movilidad en la arquitectura tardogótica hispánica” . Em Alonso Ruiz, B, Rodríguez Estévez, J.C. eds., _1514 Arquitectos tardogóticos en la encrucijada_ . Sevilla: Universidad de Sevilla, (2016) 339-352.
</li>
<li id="silva1989">Silva, J.C.V. _O tardo-gótico em Portugal: a arquitectura no Alentejo_ . Lisboa: Livros Horizonte (1989).
</li>
<li id="soja2010">Soja, E. _Postmodern geographies. The reassertion of space in critical social theory_ . London: Verso (2010).
</li>
<li id="terpstra2016">Terpstra N. e Rose, C. eds. _Mapping Space, Sense, and Movement in Florence: Historical GIS and the early modern city_ . Abingdon: Routledge (2016).
</li>
<li id="uriolsalcedo1985">Uriol Salcedo, José I. “Las calzadas romanas y los caminos del siglo XVI”  _Revista de Obras Públicas_ , 3237 (1985): 553-563.
</li>
<li id="uriolsalcedo1990">Uriol Salcedo, José I. _Historia de los caminos de España I. Hasta el siglo XIX_ . Madrid: Colegio de Ingenieros de Caminos, Canales y Puertos (1990).
</li>
<li id="vanleusen1993">Van Leusen, P. M. “Cartographic modelling in a cell-based GIS” . Em Andresen, J., Madsen, T. e Scollar, I. eds. _Computing the Past: Computer Applications and Quantitative Methods in Archaeology_ , Aarhus: Aarhus University Press (1993): 105–123.
</li>
<li id="verhagen2018">Verhagen, P. “Spatial Analysis in Archaeology: moving into New Territories” . Em Siart C., Forbriger M. e Bubenzer O. eds. _Digital Archaeology_ , Nature Science in Archaeology. Cham: Springer (2018).
</li>
<li id="verhagen2013">Verhagen, P., Nuninger, L., Tourneux, F.P., Bertoncello, F., Jeneson, K. “Introducing the human factor in predictive modelling: a work in progress” . Em Earl, G., Sly, T., Chrysanthi, A., Murrieta-Flores, P., Papadopoulos, C., Romanowka, I., Wheatley, D. eds. _Archaeology in the digital era. Papers from the 40th annual conference of computer applications and quantitative methods in archaeology (CAA), Southampton, 26–29 March 2012._ Amsterdam: Amsterdam University Press (2013): 379–388.
</li>
<li id="verhagen2016">Verhagen, P., Nuninger, L., Bertoncello, F., Castrorao Barba, A. “Estimating the “memory of landscape” to predict changes in archaeological settlement patterns” . Em Campana, S., Scopigno, R., Carpentiero, G., Cirillo, M. eds. _CAA 2015. Keep the revolution going. Proceedings of the 43rd annual conference on computer applications and quantitative methods in Archaeology, Siena_ . Oxford: Archaeopress (2016): 623–636
</li>
<li id="villuga1546">Villuga, J. _Repertorio de todos los caminos de España en el año de gracia de 1543_ . Barcelona: Institut Cartografic i Geològic de Catalunya, R: RL 3419 (1543).
</li>
<li id="villuga1902">Villuga, J.  _Reportorio de todos los caminos de España; hasta agora nunca visto enel ql allarã qlquier viaje q quierã andar muy puechosopa todos los caminantes_ . New York: De Vinne Press (1902).<a href="https://archive.org/details/reportoriodetodo00vill">https://archive.org/details/reportoriodetodo00vill</a>.
</li>
<li id="villuga1950">Villuga, J. _Repertorio de todos los caminos de España_ . Madrid: Reimpresiones Bibliográficas (1950).
</li>
<li id="vonlünen2012">von Lünen, A. e Travis, C. _History and GIS: epistemologies, considerations and reflections_ . Cham: Springer Science & Business Media (2012).
</li>
<li id="wheatley2004">Wheatley, D. “Making space for an archaeology of place” . _Internet Archaeology_ 15.
</li>
<li id="whitley2010">Whitley, T.G., Moore, G., Goel, G., Jackson, D. “Beyond the marsh: settlement choice, perception and spatial decision-making on the Georgia coastal plain” . Em Fisher, B., Crawford J., Kollers, D. eds. _CAA 2009. Making history interactive. Computer applications and quantitative methods in archaeology. Proceedings of the 37th conference, Williamsburg, VA, US_ . Oxford: Archaeopress (2010): 380-390.
</li>
</ul>
]]></content></entry><entry><title type="html">Calamari − A High-Performance Tensorflow-based Deep Learning Package for Optical Character Recognition</title><link href="https://rlskoeser.github.io/dhqwords/vol/14/2/000451/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://rlskoeser.github.io/dhqwords/vol/14/2/000451/</id><author><name>Christoph Wick</name></author><author><name>Christian Reul</name></author><author><name>Frank Puppe</name></author><published>2020-06-19T00:00:00+00:00</published><updated>2023-08-06T15:53:44-04:00</updated><content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>Optical Character Recognition (OCR) of contemporary printed fonts is widely considered as a solved problem for which many commercial and open source software products exist. However, the task of text recognition on early printed books is still a challenging task due to a high variability in typeset, additional characters, or low scan quality. To master OCR on early printed books, a book or font specific model must be trained to achieve CERs below 2%<a class="footnote-ref" href="#reul2017a"> [reul2017a] </a><a class="footnote-ref" href="#springman2017"> [springman2017] </a>. For this purpose, highly performant individual models must be trained in a short period of time using as less manually annotated GT files as possible. Currently, there exist several Free Open Source Software (FOSS) attempts for such programs: OCRopy, OCRopus 3, Kraken, or Tesseract 4, each with its own advantages or drawbacks.</p>
<p>Calamari<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> is a novel software for training and applying OCR models in order to predict<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> text lines including several up-to-date techniques to highly optimize the computation time and the performance of the models. The usage of Tensorflow allows to design arbitrary DNNs including CNN and LSTM structures that are proven to yield improved results compared to shallow network structures<a class="footnote-ref" href="#breuel2017"> [breuel2017] </a><a class="footnote-ref" href="#wick2018"> [wick2018] </a>. These networks can optionally use CUDA and cuDNN (on a supported GPU) which results in a highly reduced computation time. Compared to other FOSS Calamari supports various techniques that minimize the CER including voting and pretraining (see<a href="#reul2018a">Reul et al., 2018a</a>,<a href="#wick2018">Wick et al., 2018</a>). The software is not designed as a full OCR pipeline which includes tasks such as layout analysis, or line segmentation, but is the topic of a separate publication (in preparation), instead it focuses solely on the OCR step that transcribes line images to text. However, Calamari as Python-Package<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup> can easily be integrated in existing pipelines to manage the OCR part. Thus, by design without any changes it can directly replace the OCR-Engine of OCRopy.</p>
<h2 id="related-work">Related Work</h2>
<p>In the following, we give a short list of the existing open source OCR programs OCRopy, OCRopus 3, Tesseract 4, and Kraken. All of these systems are designed to support the full pipeline from plain page to text. Since the intention of Calamari is solely to handle the OCR of line images, only this functionality of the other programs will be described and compared.</p>
<p>Currently, there exist several versions of OCRopy which was originally published by Breuel (2008) (see also<a href="#breuel2013">Breuel et al., 2013</a>), that are still maintained. OCRopy<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup> is the first software that allowed a user to train custom LSTM based models incorporating the CTC-Loss function<a class="footnote-ref" href="#graves2006"> [graves2006] </a>. By default, it uses a slow numpy based implementation of the computation, which can be exchanged by a faster C-based clstm one. However, neither the GPU nor Deep CNN-LSTM models can be used. For training it requires a list of images and their GT and outputs a model, which then can be used to automatically transcribe the written text of other text lines.</p>
<p>Kraken<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup> is a fork of OCRopy, which has a different API, uses clstm as default, and adds support for bidirectional and top to bottom text. Currently, the usage of PyTorch<sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup> as Deep Learning engine supporting GPU training is under development.</p>
<p>While OCRopy is still maintained OCRopy 2<sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup> seems not to be developed anymore, probably due to the introduction of OCRopus 3<sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup> which changed all major OCR components to Deep Models using PyTorch. OCRopus 3 supports variable network architectures including CNNs and LSTMs and allows training and applying of the models on the GPU. The resulting models yield state-of-the-art results and can be trained with minimal effort in time.</p>
<p>Tesseract<sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup> was initially released as open source in 2005 and is still under development. The newest version 4 of Tesseract adds support for deep network architectures such as LSTM-CNN-Hybrids, however GPU support is not offered. To prototype network structures Tesseract proposes a Variable-size Graph Specification Language (VGSL) which is similar to the network prototype language of Calamari.</p>
<h2 id="methods">Methods</h2>
<p>Calamari comprises several techniques to achieve state-of-the art results on both contemporary prints and historical prints. Beside different DNN architectures (see<a href="#appendixa">Appendix A</a>), it supports confidence voting of different predictions and finetuning with codec adaption. These methods will be presented in the following.</p>
<h2 id="finetuning-and-codec-resizing">Finetuning and Codec Resizing</h2>
<p>A general approach to improve the accuracy of a model on a specific dataset is not to train from scratch but instead to start from an existing robust model and to finetune for the specific task (see e.g.<a href="#reul2017b">Reul et al., 2017b</a>). Usually, the alphabet of the base model differs from the desired labels which is why the output layer is usually fully replaced. However, in the task of OCR many letters, digits, or punctuations are shared across the base model and the target task, and only a few new letters might be added, e.g. German umlauts when starting from an English model, or erased, e.g. the character@which does not exist in historical texts. It is rational to keep those weights as is and add or remove new or unneeded labels instead of training the output layer from scratch.</p>
<p>In the area of historical printed books an individual model for each book must be trained to achieve reasonable results for OCR. To reduce the human effort required for manually transcribing GT lines the OCR model should be trained using as few lines as possible. However, if using only a few lines some characters might not be present yet, e.g. capital letters or digits. Hence, a whitelist is useful to define characters that should not be erased from the base model. Thus, the resulting model has still a chance to predict those letters, even if they have never been seen during finetuning.</p>
<p>Another benefit of using a pretrained model is the reduced computation time to train a model. Since the initial weights are not randomly chosen but set to meaningful features that are expected to generalize on the new data only small variations are required to optimize on the new data.</p>
<h2 id="voting">Voting</h2>
<p>Another technique to obtain more robust results is to vote the outcomes of different models (see e.g.<a href="#reul2018b">Reul et al., 2018b</a>). Hereby, several models are individually applied to the same data, each generating one result called a vote. The final output can be inferred by majority voting. Further refinement is obtained by weighting each vote: the higher the confidence of the model in general, or the actual prediction in particular, the higher the weight (confidence voting). The benefit of voting depends highly on the variance of the individual voters. If the voters predict very similar results, errors are less probable of being removed, as if more diverse models are used.</p>
<p>In case of OCR, confidence voting showed the best results so far. This voting mechanism does not only include the most likely predicted character but also alternatives with its probabilities into account.<a href="#figure01">Figure 1</a>shows a simple example of confidence voting. Three different voters predict the possible characters with an individual confidence. If a single voter chooses the character with the highest confidence, the capital letterIis winning in a majority vote. However, if one adds up each individual confidence values the correct letterlis chosen as the final output.</p>




























<figure ><img loading="lazy" alt="" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>An example for the confidence voting algorithm. Each row shows a part of the output of three different voters. When choosing the most frequent top result of each voter (bold) anIwould be predicted. However, when adding the confidences of each voter, the letterlis predicted.
        </p>
    </figcaption>
</figure>
<p>To obtain different voters based on a dataset several approaches are meaningful, e.g. using different training data, network architectures, base models for finetuning. Recently, we showed that variable voters can be generated by a simple but robust approach called cross-fold-training<a class="footnote-ref" href="#reul2018b"> [reul2018b] </a>.</p>
<h2 id="the-calamari-ocr-system">The Calamari OCR-System</h2>
<p>Calamari supports easy instructions to use any of the listed methods to train various models, and to apply them on existing lines. The software is implemented in Python3 and uses Tensorflow for Deep Learning of the neural net. In doing so, Calamari supports usage of the GPU including the highly optimized cuDNN kernels provided by NVIDIA for a rapid training and an automatic transcription of multiple lines (batches) simultaneously.</p>
<h2 id="preprocessing">Preprocessing</h2>
<p>Both the lines and the text are preprocessed by Calamari for all conducted experiments. The line images are converted to grayscale and are rescaled proportionally to a standard height of 48 pixels. Optionally, the lines are dewarped using OCRopy&rsquo;s dewarping algorithm. Crucial problems of the bidirectional LSTM are the predictions of the first and last few pixels of a line which can be seen as transient behaviour of the internal LSTM state. Therefore, a padding of 16 white pixels is added to each side of the line.</p>
<p>The textual preprocessing allows to resolve several visual ambiguities such as converting Roman unicode digits to Latin letters or joining repeated white space. Furthermore, Calamari adds support for mixed left-to-right and right-to-left text. This solves a challenging task: mirrored symbols, e.g. opening or closing brackets depend on the reading order which can change within a line.</p>
<h2 id="training">Training</h2>
<p>Calamari can easily be trained on new material if an appropriate model is not available yet. As input for training, Calamari expects just as OCRopy a list of line images and the corresponding text files. For efficiency, the full data is loaded and kept in memory for the complete training task instead of repeatedly reading only the current training example. The actual hyperparameters of Calamari are described in<a href="#appendixb">Appendix B</a>.</p>
<h2 id="prediction">Prediction</h2>
<p>To apply a trained model on new line images Calamari expects one or more models for automatic transcription. If several models are used, Calamari votes the results of each individual model to generate the output sentence. Sometimes, it might be useful to access additional information of the prediction. Therefore, Calamari allows to generate information about the position and its confidence of each individual character, as well as the full probability distribution of the input.</p>
<h2 id="experiments">Experiments</h2>
<p>To compare the performance of Calamari to OCRopy, OCRopus 3, and Tesseract 4 we use the datasets UW3 and DTA19. All final results of Calamari were achieved by using early stopping. Hereby we check every 20,000 iterations for a smaller CER on 20% of the training data (hence 80% are used for actual training), after ten checks without a better model, we interrupt the training. Similarly, we chose the best OCRopy or OCRopus 3 model based on the same 20% of training data. Tesseract 4 itself chooses its best model on the provided test data set. For Calamari and OCRopus 3 we use a batch size of 5, the OCRopy and Tesseract 4 do not support batching.</p>
<h2 id="datasets">Datasets</h2>
<p>For evaluating Calamari, we use two datasets with several millions of characters in the training set and three historical printed books.</p>
<p>The smaller University of Washington Database III<sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup> (UW3) consists of extracted lines images from different English scientific and technical journal reports which are printed in a modern Antiqua font. In total, more than 70,000 lines for training and almost 10,000 lines for evaluation are available. Breuel (2017) uses a different version of this dataset (UW3α) with 95,190 text lines for training and 1,253 text lines for evaluation. Unfortunately, this split is not publicly available.</p>
<p>The other large dataset German Text Archive of the 19th Century<sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup> (DTA19) extracted from the German Text Archive (see Wiegand et al., 2018) consists of 39 German Fraktur novels (mostly) printed during the 19th century (1797-1898). Eight novels consisting of 50,968 text lines are used for evaluation the remaining books with 192,974 lines for training. Thus, the evaluation measures the capability of the models to generalize for unseen fonts.</p>
<p>The three historical printed books<sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup> of the years 1476, 1478, and 1499 are written in broken script (Fraktur in a wider sense) and only consist of 3,100, 2,695, and 1,578 lines respectively. We use only 50 lines for training to simulate a human annotator that has to manually transcribe GT data, yet wants to achieve the best possible results.</p>




























<figure ><img loading="lazy" alt="" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Example lines of the used UW3, DTA19, 1476, 1478, and 1499 datasets. Note that the last line is cropped.
        </p>
    </figcaption>
</figure>
<p>An example line for each dataset is shown in<a href="#figure02">Figure 2</a>and an overview of the datasets and their statistics are shown in<a href="#table01">Table 1</a>. The codec size lists the number of characters in the alphabet. The average line width is computed after preprocessing (see<a href="#section4-1">Sec. 4.1</a>) which forces a line height of 48 pixels.</p>




























<figure ><img loading="lazy" alt="" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Overview of the used datasets. The Codec size lists the number of possible characters in the alphabet. The GT lines, the number of total characters, and the average line width in pixels are both shown for the training and the evaluation data sets. The average width of the evaluation set of UW3α is unknown.
        </p>
    </figcaption>
</figure>
<h2 id="evaluation-measure">Evaluation Measure</h2>
<p>To measure the performance of each OCR system we use the CER which is defined as the edit distance ( <em>ed</em> ) of two sequences _s 1 _ and _s 2 _ normalized by the maximum length:CER=ed(s1,s2)max(|s1|,|s2|)This measure is 0 if all characters and 1 if no character match.</p>
<h2 id="accuracy">Accuracy</h2>
<p><a href="#table02">Table 2</a>lists all results for the different datasets, software and used network architectures. First, it is notable that OCRopy yields the worst results both on the UW3 and the DTA19 dataset due to the shallow network structure of only one hidden LSTM-layer.</p>




























<figure ><img loading="lazy" alt="" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>CER using different software on the UW3 (20th cent. reports) and the DTA19 (19th cent. novels) dataset. The convolutions C each have a kernel size of with zero padding and a filter count of 64 in the first and 128 in the second layer. Max-Pooling Mp is used either with a kernel size and stride of or. Note that when using both the height and length of the line is halved while when used only the height is halved. The last hidden layer is always a LSTM with 200 nodes. Calamari results are the average of five models that are then used to apply confidence voting. The CERs on UW3α were published by Breuel (2017).
        </p>
    </figcaption>
</figure>
<p>This network is not capable of learning and generalizing the variations of fonts in the datasets. Introduction of Convolution- and Pooling-Layers highly increases the accuracy of all software and on both datasets. Hereby, the same network structure <em>C</em> , <em>Mp(2x2)</em> , <em>C</em> , <em>Mp(2x2)</em> , <em>LSTM(200)</em> yields different results on the various frameworks: 0.155% on Calamari, 0.397% on Tesseract 4 and 0.502% on OCRopus3 for the UW3 dataset. This difference must be caused by a different training setup or varying hyperparameters, e.g. using an Adam-Solver or a Momentum-Solver, differences in the learning-rate, or usage of dropout. However, the evaluation of hyperparameters is out of the scope of this paper. Note, that the overall setup is the same: All frameworks use the same dataset split for training, choosing the best trained model and evaluation.</p>
<p>The CER on UW3α published by Breuel (2017) that is evaluated on a different evaluation split are in the same order of magnitude as the computed CER on the UW3 split used in this paper. Our trained model on OCRopus3 is comparable to the Tesseract 4 model.</p>
<p>On both models the best performance is achieved by Calamari and further improved by the voting mechanism. On the UW3 dataset it achieves impressive error rates of 0.11% without a language model such as a dictionary. Calamari with voting yields 0.18% CER on German Fraktur. This task is far more difficult because this dataset has a larger alphabet with very similar characters (e.g. a, à, á, â, or ä) or different fonts especially for capital letters. Yet, most errors are due to corrupt lines or GT inconsistencies (e.g. upper and lower quotation marks).</p>
<p>The evaluation on the UW3 dataset shows that 97% of all lines are recognized without any error, however the worst 0.1% lines contribute to 20% of the remaining error. Those lines are either wrongly segmented, rotated or highly degraded.<a href="#figure03">Figure 3</a>shows three of the worst recognized examples. The first impurely segmented line contains the upper part of the following line. Therefore, the postprocessing step failed by bending the middle of the line and the line could not recognized anymore.</p>




























<figure ><img loading="lazy" alt="" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Three of the worst recognized lines during the evaluation of Calamari using voting on the UW3 dataset. The upper two lines suffer from impurities of segmentation or font. The text of the bottom line is cropped.
        </p>
    </figcaption>
</figure>
<p>The second example has a degraded font, that is also larger than the other lines. For example,watersis misclassified aswatorsbecause the middle horizontal line in theeis missing. This error could surely be fixed by a dictionary.</p>
<p>The last line is falsely cropped at the beginning, thus it is impossible to recognizeEnergy Vol. 9. Any other character is correctly recognized.</p>




























<figure ><img loading="lazy" alt="" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Most common errors of Calamari on the UW3 evaluation set. The left side shows the errors of a single voter, the right side after voting. The first two columns show the GT and the predicted character, the third column the number of occurrences, and the forth column the relative percentage compared to the total CER, respectively. The last row sums the relative error remaining mistakes. Note that &quot; is interpreted as two single quotes, which is why the relative error is doubled if those two characters are missing.
        </p>
    </figcaption>
</figure>
<p><a href="#table03">Table 3</a>lists the 20 most common errors of Calamari with and without voting on the UW3 dataset. The GT and PRED columns list the GT expectation and the automatic transcription. The third column counts the number of errors in the test data set (≈0.47 million characters in total) and the last column the relative percentage to all errors made by the respective model. The last row adds the relative percentage of all errors that are not among the 20 most common. Note, that even though the missing &quot; occurred four times in the voted model the relative percentage is doubled, because the double quote is interpreted as two single quotes.</p>
<p>As expected the most common errors in both models are confusions of similar characters such as.and,,vandy, orIandl, but also missing or inserted spaces. Most significantly, the voting mechanism reduces the error of missing spaces (25 vs 9 occurrences), but also the number of confusions ofeandcdropped (7 to 3).</p>
<p>A common postprocessing step on OCR is to apply a dictionary on the recognized words. It is to be expected that errors on letters are highly reduced but punctuation errors are not decreased. Even in this case the voting mechanism is very useful since is reduces also whitespace and punctuation errors. However, note that in the field of OCR on historical pages a dictionary might not be present or even not desired if differences in spelling are of interest. In this field of research, the voting mechanism of Calamari is very useful to reduce the CER.</p>
<h2 id="recognition-speed">Recognition Speed</h2>
<p>Another crucial measure besides its accuracy for a good OCR system is its speed. The runtimes of all softwares for training and prediction excluding preprocessing of a single line is listed in<a href="#table04">Table 4</a>. The processing time of a single line highly depends on the network architecture, the line width, and the used hardware. All the time experiments were measured on a NVIDIA GTX 1080Ti GPU if the software supports GPU usage and an Intel Xeon E5-2690 CPU. Preprocessing requires ca. 50 ms per line with a single process and drops to 6 ms when processing eight lines in parallel.</p>




























<figure ><img loading="lazy" alt="" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Average time for training or prediction of a single line of the UW3 dataset. Note that the times measured for OCRopy and Tesseract 4 are on the CPU while Calamari and OCRopus3 run on the GPU. The prediction of OCRopy and Tesseract 4 is evaluated using a single process, using multiple multithreading highly reduces their computation time. The last row was published by Breuel (2017).
        </p>
    </figcaption>
</figure>
<p>Obviously, both OCRopus 3 and Calamari are faster than Tesseract or OCRopy by a factor of almost 100 since they support batched GPU training and prediction. Incorporation of a GPU therefore allows to recognize more than 100 lines per second.</p>
<p>The time for voting scales linearly with the number of models used as voters. Thus, the prediction time per line in the voting experiments is approximately five times higher than the results shown in<a href="#table04">Table 4</a>. The time required for the aggregating the voting can be neglected.</p>
<h2 id="finetuning">Finetuning</h2>
<p>Using the models trained on the UW3 or the DTA19 dataset to directly transcribe the books 1476, 1488, and 1499 yields discardable results with up to 50% CER. Thus, it is mandatory to train individual models requiring manually labelled GT data. In the following, we use 50 lines of the three historical printed datasets for training different models with and without using the pretrained models of UW3 and DTA19 from section 5.3. This amount of lines is very low, but shows that even a small amount of GT can drastically decrease the CER even though the trained model might not have seen all possible characters of the alphabet yet (e.g. capital letters). The CERs are shown in<a href="#table05">Table 5</a>, whereby both the average of the five folds and their voting results are shown.</p>
<p>The 1476 and 1478 datasets behave similar. Both sets yield about 10% error when using an individual model and a lower CER when voting five different models. Using the pretrained UW3 and DTA19 models instead of training from scratch both the individual model CER and the voted CER drop significantly. Hereby, using DTA19 as pretrained model results in better models because the font and the German alphabet of DTA19 is closer to the historical prints than the modern English fonts of UW3.</p>
<p>Interestingly, pretraining on UW3 yields worse results on the 1499 dataset compared to the models without pretraining, but the voted results are similar. However, the model using DTA19 as initial values clearly predicts better values than the default model.</p>




























<figure ><img loading="lazy" alt="" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Results on the historical printed books using Calamari trained with 50 lines of GT and a batch size of 5. Both the results of using a pretrained model based on either the UW3 (20th cent. reports) or the DTA19 (19th cent. novels) datasets, or training from scratch are indicated by second column. The CER lists the average of the five trained folds and the last column the voted result as CER.
        </p>
    </figcaption>
</figure>
<p>Reul et al. (2018a) showed that using the same pretrained model to train different voters yields worse voted results than using different pretraining models. Thus, it is to be expected that the overall results get further improved if one mixes the pretrained models of UW3 and DTA19. Of course, increasing the number of available lines of GT for training the respective book significantly improves the accuracies of the models, however this results in a higher amount of human effort to annotate GT.</p>
<h2 id="conclusion">Conclusion</h2>
<p>The main focus of this paper was the presentation of Calamari as new line based OCR engine to replace OCRopy or Tesseract due to its low CER and fast computation times.</p>
<p>The conducted experiments clearly demonstrate the capabilities of Calamari on both contemporary and historical OCR tasks. Not only does Calamari yield outstanding performances compared to other OpenSource OCR software but also requires a minimum of time for training and prediction due to the usage of Tensorflow as Deep Learning framework including cuDNN. As already shown by Breuel (2017) and Wick et al. (2018), Deep Hybrid Convolutional-LSTM architectures increase the accuracies both on contemporary and historical prints. Our results have revealed significant differences of Tesseract 4, OCRopus 3 and Calamari due to variations of the network architectures and different sets of hyperparameters. It is to be expected that an optimization of these hyperparameters might further improve the accuracies of the OCR models. However, a very high amount of the remaining errors on UW3 and DTA19 are GT inconsistencies or impure lines, which are nearly impossible to predict correctly.</p>
<p>Voting and pretraining are two important mechanisms to increase the performance of newly trained models, especially if only few data is available. Voting has shown improved results on all conducted experiments, however on the cost of a higher prediction and training time, since several different models are independently used. Pretraining is most useful if the original model is similar to the new data and reduced both the CER and the training time. Especially, when training new individual models as required for OCR on historical prints pretraining should be used to receive the best possible results if only a few manually annotated lines are available. In general, the best OCR results are to be expected if the targeted fonts of a pretrained model are similar to the material at hand. This of course correlates with the period of publication of the various books lies in the same epoch.</p>
<p>Although Calamari supports many features such as voting, or pretraining, plans for extensions exist. A first major work is data augmentation during training which is expected to significantly drop the CER especially if only a few lines are present. The augmentations basically are different types of noise, degradation, or generated background. Obviously, synthetic data based on existing fonts can also be incorporated for data augmentation.</p>
<p>Tesseract&rsquo;s language to define network topologies (VGSL) has a very simple and compact syntax. The current syntax of Calamari should also support this language to define networks.</p>
<p>Finally, since Calamari is designed very modular, it shall be extended to support other sequence-to-sequence tasks, such as monophonic Optical Music Recognition (e.g. Gregorian chants) or Speech-To-Text. All these tasks fundamentally share the tasks of processing two dimensional sequential data and output a sequence of classes. Only the data preprocessing e.g. of audio files and postprocessing is different. Calamari is designed to easily exchange these steps but keeping a generic structure for training, evaluation, and application.</p>
<h2 id="network-architecture-building-blocks">Network Architecture Building Blocks</h2>
<p>The task of the DNN and its decoder is to process the image of a segmented text line and to output simple text. This sequence to sequence task is trained by the CTC algorithm published by Graves et al. (2006) that allows to predict shorter but arbitrary label sequences out of an input sequence that is the line image regarded as sequence. Hereby, the network outputs a probability distribution for each possible character in the alphabet for each horizontal pixel position of the line. Thus, an image with sizehxwwith a given alphabet size of|L|will result in a matrix of shapeP(x,l)∈Rwx|L|withP(x,l)being a probability distribution for all <em>x</em> . Since the network needs to see several slices in width to be certain about a single character, most of the time it does not yet know what to predict. Hence, the CTC-algorithm adds a blank label to the alphabet that is ignored by the decoder but allows the network to make empty or uncertain predictions with a high probability. In fact, the used greedy decoder that chooses the character with the highest probability at each position <em>x</em> mostly predicts blank labels, and only with one or two pixel widths the actual character is recognized. Afterwards, the final decoding result is received by unifying neighbouring predictions of the same characters and removing all blank labels. For example the sentence <em>AA&ndash;BB&ndash;CA&ndash;A-¿</em> of length <em>w</em> is reduced to <em>ABCAA</em> .</p>
<p>The network is trained by the CTC-Loss-Function that basically computes the probability of the GT label sequence given the probability output of the network. This probability is computed by summing up all possible paths through the probability matrix P that yield the GT using an efficient forward backward algorithm. Fortunately, this computation can be derived to receive the gradient that is required for the learning algorithm.</p>
<p>The supported network architectures of Calamari are CNN-LSTM-Hybrids that act on a full line in one step. CNNs are widely used in image processing because they are designed to detect meaningful features (e.g. curves, circles, lines, or corners) that can be located anywhere in an image. These features are processed afterwards by a (bidirectional) LSTM based recurrent network to compute the probability matrix <em>P</em> . Max-Pooling is a common technique in CNNs to reduce the computation effort and keep only the most important features. In general image processing settings pooling is applied both in vertical and horizontal direction. This means that the processed lines get shortened. Thus, it is important that the final layer of the CNN in the full network has an image width, that is long enough to produce the full label sequence. For example, if the GT has a length of 40 characters, a minimum of 80 predictions are required to allow for a blank prediction between any pair of adjacent characters. Thus, if two2x2pooling layers are used in the CNN the width of the image lines should be at leastw=2·2·80px=320px.</p>
<h2 id="training-hyperparameters">Training hyperparameters</h2>
<p>The default network consists of two pairs of convolution and pooling layers with a ReLU-Activation function, a following bidirectional LSTM layer, and an output layer which predicts probabilities for the alphabet. Both convolution layers have a kernel size of3x3with zero padding of one pixel. The first layer has 64 filters, the second layer 128 filters. The pooling layers implement Max-Pooling with a kernel size and stride of2x2. Each LSTM layer (forwards and backwards) has 200 hidden states that are concatenated to serve as input for the final output layer. During training we apply dropout<a class="footnote-ref" href="#srivastava2014"> [srivastava2014] </a>with a rate of 0.5 to the concatenated LSTM output to prevent overfitting. The loss is computed by the CTC-Algorithm given the output layer’s predictions and the GT label sequence.</p>
<p>Calamari uses Adam<a class="footnote-ref" href="#kingma2014"> [kingma2014] </a>as standard solver with a learning rate of 0.001. To tackle the exploding gradient problem of the LSTMs we implement gradient clipping on the global norm of all gradients as recommended by Pascanu et al. (2013).</p>
<ul>
<li id="breuel2008">Breuel, T. M. (2008) “The OCRopus open source OCR system” . In: _Document Recognition and Retrieval XV_ . International Society for Optics and Photonics, Vol. 6815, p. 68150F.
</li>
<li id="breuel2017">Breuel, T. M. (2017) “High performance text recognition using a hybrid convolutional-lstm implementation” . In: _Document Analysis and Recognition (ICDAR), 2017 14th IAPR International Conference on_ . IEEE, Vol. 1, pp. 11-16.
</li>
<li id="breuel2013">Breuel, T. M., et al. (2013) “High-performance OCR for printed English and Fraktur using LSTM networks” . In: _Document Analysis and Recognition (ICDAR), 2013 12th International Conference on_ . IEEE, pp. 683-687.
</li>
<li id="graves2006">Graves, A., et al. (2006) “Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks” . In: _Proceedings of the 23rd international conference on Machine learning_ . ACM, pp. 369-376.
</li>
<li id="kingma2014">Kingma, D. P. and BA, J. (2014) “Adam: A method for stochastic optimization” . _arXiv preprint arXiv_ :1412.6980.
</li>
<li id="pascanu2013">Pascanu, R., et al. (2013) “On the difficulty of training recurrent neural networks” . In: _International Conference on Machine Learning_ . pp. 1310-1318.
</li>
<li id="reul2017a">Reul, C., et al. (2017a) “Case Study of a highly automated Layout Analysis and OCR of an incunabulum: “Der Heiligen Leben” (1488)” . In: _Proceedings of the 2nd International Conference on Digital Access to Textual Cultural Heritage_ . ACM, pp. 155-160.
</li>
<li id="reul2018a">Reul, C., et al. (2018a) “Improving OCR Accuracy on Early Printed Books by combining Pretraining, Voting, and Active Learning” . _JLCL: Special Issue on Automatic Text and Layout Recognition_ , 33 (1), 3-24.
</li>
<li id="reul2018b">Reul, C., et al. (2018b) “Improving OCR Accuracy on Early Printed Books by utilizing Cross Fold Training and Voting” . In: _2018 13th IAPR International Workshop on Document Analysis Systems (DAS)_ . IEEE, pp. 423-428.
</li>
<li id="reul2017b">Reul, C., et al. (2017b) “Transfer Learning for OCRopus Model Training on Early Printed Books” . _Journal for Library Culture_ , 5 (1), 38-51.
</li>
<li id="springman2017">Springmann, U. and Lüdeling, A. (2017) “OCR of historical printings with an application to building diachronic corpora: A case study using the RIDGES herbal corpus” . _Digital Humanities Quarterly_ , 11 (2).
</li>
<li id="springman2018">Springmann, U., et al. (2018) “Ground Truth for training OCR engines on historical documents in German Fraktur and Early Modern Latin” .JLCL: Special Issue on Automatic Text and Layout Recognition, 33 (1), 97-114.
</li>
<li id="srivastava2014">Srivastava, N., et al. (2014) “Dropout: a simple way to prevent neural networks from overfitting” . _The Journal of Machine Learning Research_ , 15 (1), 1929-1958.
</li>
<li id="wick2018">Wick, C., et al. (2018) “Comparison of OCR Accuracy on Early Printed Books using the Open Source Engines Calamari and OCRopus” . _JLCL: Special Issue on Automatic Text and Layout Recognition_ , 33 (1), 79-96.
</li>
<li id="wiegand2018">Wiegand, F., et al. (2018) “Recherchieren, Arbeiten und Publizieren im Deutschen Textarchiv: ein Praxisbericht” . _Zeitschrift für germanistische Linguistik_ , 46 (1), 147-161.
</li>
</ul>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p><a href="https://github.com/Calamari-OCR">https://github.com/Calamari-OCR</a>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Throughout this paper, the term “predict” is used whenever machine-readable text is automatically transcribed from a line image.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p><a href="https://pypi.org/project/calamari_ocr/">https://pypi.org/project/calamari_ocr/</a>&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p><a href="https://github.com/tmbdev/ocropy">https://github.com/tmbdev/ocropy</a>&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p><a href="http://kraken.re/">http://kraken.re/</a>&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p><a href="https://pytorch.org/">https://pytorch.org/</a>&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p><a href="https://github.com/tmbdev/ocropy2">https://github.com/tmbdev/ocropy2</a>&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p><a href="https://github.com/NVlabs/ocropus3">https://github.com/NVlabs/ocropus3</a>&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p><a href="https://github.com/tesseract-ocr/tesseract">https://github.com/tesseract-ocr/tesseract</a>&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p><a href="http://www.tmbdev.net/ocrdata-split/">http://www.tmbdev.net/ocrdata-split/</a>&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>Published as part of the GT4HistOCR dataset<a class="footnote-ref" href="#springman2018"> [springman2018] </a>,<a href="https://zenodo.org/record/1344132">https://zenodo.org/record/1344132</a>.&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p>Published as part of the GT4HistOCR dataset<a class="footnote-ref" href="#springman2018"> [springman2018] </a>,<a href="https://zenodo.org/record/1344132">https://zenodo.org/record/1344132</a>.## Bibliography&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry><entry><title type="html">Crowdsourcing Image Extraction and Annotation: Software Development and Case Study</title><link href="https://rlskoeser.github.io/dhqwords/vol/14/2/000469/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://rlskoeser.github.io/dhqwords/vol/14/2/000469/</id><author><name>Ana Jofre</name></author><author><name>Vincent Berardi</name></author><author><name>Kathleen P.J. Brennan</name></author><author><name>Aisha Cornejo</name></author><author><name>Carl Bennett</name></author><author><name>John Harlan</name></author><published>2020-06-19T00:00:00+00:00</published><updated>2023-08-06T15:53:44-04:00</updated><content type="html"><![CDATA[<h2 id="1-introduction">1. Introduction</h2>
<p>The amount of multimedia data available is steadily increasing<a class="footnote-ref" href="#james2014"> [james2014] </a>, which has led to many instances where it is desirable to identify and annotate objects located within an image. Examples include the detection of features from outdoor cameras<a class="footnote-ref" href="#hipp2013"> [hipp2013] </a><a class="footnote-ref" href="#hipp2015"> [hipp2015] </a>and the classification of animal species<a class="footnote-ref" href="#welinder2010"> [welinder2010] </a><a class="footnote-ref" href="#caltech2018"> [caltech2018] </a>. Machine learning and other quantitative methodologies can be used to identify objects within images (see<a class="footnote-ref" href="#lecun2015"> [lecun2015] </a>for an example), but their complexity and the requirement for an optimized training set often limit the use of these approaches. A viable alternative is crowdsourcing, the process of enlisting untrained individuals to perform computationally intensive tasks, which has been extensively used in a variety of projects<a class="footnote-ref" href="#kuang2015"> [kuang2015] </a><a class="footnote-ref" href="#manovich2016"> [manovich2016] </a><a class="footnote-ref" href="#yu2013"> [yu2013] </a><a class="footnote-ref" href="#tomnod2018"> [tomnod2018] </a><a class="footnote-ref" href="#clickworkers2018"> [clickworkers2018] </a>. Amazon’s Mechanical Turk (AMT) service for crowdsourcing work is popular with many researchers across disciplines and allows requesters to post tasks, and matches these tasks with anonymous workers who complete them.</p>
<p>Our specific interest is in identifying and labeling images of faces from the <em>Time</em> magazine archive to gain historical insight on American cultural trends. Collecting such a data set requires a two-step process: 1) identify all faces within the corpus and 2) annotate each face according to standardized protocols for feature designation. In this paper, we detail the development of a web-based image-cropping and annotation software for performing these tasks, and we describe our rigorous verification methods for both the cropping and the annotation. Notably, we developed a verification procedure that required only two annotations per image to produce high-fidelity data. The data collected using the methods described here was then used to train an object detector and image classifier machine learning models.</p>
<p>Our methods are illustrated through a case study where the software was used to crop and label human faces from an archive of <em>Time</em> magazine. While our web-based interface is platform-independent, it was administered as an external survey link on AMT. The design process, details of our data collection methods, and instructions for others to customize the software to crop alternative objects from other archives are all described. The software and methodology described here has been used for our own digital humanities project<a class="footnote-ref" href="#jofre2020a"> [jofre2020a] </a><a class="footnote-ref" href="#jofre2020b"> [jofre2020b] </a>, and we believe it may be useful to other researchers.</p>
<h2 id="2-motivation-and-background">2. Motivation and Background</h2>
<p>This work was motivated by an interest in using large, image-heavy corpora, in particular periodical archives, to gain insights into cultural history. Interpreting large cultural corpora requires both quantitative methods drawn from data science and qualitative methods drawn from technology, cultural, and social studies. From this perspective, we are interested in questions concerning what the faces in a magazine archive could reveal about the larger, historical context of a publication, questions such as how gender/race/age representation have changed over time, and how these correlate with the magazine’s text and with broader cultural trends.</p>
<p>The archive under consideration for our case study consists of approximately 4,500 issues from <em>Time</em> magazine, ranging from 1923 through 2014. The corpus comprises approximately 500,000 .jpg files, with each page of each issue, including the cover, representing one file. We selected <em>Time</em> magazine for a number of reasons. First, while there are a few existing studies of this corpus (see<a class="footnote-ref" href="#desouza2014"> [desouza2014] </a>and<a class="footnote-ref" href="#manovich2009"> [manovich2009] </a>), there is certainly more work to be done on the visual aspects of the archive by moving beyond the cover images and text. Second, <em>Time</em> has been a mainstay publication in the United States for nearly a century, and in that period has witnessed vast cultural, political, and technological changes. Third, it has a relatively well documented corporate history (see<a class="footnote-ref" href="#prendergast1986"> [prendergast1986] </a>), which allows us to examine the internal context of the production of the magazine vis-à-vis its external context like wars, political movements, changes in fashion, and so on. Finally, the <em>Time</em> corpus is widely held in library collections in the United States, and available online through <em>The Vault</em> at<a href="https://time.com/vault/">https://time.com/vault/</a>.</p>
<p>The data we collected using the crowdsourcing methods described in this paper has been published as a dataset in the Journal of Cultural Analytics<a class="footnote-ref" href="#jofre2020a"> [jofre2020a] </a>, available for use to all researchers in the digital humanities. We used the crowdsourced data to train an algorithm to extract all the images of faces from our <em>Time</em> magazine archive and classify their gender. The high-granularity of the automatically generated data allowed us to undertake a detailed study on gender representation in <em>Time</em> magazine<a class="footnote-ref" href="#jofre2020b"> [jofre2020b] </a>.</p>
<p>Previous studies have successfully used crowdsourcing to achieve goals similar to ours. For instance, when examining features of traffic intersections, the correlation between crowdsourced results and experts was 0.86 for vehicles, 0.90 for pedestrians, and 0.49 for cyclists<a class="footnote-ref" href="#hipp2013"> [hipp2013] </a>. Similarly, when assessing 99 Flickr images for the presence of 53 features, the correlation between crowdsourced results and expert raters was 0.92<a class="footnote-ref" href="#nowak2010"> [nowak2010] </a>. In both of these studies, crowdsourced labels were derived by averaging the labels produced by multiple individuals. While these correlations are encouraging, there are known challenges associated with crowdsourcing. Occasionally, crowdsource workers have been shown to arbitrarily select answers or give vague responses in an effort to complete jobs more quickly<a class="footnote-ref" href="#downs2010"> [downs2010] </a>. This behavior can be reduced by adding verifiable qualification questions, often called honeypots, to crowdsourcing procedures<a class="footnote-ref" href="#kittur2008"> [kittur2008] </a>. Furthermore, the demographics of crowdsource workers are typically skewed towards low income workers from India and the United States, who tend to be young and female<a class="footnote-ref" href="#casey2017"> [casey2017] </a>. Our data collection crowdsourcing methods are mindful of concerns about the potential of inadvertently exploiting low-visibility and/or vulnerable populations and intentionally aim to provide reasonable compensation (for further discussion of these issues, see<a class="footnote-ref" href="#irani2015"> [irani2015] </a>).</p>
<p>While there are many other solutions for researchers seeking to perform image extraction and annotation via crowdsourcing, we believe that our software fills a unique niche for humanities researchers who want to have full control of the data collection and quality controls. Most solutions are geared towards machine learning researchers and provide these services as a bundle, where the client receives the requested clean data. These include LabelBox (<a href="https://labelbox.com/product/platform">https://labelbox.com/product/platform</a>), LionBridge (<a href="https://lionbridge.ai/services/image-annotation/">https://lionbridge.ai/services/image-annotation/</a>), Hive (<a href="https://thehive.ai/">https://thehive.ai/</a>), Figure Eight (<a href="https://www.figure-eight.com/">https://www.figure-eight.com/</a>), and Appen (<a href="https://appen.com/">https://appen.com/</a>). Such black-box solutions are not suitable for the humanities, where we must be mindful of who is doing the tagging. Our software allows the researcher to track individual workers to examine their effect on the data. Furthermore, it is platform-independent, allowing it to be deployed on any crowdsourcing site. We are aware of one other standalone image cropping and tagging software package, labelImg (<a href="https://github.com/tzutalin/labelImg">https://github.com/tzutalin/labelImg</a>), but it is not web-based, which limits its deployment.</p>
<p>The software package and methodology we developed are intentionally flexible, both in the corpora they can analyze and in the crowdsourcing platform on which they can be deployed. For the former, our motivation was to allow our tools to be used with a variety of sources, such as the Look Magazine archive, hosted by the Library of Congress<a class="footnote-ref" href="#lookmagazine2012"> [lookmagazine2012] </a>. For the latter, we did not want to exclusively link the project to AMT because we want the option of using other crowdsourcing platforms.</p>
<h2 id="3-development-and-deployment-of-interface">3. Development and Deployment of Interface</h2>
<h2 id="31-determination-of-image-features-to-be-assessed">3.1 Determination of Image Features to Be Assessed</h2>
<p>In preliminary work, project leaders identified the following nine facial features of interest: 1.) Gender, classified as Male, Female or Unknown; 2.) Race, classified according to current U.S. census categories as American Indian, Asian, Black, Pacific Islander, White, or Unknown; 3.) Emotion, classified according to Ekman’s six basic emotions as Anger, Disgust, Fear, Happy, Sad, or Surprise (Ekman and Friesen 1986); 4.) Racial Stereotype, classified as Yes or No; 5.) Magazine Context, classified as Advertisement, Cover, or Feature Story; 6.) Image Type, classified as Photograph versus Illustration; 7.) Image Color, classified as Color or Black &amp; White; 8.) Multiple Faces in the Image, classified as Yes or No; and 9.) Image Quality, classified as Good, Fair, or Poor.</p>
<p>One issue from each of the ten decades spanned by the data (1920s-2010s) was selected at random and analyzed by student research assistants. The student coders proceeded through all pages in an issue (range: 50-160), identified faces, and annotated the features according to the above categories. Throughout this process, coders were asked to keep track of anomalous faces that were not easily classified, a process that was extremely valuable in refining our procedures. For example, due to the presence of animal faces and masks, the operational definition of a classifiable face was changed to human faces where at least one eye and clear facial features are present. Single color images required the Image Color classification levels to be changed to Color versus Monochrome and an “Author” category was added to Magazine Context. There was little agreement among raters concerning the presence of stereotypes and facial emotions, so these categories were eliminated. The emotion variable was replaced by a binary Smile variable and a Face Angle variable (whether the face is in profile or facing the viewer). Furthermore, most of the Unknown labels for the Race and Gender categories were assigned to babies or young children, so a binary Adult variable was also added. The final list of facial features is provided in Table 1.</p>
<p>With the updated feature list established, three coders reviewed a single issue and annotated the 185 faces that were identified by all three individuals when reviewing the issue. To assess interrater reliability (IRR), Cohen’s kappa (κ) was calculated for each facial category.κvalues between 0.60 and 0.75 are typically interpreted as representing good agreement whileκ&gt; 0.75 characterizes excellent agreement. The averageκwas 0.809 and all values were above 0.721 with the exception of image quality, withκ=0.363. These values are summarized in Table 1. This IRR exercise revealed that, when reviewing the coder data for pages with multiple faces, it was challenging to make interrater comparisons since it was often difficult to determine which exact face corresponded with a given set of labels. This led to a major revision in our protocol where, rather than having individuals annotate faces and store the results as they reviewed pages, they would first crop each face, so that each set of assigned labels could be associated with a specific cropped face.<br>
Classification features and categories used for annotating facial images along with κ, quantifying the interrater reliability among three raters over 185 faces from the same magazine issue. * Denotes that this category was classified according to the current U.S. Census<a class="footnote-ref" href="#aboutrace2018"> [aboutrace2018] </a>Variable NameClassification OptionsCohen’sκAdultYes or No0.771Face AngleProfile or Straight0.819GenderFemale, Male, or Unknown0.932Image ColorColor or Monochrome0.985Image QualityGood, Fair, or Poor0.363Image TypePhoto or Illustration0.928ContextAdvertisement, Cover Page, or Feature Story0.974MultifaceYes or No0.869RaceAmerican Indian, Asian, Black, Pacific Islander, White, or Unknown*0.721SmileYes or No0.731</p>
<h2 id="32-deployment-of-web-based-application">3.2 Deployment of Web-Based Application</h2>
<p>To scale up data collection, we created a web-based form in PHP, coupled to an SQL database, that could be deployed within crowdsourcing platforms to perform the two tasks required to obtain the data of interest. In Task 1, a magazine page was presented, and participants were instructed to crop any faces that are present; in Task 2, participants were instructed to categorize the faces identified in Task 1 according to the specifications in Table 1. The data collection protocol was to first complete Task 1 (cropping) on all our selected pages before moving on to the annotation phase, which allowed cropping errors to be eliminated before sending the extracted images for annotation. Task 1 was separated from Task 2 so that crowdsource workers would only have to be trained for and perform one scope of work.</p>
<p>While the data-collection interface is platform-independent and can be used to directly collect data, we found it beneficial to use AMT to recruit participants and manage payments.Jobs(or human interface tasks (HITs) in AMT vernacular) were deployed in AMT as a survey link. For Task 1, each job consisted of reviewing 50 pages and cropping all of the observed faces within each page. AMT workers were paid $5 USD (all payment rates cited here are in USD) for each completed job, which was based on the time it took student coders to complete similarly-sized jobs (30-40 minutes) with a goal of paying between $8-$10/hour, above U.S. federal minimum wage<a class="footnote-ref" href="#silberman2018"> [silberman2018] </a>. For Task 2 jobs, AMT workers were required to categorize 25 to 50 faces, each of which was previously cropped from a page in Task 1. Student coders spent 10-15 minutes to complete jobs consisting of 50 faces on our context-free interface (discussed in section 3.4.1) and 15-20 minutes on jobs consisting of 25 faces on our default interface; therefore, AMT workers were paid $2.25 for these jobs. Once an assigned job was completed, the software generated a completion code that workers entered into AMT to receive payment. Using this code as an identifier, we were able to verify the quality of the work (see sections 3.3 and 3.4 for details) and process payments. For borderline or questionable work quality, we intentionally erred towards payment and only withheld payment for the most extreme circumstances. Each job also included an optional demographic survey, which will inform future studies exploring relationships between demographics and face annotation outcomes. All procedures were approved by the SUNY Polytechnic Internal Review Board.</p>
<h2 id="33-description-of-task-1-cropping-interface">3.3 Description of Task 1 (Cropping) Interface</h2>
<p>In this task, workers were presented with a job consisting of 50 images, 47 of which were randomly-selected magazine pages and three of which were validation pages. On each assigned page, AMT workers were asked to crop a rectangle around individual faces by clicking and dragging from one corner of a rectangle to the opposite corner. (See Figure 1). If there was more than one face on the page, workers selected an option to remain on the page and continue cropping. Once all the faces were cropped, or if there were no faces on the page, workers selected an option to move onto the next page in their job. We observed that workers often abandoned an assigned job after the first few pages, resulting incomplete jobs within our system. To eliminate these jobs, a script was created that ran in the background to look for pages that had been assigned within a job that had been inactive (i.e. no faces cropped) for more than 2 hours. Any data collected from these jobs was deleted and the pages within them were made available for a new job assignment.</p>




























<figure ><img loading="lazy" alt="This image shows three stages of the process of the cropping interface. A face is selected and cropped, leaving a red box covering the face in the final stage." src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>The cropping interface. Left: page as first presented to worker. Middle: worker selects face to crop. Right: Faces that have already been selected and submitted are covered up to help workers keep track of cropped faces.
        </p>
    </figcaption>
</figure>
<p>Within each job, 3 of the 50 pages that the workers analyzed were validation pages, whose inclusion was meant to help detect workers that attempted to quickly receive payment by repeatedly indicating that there were no faces on each page, regardless of content. These pages were selected randomly from a database which contains a list of magazine pages and the known number of faces on each page, as determined by trained project personnel. These are ourground-truthfaces. Worker quality was assessed by comparing the number of cropped faces on these pages to the known number of faces. Workers’ validation page was flagged if they cropped more than one face on a validation page with only 1 face or cropped±1 face beyond the known number of faces on pages with &gt; 1 face. When determining if payment should be provided, workers with 2 or 3 flags were subject to additional review while payment was immediately processed for all others.</p>




























<figure ><img loading="lazy" alt="A screenshot of a _Time_ magazine page with faces identified with red boxes around them." src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Screenshot of a page within our in-house review interface. Selections cropped by workers are outlined with a red rectangle.
        </p>
    </figcaption>
</figure>
<p>To facilitate the further inspection of AMT workers with a high number of flags, an easy-to-use, in-house review interface was built (Figure 2). On a single webpage, this interface displayed all of the magazine pages assigned to any worker, along with frames around the image areas that the worker selected for cropping. Using this interface, project personnel were able to rapidly scroll through the pages, inspect the work, and make note of pages with mistaken crops or faces left uncropped. If a worker had errors on more than half of their pages, then payment was not provided and all pages in their job were re-analyzed. We paid all other workers but used our revision process to identify pages with egregious errors, which were returned to the pool to have their analysis redone.</p>
<h2 id="34-description-of-task-2-annotating">3.4 Description of Task 2 (Annotating)</h2>
<p>In this task, workers were presented with a job consisting of either 25 or 50 images of faces, and were asked to enter appropriate tags for each face. The faces were randomly selected from the images that were cropped in Task 1. Procedures similar to those outlined in Task 1 were used to simultaneously manage multiple jobs, ensure that a sufficient number of images are available to populate each job, and cancel jobs that have timed out. For each face in a job, workers classified facial features according to the categories in Table 1 with an additional “not a face” option that served as a quality check for the collection of cropped faces. To maximize task efficiency, the options for each classification were presented as clickable radio buttons, rather than as drop-down menus. As in Task 1, once the job was completed, the workers were given a randomly-generated completion code that was used to secure payment through the AMT platform.</p>
<p>In a similar process to Task 1, each job contained 3 validation faces, also known as ground-truth faces, each of which was consistently labeled the same by three student coders over all categories. To create a flagging system, we focused on the three categories that had the highest rates of agreement in our preliminary data collection: gender, image color, and image type. Magazine context had the second-highest interrater reliability, but as will be discussed in section 3.4.1, our software was configured to assess this feature in two different ways so it could not be used for validation. When the classifications matched the known values for a given validation image, the flag value was set to zero. Each mismatch contributed a value of 1 to the flag, with a maximum of 3. Images with large flag values were subject to further scrutiny. For the cases where an AMT worker had mismatches with the validation images, it was not possible to build a succinct visual inspection tool for all images as was done in Task 1 since category selections cannot easily be represented visually. Furthermore, there is a degree of subjectivity and ambiguity in certain categories, such as the presence of a smile, so we chose not to develop explicit criteria for processing AMT payments and all workers were paid. To navigate the potential for erroneous data and/or ambiguous categories, we obtained multiple annotations for each face, which were aggregated to obtain a crowdsourced label. As will be described in a subsequent section, we had each face annotated twice and resolved inconsistencies by choosing the label associated with the worker who was most consistent with other workers over all annotated faces, and who had the lowest number of flags.</p>
<h2 id="341-examination-of-variations-in-the-interface">3.4.1 Examination of variations in the interface</h2>
<p>We also took this opportunity to examine how variations in the interface affected annotation results (see Figure 3). In particular, we were curious about whether faces taken out of context were more likely to be erroneously labeled. For example, a closely cropped face may not include gender cues, such as hair and clothing. To address this question, we developed two different annotation interfaces. In the context-free version, we show only a cropped face to workers, who then determine the characteristics. Because there is no context around the face, the magazine context (ad, feature, cover, etc.) and multi-face (whether the face being tagged is accompanied by other faces) categories were required to be determined in Task 1 while workers did the cropping. In the second (default) version of the task, workers see the full page with a rectangle around the face of interest when labeling the face and workers answer questions about the face as well as about the context around it. We default to this later version of the interface since we were able to automate Task 1 (see section 5.2), requiring the context annotations to be assigned in Task 2. We found that, despite there being only two additional questions in the default version of the interface compared to the context-free version, it took almost twice as long to complete the labeling tasks, which is why AMT jobs consisted of 25 rather than 50 faces with the default version.</p>




























<figure ><img loading="lazy" alt="This image depicts two options in the interface. On the left is a context-free face, with a picture of only the selected face on a black background. On the right is a face highlighted in green on the page it is from which is the default option." src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Left: The context-free version of the interface shows workers only the face to be annotated. Right: The default version of the interface shows the full page with the face in question outlined in green.
        </p>
    </figcaption>
</figure>
<h2 id="4-software-evaluation-case-study-with-magazine-archive">4. Software Evaluation: Case Study with Magazine Archive</h2>
<p>A case study was performed using a subset of our magazine archive consisting of one July issue selected from every year between 1961 and 1991, which corresponds with our historic period of interest. Additionally, each of the one-per-decade issues that the student coders manually labeled during our preliminary studies were used as a second data set. The first data set was denoted as 30YR (it spans 30 years) while the second was called OPD (as we selected One issue Per Decade). After being cropped, both the 30YR and OPD data were each labeled by two distinct AMT workers.</p>
<h2 id="41-summary-of-amt-accuracy">4.1 Summary of AMT Accuracy</h2>
<p>A total of 87 AMT workers cropped 3,722 total pages in Task 1. Due to various glitches that were discovered during deployment and eventually rectified, certain jobs contained less than 50 pages with the average being 47.18 pages per job. The average time to complete a job was 47 minutes. Three validation pages were randomly included within each job to address concerns about individuals incorrectly indicating there were no faces on a given page. However, this behavior was not widely observed, as less than 5% of all validation pages were characterized as having no faces. More common errors appear to have been cropping only a fraction of the faces present on a given page or including many faces within a single crop. For example, 20.0% of validation pages with 3 or more ground truth faces were characterized as having only 1 face. The cropping error rate was significantly reduced when workers were required to acknowledge that they read our instructions before beginning the job. Overall, for 72.8% of validation pages, the number of faces identified by the AMT workers agreed with known number of faces. For an additional 7.6% of validation pages, AMT workers cropped more faces than the known number. It is likely that these cases represent genuine attempts at completing the task, where the known faces along with additional small, poor quality faces were cropped. Processes were implemented to eliminate poor quality faces (see section 4.3). Therefore, the cropping accuracy should consider true positives to be those validation pages where the number of cropped faces either matched or exceeded the ground truth, which led to an effective accuracy of 80.4%. Each page was verified with our inspection interface described above and crop errors were corrected before proceeding to Task 2.</p>
<p>In Task 2, a total of 342 workers annotated 9,369 faces. One AMT assignment consisted of either 25 or 50 faces, depending on whether the default or context-free interface was being used. Technical glitches, which were later corrected, occasionally caused the number of faces in a job to slightly vary. The average time to complete a job was 30 minutes using the context-free interface, with a job consisting of 50 faces, and 25 minutes using the default interface with a job consisting of 25 faces. Table 2 illustrates the consistency of image annotations with the known labels of the validation images. With the exception of image quality, the accuracy for each category was above 87%.<br>
Proportion of images where AMT worker’s label matched the known validation image label in Task 2. Results are not provided for Magazine Context or Multi face since these categories were assessed in Task 1 when the context-free interface was used.PhotoColorAngleQualityGenderRaceSmileAdult0.960.930.880.480.940.880.890.99</p>
<h2 id="42-comparison-of-default-versus-context-free-interface-for-task-2">4.2 Comparison of Default versus Context-Free Interface for Task 2</h2>
<p>As described in section 3.4.1, Task 2 was deployed with two different interfaces. In the default case, faces were presented in the context of the original page they were cropped from, while in the context-free case, the face alone was presented. To investigate whether the interfaces affected the labeling task, we used the default interface for both rounds of OPD labeling, but varied the interface for the 30YR data, as shown in Table 3. We then examined the consistency of labels over these two cases.<br>
Deployment of Task 2 over various interfaces.Round 1Round 2OPDDefault InterfaceDefault Interface30YRContext-free interfaceDefault interface<br>
For each of the 10 labeled features, the proportion of images where the ratings agreed was calculated for both the 30YR and OPD data sets. The results are illustrated in Table 4. According toχ2analyses, the differences between the proportion of matches was significant for 5 of the 10 features with the largest differences being between magazine context and image quality. This is to be expected since the two different interfaces used for the 30YR data primarily differed in ways that can be expected to affect these features. There were relatively large differences in image quality based on the presence of context, with 54.1% and 9.6% of faces labeled as good and poor quality, respectively, compared to 3.4% and 20.5% of images labeled good and poor, respectively, for the context-free design. It is possible that the presence of context increased the readability of the face.</p>
<p>Interestingly, the correspondence in magazine context was larger across different interfaces in the 30YR data than across the consistent interfaces in the OPD data. The observed statistically significant differences may be due to the large sample size, which is bolstered by effect sizes (Cohen’s <em>f</em> ) that are well below 0.1 in every case; typically, a moderate effect is considered 0.3. As a result, we conclude that the differences in annotation quality according to the interface design are relatively small.<br>
Proportion of ratings agreeing for both 30YR and OPD data withχ2analysis <em>p</em> -values and effect sizes (Cohen’s <em>f</em> ) for the differences in proportions provided. * indicates a p-value &lt; 0.05.MultifaceColorContextPhotoAngle30YR0.680.900.690.920.79OPD0.740.880.600.910.78p0.004<em>0.11&lt;0.001</em>0.480.47effect0.040.020.060.010.01GenderRaceAdultSmileQuality30YR0.900.710.930.810.45OPD0.890.770.970.820.53p0.180.002<em>0.001</em>0.72&lt;0.001*effect0.020.050.050.0050.05</p>
<h2 id="43-effect-of-image-quality">4.3 Effect of Image Quality</h2>
<p>We next explored the effect of image quality on the consistency between raters. Each image was classified as having <em>Satisfactory Quality</em> (SQ) if both raters scored its quality as either good or fair, or <em>Non-Satisfactory Quality</em> (NSQ) otherwise. Approximately 27% of the observations were classified as NSQ. The proportion of matches for each feature was then calculated separately for both the SQ and NSQ cases. The results are illustrated in Table 5. For 6 of the 10 features,χ2analyses indicated that the concordance between raters was significantly different for SQ and NSQ images. The effect sizes (Cohen’s <em>f</em> ) were larger than when comparing 30YR to OPD images with the adult and image quality features approaching a moderate effect.<br>
Proportion of ratings agreeing for both SQ and NSQ dataχ2analysis <em>p</em> -values and effect sizes (Cohen’s <em>f</em> ) for the differences in proportions provided. * indicates a p-value &lt; 0.05.Multi-faceColorContextPhotoAngleSQ0.680.900.680.940.81NSQ0.710.890.680.850.75p0.110.110.60&lt;0.001*&lt;0.0001<em>effect0.020.020.0080.140.06GenderRaceAdultSmileQualitySQ0.940.750.970.820.56NSQ0.810.640.850.810.21p&lt;0.001</em>&lt;0.001*&lt;0.001<em>0.52&lt;0.001</em>effect0.180.110.210.010.31<br>
The results in Table 5 indicate that it may be advantageous to eliminate NSQ data from subsequent analyses. Before doing so, it is important to determine if this will introduce a bias. Due to changes in printing technology and subject matter over the 90+ years spanned by the data, there is the potential for image quality to differ by time. This possibility was assessed by separately calculating the frequency of SQ and NSQ images in each issue. Aχ2analysis was then performed, which indicated that there was no significant difference between the SQ and NSQ frequency distributions. Therefore, eliminating the NSQ images will not introduce temporal bias.</p>
<h2 id="44-aggregation-of-multiple-image-labels">4.4 Aggregation of Multiple Image Labels</h2>
<p>Each face was annotated twice, each time by distinct AMT workers. While the majority of labels (~ 80%) were in agreement, we required a methodology to resolve disagreements between labels in order to have a definitive value for each annotation. When crowdsourcing data, this is often achieved by having multiple individuals rate a given image and then using a majority rules approach for each feature<a class="footnote-ref" href="#hipp2013"> [hipp2013] </a><a class="footnote-ref" href="#hipp2015"> [hipp2015] </a><a class="footnote-ref" href="#nowak2010"> [nowak2010] </a>. However, this approach can be resource intensive. More targeted approaches have been developed that implement an expectation-maximization algorithm to determine the most likely label for a given object in order to ultimately determine a score for the quality of each work<a class="footnote-ref" href="#dawid1979"> [dawid1979] </a><a class="footnote-ref" href="#wang2011"> [wang2011] </a><a class="footnote-ref" href="#organisciak2012"> [organisciak2012] </a><a class="footnote-ref" href="#welinder2010"> [welinder2010] </a><a class="footnote-ref" href="#whitehill2009"> [whitehill2009] </a>. Lower-performing workers can then be filtered out of the rating system. We aimed to emulate such approaches, but with a simplified procedure that functions over only two coders per image. Our strategy was to calculate a proficiency score for each of the raters and to resolve inconsistencies by selecting the response recorded by the individual with the better proficiency score. Proficiency scores were determined for each worker by examining their validation images and calculating the fraction of annotations matched between the worker’s input and the ground truth. A proficiency score of 1 is a perfect score. The average proficiency score (μ) was 0.87 with a standard deviation (SD) of 0.09. An alternate way to calculate the proficiency score was by considering all of the images tagged by a given rater and computing the average fraction of image features that matched the images’ other raters. The average proficiency score with this convention wasμ= 0.81 with SD = 0.06.<br>
Mean proficiency score stratified by the sum of flag values over all validation images.Flag Sum0123Mean Proficiency (All Rated Image)0.820.800.780.64Mean Proficiency (Validation Images)0.900.850.770.76<br>
Table 6 compares our two methods of calculating the proficiency score with the flagging system for image annotations. The sum of the flags for each participant was calculated and proficiency scores were stratified by these values. As shown in Table 6, lower proficiency scores were associated with larger flag values, which indicates that our flagging system provides a reasonably good indicator of worker proficiency. An ANOVA test indicated that the differences in proficiency score values among the flag values were significant (p&lt;0.001) for both varieties of the proficiency score.</p>
<h2 id="45-validation-of-proficiency-score">4.5 Validation of Proficiency Score</h2>
<p>Prior to deploying the proficiency score methodology to resolve annotation inconsistencies throughout the entire corpus, it was necessary to determine the consistency of this methodology with the more established majority-rules procedure. To assess this, a subset of 1,000 SQ images were selected from the corpus at random and then submitted to AMT for three additional annotations (i.e., five total annotations). The annotation label selected most frequently was selected for this image with ties between annotation labels (&lt; 1% of all annotations) chosen at random. Table 7 summarizes the proportion of faces for which the annotation labels in the five-rater consensus and proficiency score (using the all rated images option) matched. These results indicate that the proficiency scoring procedure is sufficiently accurate to allow future iterations of this system to proceed with only two raters per image, which will allow for a more resource-efficient project.<br>
Proportion of images where the five-rater consensus and proficiency score labels matched, stratified by annotation category.PhotoColorAngleQualityGenderRaceSmileAdultContextMultiface0.970.970.920.740.970.930.910.990.900.85</p>
<h2 id="5-software-applications">5. Software Applications</h2>
<h2 id="51-applying-software-to-other-data-sets">5.1 Applying Software to Other Data Sets</h2>
<p>While this software was built for our specific purpose of cropping and annotating faces from <em>Time</em> magazine, we were mindful about its generalizability and developed it with the hope that it could serve as a useful tool for other researchers with other corpora. To this end, the code is hosted on GitHub (<a href="https://github.com/Culture-Analytics-Research-Group/Data-Collection">https://github.com/Culture-Analytics-Research-Group/Data-Collection</a>) and is written so that both tasks (image cropping and annotation) are easily generalized, and the annotation variables are straightforward to modify. Instructions for modifying this software to a different archive, along with detailed instructions on how to use the software, are provided in the Appendix.</p>
<p>As a demonstration of this flexibility, we hosted a proof-of-concept workshop in December 2018 demonstrating the use of our tool on selected pages from the GQ Magazine corpus<a class="footnote-ref" href="#jofre2018"> [jofre2018] </a>. Prior to the workshop, we used our trained face detector to identify and crop faces from these pages, and the workshop demonstrated Task 2 (annotating the selected images) to explore trends in facial hair.</p>
<p>The cropping part of the software (Task 1) is particularly easy to adapt for cropping other objects. In our own research, we are currently using the cropping part of the software to extract the advertisements from the corpus. The software is also being used to identify measures of neighborhood distress (graffiti, abandoned vehicles, etc.) in a study that examines the role of environmental factors in promoting physical activity.</p>
<h2 id="52-task-automation">5.2 Task Automation</h2>
<p>Our case-study data has provided us with a corpus-specific training set that we have used to train a RetinaNet detector<a class="footnote-ref" href="#lin2017"> [lin2017] </a><a class="footnote-ref" href="#lin2018"> [lin2018] </a>to automatically identify and extract the rest of the faces from the archive<a class="footnote-ref" href="#jofre2020a"> [jofre2020a] </a><a class="footnote-ref" href="#jofre2020b"> [jofre2020b] </a>. Our case-study data set of 1,958 pages with 4739 face annotations and 1708 pages containing zero faces was used to train the detector. The detector was trained for twenty epochs, since training for more resulted in overfitting and poor generalization in face detection across different historical eras. After running the detector on every page from the archive over 400 thousand facial images were extracted, using a threshold of 50% certainty. When we increased the accuracy threshold to 90%, we were able to extract over 327 thousand faces with very high accuracy. In comparison, our first attempts at automated extraction with OpenCV yielded only 117 thousand facial images from the entire corpus, and 5% of these were false positive (i.e. not actually faces). Compared to OpenCV, the trained RetinaNet detector was able to extract more faces, particularly those with a profile orientation, and those that were illustrated instead of photographed.</p>
<p>We have also trained classifiers to automatically label the gender of the face by fine-tuning a pre-trained VGG Face CNN Descriptor network<a class="footnote-ref" href="#parkhi2015"> [parkhi2015] </a><a class="footnote-ref" href="#malli2018"> [malli2018] </a>with our crowd-sourced data. From the initial set of data described here, 3,274 faces were male, and only 1,131 were female, which skewed our results on the first run. To expand the training set, we employed a bootstrapping technique to acquire additional, more balanced, training data and thus improve our classifier. The model trained on the AMT data was used to classify all 327,322 faces from the archive. From these faces, we randomly selected images and manually verified the classification results. These new images plus the AMT data yielded a new dataset of 17,698 faces for the second round of training, with roughly equal male/female representation. This yielded a 95% accuracy<a class="footnote-ref" href="#jofre2020a"> [jofre2020a] </a><a class="footnote-ref" href="#jofre2020b"> [jofre2020b] </a>.</p>
<h2 id="53-visualizing-annotation-results">5.3 Visualizing Annotation Results</h2>
<p>We created an additional piece of software, also available on our Github page (<a href="https://github.com/Culture-Analytics-Research-Group/Metadata-Analysis">https://github.com/Culture-Analytics-Research-Group/Metadata-Analysis</a>), that pulls the data directly from the database where the crowdsourced annotations are stored and creates visual summaries of image annotations versus time. The user can select any annotation category and easily generate a chart of the selection as a function of time, aggregated by year or by month. In addition, the tool allows users to select subsets of categories. The example in Figure 4 shows the percentage of women’s faces out of the subset of faces identified in the context of advertisements. This tool is intended for preliminary analysis that allows researchers to quickly identify temporal trends and patterns.</p>




























<figure ><img loading="lazy" alt="This image depicts a graph of faces being tagged female out of all faces tagged in advertisements between 1930 and 2010. There is a sharp spike in female faces being tagged in 1960." src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Screenshot showing the percentage of faces that are tagged female out of faces that are tagged as being within advertisements.
        </p>
    </figcaption>
</figure>
<h2 id="54-digital-humanities-studies">5.4 Digital humanities studies</h2>
<p>The data we collected with these methods have allowed us to generate more data via machine learning, and has allowed us to ask the following questions<a class="footnote-ref" href="#jofre2020a"> [jofre2020a] </a>. How has the importance of the image of the face changed over time? How has gender representation changed over time? How does gender representation correlate with the magazine’s text and with the historical context? How has race representation changed over time? How has the representation of children changed over time? How does race and/or age correlate with the magazine’s text and with the historical context? What types of faces are more likely to be smiling? In what context (ads or news) do certain types of faces tend to appear, and how does this change over time? What types of faces are more likely to be presented as individualized portraits?</p>
<p>In our own work, we used the data collected through this method (as well as the automatically-extracted data that this work made possible) to examine how the percentage of female faces found in <em>Time</em> magazine between the 1940s and 1990s correlates with changing attitudes towards women. We found that the percentage of images of women’s faces peaks during eras when women have been more active in public life, and wanes in eras of backlash against women’s rights. The changes in the representation of women in the magazine over time tracked closely not only with the overall historical context, but also with the internal policies of the publication, and with a close reading of the magazine’s content. We believe that this finding is particularly relevant in our contemporary post-literate world in which people absorb culture primarily through images<a class="footnote-ref" href="#jofre2020b"> [jofre2020b] </a>.</p>
<h2 id="6-discussion-and-future-work">6. Discussion and Future work</h2>
<h2 id="61-observations">6.1 Observations</h2>
<p>We were successful in building and deploying software to manage the crowdsourced extraction and labeling of features from an image-heavy corpus. While the software is generalizable, we focused on an application where faces were required to be extracted and labeled from <em>Time</em> magazine. The accuracy for both Task 1 and Task 2 were in line with those seen for other studies that have used crowdsourcing for similar tasks<a class="footnote-ref" href="#hipp2013"> [hipp2013] </a><a class="footnote-ref" href="#hipp2015"> [hipp2015] </a><a class="footnote-ref" href="#nowak2010"> [nowak2010] </a>. In contrast to these other studies that required multiple workers for each image, our method only requires two individuals to annotate each image to gain results with a similar accuracy.</p>
<p>Our case-study results show that the differences between labeling performed on context-free versus context-rich interfaces were small. However, there was a notable difference when we instead compared images that were tagged as “good” quality with images tagged as “poor” quality, an effect likely due to challenges in reading poor quality figures. This indicates that there is value in requiring workers to evaluate image quality, as it allows us to flag potentially ambiguous annotations. Interestingly, faces that were viewed in the context of a full image were less likely to be labeled as having poor quality compared to faces that were viewed in the context-free interface. It seems that context increases the readability of the face in question, which makes our default interface advantageous. On the other hand, a disadvantage of the default interface is that it takes nearly twice as long to label a single face compared to the context-free interface. While the default interface contains two additional features to be assessed, we speculate that providing a full image rather than a cropped image adds a significant cognitive load to the task. We anecdotally note that personnel who tested both interfaces observed that the default interface felt “less tedious” than the context-free interface: viewing pages from vintage magazines was “more entertaining” than viewing decontextualized images of faces. In the end, we likely will opt for the default interface in our future studies. This is in part because we have been able to fully automate image extraction, but also because the context-rich environment seems to increase the readability of the selected face. An image of a face alone loses the rich contextual information of the complete page in which it appeared.</p>
<p>Using the methods described in this case study, we successfully collected data that was 1) used to train an object detector and an image classifier, 2) published and made accessible to other digital humanities researchers<a class="footnote-ref" href="#jofre2020a"> [jofre2020a] </a>, and 3) used to undertake a study on gender representation in <em>Time</em> magazine<a class="footnote-ref" href="#jofre2020b"> [jofre2020b] </a>.</p>
<h2 id="62-advantages-of-a-standalone-application">6.2 Advantages of a Standalone Application</h2>
<p>While AMT offers multiple options, including developer tools and a sandbox, for creating image cropping and tagging interfaces, we chose to build our own web-based application for several reasons. For one, this allows complete customizability, which was beneficial as we tweaked our approach in response to preliminary data. Also, this web-form enables us to collect data in a manner that is independent of any service providers, which allows us to use different services without compromising our methods. In this work, we used AMT to provide a proof-of-principle, but we plan to deploy this system on other crowdsourcing platforms. The stand-alone interface also opens the possibility of collecting data with volunteer crowdsourcing, as has been done in projects from the New York City Public Library<a class="footnote-ref" href="#nyplmap2018"> [nyplmap2018] </a><a class="footnote-ref" href="#nypllabs2018"> [nypllabs2018] </a><a class="footnote-ref" href="#allhands2018"> [allhands2018] </a>. The biggest challenge in using volunteers is generating sufficient interest to collect a significant amount of data. We may have to consider methods of gamifying the tasks to make them more appealing, and our hope is that once our results are presented publicly, people may become interested in participating in the project. Lastly, a standalone application can be shared with other researchers and adapted to different types of projects in a way that is not possible with platform-specific approaches.</p>
<h2 id="63-limitations">6.3 Limitations</h2>
<p>From a humanistic perspective, there is a limitation in using only visual data to classify race and gender. In the case of gender, our data doesn’t distinguish between someone who identifies as a woman (or man) and someone who presents as female (or male), and the automatic classification trained on this data assumes that gender is binary, which is problematic. Human coders, who see the context of the page can mitigate this problem by labeling the gender as ‘unknown’, which accounted for 6% of the faces. However, upon closer inspection, we found that none of these were actually gender non-binary adult individuals: many were not faces at all (errors in the face extraction), many were very small low resolution images that were hard to read, some were non-gendered cartoon illustrations (a face drawn onto an object, for example), and some were infants or small children. So, while problematic, the assumption of a binary gender may be suitable for examining certain mainstream 20th century publications such as <em>Time</em> magazine. In the case of race, we found its classification was difficult because race categories are somewhat arbitrary, and because the concept of race is highly context-dependent. Census categories have changed significantly over the past century and they continue to be contentious. In our experience with human coders, we found that the race of a face is often not recognized unless it is embedded within a stereotyped setting, and that when the face was not white, coders tended to disagree on race more than with other categories.</p>
<p>A second, more practical, limitation is that this software requires that the user have some familiarity with PHP and with managing SQL databases. Our goal was to make a useful tool for researchers, rather than a polished commercial product. Researchers using this software need to have someone on their team with basic programming experience. The tradeoff, however, is that this software allows researchers to have full control of the data collection and quality controls.</p>
<h2 id="64-long-term-project-goals">6.4 Long term project goals</h2>
<p>Our next steps are to continue using this crowdsourced data we collected to automate the classification of other categories, and to undertake a close examination of the context in which faces appear, particularly advertisements. To this end, we are using our software to crowdsource the extraction of all advertisements from selected issues of the corpus. These will be used to train an algorithm that will extract all the advertisements from the corpus. Using this advertising data in conjunction with our face data will allow us to undertake a study on trends in advertising in this particular media outlet.</p>
<p>The ultimate goal of this project is to create web-based interactive visualizations of the data we extract from our <em>Time</em> magazine archive, and of the results of our analysis. We hope to provide insights into how depictions of faces have changed over time and what such changes in visual representation can tell us about the intersection of politics, culture, race, gender, and class over time. We hope that the online resource we create will be of interest to researchers and students of media and cultural history, as well as to the general public. Our visualization approach is inspired by Manovich’s Selfie-city and Photo-trails work<a class="footnote-ref" href="#manovich2016"> [manovich2016] </a><a class="footnote-ref" href="#douglass2011"> [douglass2011] </a><a class="footnote-ref" href="#hochman2016"> [hochman2016] </a>, and by his team’s use of direct visualization<a class="footnote-ref" href="#crockett2016"> [crockett2016] </a>, which is an effective way to engage broad audiences into complex corpuses. We also draw inspiration from Robots Reading Vogue<a class="footnote-ref" href="#king2016"> [king2016] </a>and Neural Neighbors<a class="footnote-ref" href="#leonard2018"> [leonard2018] </a>, which are projects based in the Yale University library system. Most recently, we have been using and modifying software from Yale’s DH lab, <em>PixPlot</em> <a class="footnote-ref" href="#duhaime2018"> [duhaime2018] </a>, to sort the images with unsupervised clustering.</p>
<p>In addition to gaining insights from our corpus and making these publicly accessible, we also aim to develop novel methodologies for the visual analytics of large, image-based data sets that can be applied to a variety of projects and shared with other researchers.</p>
<h2 id="acknowledgements">Acknowledgements</h2>
<p>We would like to acknowledge Michael Reale for his help with automating image extraction and tagging. We would also like to acknowledge generous research support from our institutions, SUNY Polytechnic and Chapman University, for the start-up funding that made this research possible. Finally, we acknowledge IPAM at UCLA for bringing this collaboration together at the Culture Analytics Long Program and for equipping us with the tools to undertake this research.</p>
<h2 id="appendix-using-and-modifying-the-software">Appendix: Using and modifying the software</h2>
<h2 id="part-1-details-about-the-code">Part 1: Details About the Code</h2>
<p>This is a web interface for gathering data from images on a large scale. Users should serve it with accompanying writable SQL databases. We provide the accompanying database structures here and on Github, along with the code.</p>
<p>This web-based interface facilitates gathering data from images: it allows users to crop a selection from a larger image and to input information about the crop. In our case, we are selecting faces out of images from a magazine archive, but with some minor edits this code can be used to select anything else from an image archive (cars, trains, signs, etc.).</p>
<p>This web interface is platform independent. Users only need a link to access it.</p>
<p>The code itself has three different data gathering surveys that are part of it.</p>
<p>The first survey allows participants to select and save a cropped portion of an image. The survey contains multiple pages (in our case 50), and the participant has to select and submit all the faces from each page. To access the cropping survey use the linksurvey.php?load=crop.</p>
<p>For the crop, we used<a href="https://github.com/odyniec/imgareaselect">https://github.com/odyniec/imgareaselect</a>imgareaselectby Michal Wojciechowski.</p>
<p>The second survey allows users to classify the already cropped images from a selection of categories. To access the cropping survey use the linksurvey.php?load=tag.</p>
<p>The third survey is simply a demographics survey that allows users to enter their demographic information, and is presented at the end of each of the previous two surveys.</p>
<p>The code of this survey is split into 4 different files <em>instructions.php</em> , <em>survey.php</em> , <em>post.php</em> , and <em>functions.php</em> .</p>
<p><em>instructions.php</em> is a landing page that presents the user with instructions for the current survey either the cropping survey or the classify survey. The survey and instructions that will be presented are determined by the GET variable load in the URL. If load=crop the crop instructions are presented if load=tag then the classifying survey is presented. Users must select that they have read the instructions in order to move onto the survey.</p>
<p><em>survey.php</em> is the main interface of the survey that the user interacts with.</p>
<p>If the job is to crop images, the urlsurvey.php?load=cropshould be used. The image to be cropped is presented and users are asked if the object to be cropped is present (faces in the case of the original purpose) in the image. If the object is present users can crop it be clicking and dragging over the object in the image. If multiple objects are present users may select that there are more objects (faces) on the page. Any previous cropped objects will be covered when cropping another object. If it is not present users may simply select that the object is not there and move to the next image.</p>
<p>If the job is classifying images that were previously cropped, the urlsurvey.php?load=tagshould be used. The user is presented the image from which an object of interest was cropped, with the cropped portion highlighted along with questions about the classification of the object.</p>
<p>Each job within the survey has a total number of images to be done at one time that can be set along with three check points that can be set (in <em>functions.php</em> ). The check points present the user with ground truth pages where the classification or number of objects cropped is already known in order to check whether a user has properly completed the survey. These variables can be set in <em>functions.php</em> .</p>
<p><em>post.php</em> handles all submission of data to the data base after a user has hit the submit button. If the job was cropping, data is submitted to the database and the selected portion is cropped and saved to a folder on the server. If the job was classifying, data is just submitted to the database. If a user has completed a check page then information on the page is placed in an array to later be checked and entered at the end of the survey. If the user has reached the end of the survey and filled out the demographics information then the demographics data and check data is submitted and a completion code is generated. If a user has no activity for 2 hours and then tries to submit data <em>post.php</em> will cause the session to timeout.</p>
<p><em>functions.php</em> contains all the functions that are used in the survey and is included in both <em>survey.php</em> and <em>post.php</em></p>
<h2 id="functionsphp-overview">functions.php Overview</h2>
<p>$job — php $_GET variable that indicates whether the job I for cropping or tagging so that the proper page is loaded. Obtained from the url, for example, in the urlsurvey.php?load=crop$job=crop.</p>
<p>$batch_size — variable controlling the number of images per job</p>
<p>$check — array variable that contains when ground truth images will be shown in the job</p>
<p>$face_total — variable for cropping that keeps track of the number of objects cropped from a specific image</p>
<p>$file_array — holds image file names to have a group number added at the end of each job</p>
<p>$check_data1, $check_data2, $check_data3 — holds data submitted by users on each of the three ground truth images</p>
<p>db_connect() — returns a mysqli_connection object for connecting to the database, set $servername, $username, $password, and $database you wish to connect to</p>
<p>select($job, $batch_size, $connection) — selects images one at a time as long as there is enough images available for another job, otherwise users are presented with a message that requests are currently at capacity. This function also marks pages as being worked on in the database and adds a timestamp for clearing data on a job that was never finished. The file name of the image is returned</p>
<p>check_select($job, $connection) — similar to select, except it selects ground truth images from their tables.</p>
<p>parse_filename($job, $filename) — parses information from the file name of the image. If the job is cropping, then this information is used to create the path that cropped images will be stored in. If the job is classifying, then this information is used to determine the path of the original image. The parsed data is stored in the $file_data array to later be displayed and submitted to the database. This function is based on the file name scheme of the images originally used with this code.</p>
<p>display($job, $file_data) — handles what is displayed for the user depending what the job is. Inputs for the survey questions are printed out as radio buttons</p>
<p>hidden($job, $batch_current, $filename, $file_data, $file_array, $check_data1, $check_data2, $check_data3) — prints out the hidden inputs for each job mainly the data parsed from the filename. If the job is cropping the hidden inputs containing information for cropping the data is printed out.</p>
<p>post_hidden() — prints out hidden inputs for <em>post.php</em> that need to be sent back to <em>survey.php</em></p>
<p>crop_image() — handles the cropping of images for the crop job and accounts for offset of different window resolutions and sizes.</p>
<p>post_variables($job) — sets the variables in post that will be submitted to the database for each job along with variables needed for post functions</p>
<p>submit($job, $connection) — submits data to the database for each job and marks images as no longer being worked on. If the job is cropping and no object was cropped then no data is submitted. If the job was cropping and the page was a ground truth page a temporary entry is mad in a table so that covering previously cropped objects on pages with multiple objects will work properly.</p>
<p>final_submit($job, $connection) — submits the demographics information to the database. A group number is generated by selecting the highest group number from the database group tables for each job and adding one.</p>
<p>This group number is assigned to each image that was part of the job. It is also inserted into the check table for each job along with possible flags raised from the information in the check arrays and a randomly generated code that will be presented to the user. This code is for admins to manage payment via Amazon Mechanical Turk.</p>
<p>demographic($job, $file_array, $check_data1, $check_data2, $check_data3)- displays the form and the inputs for users to enter their demographic information</p>
<p>coverfaces($job, $connection, $filename, $file_data) —</p>
<p>If the job is set tocrop, covers previously cropped objects (faces) on images where multiple objects need to be cropped, by selecting previously submitted x and y coordinates from the database. If the image is a ground truth image then it selects from the temporary entry in the table for crop checks. If the job is set totag, this function is used to find the coordinates and draw the rectangle around the object to be classified.</p>
<h2 id="part-2-the-data-tables">Part 2: The Data Tables</h2>
<p>Below is thepagestable structure — Used for the cropping task.<br>
ColumnDescriptionpage_fileFile name of magazine page imagefacesTotal number of faces on that page (starts out as null until page is analyzed)group_numIdentifies a completed job. This cell is null until a job is completed, when the job is completed, all the pages that belonged to that job are marked with this group number. This number is unique and increments each time a job is completed.workingflags whether that particular page is being worked on by another worker.timestampwhich marks the date/time a page is displayed. If a page was displayed more than 2 hours ago and does not have an associatedgroup number, then any data collected on that page is cleared, timestamp is marked null, and the page is made available again for selection.<br>
Below is thecrop_groupstable structure — Used to track workers in cropping task.<br>
ColumnDescriptiongroup_numJob identifierflag1Results fromground-truthcomparisons.flag2flag3codeUnique completion code. Randomly generated by our software, to be entered into mechanical turk.<br>
Below is theground_truth_cropstable structure. This is the ground truth table that is used for the cropping task.<br>
ColumnDescriptionfileFile name of the imagenfacesNumber of faces on this imageworkingMarks whether the file is currently being usedtimestampMarks time that file was displayed. Resets after 1 hour.<br>
Below is thetag_groupstable structure – Used to track workers in tagging task.<br>
ColumnDescriptiontag_groupJob identifierflag1Results fromground-truthcomparisons.flag2flag3codeUnique completion code. Randomly generated by our software, to be entered into mechanical turk.<br>
Below is thedatatable structure — this is the table that contains the collected data. Year, month, day, page, image, and coordinates are populated during the cropping task. The rest of the columns are populated in the tagging task.<br>
ColumnDescriptionyearThese identify the source image, which is labeled by issue date and page number.monthdaypagemultifaceIs there more than one person in the image (yes/no)?categoryIs the image part of a feature story, an ad, or the cover page?colorIs the image in color or monochrome?photoIs the face a photograph or an illustration?angleIs the face in profile or looking straight ahead?genderIs the face male or female (or other)?raceWhat is the race of the face? (select from 5 census categories: White, Black, Asian, American Indian, Pacific Islander)adultIs it an adult or a child?smileIs the face smiling?qualityWhat is the image quality like? (Good — face is clearly visible, Fair — face is small or slightly blurry, Poor — face is barely visible, Discard — this is not a human face)imagethe name of the cropped image that is saved in the data folder on the back end.x1These are the diagonal corner coordinates of the cropped selection.y1x2y2tag_grouptracks completed tagging jobs. This cell is null until a job is completed, when the job is completed, all the crops that belonged to that job are marked with this group number. This number is unique and increments each time a job is completed.working,workingflags whether that crop is currently being tagged by another workertimestamp, andtimestampmarks the date/time an object is displayed for tagging. If an image was displayed more than 2 hours ago and does not have an associatedtag_group, then any data collected on that crop is cleared and the crop is made available again for selection.<br>
Theground_truthtable has the same structure as the data table — This is the ground truth table for the tagging task.</p>
<p>Thecrop_checktable stores the year, month, day, page, and coordinates of the ground truth pages that the user crops. This keeps track of the objects cropped out of theground truthpages. It is used to cover objects that a user has already cropped from a single page when multiple objects are present, and it is used to calculate the flags in thecrop_groupstable. Once the job is finished and the flags are calculated, the entries in this table are deleted.</p>
<p>tag_checktable structure (this table records workers’ entries on the validation pages)<br>
ColumnDescriptiontag_groupJob identifiermultifaceIs there more than one person in the image (yes/no)?categoryIs the image part of a feature story, an ad, or the cover page?colorIs the image in color or monochrome?photoIs the face a photograph or an illustration?angleIs the face in profile or looking straight ahead?genderIs the face male or female (or other)?raceWhat is the race of the face? (select from 5 census categories: White, Black, Asian, American Indian, Pacific Islander)adultIs it an adult or a child?smileIs the face smiling?imageThe validation image used</p>
<h2 id="part-3-instructions-for-modifying-the-software-for-use-in-other-studies">Part 3: Instructions for Modifying the Software for Use in Other Studies</h2>
<p>While this software was built for our specific purpose of cropping and annotating faces from a specific periodical archive, we were mindful about its generalizability and developed it with the hope that it could serve as a useful tool for other researchers. We share our code and database structure on GitHub with this intent. The code is written so that the cropping job is easily generalized and the annotation variables are easy to modify.</p>
<p>The most straightforward application of this software is for researchers interested in cropping and annotating objects from other magazine archives. To use our application, the archive needs to be stored as a collection of .jpg images named using the following convention: YYYY-MM-DD page X.jpg (where YYYY is the year, MM is the month, DD is the day, X is the page number). We share the database structure so that users can easily configure it from their server. Users can change column names (and corresponding variable names in the code) as needed.</p>
<p>The key part of the code consists of four php files: <em>instructions.php</em> is a landing page in case users want to present workers with instructions at the beginning of a task, <em>survey.php</em> contains the interface the worker interacts with, <em>post.php</em> handles all the submission of data to the database, and <em>functions.php</em> contains all the functions used in <em>survey.php</em> and <em>post.php</em> . The user will have to modify these files, depending on the application. At a minimum, the user will need to edit the <em>db_connect()</em> function in the <em>functions.php</em> file with their own server configurations.</p>
<p>To use the cropping task, users should list the images they want analyzed in the <em>page_file</em> column in the <em>pages</em> data table and serve the ‘survey.php?load=crop’ URL to display the cropping task. (A link to the demo will be included here if this paper is accepted, after anonymity is lifted.) In the <em>function.php</em> file, users can adjust the number of pages that comprise a job, the number of validation images per job, and the location of the validation images (2nd image seen, 5th image seen, etc.). The validation images are drawn from the <em>ground_truth_crop</em> table, which the user must populate.</p>
<p>When a worker crops a face with this interface, a copy of the cropped image is stored on the backend and the <em>data</em> table is populated with information about this face. The user must specify the name and path of the folder where the cropped images will be stored: this is done in the <em>crop_image</em> function in <em>functions.php</em> . The information stored in the data table is the year, month, day, and page number, parsed from the source image name; the coordinates of the crop; and the name of the cropped image. If users need to have a different file naming convention and need their source image names parsed differently, they can modify the <em>parse_filename()</em> function in the <em>functions.php</em> file. The total number of crops made per page is stored in the <em>faces</em> column of the <em>pages</em> table. If the user is cropping an object other than a face, the names of variables, data columns, and the descriptors on the frontend can be changed to more appropriate terms.</p>
<p>To display the annotation task, users should serve thesurvey.php?load=tagURL. (a demo page can be viewed here:<a href="https://magazineproject.org/TIMEvault/survey.php?load=tag">https://magazineproject.org/TIMEvault/survey.php?load=tag</a>.) To use the tagging task, the <em>data</em> table should be populated with the source image identifiers (year, month, day, and page) and with the coordinates of the crop. If the user wants to use the context-free version of the interface, they will only need to provide the name of the cropped image in the <em>data</em> table and modify the source image in the “content-div” html element in the <em>survey.php</em> file.</p>
<p>If users want to annotate features that are different from the ones we listed, the names of the data columns can be changed, as well as the corresponding variable names in the functions <em>post_variables()</em> , <em>submit()</em> , and <em>display()</em> , which are in the <em>functions.php</em> file. Data columns and corresponding variables can be added or removed as needed.</p>
<h2 id="bibliography">Bibliography</h2>
<ul>
<li id="aboutrace2018">About Race, United States Census Bureau. URL<a href="https://www.census.gov/topics/population/race/about.html">https://www.census.gov/topics/population/race/about.html</a>(accessed 9.20.18).
</li>
<li id="allhands2018"> “All Hands on Deck: NYPL Turns to the Crowd to Develop Digital Collections” . _The New York Public Library_ . URL<a href="https://www.nypl.org/blog/2011/09/15/all-hands-deck-nypl-turns-crowd-develop-digital-collections">https://www.nypl.org/blog/2011/09/15/all-hands-deck-nypl-turns-crowd-develop-digital-collections</a>(accessed 9.6.18).
</li>
<li id="bradski2000">Bradski, G. “The OpenCV Library” , _Dr. Dobb’s Journal of Software Tools_ (2000).
</li>
<li id="caltech2018">Caltech-UCSD Birds 200. URL<a href="http://www.vision.caltech.edu/visipedia/CUB-200.html">http://www.vision.caltech.edu/visipedia/CUB-200.html</a>(accessed 9.18.18).
</li>
<li id="casey2017">Casey, L. S., Chandler, J., Levine, A. S., Proctor, A., Strolovitch, D.Z. “Intertemporal Differences Among MTurk Workers: Time-Based Sample Variations and Implications for Online Data Collection” , _SAGE Open_ 7, 215824401771277.<a href="https://doi.org/10.1177/2158244017712774">https://doi.org/10.1177/2158244017712774</a>(2017).
</li>
<li id="clickworkers2018">CLICKWORKERS: Home. URL<a href="http://www.nasaclickworkers.com/">http://www.nasaclickworkers.com/</a>(accessed 9.18.18).
</li>
<li id="crockett2016">Crockett, D. “Direct visualization techniques for the analysis of image data: the slice histogram and the growing entourage plot” , _International Journal for Digital Art History_ 2.<a href="https://doi.org/10.11588/dah.2016.2.33529">https://doi.org/10.11588/dah.2016.2.33529</a>(2016).
</li>
<li id="dawid1979">Dawid, A. P., Skene, A. M. “Maximum likelihood estimation of observer error‐rates using the EM algorithm” ,  _Journal of the Royal Statistical Society: Series C (Applied Statistics)_  28(1): 20-28. doi: 10.2307/2346806 (1979).
</li>
<li id="desouza2014">de Souza, R.C. “Chapter 2.3 dimensions of variation in _TIME_ magazine.” In T. Berber Sardinha, and M. Veirano Pinto (eds), _Multi-Dimensional Analysis, 25 Years on: A Tribute to Douglas Bieber_ , Amsterdam: 177-194.<a href="https://doi.org/10.1075/scl.60.06sou">https://doi.org/10.1075/scl.60.06sou</a>(2014).
</li>
<li id="douglass2011">Douglass, J., Huber, W., Manovich, M. “Understanding scanlation: how to read one million fan-translated manga pages” , _Image & Narrative_ 12: 190–227 (2011).
</li>
<li id="downs2010">Downs, J. S., Holbrook, M. B., Sheng, S., Cranor, L. F. “Are your participants gaming the system?: screening mechanical turk workers” , _CHI 2010 Proceedings of the SIGCHI Conference on Human Factors in Computing Systems_ , Atlanta, Georgia, April 2010: 2399–2402.<a href="https://doi.org/10.1145/1753326.1753688">https://doi.org/10.1145/1753326.1753688</a>(2010).
</li>
<li id="duhaime2018">Duhaime, D. PixPlot. Yale Digital Humanities Lab (2018).
</li>
<li id="ekman1986">Ekman, P., Friesen, W. V. “A new pan-cultural facial expression of emotion” , _Motivation and Emotion_ 10: 159–168.<a href="https://doi.org/10.1007/BF00992253">https://doi.org/10.1007/BF00992253</a>(1986).
</li>
<li id="han2014">Han, H., Jain, A. K. “Age, gender and race estimation from unconstrained face images” , Dept. Comput. Sci. Eng., Michigan State Univ., East Lansing, MI, USA, MSU Tech. Rep.(MSU-CSE-14-5). (2014).
</li>
<li id="han2015">Han, H., Otto, C., Liu, X., Jain, A. K. “Demographic estimation from face images: Human vs. machine performance”  _IEEE Transactions on Pattern Analysis & Machine Intelligence_ : 1148–1161. (2015).
</li>
<li id="hipp2013">Hipp, J. A., Adlakha, D., Gernes, R., Kargol, A., Pless, R. “Do you see what I see: crowdsource annotation of captured scenes”  _SenseCam 2013 Proceedings the 4th International SenseCam & Pervasive Imaging Conference_ , San Diego, California, November 2013: 24–25.<a href="https://doi.org/10.1145/2526667.2526671">https://doi.org/10.1145/2526667.2526671</a>(2013).
</li>
<li id="hipp2015">Hipp, J. A., Manteiga, A., Burgess, A., Stylianou, A., Pless, R. “Cameras and crowds in transportation tracking”  _WH 2015 Proceedings of the conference on Wireless Health_ , Bethesda, Maryland October 2015: 1–8.<a href="https://doi.org/10.1145/2811780.2811941">https://doi.org/10.1145/2811780.2811941</a>(2015).
</li>
<li id="hochman2016">Hochman, N., Manovich, L., Chow, J. _Phototrails: Visualizing 2.3 M Instagram photos from 13 global cities_ . URL<a href="http://lab.culturalanalytics.info/2016/04/phototrails-visualizing-23-m-instagram.html">http://lab.culturalanalytics.info/2016/04/phototrails-visualizing-23-m-instagram.html</a>(accessed 12.30.16).
</li>
<li id="irani2015">Irani, L. “The cultural work of microwork” , New Media & Society 17: 720–739.<a href="https://doi.org/10.1177/1461444813511926">https://doi.org/10.1177/1461444813511926</a>(2015).
</li>
<li id="james2014">James, J. Data Never Sleeps 2.0 | Domo. URL<a href="https://www.domo.com/blog/data-never-sleeps-2-0/">https://www.domo.com/blog/data-never-sleeps-2-0/</a>(accessed 9.20.18) (2014).
</li>
<li id="jofre2018">Jofre, A., Berardi, V., and Brennan, K.. “Time magazine archive: Annotating Faces, Visualizations, and Alternative Applications” (Workshop), _IPAM Culture Analytics Reunion Conference II_ , Lake Arrowhead, California, December. (2018).
</li>
<li id="jofre2020a">Jofre, A., Berardi, V., Bennett, C., Reale, M., Cole, J.. “Dataset: Faces extracted from _Time_ Magazine 1923-2014” , _Journal of Cultural Analytics_ . March 16, 2020,<a href="https://doi.org/10.22148/001c.12265">https://doi.org/10.22148/001c.12265</a>(2020)
</li>
<li id="jofre2020b">Jofre, A., Cole, J., Berardi, V., Bennett, C., Reale, M. “What’s in a Face? Gender representation of faces in _Time_ , 1940s-1990s” , _Journal of Cultural Analytics_ . March 16, 2020<a href="https://doi.org/10.22148/001c.12266">https://doi.org/10.22148/001c.12266</a>(2020)
</li>
<li id="king2016">King, L., Leonard, P. _Robots Reading Vogue_ . Yale DHLab. URL<a href="http://dh.library.yale.edu/projects/vogue/">http://dh.library.yale.edu/projects/vogue/</a>(accessed 11.8.16).
</li>
<li id="kittur2008">Kittur, A., Chi, E. H., Suh, B. “Crowdsourcing user studies with Mechanical Turk” . In: _CHI 2008 Proceedings of the SIGCHI Conference on Human Factors in Computing Systems_ , Florence, Italy, April 2008: 453-456.<a href="https://doi.org/10.1145/1357054.1357127">https://doi.org/10.1145/1357054.1357127</a>(2008).
</li>
<li id="kuang2015">Kuang, J., Argo, L., Stoddard, G., Bray, B. E., Zeng-Treitler, Q. “Assessing Pictograph Recognition: A Comparison of Crowdsourcing and Traditional Survey Approaches” . _J Med Internet Res_ 17.<a href="https://doi.org/10.2196/jmir.4582">https://doi.org/10.2196/jmir.4582</a>(2015).
</li>
<li id="lecun2015">LeCun, Y., Bengio, Y., Hinton, G. “Deep learning” , _Nature_ 521: 436–444.<a href="https://doi.org/10.1038/nature14539">https://doi.org/10.1038/nature14539</a>(2015).
</li>
<li id="lin2017">Lin, T., Goyal, P, Girshick, R., He, K., & Dollar, P. “Focal Loss for Dense Object Detection” , _The IEEE International Conference on Computer Vision (ICCV)_ , October.<a href="https://arxiv.org/abs/1708.02002">https://arxiv.org/abs/1708.02002</a>(2017).
</li>
<li id="lin2018">Lin, T., Goyal, P, Girshick, R., He, K., & Dollar, P. GitHub Repository.<a href="https://github.com/fizyr/keras-retinanet">https://github.com/fizyr/keras-retinanet</a>(accessed 2018).
</li>
<li id="leonard2018">Leonard, P., Duhaime, D. _Yale DHLab - Neural Neighbors: Capturing Image Similarity_ . Yale DHLab. URL<a href="http://dhlab.yale.edu/projects/neural_neighbors.html">http://dhlab.yale.edu/projects/neural_neighbors.html</a>(accessed 9.14.18).
</li>
<li id="lookmagazine2012">Look Magazine Photograph Collection, Library of Congress, Prints & Photographs Division. Library of Congress, Washington, D.C. 20540 USA. URL<a href="https://www.loc.gov/collections/look-magazine/about-this-collection/">https://www.loc.gov/collections/look-magazine/about-this-collection/</a>(accessed 9.20.18).
</li>
<li id="malli2018">Malli, R.C., Suri A., & Ramírez S. Github Repository<a href="https://github.com/rcmalli/keras-vggface">https://github.com/rcmalli/keras-vggface</a>(accessed 2018).
</li>
<li id="manovich2009">Manovich, L., and Douglass, J. Timeline: 4535 _Time_ magazine covers, 1923-2009.<a href="https://www.flickr.com/photos/culturevis/3951496507/">https://www.flickr.com/photos/culturevis/3951496507/</a>(2009).
</li>
<li id="manovich2016">Manovich, L., Stefaner, M., Yazdani, M., Baur, D., Goddemeyer, D., Tifentale, A., Chow, J. selfiecity, selfiecity. URL<a href="http://selfiecity.net/">http://selfiecity.net/</a>(accessed 12.30.16).
</li>
<li id="nowak2010">Nowak, S., Rüger, S. “How reliable are annotations via crowdsourcing: a study about inter-annotator agreement for multi-label image annotation”  _MIR 2010 Proceedings of the international conference on Multimedia information retrieval_ , Philadelphia, Pennsylvania, March 2010: 557-566.<a href="https://doi.org/10.1145/1743384.1743478">https://doi.org/10.1145/1743384.1743478</a>(2010).
</li>
<li id="nypllabs2018">NYPL Labs. The New York Public Library. URL<a href="https://www.nypl.org/collections/labs">https://www.nypl.org/collections/labs</a>(accessed 9.14.18).
</li>
<li id="nyplmap2018">NYPL Map Warper: Home. URL<a href="http://maps.nypl.org/warper">http://maps.nypl.org/warper</a>(accessed 9.14.18).
</li>
<li id="opencv">OpenCV - CiteOpenCV - OpenCV DevZone. URL<a href="http://code.opencv.org/projects/opencv/wiki/CiteOpenCV">http://code.opencv.org/projects/opencv/wiki/CiteOpenCV</a>(accessed 1.5.17).
</li>
<li id="organisciak2012">Organisciak, P., Efron, M., Fenlon, K., Senseney, M. “Evaluating rater quality and rating difficulty in online annotation activities” , _Proceedings of the American Society for Information Science and Technology_ 49(1): 1-10.<a href="https://doi.org/10.1002/meet.14504901166">https://doi.org/10.1002/meet.14504901166</a>(2012).
</li>
<li id="parkhi2015">Parkhi, O. M., Vedaldi, A., Zisserman, A. “Deep Face Recognition”  _Proceedings of the British Machine Vision Conference 2015_ , Swansea, UK, September 2015: 41.1-41.12.<a href="https://doi.org/10.5244/C.29.41">https://doi.org/10.5244/C.29.41</a>(2015).
</li>
<li id="prendergast1986">Prendergast, C., and Colvin, G. _The world of Time Inc.: The intimate history of a changing enterprise_ , Volume 3: 1960-1980. New York (1986).
</li>
<li id="silberman2018">Silberman, M. S., Tomlinson, B., LaPlante, R., Ross, J., Irani, L., Zaldivar, A. “Responsible research with crowds: pay crowdworkers at least minimum wage” , _Communications of the ACM_ 61: 39–41.<a href="https://doi.org/10.1145/3180492">https://doi.org/10.1145/3180492</a>(2018).
</li>
<li id="tomnod2018">Tomnod, Tomnod. URL<a href="https://www.tomnod.com">https://www.tomnod.com</a>(accessed 9.17.18).
</li>
<li id="wang2011">Wang, J., Ipeirotis, P.G., Provost, F. “Managing Crowdsourcing Workers”  _The 2011 Winter Conference on Business Intelligence_ , Salt Lake City, Utah: 10-12. (2011).
</li>
<li id="welinder2010">Welinder, P., Perona, P. “Online crowdsourcing: Rating annotators and obtaining cost-effective labels”  _2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition – Workshops_ , San Francisco, California, June 2010: 25–32.<a href="https://doi.org/10.1109/CVPRW.2010.5543189">https://doi.org/10.1109/CVPRW.2010.5543189</a>(2010).
</li>
<li id="whitehill2009">Whitehill, J., Ruvolo, P., Wu, T., Bergsma, J., Movellan, J. “Whose vote should count more: Optimal integration of labels from labelers of unknown expertise” , _Advances in Neural Information Processing Systems_ 22: 2035-2043. (2009).
</li>
<li id="yu2013">Yu, B., Willis, M., Sun, P., Wang, J. “Crowdsourcing Participatory Evaluation of Medical Pictograms Using Amazon Mechanical Turk” , _J Med Internet Res_ 15.<a href="https://doi.org/10.2196/jmir.2513">https://doi.org/10.2196/jmir.2513</a>(2013).
</li>
</ul>
]]></content></entry><entry><title type="html">Curadoria Digital e Custos – Exploração de abordagens e perceções</title><link href="https://rlskoeser.github.io/dhqwords/vol/14/2/000459/?utm_source=atom_feed" rel="alternate" type="text/html"/><link href="https://rlskoeser.github.io/dhqwords/pt/vol/14/2/000459/?utm_source=atom_feed" rel="alternate" type="text/html" hreflang="pt"/><id>https://rlskoeser.github.io/dhqwords/vol/14/2/000459/</id><author><name>Luís Corujo</name></author><author><name>Jorge Revez</name></author><author><name>Carlos Guardado da Silva</name></author><published>2020-06-19T00:00:00+00:00</published><updated>2023-08-06T15:53:44-04:00</updated><content type="html"><![CDATA[<h2 id="note-on-translation">Note on Translation</h2>
<p>For articles in languages other than English, DHQ provides an English-language abstract to support searching and discovery, and to enable those not fluent in the article&rsquo;s original language to get a basic understanding of its contents. In many cases, machine translation may be helpful for those seeking more detailed access. While DHQ does not typically have the resources to translate articles in full, we welcome contributions of effort from readers. If you are interested in translating any article into another language, please contact us at <a href="mailto:editors@digitalhumanities.org">editors@digitalhumanities.org</a> and we will be happy to work with you.</p>
<h2 id="bibliography">Bibliography</h2>
<ul>
<li id="project4c2013">4C Project 2013, _4C Project Glossary_ , 4C Project - Colaboration to Clarify the Costs of Curation. Glossary, viewed 4 March 2019,<a href="http://www.4cproject.eu/community-resources/glossary/">http://www.4cproject.eu/community-resources/glossary/</a>.
</li>
<li id="abbott2008">Abbott, D 2008, _DCC Briefing Paper: What is digital curation?_ , Digital Curation Centre, Edinburgo.
</li>
<li id="akers2014">Akers, K & Green, J 2014, “Towards a Symbiotic Relationship Between Academic Libraries and Disciplinary Data Repositories: A Dryad and University of Michigan Case Study” , _International Journal of Digital Curation_ , vol. 9, no. 1, pp. 119–131.
</li>
<li id="atkins2003">Atkins, DE 2003, _Revolutionizing science and engineering through cyberinfrastructure: report of the National Science Foundation Blue-Ribbon Advisory Panel on Cyberinfrastructure_ , National Science Foundation, Arlington, VA.
</li>
<li id="ayris2009">Ayris, P 2009, “LIBER’s Involvement in Supporting Digital Preservation in Member Libraries” , _Liber Quarterly: The Journal of European Research Libraries_ , vol. 19, no. 1, pp. 22–43.
</li>
<li id="bachell2014">Bachell, A & Barr, M 2014, “Video Game Preservation in the UK: A Survey of Records Management Practices” , _International Journal of Digital Curation Volume_ , vol. 9, no. 2, pp. 139–170.
</li>
<li id="bardin2011">Bardin, L 2011, _Análise de Conteúdo_ , Edições 70, São Paulo.
</li>
<li id="barros2014">Barros, N 2014, “Apropriação da curadoria na web por uma empresa de mídia tradicional : um caso de convergência entre narrativa e banco de dados” , Unicamp, Campinas.
</li>
<li id="beagrie2006">Beagrie, N 2006, “Digital Curation for Science, Digital Libraries, and Individuals” , _International Journal of Digital Curation, Vol 1, Iss 1, Pp 3-16 (2006)_ , no. 1, p. 3.
</li>
<li id="beagrie2002">Beagrie, N & Pothen, P 2002, “Digital Curation: Digital Archives, Libraries and e-Science Seminar” , _Ariadne_ , no. 30, viewed 14 March 2018,<a href="http://www.ariadne.ac.uk/issue30/digital-curation">http://www.ariadne.ac.uk/issue30/digital-curation</a>.
</li>
<li id="benardou2010">Benardou, A, Constantopoulos, P, Dallas, C & Gavrilis, D 2010, “Understanding the Information Requirements of Arts and Humanities Scholarship” , _International Journal of Digital Curation_ , vol. 5, no. 1, pp. 18–33.
</li>
<li id="bicarregui2013">Bicarregui, J, Gray, N, Henderson, R, Jones, R, Lambert, S & Matthews, B 2013, “Data Management and Preservation Planning for Big Science” , _International Journal of Digital Curation_ , vol. 8, no. 1, pp. 29–41.
</li>
<li id="borgman2007">Borgman, CL 2007, _Scholarship in the digital age: information, infrastructure, and the Internet_ , MIT Press, Cambridge, Mass.
</li>
<li id="borgman2015">― 2015, _Big data, little data, no data: scholarship in the networked world_ , The MIT Press, Cambridge, Massachusetts.
</li>
<li id="borgmanetal2007">Borgman, CL, Wallis, JC & Enyedy, N 2007, “Little science confronts the data deluge: habitat ecology, embedded sensor networks, and digital libraries” , _International Journal on Digital Libraries_ , vol. 7, no. 1–2, pp. 17–30.
</li>
<li id="buckland2011">Buckland, M 2011, “Data Management as Bibliography” , _Bulletin of the American Society for Information Science & Technology_ , vol. 37, no. 6, pp. 34–37.
</li>
<li id="ccsds2002">CCSDS 2002, “Reference Model for an Open Archival Information System (OAIS)” , viewed 16 June 2016,<a href="http://nssdc.gsfc.nasa.gov/nost/isoas/us19/650x0_20010226rl.pdf">http://nssdc.gsfc.nasa.gov/nost/isoas/us19/650x0_20010226rl.pdf</a>.
</li>
<li id="constantopoulos2009">Constantopoulos, P, Dallas, C, Androutsopoulos, I, Angelis, S, Deligiannakis, A, Gavrilis, D, Kotidis, Y & Papatheodorou, C 2009, ‘DCC&U: An Extended Digital Curation Lifecycle Model’, _International Journal of Digital Curation_ , vol. 4, no. 1, pp. 34–45.
</li>
<li id="corujo2015">Corujo, L 2015, “Repositórios digitais e confiança: um exemplo de prevenção digital: o RODA” , Dissertação de Mestrado, Faculdade de Letras da Univerdidade de Lisboa, Lisboa, viewed 25 January 2018,<a href="http://repositorio.ul.pt/handle/10451/18109">http://repositorio.ul.pt/handle/10451/18109</a>.
</li>
<li id="corujo2016">Corujo, L, Silva, CG da & Revez, J 2016, “Digital Curation and Costs: Approaches and Perceptions” , in _TEEM’16 Proceedings_ , ACM, pp. 277–284.
</li>
<li id="cragin2010">Cragin, MH, Palmer, CL, Carlson, JR & Witt, M 2010, “Data sharing, small science and institutional repositories” , _Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences_ , vol. 368, no. 1926, pp. 4023–4038.
</li>
<li id="cruzmundet2015">Cruz Mundet, JR & Díez Carrera, C 2015, “El cálculo de costes de la preservación digital: un análisis de modelos” , _Anales de Documentación_ , vol. 18, no. 2, viewed 16 June 2016,<a href="http://revistas.um.es/analesdoc/article/view/228411">http://revistas.um.es/analesdoc/article/view/228411</a>.
</li>
<li id="currall2007">Currall, J, Johnson, C & McKinney, P 2007, “The world is all grown digital.... How shall a man persuade management what to do in such times?” , _International Journal of Digital Curation_ , vol. 2, no. 1, pp. 12–28.
</li>
<li id="dallas2016">Dallas, C 2016, “Digital Curation beyond the wild frontier : a Pragmatic Approach” , _Archival Science_ , no. 16, pp. 421–457.
</li>
<li id="dallas2009">Dallas, C & Doorn, P 2009, “Report on the Workshop on Digital Curation in the Human Sciences at ECDL 2009” , _D-Lib Magazine_ , vol. 15, no. 11/12.
</li>
<li id="davies2007">Davies, R, Ayris, P, McLeod, R, Shenton, H & Wheatly, P 2007, “How much does it cost? The LIFE Project - Costing Models for Digital Curation and Preservation” , _Liber Quarterly: The Journal of European Research Libraries_ , vol. 17, no. 3/4.
</li>
<li id="dcc2004">DCC 2004, _DCC Curation Lifecycle Model_ , DCC Curation Lifecycle Model, viewed 12 March 2018,<a href="http://www.dcc.ac.uk/resources/curation-lifecycle-model">http://www.dcc.ac.uk/resources/curation-lifecycle-model</a>.
</li>
<li id="dcc2014">― 2014, _Cost Model for Digital Preservation_ , DCC Resources - Cost Model for Digital Preservation, viewed 5 March 2019,<a href="http://www.dcc.ac.uk/resources/external/cost-model-digital-preservation">http://www.dcc.ac.uk/resources/external/cost-model-digital-preservation</a>.
</li>
<li id="delasalle2013">Delasalle, J 2013, “Research Data Management at the University of Warwick: recent steps towards a joined-up approach at a UK university.” , _Libreas Library Ideas_ , vol. 9, no. 2, pp. 97–105.
</li>
<li id="dillon2013">Dillon, C 2013, “The Research Library as Digital Curator at Virginia Tech.” , _College Undergraduate Libraries_ , vol. 20, no. 2, pp. 232–238.
</li>
<li id="donnelly2010">Donnelly, M, Jones, S & Pattenden-Fail, J 2010, “DMP Online: The Digital Curation Centre’s Web-based Tool for Creating, Maintaining and Exporting Data Management Plans” , _International Journal of Digital Curation_ , vol. 5, no. 1, pp. 187–193.
</li>
<li id="donnelly2011">Donnelly, M & North, R 2011, “The Milieu and the MESSAGE: Talking to Researchers about Data Curation Issues in a Large and Diverse e-Science Project” , _International Journal of Digital Curation_ , vol. 6, no. 1, pp. 32–44.
</li>
<li id="dürr2008">Dürr, R, Meer, K, Luxemburg, W & Dekker, R 2008, “Dataset Preservation for the Long Term: Results of the DareLux Project” , _International Journal of Digital Curation_ , vol. 3, no. 1, pp. 29–43.
</li>
<li id="edmond2015">Edmond, J & Garnet, V 2015, “APIs and Researchers: The Emperor’s New Clothes?” , _International Journal of Digital Curation_ , vol. 10, no. 1, pp. 287–297.
</li>
<li id="edwards2010">Edwards, PN 2010, _A vast machine: computer models, climate data, and the politics of global warming_ , MIT Press, Cambridge, Mass.
</li>
<li id="edwards2007">Edwards, PN, Jackson, SJ, Bowker, GC & Knobel, CP 2007, _Understanding Infrastructure: Dynamics, Tensions, and Design_ , University of Michigan, Ann Arbor, viewed 9 March 2019,<a href="http://hdl.handle.net/2027.42/49353">http://hdl.handle.net/2027.42/49353</a>.
</li>
<li id="edwards2011">Edwards, PN, Mayernik, MS, Batcheller, AL, Bowker, GC & Borgman, CL 2011, “Science friction: Data, metadata, and collaboration” , _Social Studies of Science_ , vol. 41, no. 5, pp. 667–690.
</li>
<li id="evans2014">Evans, T & Moore, R 2014, “The Use of PDF/A in Digital Archives: Study from Archaeology” , _International Journal of Digital Curation_ , vol. 9, no. 2, pp. 123–138.
</li>
<li id="faria2012">Faria, L & Ferreira, M 2012, “Plataforma de colaboração para custear a curadoria digital” , paper presented at 12.o Congresso Nacional BAD, Évora, APBAD.
</li>
<li id="faria2015">Faria, L & Ferreira, M 2015, “Plataforma de colaboração para custear a curadoria digital” , in _12_ o _Congresso Nacional BAD, 2015_ , Associação Portuguesa de Bibliotecários, Arquivistas e Documentalistas (APBAD), viewed 27 January 2018,<a href="http://repositorium.sdum.uminho.pt/handle/1822/37858">http://repositorium.sdum.uminho.pt/handle/1822/37858</a>.
</li>
<li id="ferreira2014">Ferreira, M, Faria, L & Silva, H 2014, _D2. 1: baseline study of stakeholder & stakeholder initiatives_ , 4C Project, viewed 16 June 2016,<a href="http://repositorium.sdum.uminho.pt/handle/1822/30903">http://repositorium.sdum.uminho.pt/handle/1822/30903</a>.
</li>
<li id="ferreira2012">Ferreira, M, Saraiva, R & Rodrigues, E 2012, _Estado da arte em preservação digital_ , Universidade do Minho, Braga.
</li>
<li id="flick2018">Flick, U 2018, _Doing grounded theory_ , U Flick (ed.), 2nd edition, The SAGE qualitative research kit, SAGE, Los Angeles London New Delhi Singapore Washington, DC Melbourne.
</li>
<li id="foster2013">Foster, I, Borgman, CL & Heidorn, PB 2013, _Empowering Long Tail Research_ , viewed 9 March 2019,<a href="https://sites.google.com/site/ieltrconcept/">https://sites.google.com/site/ieltrconcept/</a>.
</li>
<li id="glaser1965">Glaser, BG 1965, “The Constant Comparative Method of Qualitative Analysis” , _Social Problems_ , vol. 12, no. 4, pp. 436–445.
</li>
<li id="heidorn2011">Heidorn, PB 2011, “The Emerging Role of Libraries in Data Curation and E-science” , _Journal of Library Administration_ , vol. 51, no. 7/8, pp. 662–672.
</li>
<li id="hey2005">Hey, T & Trefethen, AE 2005, “Cyberinfrastructure for e-Science” , _Science_ , vol. 308, no. 5723, pp. 817–821.
</li>
<li id="higgins2011">Higgins, S 2011, “Digital Curation: The Emergence of a New Discipline” , _The International Journal of Digital Curation_ , vol. 6, no. 2, pp. 78–88.
</li>
<li id="kejser2011">Kejser, U, Nielsen, A & Thirifays, A 2011, “Cost Model for Digital Preservation: Cost of Digital Migration” , _International Journal of Digital Curation_ , vol. 6, no. 1, pp. 255–267.
</li>
<li id="kejser2014">Kejser, UB, Johansen, KHE, Thirifays, A, Nielsen, A, Wang, D, Strodl, S, Miksa, T, Davidson, J, McCann, P, Krupp, J & Tjalsma, H 2014, _D3.1 - Evaluation of Cost Models and Needs & Gaps Analysis. 4C Project_ , report, 30 June, viewed 27 January 2018,<a href="http://www.4cproject.eu/documents/D3.1_final_report_10May2014-v1.02.pdf">http://www.4cproject.eu/documents/D3.1_final_report_10May2014-v1.02.pdf</a>.
</li>
<li id="kilbride2014">Kilbride, W & Norris, S 2014, “Collaborating to Clarify the Cost of Curation” , _New Review of Information Networking_ , vol. 19, no. 1, pp. 44–49.
</li>
<li id="laney2001">Laney, D 2001, _3D Data Management: Controlling Data Volume, Velocity, and Variety_ , February, META Group, viewed 9 March 2019,<a href="https://blogs.gartner.com/doug-laney/files/2012/01/ad949-3D-Data-Management-Controlling-Data-Volume-Velocity-and-Variety.pdf">https://blogs.gartner.com/doug-laney/files/2012/01/ad949-3D-Data-Management-Controlling-Data-Volume-Velocity-and-Variety.pdf</a>.
</li>
<li id="lee2016">Lee, CA, Allard, S, McGovern, N & Bishop, A 2016, “Open Data Meets Digital Curation: An Investigation of Practices and Needs” , _International Journal of Digital Curation_ , vol. 11, no. 2, pp. 115–125.
</li>
<li id="lee2011">Lee, CA & Tibbo, H 2011, “Where’s the archivist in digital curation? Exploring the possibilities through a matrix of knowledge and skills” , _Archivaria_ , vol. 72, no. Fall 2011, pp. 123–168.
</li>
<li id="macdonald2003">Macdonald, A & Lord, P 2003, _Digital Data Curation Task Force Report of the Task Force Strategy Discussion Day_ , Digital Archiving Consultancy, London, p. 18, viewed 14 March 2018,<a href="https://www.webarchive.org.uk/wayback/archive/20140616043831/http://www.jisc.ac.uk/uploaded_documents/CurationTaskForceFinal1.pdf">https://www.webarchive.org.uk/wayback/archive/20140616043831/http://www.jisc.ac.uk/uploaded_documents/CurationTaskForceFinal1.pdf</a>.
</li>
<li id="machado2015">Machado, D 2015, “Dados de pesquisa em repositório institucional: o caso do Edinburgh DataShare” , Universidade Federal do Rio Grande do Sul, Porto Alegre.
</li>
<li id="machadoetal2015">Machado, H, Soares, P & Silva, T 2015, “Em busca duma anamnese universitária: a materialização do arquivo do ISSSL na Internet” , paper presented at 12.o Congresso Nacional BAD, Évora, APBAD.
</li>
<li id="manyika2013">Manyika, J, Chui, M, Farrell, D, Kuiken, SV, Groves, P & Doshi, EA 2013, _Open data: Unlocking innovation and performance with liquid information_ , McKinsey & Company, viewed 9 March 2019,<a href="https://www.mckinsey.com/business-functions/digital-mckinsey/our-insights/open-data-unlocking-innovation-and-performance-with-liquid-information">https://www.mckinsey.com/business-functions/digital-mckinsey/our-insights/open-data-unlocking-innovation-and-performance-with-liquid-information</a>.
</li>
<li id="mayer-schonberger2013">Mayer-Schonberger, V & Cukier, K 2013, _Big data: la revolución de los datos masivos_ , Turner, Madrid.
</li>
<li id="minor2010">Minor, D, Sutton, D, Kozbial, A, Westbrook, B, Burek, M & Smorul, M 2010, “Chronopolis Digital Preservation Network” , _International Journal of Digital Curation_ , vol. 5, no. 1, pp. 119–133.
</li>
<li id="molloy2011">Molloy, JC 2011, “The Open Knowledge Foundation: Open Data Means Better Science” , _PLoS Biology_ , vol. 9, no. 12, pp. 1–4.
</li>
<li id="mundet2015">Mundet, JRC & Carrera, CD 2015, “El cálculo de costes de la preservación digital: un análisis de modelos” , _Anales de Documentación_ , vol. 18, no. 2, viewed 27 January 2018,<a href="http://revistas.um.es/analesdoc/article/view/228411">http://revistas.um.es/analesdoc/article/view/228411</a>.
</li>
<li id="murray-rust2008">Murray-Rust, P 2008, “Open Data in Science” , _Serials Review_ , vol. 34, no. 1, pp. 52–64.
</li>
<li id="murray-rust2004">Murray-Rust, P & Rzepa, HS 2004, “The Next Big Thing: From Hypermedia to Datuments” , _Journal of Digital Information_ , vol. 5, no. 1, viewed 9 March 2019,<a href="https://journals.tdl.org/jodi/index.php/jodi/article/view/130">https://journals.tdl.org/jodi/index.php/jodi/article/view/130</a>.
</li>
<li id="ogburn2010">Ogburn, JL 2010, “The Imperative for Data Curation” , _portal: Libraries and the Academy_ , vol. 10, no. 2, pp. 241–246.
</li>
<li id="odc2013"> _Open Data Commons_ 2013, Open Data Commons, viewed 9 March 2019,<a href="https://opendatacommons.org/">https://opendatacommons.org/</a>.
</li>
<li id="ocde2007">Organização para a Cooperação e Desenvolvimento Económico 2007, _OECD Principles and Guidelines for Access to Research Data from Public Funding_ , OCDE, viewed 9 March 2019,<a href="http://www.oecd.org/sti/inno/38500813.pdf">http://www.oecd.org/sti/inno/38500813.pdf</a>.
</li>
<li id="oed"> _Oxford English Dictionary_ n.d., viewed 9 March 2019,<a href="http://www.oed.com/">http://www.oed.com/</a>.
</li>
<li id="poole2013">Poole, AH 2013, “Now is the Future Now? The Urgency of Digital Curation in the Digital Humanities” , _Digital Humanities Quarterly_ , vol. 007, no. 2.
</li>
<li id="poole2015">― 2015, “How has your science data grown? Digital curation and the human factor: a critical literature review” , _Archival Science_ , vol. 15, no. 1, pp. 101–139.
</li>
<li id="price1963">Price, DJ de S 1963, _Little science, big science_ , George B. Pegram lectures, Columbia University Press, New York.
</li>
<li id="queiroz2013">Queiroz, B 2013, “A preservação da informação na Universidade Federal de Goiás: uma proposta de curadoria digital” , Universidade Federal de Goiás, Goiâna.
</li>
<li id="ramos2012">Ramos, D 2012, “Anotações para a compreensão da atividade do “curador de informação digital” , in E Correa (ed.), _Curadoria Digital e o Campo da Comunicação_ , ECA-USP, São Paulo, pp. 11–21, viewed 16 June 2016,<a href="https://issuu.com/grupo-ecausp.com/docs/ebook_curadoria_digital_usp">https://issuu.com/grupo-ecausp.com/docs/ebook_curadoria_digital_usp</a>.
</li>
<li id="rice2013">Rice, R, Ekmekcioglu, Ç, Haywood, J, Jones, S, Lewis, S, Macdonald, S & Weir, T 2013, “Implementing the Research Data Management Policy: University of Edinburgh Roadmap” , _International Journal of Digital Curation_ , vol. 8, no. 2, pp. 194–204.
</li>
<li id="rodrigues2015">Rodrigues, A, Barbedo, F, Runa, L & Sant’Ana, M 2015, _Continuidade digital: relatório final do projecto_ , DGLAB, Lisboa.
</li>
<li id="rosenthal2013">Rosenthal, D & Vargas, D 2013, “Distributed Digital Preservation in the Cloud” , _International Journal of Digital Curation_ , vol. 8, no. 1, pp. 107–119.
</li>
<li id="rs2012">Royal Society (Reino Unido) & Policy Studies Unit 2012, _Science as an open enterprise._ , viewed 9 March 2019,<a href="https://royalsociety.org/~/media/Royal_Society_Content/policy/projects/sape/2012-06-20-SAOE.pdf">https://royalsociety.org/~/media/Royal_Society_Content/policy/projects/sape/2012-06-20-SAOE.pdf</a>.
</li>
<li id="rusbridge2007">Rusbridge, A & Ross, S 2007, “The UK LOCKSS Pilot Programme: A Perspective from the LOCKSS Technical Support Service” , _International Journal of Digital Curation_ , vol. 2, no. 2, pp. 111–122.
</li>
<li id="santos2014">Santos, T 2014, “Curadoria Digital: o conceito no período de 2000 a 2013” , Universidade de Brasília, Brasília.
</li>
<li id="saraiva2015">Saraiva, P & Quaresma, P 2015, “Bibliotecas Universitárias: tendências, modelos e competências” , paper presented at 12.o Congresso Nacional BAD, Évora, APBAD.
</li>
<li id="sayão2012">Sayão, L & Sales, L 2012, “Curadoria digital: um novo patamar para preservação de dados digitais de pesquisa” , _Informação & Sociedade: Estudos (I&S)_ , vol. 22, no. 3, pp. 179–191.
</li>
<li id="schroeder2014">Schroeder, R 2014, “Big Data: Towards a More Scientific Social Science and Humanities?” , in M Graham, WH Dutton & M Castells (eds), _Society and the internet: how networks of information and communication are changing our lives_ , First edition, Oxford University Press, Oxford ; New York, NY.
</li>
<li id="strasser2014">Strasser, C, Abrams, S & Cruse, P 2014, “DMPTool 2:Expanding Functionality for Better Data Management Planning” , _International Journal of Digital Curation_ , vol. 9, no. 1, pp. 324–330.
</li>
<li id="suchodoletz2013">Suchodoletz, D, Rechert, K & Valizada, I 2013, “Towards Emulation-as-a-Service: Cloud Services for Versatile Digital Object Access” , _International Journal of Digital Curation_ , vol. 8, no. 1, pp. 131–142.
</li>
<li id="taper2004">Taper, ML & Lele, S (eds) 2004, “Models of Scientific Inquiry and Statistical Practice: Implications for the Structure of Scientific Knowledge” , in _The nature of scientific evidence: statistical, philosophical, and empirical considerations_ , University of Chicago Press, Chicago, pp. 17–51.
</li>
<li id="thibodeau2002">Thibodeau, K 2002, “Overview of technological approaches to digital preservation and challenges in coming years” , in Council on Library and Information Resources & Institute for Information Science (eds), _The state of digital preservation: an international perspective ; conference proceedings, Documentation Abstracts, Inc. Institutes for Information Science, Washington, D.C., April 24-25, 2002_ , CLIR, Washington, D.C, viewed 5 February 2019,<a href="http://www.clir.org/pubs/reports/reports/pub107/pub107.pdf">http://www.clir.org/pubs/reports/reports/pub107/pub107.pdf</a>.
</li>
<li id="unsworth2006">Unsworth, J, Courant, P, Fraser, S, Goodchild, M, Hedstrom, M, Henry, C, Kaufman, PB, McGann, J, Rosenzweig, R & Zuckerman, B 2006, _Our Cultural Commonwealth: The report of the American Council of Learned Societies Commission on Cyberinfrastructure for the Humanities and Social Sciences_ , text, 13 December, ACLS: New York, viewed 9 March 2019,<a href="https://www.ideals.illinois.edu/handle/2142/189">https://www.ideals.illinois.edu/handle/2142/189</a>.
</li>
<li id="walters2011">Walters, T & Skinner, K 2011, _New roles for new times: Digital curation for preservation_ , Association of Research Libraries, viewed 16 June 2016,<a href="https://vtechworks.lib.vt.edu/handle/10919/10183">https://vtechworks.lib.vt.edu/handle/10919/10183</a>.
</li>
<li id="waters1996">Waters, D & Garrett, J 1996, _Preserving Digital Information. Report of the Task Force on Archiving of Digital Information._ , CPA/RLG, viewed 16 June 2016,<a href="http://eric.ed.gov/?id=ED395602">http://eric.ed.gov/?id=ED395602</a>.
</li>
<li id="weinberg1961">Weinberg, AM 1961, “Impact of Large-Scale Science on the United States: Big science is here to stay, but we have yet to make the hard financial and educational choices it imposes” , _Science (New York, N.Y.)_ , vol. 134, no. 3473, pp. 161–164.
</li>
<li id="whyte2011">Whyte, A & Pryor, G 2011, “Open Science in Practice: Researcher Perspectives and Participation” , _International Journal of Digital Curation Volume pp_ , vol. 6, no. 1, pp. 199–213.
</li>
<li id="wilson2013">Wilson, J & Jeffreys, P 2013, “Towards a Unified University Infrastructure: The Data Management Roll-Out at the University of Oxford” , _International Journal of Digital Curation_ , vol. 8, no. 1, pp. 235–246.
</li>
<li id="wright2009">Wright, R, Miller, A & Addis, M 2009, “The Significance of Storage in the “Cost of Risk” of Digital Preservation” , _International Journal of Digital Curation_ , vol. 4, no. 3, pp. 104–122.
</li>
</ul>
]]></content></entry><entry><title type="html">Digital Editions and Version Numbering</title><link href="https://rlskoeser.github.io/dhqwords/vol/14/2/000455/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://rlskoeser.github.io/dhqwords/vol/14/2/000455/</id><author><name>Paul A. Broyles</name></author><published>2020-06-19T00:00:00+00:00</published><updated>2023-08-06T15:53:44-04:00</updated><content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>Digital editions can change long after publication: errors can be corrected; new materials can be added; the scholarship can be updated. The fact that such changes can occur on an ongoing basis is both one of the great potentials and one of the great terrors of digital scholarly resources. Printed books are comparatively static; while students of bibliography know that changes to books can and do happen in the course of a print run, in most circumstances readers instinctively recognize G. Thomas Tanselle’s “central truth . . . that books are not meant to be unique items and are normally printed in runs of what purport to be duplicate copies” <a class="footnote-ref" href="#tanselle1980"> [tanselle1980] </a>.<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> Moreover, printed volumes are self-identical: a single copy of a book remains the same object and carries the same text unless acted upon by outside forces, like the environment, natural deterioration, or a human hand. Standards for scholarly citation, which solidified around print resources, take advantage of this objectual stability. Referencing a book means identifying its author and title, its edition number (if specified), and the details of its publication (perhaps including the year of its most recent printing or issue). Where the details of a particular copy matter (for instance for incunabula, or where the argument is bibliographic), the writer might go so far as to specify a library or archive and shelfmark. Armed with that information, readers can find and consult an appropriate copy.</p>
<p>By contrast, online digital resources can be expanded or corrected long after their initial release<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> — not necessarily by publishing a new edition that can occupy the shelf beside the previous, nor even through a stop-press correction that will affect volumes printed after it is made, but simply by updating some files on a webserver, with the result that anyone accessing the resource from that point on will see the revised form. Indeed, this mutability is one of the defining promises of digital textuality. Jerome McGann, in his influential essay “The Rationale of Hypertext” (first published online in 1995), contrasts the physical book, which “literally closes its covers on itself” when it is published, with the hypertext archive that “need never be complete ” and “will evolve and change over time, it will gather new bodies of material, its organizational substructures will get modified, perhaps quite drastically” <a class="footnote-ref" href="#mcgann1995"> [mcgann1995] </a><a class="footnote-ref" href="#mcgann1996"> [mcgann1996] </a>. Less poetically perhaps, but no less significantly, errors can be corrected with relative ease. And anyone who has tried to maintain a digital resource over any duration knows that, quite apart from willful content revisions, changes may not merely be possible but required in order to keep it operational.<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup></p>
<p>While the open-endedness of digital resources, the potential for evolution and infinite expansion, has excited scholars (the creators of large digital editorial projects among them), the inherent changeability of digital materials also poses threats to the scholarly ecosystem. Looking beyond the by now well-known problem oflink rot, in which online resources linked in references simply disappear from the internet, research on science communication has identified the problem ofcontext drift, in which links function but the content on the website has changed since it was referenced<a class="footnote-ref" href="#klein2014"> [klein2014] </a>; a study published in 2016 found that as many as 75% of webpages referenced in scholarly literature in Science, Technology, and Medicine have changed since they were cited<a class="footnote-ref" href="#jones2016"> [jones2016] </a>. Though results would likely be different if examining citations of digital scholarly editions, which are my concern in this article, the issue ofcontext drifthighlights the problems that digital mutability poses to the scholarly record. Indeed, the Committee on Scholarly Editions of the Modern Language Association (MLA CSE) has identified “the challenge of maintaining the scholarly ability to be referenced in view of the ways that interfaces change over time” as a central issue facing digital scholarly editions<a class="footnote-ref" href="#MLAC2016"> [MLAC2016] </a>.</p>
<p>In this article, I focus on digital scholarly editions, arguing that in order to make sure such editions are citeable and their history is intelligible, their creators and publishers must assign version numbers in tandem with any changes made to edition content. By digital scholarly editions, I refer to any electronic resources that encode textual objects for scholarly study.<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup> While the same considerations might apply to many kinds of digital scholarly resource, I choose to focus on digital editions for a few reasons. For one thing, perhaps more than other areas of digital scholarship in the humanities, digital scholarly editing constitutes a clear community of practice, with a longstanding tradition of editorial theory and a widely (though certainly not universally) shared technical standard in the form of the Guidelines of the Text Encoding Initiative (TEI). For another, the concern with textual histories within scholarly editing and other fields under the umbrella of textual scholarship suggests that editors of all people ought to be particularly attentive to the way textual resources transform in time.</p>
<p>But perhaps most significantly, digital editions occupy a hybrid position in the scholarly ecosystem that makes it especially important to be able to identify and track the changes they go through and the states in which they exist. Digital editions, as I understand them, are simultaneously scholarly publications and data sources. Like all scholarly editions, digital editions are a product of interpretation, scholarly judgment, and the imposition of codes and conventions onto the material being remediated; that is to say, they are works of scholarship, produced according to the research and critical judgment of their creators. Like other scholarly publications, digital editions can be formally vetted through peer review processes.<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup> And in general, digital editions are presented in user interfaces that support the reading and study of the provided text, rather than simply providing encoded files for a user to download.<sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup> But editions also provide data on multiple levels. Most basically, they provide texts corresponding to particular documents or works that scholars may reference and cite in publications, treating the edition as a surrogate for the object it edits and using the text it offers as a basis for analysis: that is, as data.<sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup> Humans are by no means the only potential consumers of the data embedded in digital editions. Texts and metadata can form raw material for analysis, including computer-aided study and incorporation into large corpora. And the dream of the fully networked digital edition, functionally integrated and cross-referenced by other editions and systems, grows ever more practical with the development of shared infrastructures and standards.<sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup> Digital editions, then, are simultaneously publications and data, potentially offering interfaces both to humans and to machines, and it is essential that these multiple consumers be able to understand the evolution of digital editions and precisely reference different states as editions are revised.</p>
<p>Version numbers, I argue, offer a simple and practical method not merely for identifying a state of a resource, but for communicating something of its history and the relationship among its states. Yet defined, citable version numbers still seem to be a rarity in the world of digital editions, and no consensus practice exists in the field regarding how different versions of an electronic textual resource should be identified, or what it is that version numbers should communicate.<sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup> Although textual scholarship has created sophisticated frameworks for understanding revision and the evolution of texts, I suggest that software developers have much to teach editors about versioning living resources in ongoing development and publication. This essay argues that a new version number should be attached each time an edition is updated, that version numbers should communicate something meaningful about the scope of changes to the resource, and that the encoded informational content of an edition should be versioned separately from the interfaces through which users access that information. After outlining considerations involved in assigning version numbers, I conclude with a case study of the development of a versioning policy for the <em>Piers Plowman Electronic Archive</em> , a longstanding scholarly resource that has published editions of multiple texts in evolving formats.</p>
<h2 id="approaches-to-change">Approaches to Change</h2>
<p>The fundamental changeability of digital resources, including digital editions, poses challenges to longstanding scholarly paradigms of authority and completeness. Kathleen Fitzpatrick has suggested that the capacity — indeed, in some circumstances, the necessity — for digital writing to change over time might suggest a fundamental change in our understanding of scholarly writing, from product to process<a class="footnote-ref" href="#fitzpatrick2011"> [fitzpatrick2011] </a>. But new paradigms only slowly beget the practices needed to support them. Paul Fyfe has argued that we have not sufficiently theorized how digital scholarship deals with the problem of error<a class="footnote-ref" href="#fyfe2012"> [fyfe2012] </a>. Correcting error is relatively simple, but scholarly practices surrounding correction lag behind.</p>
<p>For digital editions in particular, change is a double-edged sword. Although few editors would now claim to be producing “definitive editions,” the goal of any editor is presumably to produce an accurate text representing the document or work being edited according to that editor’s theory of the object of study.<sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup> Thus, the ability to incorporate corrections and continuously present the most accurate possible text in one sense enhances the reliability and scholarly value of digital editions by contrast to print, where errors discovered after publication can be corrected only in later printings or by issuing errata. But that same flexibility underscores the need to be able to clearly identify particular states of the resource. Consider a scholar who bases an argument on a particular reading taken from a diplomatic text presented in a digital edition. The editors later discover that they have made an error in their transcription and update the edition. Without a way to identify the specific state of the resource when it was cited, the error may appear to have been the scholar’s, and the scholarly record is muddled. Similarly, archivists seeking to preserve a digital edition can more effectively capture its history if the resource clearly signals when changes occur. And computer systems that ingest and process data from digital editions (for instance aggregating texts from multiple publications, or analyzing the text of an edition and recording statistical information in a database) have the same needs as human researchers: to know in what form they have accessed a resource and when changes have occurred. Citation styles, clerical practices, and technical measures have all attempted to offer solutions to the problem of digital change, but I argue that explicit versioning of resources can more effectively meet the needs of digital reference.</p>
<p>When citation guides were first faced with the problem of the mutability of digital resources, some suggested that researchers citing online publications should include in their citations the date on which they accessed the material<a class="footnote-ref" href="#gibaldi1995"> [gibaldi1995] </a><a class="footnote-ref" href="#turabian1996"> [turabian1996] </a><a class="footnote-ref" href="#gibaldi1998"> [gibaldi1998] </a><a class="footnote-ref" href="#PM2001"> [PM2001] </a>.<sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup> Access dates recognize that online resources evolve, but they are concerned with a researcher’s activity (visiting a website) rather than with the resource itself. Unless a resource happens to have been archived on that particular day, a date of access does not point to a particular form of the material (and there is, of course, no guarantee that the resource did not change later on the day it was cited). <em>The Chicago Manual of Style</em> accordingly suggests that “access dates in online citations are of limited value” and does not recommend including them in citations<a class="footnote-ref" href="#CM2017"> [CM2017] </a>. And for computer systems interacting with a resource, recording the date of last access does nothing to determine whether it has changed since that last access.</p>
<p>Another way of dealing with the problem of mutating resources — the method most endorsed by the TEI, and the most widely used way in which digital editing projects appear to deal with textual change (see<a href="#appendix1">Appendix</a>) — is manually creating a log of revisions. Attaching revision metadata to files allow records of revisions to be closely associated with the files themselves, though this information does not make it possible to reference particular states of the text. The TEI Guidelines provide the XML element <code>&lt;revisionDesc&gt;</code> in the header of each file to record narrative explanations of changes and the reasons and agents behind them in individual <code>&lt;change&gt;</code> elements associated with each revision<a class="footnote-ref" href="#TEI2019"> [TEI2019] </a>. Because change logs are simply written records of modifications, they are not tied to the TEI, or to any particular metadata format. The <em>Walt Whitman Archive</em> &rsquo;s<a class="footnote-ref" href="#folsomprice"> [folsomprice] </a>, for example, maintains a public change log in the form of a blog that provides clear descriptions of modifications to the <em>Archive</em> , from corrections of typos to pervasive metadata updates<a class="footnote-ref" href="#WWAC2019"> [WWAC2019] </a>.<sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup> Individual XML files also carry the TEI <code>&lt;revisionDesc&gt;</code> element. <em>The Whitman Archive</em> ’s approach models thoroughness and transparency in disclosing ongoing modifications to a digital resource. But the <em>Archive</em> largely obscures these revision histories from users of the site. The change blog is hosted at a different web domain from the <em>Archive</em> itself, and the revision lists embedded in file metadata are not displayed in the reading interface provided on its website — probably the context in which most users will encounter the texts. In contrast, the <em>William Blake Archive</em> <a class="footnote-ref" href="#eaves2017"> [eaves2017] </a>extracts this revision history and presents it in a human–readable format in an Electronic Edition Information section associated with each object in its collection. This section of the display makes the file history more directly available to readers conducting research within the <em>Archive</em> and conceivably allows readers to cite the date of the last revision, but still does not supply a specific identifier pointing unambiguously to a particular state of the file. The many editorial projects that use change logs store and expose that information in a wide variety of ways, but share an interest in recording what kinds of changes were made, when, and (often) by whom — without necessarily offering a way to reference a state of the resource resulting from a particular set of changes.</p>
<p>Nor do change logs offer any way to get back to prior versions of a resource; a user can understand what has changed, but not access an earlier form. The increasing embrace of revision control systems (RCSs) such as Git in the digital humanities has suggested the possibility of automated, systematized methods for tracking revision history and providing access to specific states of a project or file.<sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup> Elena Pierazzo proposes that RCSs should be embedded in digital edition software, exposing the evolution of an edition and providing access to previous states<a class="footnote-ref" href="#pierazzo2015"> [pierazzo2015] </a>.<sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup> Wiki-based editions, such as <em>A Social Edition of the Devonshire MS</em> <a class="footnote-ref" href="#siemens"> [siemens] </a>, are one existing model enacting Pierazzo’s hope for editions with built-in RCSs. Christian Wittern goes even further, suggesting that distributed RCSs such as Git might furnish a new ecosystem for scholarly publishing of digital editions, allowing the maintenance of fine-grained revision histories as well as the coexistence of multiple revisions of a single file carried out by different scholars<a class="footnote-ref" href="#wittern2013"> [wittern2013] </a>.</p>
<p>RCSs make file history accessible, but do not necessarily identify or make intelligible meaningful developmental stages. While different RCSs provide different features, broadly, they operate by storing the content of each file as well as a precise record of each change made to any file. As a result, all changes are reversible, and it is possible to retrieve any previous state of a file as it was stored in the RCS repository, as well as any previous state of the repository as a whole. In order to facilitate retrieving earlier states, RCSs do (unlike change logs) provide unambiguous identifiers for a particular state of a file. In Git, for instance, each commit has an associated hash: a cryptographically generated key that can be used to identify and retrieve a particular state of the repository. A particular version of a file, or of the whole project, can thus be identified through an associated hash. However, they are not necessarily meaningful to human users. Git hashes, produced using the SHA-1 algorithm, take the form of forty-digit hexadecimal numbers (usually cited only by their first few digits). The hashes of successive commits bear no visible relationship to each other; indeed, given two hashes but no access to the repository containing the data, it is not possible to determine which represents the more recent state of the data. Other RCSs use different mechanisms, some of which are more straightforwardly numeric, but identifiers within RCSs are inevitably tied to the details of the system and may not correspond to human editors’ understanding of their processes. They cease to identify states of a file or resource if that file is archived elsewhere, or even if the project migrates to a new RCS.<sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup></p>
<p>Revision control is an important tool for data management in the digital humanities. But explicit, deliberate versioning of data should go beyond recording revision history or providing an arbitrary identifier for a particular state of a file (bound to a specific RCS). Versioning should communicate information that helps both humans and computers understand how that version relates to others and the context in which users should approach it. Assigning version numbers to digital editions would permit humans and computer systems not only to refer to a particular state of the edition, but to understand the relationship between any two copies. Adopting clear versioning practices aids both the preservation and the reuse of data, and the producers of digital editions can benefit from practices developed both in the fields of textual scholarship and software development in producing useful version numbers for digital editions.</p>
<p>The problem of versioning data is by no means unique to digital editions; it is a pressing issue of research data management and publication across disciplines. The W3C recommendation on “Data on the Web Best Practices” highlights the importance of versioning data and specifically indicates the value of standardized, meaningful version numbers that not only identify versions, but suggest how they differ<a class="footnote-ref" href="#lóscio2017"> [lóscio2017] </a>. But despite increasing recognition of the importance of clearly versioning research data, standard practices around versioning have yet to cohere in the research data community; a guide to data versioning from the Australian National Data Service is replete with language like “no agreed standard or recommendation” and “no one way” <a class="footnote-ref" href="#ANDS"> [ANDS] </a>. Still, emerging data infrastructures support a move toward more transparent and explicit versioning. For instance, the research data repository Zenodo introduced support for versioned Digital Object Identifiers (DOIs) in 2017, allowing depositors to update their data and permitting researchers to cite both specific versions and a whole concept independent of version<a class="footnote-ref" href="#nielsen2017"> [nielsen2017] </a>.<sup id="fnref:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup> Particularly within the Open Science movement, the growing emphasis on data publication has translated into attention on data versioning.</p>
<p>However, the textual digital humanities, and digital editing in particular, have been slower to adopt versioning practices. Editors’ awareness of the problem of textual variance should make them more attuned to the need to track and publicize the evolution of their own editions. The practices of textual criticism point to the value of developing versioning protocols for digital editions.</p>
<h2 id="textual-versioning">Textual Versioning</h2>
<p>When textual scholars use the termversion, they mean something different from (though related to) the way the term is used in software development. Because my argument that digital editions need versioning policies lies at the intersection of these fields, it will be useful to survey the ways in which the two fields think about versioning. Literary scholars and textual critics might speak of the Quarto and First Folio versions of Shakespeare’s <em>King Lear</em> , or the A, B, and C versions of the fourteenth-century alliterative poem <em>Piers Plowman</em> — or to distinct draft versions produced during the course of Thoreau’s revision of the single manuscript of <em>Walden</em> . Software developers (or users), on the other hand, might refer to version 5.2.1 of the Linux kernel, or to Apple’s iOS 12.4 (eliding the wordversionentirely). Do these concepts have anything to do with each other? Though their orientation is different — textual scholarship is focused on historical analysis, software development on ongoing maintenance, publication, and support — both fields share a common concern with making it easier to understand variation and evolution, a concern likewise relevant to the problem of changes in digital editions.</p>
<p>Version, as used by textual critics and editors, generally denotes a distinct state of a work or a document that has transformed in time. Literary works exist in different versions because of alterations during the course of their composition and transmission — alterations by the author or by someone else, willed or unwilled. So, a campaign of authorial revision of a work would produce a new version of that work, as might the copying of a medieval manuscript in which a scribe introduced changes (even unintentional ones), or the publication of an expurgated edition long after an author’s death. These versions have independent value as forms in which creators conceived and audiences encountered the work. Donald H. Reiman in 1987 argued for what he called “versioning” , as a counterpoint to editing: rather than producing complicated, expensive critical editions, he suggests, it may be more productive to publish accessible texts of major forms in which a work existed, such as important editions and authorial manuscripts, allowing readers to compare the texts themselves<a class="footnote-ref" href="#reiman1987"> [reiman1987] </a>. (The profusion of digital documentary editions suggests that Reiman’s dream is increasingly being realized.)<sup id="fnref:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup></p>
<p>Both genetic critics and those concerned with thesociology of the texthave emphasized the coherence and vitality of individual versions of developing works, pointing to the inadequacy of the notion of final authorial intention and calling for editorial and critical engagement with versions as coherent units.<sup id="fnref:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup> Hans Zeller argued that individual variants in witnesses to a work cannot be considered in isolation, as had been common under the principles of eclectic editing; rather, we must recognize “the relationship of its elements to one another and to the whole, and therefore to what constitutes a text as a text, to what makes it into a particular version” <a class="footnote-ref" href="#zeller1975"> [zeller1975] </a>. Peter L. Shillingsburg identifies the concept of version as “a means of classifying copies of a Work according to one or more concepts that help account for the variant texts or variant formats that characterize them” <a class="footnote-ref" href="#shillingsburg1991"> [shillingsburg1991] </a>. A version is thus a concept, not a thing; it is distinct from any physical embodiment (which might not represent it reliably), and versions come into being through the act of reading, as readers create them to organize textual variants<a class="footnote-ref" href="#shillingsburg1991"> [shillingsburg1991] </a>. John Bryant, articulating his concept of the “fluid text” defined by the flow among different versions, echoes the notion of versions as “critical constructs” but also emphasizes their relationality: all versions exist in relation to other versions; they come into being through revision (which may or may not be intended); they are “pulsings of . . . collective energy” that can involve both authors and the editorial and cultural forces surrounding and following them; they have their own conceptions of the work and speak to their own readerships<a class="footnote-ref" href="#bryant2002"> [bryant2002] </a>. While these and other theories of the concept of version differ on points such as the precise degree, nature, and agency of the changes that can produce a new version, they share a sense that versions are distinct and alive, and their coexistence is part of what constitutes a work.</p>
<p>These sophisticated frameworks for textual change may seem far from the problems of labeling changes as a digital edition is revised, and from the straightforward numerical approaches that I will draw from software development. But textual-critical accounts of versioning remind us that readers (and, we might add for our purposes, machines) encounter individual texts as coherent units, and these discrete forms have existence and meaning independent of the work as a whole. The kind of versioning this article focuses on is not teasing out key moments in the life of a work that is the object of study, but identifying moments of change in the evolving life of the published edition. In other words, echoing Hans Walter Gabler’s understanding of the contents of an edition, this article is concerned with versioning the editor’s text and the editorial discourses attached to it<a class="footnote-ref" href="#gabler2010"> [gabler2010] </a>. An edition might present one or more versions of a work or of a document (indeed, the ability to present more versions in more dynamic forms has long been heralded as one of the most exciting potentials of digital editions), but what I address in this article is the need to version that edition as an edition, to keep track of the changes that occur within the edition itself.<sup id="fnref:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup> Versioning, as I use the term, means assigning version identifiers to public materials as they develop; it is a publication practice rather than a critical practice.</p>
<p>That leaves the practical problem (unresolved by textual-critical theories of versions, which focus on more complicated analyses) of how to communicate the state of the edition to its users, whether humans or software programs. Existing publishing practices a not great help. Print publication simply has not developed conventions for describing ongoing revisions. Minor errors discovered after printing might be dealt with by issuing a list of errata; a more thorough revision might occasion the publication of a new edition. This publishing logic features in the closest the TEI Guidelines come to addressing the versioning of texts. The <code>&lt;editionStmt&gt;</code> section of the TEI header groups together information about an “edition” of a TEI-encoded text<a class="footnote-ref" href="#TEI2019"> [TEI2019] </a>. The Guidelines link the intellectual foundations of the concept of edition to the idea of amaster copy, while simultaneously noting that the concept does not really apply to electronic texts. Nevertheless, the primacy of the print concept of edition leads the Guidelines to distinguish between “substantive changes” (such as the encoding of new information throughout the file) and “minor changes . . . which do not amount to a new edition” (such as error corrections or conversation between encodings) — a distinction that the Guidelines themselves acknowledge to be somewhat arbitrary and subjective. These “minor changes” can be recorded in <code>&lt;revisionDesc&gt;</code> , but there is no mechanism for labeling them. Confusing the issue still more, the Guidelines treat edition as synonymous with version, level, and release, while using the terms revision and update for minor changes below the level of edition. Finally, the Guidelines offer two rather different ways of recording version information in the same element. The edition (or version) can be recorded either descriptively, with a phrase likenew editionas the content of the <code>&lt;edition&gt;</code> element, or with a “formal identification (such as a version number) for the edition” in the <code>@n</code> attribute. The Guidelines introduce a concept broadly similar to the print concept of edition, but one that lacks the technical underpinnings (the setting of type) that gave the concept its meaning in print, and that lacks the expressive power for dealing with digital textuality in a comprehensive way.</p>
<h2 id="technical-versioning">Technical Versioning</h2>
<p>The shortcomings of the TEI’s print-inspired model suggest that we might look elsewhere for models to describe changes in computer-encoded data files with sufficient granularity. The field of software development has, over a period of decades, developed software version numbers as a system of practice for tracking the development of complex, digital objects — pieces of software — as they are published and revised. Software version numbers also situate the objects they describe in their developmental histories, but from the inside: rather than analytically describing objects after the fact, they are assigned during the development and release process to track ongoing work. Software version numbers facilitate many kinds of reference: they track changes to a piece of software, help users know when updates are available, facilitate technical support by unambiguously identifying a particular state of that software with all its particularities, and promote interoperability by allowing computer programs to determine whether they are compatible. Despite its straighforwardness, software versioning is a rich signifying practice, and it offers a model that suggests practical solutions for editors of digital editions.</p>
<p>Version numbers, at their most basic, delineate stages in the development of an object — for instance, a piece of software — by quantifying them and assigning ordered numbers to the object. It would be possible in principle to use a single whole number, which increases with every change. However, this approach, which fails to distinguish the scope of the changes that have been made, is insufficient for dealing with complex software objects. It is instead common practice to subdivide the version number into parts according to the scale of the difference from what has come before. The most common approach is to segment the version number, using a period to divide the parts. Version numbers with either two or three segments are common. A piece of software with version 2.7.4 would thus signify major version 2, minor version 7, revision or patch 4.<sup id="fnref:20"><a href="#fn:20" class="footnote-ref" role="doc-noteref">20</a></sup> (The meanings of the first two numbers are typically major and minor version; what, exactly, later numbers communicate is less consistent, though they often indicate small revisions intended to fix errors without adding features or altering behavior.)</p>
<p>The meanings of these sequences are not fixed; different software creators are free to construct their version numbers in different ways, and there are no universal criteria for distinguishing major and minor releases — although some recent efforts, which I will discuss, have attempted to make version numbers more systematically intelligible. But broadly, major version releases are likely to introduce significant changes to a product: for instance, a new user interface, a large set of new capabilities, or technical changes that make files produced with the new version incompatible with previous versions. Minor versions might introduce features that do not substantially alter the nature of the product, or correct problems that have been discovered. Smaller releases, like patches, are likely to fix individual errors.</p>
<p>This way of conceptualizing versions is at heart hierarchical, with each level in effectcontainingthose below it. In general, bumping the version number at any level resets all the levels below it to zero, so that, for instance, the major release that follows 2.4.7 is given version number 3.0.0. Conceptually, the life of a major version consists of all the releases under that major version number, not just the original point zero release. This hierarchy roughly parallels the way the edition–impression–issue–state model subdivides the bibliographic object (see<a href="#bowers2005">Bowers (2005), 37-42, 406-411</a>;<a href="#tanselle1975">Tanselle (1975)</a>). An edition, in bibliographical terms, is created whenever a given text is typeset; setting new type constitutes a fundamental change in the essence of the object even if the text remains unchanged. A new edition is a kind of new major version, an object that on some level shares identity with what came before but also represents a significant break. Other categories are grouped beneath this, expressing different levels of identity change with an edition subdivided into impressions (the copies printed at once) and impressions divided into issues (copies intended as a unit of sale) (see<a href="#tanselle1975">Tanselle (1975), 28n14</a>). Sheets of books even getpatches, changes correcting individual errors; in his attempts to distinguish issue from state, Fredson Bowers suggests that minor textual corrections, along with small supplements, produce only new states and not new issues because they are simply “delayed attempts to construct an ideal copy ” , much as software patches do not seek to extend functionality or change intended behavior but merely make the software conform to existing expectations<a class="footnote-ref" href="#bowers2005"> [bowers2005] </a>.</p>
<p>The point, of course, is not that software versioning and bibliographic description map the same procedures to different media. Each practice is informed by different practical needs, disciplinary contexts, and underlying technologies. Rather, I wish to point to a broad correspondence in approach between the two procedures, even though they bear different relationships to their subject matter: both organize intellectual objects hierarchically, categorizing and subdividing around questions of essential identity and of imagined ideal state.</p>
<p>But bibliographic classification, as an analytical practice, is rooted in the evidence of specific changes. Software versioning, by contrast, has been accused of being arbitrary and inconsistent — and at times of being driven by market forces rather than technological logic. A few efforts to make versioning practices more consistently meaningful help clarify what version numbers can actually assert about an object.</p>
<h2 id="calendar-versioning">Calendar Versioning</h2>
<p>One approach to versioning software, which has been called Calendar Versioning, highlights the temporality of releases<a class="footnote-ref" href="#hashemi2019"> [hashemi2019] </a>.<sup id="fnref:21"><a href="#fn:21" class="footnote-ref" role="doc-noteref">21</a></sup> This approach recognizes that knowing when a software object was released may be the most important way to identify and evaluate it. Microsoft has offered the most widely visible version of this practice, with releases like Windows 95, 98, and 2000. (It is worth noting, however, that these are merely public release names and the software actually carries a different version number distinct from the release name.) But a variety of other software uses Calendar Versioning in less dramatic ways: the Ubuntu Linux distribution, for instance, offers what look like fairly traditional version numbers, but the first segment of the version numbers is the last two digits of the current year, followed by the month, so that as of the time of writing, the most recent version (released in April 2019) is 19.04. This approach has appealed to at least one digital editor; the texts edited by Jeffrey C. Witt from Peter Plaoul’s commentary on Peter Lombard’s Sentences carry version numbers that employ a form of Calendar Versioning, as detailed in<a href="#appendix1">the Appendix</a>.</p>
<p>In emphasizing date as what identifies an object, calendar versioning resembles scholarly citation practices, which emphasize publication dates, and sometimes access dates — although calendrical version numbers clearly and uniquely identify particular resource states, as access dates do not. Calendar Versioning privileges temporal sequence above all else; it suggests that when an object was produced is the most salient information for assessing it. It also establishes a sequence of versions, chiefly by relating them in time. Thus, Calendar Versioning is effective for allowing users to assess the age of a particular resource, to understand which versions were produced earlier and later, and to determine whether a more recent version is available. But it does not indicate not scope: it is impossible to tell from version numbers alone whether two versions are differentiated by the correction of a minor error or by a significant overhaul.</p>
<h2 id="semantic-versioning">Semantic Versioning</h2>
<p>Even generic versioning practices have tended to make the degree of difference among versions greater than in Calendar Versioning, differentiating major and minor versions according to the relative degree of change. However, these approaches have appeared inconsistent to some critics: different developers or companies make different decisions regarding what constitutes minor and major versions, and these decisions are sometimes driven by market forces as a new major version might generate excitement or drive customers to upgrade. The Semantic Versioning specification, created by Tom Preston-Werner, is an attempt to specify rigorously and technically what version numbers (or, more precisely, what changes in version numbers) actually mean<a class="footnote-ref" href="#preston-werner"> [preston-werner] </a>.<sup id="fnref:22"><a href="#fn:22" class="footnote-ref" role="doc-noteref">22</a></sup> I will dwell slightly longer on Semantic Versioning, because it has provoked a debate that exposes a fundamental question not merely of how versions should be identified but what versioning is for — a debate that helps expose for the creators of digital editions the role that versioning practices might play in communicating with the public and interfacing with larger systems.</p>
<p>Semantic versioning is based on the traditional [major].[minor].[patch] format, but attempts to codify something largely implicit but inconsistently practiced in community practices for giving version numbers to software: that the different portions of a version number reflect different kinds of change. Semantic versioning is most concerned with libraries and packages (that is, pieces of software designed to be used by other pieces of software), and specifically with what are called their APIs (Applications Programming Interfaces): the formal methods through which other programs interact with the package. (It is worth noting, however, that the Semantic Versioning specification is itself semantically versioned; the application of these principles is not restricted to packages or libraries.)</p>
<p>Semantic Versioning is primarily concerned with whether changes to a package break backwards compatibility. That is, have you changed the way your API works so that the same command, issued to the new version, will produce different results? The central principle is that any breaking change to the API (that is, one that will cause the same command to have different results) is a new major version. A release that adds new functions while maintaining backwards compatibility is a new minor version, while a patch version is one that simply fixes bugs, provided the fix does not break backwards compatibility. Semantic versioning is designed especially for use with package managers, programs that can automate procuring and updating the packages needed to build or run a piece of software.</p>
<p>Semantic Versioning sits especially uneasily at the intersection of intellectual and mechanistic understandings of versioning. Jeremy Ashkenas, an influential JavaScript developer and vocal critic of Semantic Versioning, argues that the system “prioritize[s] a mechanistic understanding of a codebase over a human one. . . . It’s alright for robots, but bad for us” <a class="footnote-ref" href="#ashkenas2015"> [ashkenas2015] </a>.<sup id="fnref:23"><a href="#fn:23" class="footnote-ref" role="doc-noteref">23</a></sup> Ashkenas suggests that, in an environment where other developers may write source code relying on a project’s bugs, the definition of abreakingchange is subjective — a point others contest. Perhaps most significantly for Ashkenas and other detractors, small function changes might under Semantic Versioning require an increase in the major version number (say, from 2.3.1 to 3.0.0) — a change that implies a major rethinking of the software that may not, in fact, exist (and can cause version numbers to balloon). Ashkenas agitates in favor of what others disparagingly callSentimental Versioningand he playfully labels “Romantic Versioning” : a system under which a developer’s understanding of the magnitude of the change and the relationship between versions defines the version number.<sup id="fnref:24"><a href="#fn:24" class="footnote-ref" role="doc-noteref">24</a></sup></p>
<p>The crux of the debate around Ashkenas’s rejection of Semantic Versioning, which riled a community of developers whose projects were affected by an update that Ashkenas declined to label a new major version, is whether version numbers are intended for human or machine consumption. Software processes that decide whether it is safe to update a given library do not care what a developer’s sense of the change is; humans, on the other hand, may be misled by seeing a major release that actually consists of a conceptually minor change.</p>
<p>Why should scholars at the intersection of physical book study and digital scholarship be concerned with a four-year-old squabble among software developers, much of which involved how developer practices integrate with automated systems? The Semantic Versioning debate is particularly interesting for digital textuality because it draws attention to the different kinds of weight that version information can carry, and the different systems into which it integrates. Software developer Niels Roesen Abildgaard has attempted to nuance the Semantic Versioning debate by suggesting that software exists on a continuum between interfaces directly with users and interfaces exclusively with other software; user-facing software, like games or (to a lesser extent) desktop applications, is most suitable for Romantic Versioning, since human understanding is paramount, while software libraries would benefit from Semantic Versioning because relatively few humans will look at them directly, but they will often be included in other software systems<a class="footnote-ref" href="#abilgaard2015"> [abilgaard2015] </a>. The Semantic/Romantic debate draws our attention to the fact that version numbers provide an interface for understanding software changes, and that this interface is conditioned by purpose and audience: a key insight for considering how users of digital editions might interact with version numbers and what information they can convey.</p>
<p>The focus on versioning as a communicative interface, designed to work in a system with a clear audience to satisfy a defined purpose, helps us understand the complexity of digital editions as objects to be versioned. Digital editions operate within multiple systems at once. They are typically created first and foremost as objects for <em>reading</em> , to be studied closely by individuals. They are also sources of data, furnishing both character data and metadata that can be manipulated and analyzed in a variety of ways. And they are objects of citation, which must be unambiguously referenced in scholarly environments. McDonough et al. point out that people using and analyzing digital objects for different purposes may have profoundly different (though interrelated) needs in terms of how they are categorized<a class="footnote-ref" href="#mcdonough2010"> [mcdonough2010] </a>.</p>
<p>Moreover, digital editions are complex, layered objects. At base, they consist of one or more transcriptions or constructed texts, which may have been collated or further analyzed to produce altered texts. In most cases, these texts have been encoded using a markup language to identify features, define structure, and incorporate metadata; at the level of information, they are accompanied by products of scholarly analysis, such as a critical apparatus and various annotations. And in most cases, they are accessed through a software interface for reading, which may well be unique to the edition or project in question, and perhaps through APIs that provide data upon request. Even if the content of the edition began life encapsulated in a manageable format like a single XML file, the reading interface will encompass a multitude of files and technologies, like CSS and JavaScript files executed on a user’s computer and other processes, such as XSLT transformations, that may occur on a server entirely out of a user’s sight (so that the user may not even directly receive the underlying data files without requesting them). Any APIs the edition provides will operate similarly, extracting and transforming data to answer the requests it receives.</p>
<h2 id="describing-electronic-literature">Describing Electronic Literature</h2>
<p>Given the complexity of digital editions as textual objects, one place we might turn for more robust ways to describe them as temporal, bibliographic objects is work done by electronic literature scholars in classifying and categorizing their materials. (Digital editions are indeed a form of electronic literature, albeit one that has not attracted much study outside the field of editorial theory.) Matthew Kirschenbaum in a 2002 article postulated a set of terms for describing first-generation electronic objects inspired by Bowers’s classic bibliographical typology<a class="footnote-ref" href="#kirschenbaum2002"> [kirschenbaum2002] </a>. Layer, version, and release refer to the whole software object — another hierarchy. Layer refers to a whole integrated environment of software and data; adding a brand new software interface, for example, might constitute a new layer.Versionis somewhat subordinate to layer and describes the life sequence of the software; a new layer creates a new major version, while refining an existing layer creates a new minor version.Releaseseems to be primarily a matter of distribution channel: releases are “computationally compatible . . . but . . . not functionally integrated” , and Kirschenbaum’s example is of a work released both online and on CD-ROM (presumably with the same underlying software)<a class="footnote-ref" href="#kirschenbaum2002"> [kirschenbaum2002] </a>.</p>
<p>Within the total software object so described are individual objects — individual digital entities. Kirschenbaum offers a file as an example of an object, but it is worth noting that Kirschenbaum’s objects are independent of the data format in which they are stored. These are described bystates: “the computational composition of an object in some particular data format.” For example, separate PNG and JPEG files representing the same image are different states of the same underlying object. Instance exists at the interplay between state and the software environment in which it operates: an image displayed in a particular program, which might (intentionally or inadvertently) render it differently from other programs. And finally, there iscopy, a single instance of a state of an object, for example, the copy of an image that a web browser downloads and stores on a user’s computer (as distinct from the copy on the server).</p>
<p>I rehearse this categorization at length because it represents a particularly thorough and robust attempt to think through the distinctive properties of electronic objects, and it, too, points to some of the properties we must consider when evaluating digital editions. Kirschenbaum’s seven-part system is certainly too detailed and cumbersome to be used as a versioning system in itself — though a refined version, adapted for the era of networked publishing, might ultimately prove valuable when scholars of future decades write bibliographic accounts of digital editions. But his approach might suggest what sorts of features versioning needs to account for.</p>
<p>From the perspective of digital editions, the central insight of Kirschenbaum’s proposal is his distinction between the whole software environment and the individual components that compose it. An electronic text, consisting of both data and a technical environment in which that data is remediated for a reader, cannot usefully be described in total; media objects simultaneously precede their instantiation in a particular technical environment and become entangled in the systems that display them. Kirschenbaum applied this schema to works of electronic literature; those discussed in his account appear to have evolved in relatively well-defined, separable releases that can be thought of as an issue of all parts at once. His descriptive vocabulary seems to reflect this tendency: terms addressing the whole software environment are concerned with evolution, while those concerned with individual objects are concerned with instantiation.</p>
<p>This particular division would work well for describing digital editions of the CD-ROM era, where the production of physical copies created a distinct issue of the whole, including both data and display software. But versioning all the parts together appears less appropriate for thecontinuous publishingpractices of the web era, where individual components (and most importantly individual documents) may be updated independently, not to mention the prevalence of digital editions in large archives containing many documents, and even in semi-distributed systems like Jeffrey Witt’s Scholastic Commentary and Text Archive (SCTA), which promises to aggregate related, interoperable editions<a class="footnote-ref" href="#witt"> [witt] </a><a class="footnote-ref" href="#witt2018"> [witt2018] </a>. If Peter Boot and Joris van Zundert are correct that distributed, networked systems combining many data sources and services are the future of digital editions (the digital edition 2.0, as they call it), versioning all the components of an edition as a single unit may well become completely impossible<a class="footnote-ref" href="#boot2011"> [boot2011] </a>.</p>
<h2 id="versioning-object-and-environment">Versioning Object and Environment</h2>
<p>Accordingly, rather than versioning whole systems, we should offer separate treatment of objects and environments.Objects, here, are the edition content: the texts or other resources being presented online, as represented in the edition ( <em>not</em> the physical objects being edited).Environmentdescribes the whole system within which these objects are rendered and consumed: a web of server-side and client-side electronic processes that work in tandem with a user’s local computer environment to display an edition, or that provide data to other systems upon request.<sup id="fnref:25"><a href="#fn:25" class="footnote-ref" role="doc-noteref">25</a></sup> Put another way: objects are the underlying data, the textual and editorial content that editors create and incorporate into the edition, regardless of the specifics (technical or visual) of its realization. Environment encompasses the interfaces through which that data is made available to users (the visual layout of an edition on the screen, APIs that permit machine-driven access to edition data), as well as the software environments that enable these forms of access.<sup id="fnref:26"><a href="#fn:26" class="footnote-ref" role="doc-noteref">26</a></sup></p>
<p>In arguing for the separation of object from interface, I do not mean to imply that interfaces aremeretechnical contributions, separate from the intellectual work of editing.<sup id="fnref:27"><a href="#fn:27" class="footnote-ref" role="doc-noteref">27</a></sup> Nor am I suggesting that a reader or user of an edition can experience content in some pure way, uninflected by the way it is presented. The layout (interface) even of traditional print editions constitutes an argument about the material and its character<a class="footnote-ref" href="#eggert2013"> [eggert2013] </a>. The vaunted flexibility of digital editions means an edition may contain and present its material in multiple ways, indeed, through quite different interfaces, yet these interfaces will inescapably condition the material and make arguments about understanding it<a class="footnote-ref" href="#andrews2018"> [andrews2018] </a>. Certainly, the form in which a text is encountered conditions understandings of that text, and citing the environment in which it is encountered will be necessary both to understanding conclusions drawn from the edition and to recognizing intellectual contributions to it.<sup id="fnref:28"><a href="#fn:28" class="footnote-ref" role="doc-noteref">28</a></sup></p>
<p>Despite the entanglement of content and presentation for scholarly understanding, versioning objects separately from their environments has both intellectual and practical benefits. One influential idea in software design, and key principle of the modern web, is the separation of form and content: the principle that documents should be encoded according to their underlying structural logic, without intermixing instructions regarding how that content is to be displayed.<sup id="fnref:29"><a href="#fn:29" class="footnote-ref" role="doc-noteref">29</a></sup> The separation of form and content has understandably been drawn into question by scholars gesturing toward the outpouring of work on the materiality of text.<sup id="fnref:30"><a href="#fn:30" class="footnote-ref" role="doc-noteref">30</a></sup> Nevertheless, this distinction operates at a technical level in many digital editions, and offers a model by which digital editions may be implemented and preserved. The TEI guidelines, perhaps the most common standard for textual encoding of digital editions (see<a href="#franzini2019">Franzini et al. (2019), 16</a>), endorse and support the separation of form and content.<sup id="fnref:31"><a href="#fn:31" class="footnote-ref" role="doc-noteref">31</a></sup> C. M. Sperberg-McQueen has gone so far as to suggest that it is a best practice for digital editions to provide multiple interfaces, not just to support multiple ways of interacting with the text but also to force editors to make sure they are not basing their encoding on desired display rather than the logic of the content<a class="footnote-ref" href="#sperberg-mcqueen2009"> [sperberg-mcqueen2009] </a>. While others might argue for tighter control over the presentation of an edition as an editorial responsibility,<sup id="fnref:32"><a href="#fn:32" class="footnote-ref" role="doc-noteref">32</a></sup> the edition content and presentation are still technically and intellectually separable even when they are thought of as forming a single intellectual unit. Put another way: Sperberg-McQueen distinguishes among the infinite array of facts concerning a particular text, the selection of facts that are contained as information within a particular edition, and the presentation of those facts, for instance through arrangement on the screen. The selection of facts — the total information content available in an edition, whether exposed in a particular form or not — exists, as encoded data, apart from the mechanisms that present those facts, even where the selection of facts and the development of the user interface have informed each other and where they are intended to go together<a class="footnote-ref" href="#sperberg-mcqueen2009"> [sperberg-mcqueen2009] </a>.<sup id="fnref:33"><a href="#fn:33" class="footnote-ref" role="doc-noteref">33</a></sup> Unless a creator goes to extreme lengths, against all norms of software design, to create a boutique piece of software in which data and display are fully entangled, it is likely that any digital edition (regardless of the standards or ad-hoc principles followed) will contain data objects that can be meaningfully versioned apart from their display systems.</p>
<p>Moreover, at a practical level, objects and environments are likely to evolve separately, both before and after an edition is published. An editor who learns of an error in a reading can correct it by making a change in a data file; in many online publishing environments, no further action will be required for the correction to appear in the edition.<sup id="fnref:34"><a href="#fn:34" class="footnote-ref" role="doc-noteref">34</a></sup> Similarly, the developers and maintainers of digital editions can often make changes from tweaking the text styling to rearranging the graphical user interface to adding major new features for textual analysis without altering the data files. Versioning data objects can also aid preservation. Because the underlying data in most edition objects is at heart textual, data files can be relatively easily archived in repositories designed for storing texts, like the Oxford Text Archive<a class="footnote-ref" href="#OTA"> [OTA] </a>and TextGrid<a class="footnote-ref" href="#TGC2006"> [TGC2006] </a>. Depositing an edition’s data is not the same as preserving the edition, and work on the preservation of the interfaces should continue (informed by work in the field of software preservation), but such deposits can help allow the labor represented by an edition to live on as data even if its software becomes inaccessible.<sup id="fnref:35"><a href="#fn:35" class="footnote-ref" role="doc-noteref">35</a></sup></p>
<p>As publishing practices evolve, being able to refer to objects separate from their environments may increasingly become a practical necessity. Boot and van Zundert’s vision of a networked, distributed “digital edition 2.0” involves bringing data together with services offered by different providers, and they explicitly argue that editions should not provide their own “advanced services” <a class="footnote-ref" href="#boot2011"> [boot2011] </a>. Users of digital editions may already be prepared to work with data apart from interfaces; in a recent survey about digital editions, a majority of respondents rated the ability to download and reuse data from editions “very important” <a class="footnote-ref" href="#franzini2019"> [franzini2019] </a>. Even without such a shared infrastructure, thinking separately about object and interface helps prepare us for the future agitated for by Peter Robinson, where digital editors abandon the practice of providing their own interfaces and leave textual display to others<a class="footnote-ref" href="#robinson2013"> [robinson2013] </a>. Increasingly, those working in the field of digital editing recognize the value of publication frameworks and software packages that allow editors to present their work without having to develop entirely new software.<sup id="fnref:36"><a href="#fn:36" class="footnote-ref" role="doc-noteref">36</a></sup> Alternative environments need not be software systems; as discussed below, the <em>Piers Plowman Electronic Archive</em> has begun publishing printed volumes produced from its XML data, providing paper-based access to the edition. Versioning objects and environment separately means that our versioning practices can recognize the intellectual identity between an encoded document (for example, an XML file in the TEI vocabulary) and its rendering (for example, its rendering as an HTML page as a result of an XSLT transformation). They remain the same object, even if the mediating layers change.<sup id="fnref:37"><a href="#fn:37" class="footnote-ref" role="doc-noteref">37</a></sup></p>
<p>Although the remainder of this article focuses on versioning data objects, versioning software environments (including the web platforms that display digital editions) is also important for the future health of the digital scholarly ecosystem, and should be an area of further work for the field. A FORCE11 working group has emphasized the importance of scholars citing the software they use in their research, for reasons of credit, provenance, and reproducibility, and has indicated that one goal of software citation should be to identify and facilitate access to a specific version of the software<a class="footnote-ref" href="#smith2016"> [smith2016] </a>. Although the primary focus of software citation movements has been software executed locally by researchers, such principles might furnish a starting place for citation of web-based environments in which the contents of digital editions are accessed. Publishers of digital editions might facilitate such citations by assigning their online software platforms specific version numbers, incremented with every update, even when the web interface is specific to a single project. Researchers citing a digital edition might then cite both the data underlying the edition and the platform in which they accessed the edition. Publishers might also consider making the source code of online platforms available under open source licenses, potentially enabling future researchers to recreate an earlier version of an online platform that has since been updated or discontinued. Of course, such steps are at best partial. The ways in which a user experiences the data mediated by an online edition platform depend not merely on website code, but on underlying elements of the web architecture (such as the specific versions of software running on the web server) and on features of a user’s own computer, such as operating system, web browser, and specific settings. Research into the preservation and curation of software as a part of the scholarly record is ongoing, and as the field of digital editing and publication continues to mature, it will need to become involved in these broader conversations.<sup id="fnref:38"><a href="#fn:38" class="footnote-ref" role="doc-noteref">38</a></sup></p>
<p>But there is lower-hanging fruit for editors and publishers of scholarly editions, who have yet to develop standards for the comparatively straightforward versioning of edition contents, standards that would benefit the field of scholarly editing. Versioning the contents of digital editions would represent a significant step forward for citeability and preservation of the scholarly record even while difficult issues regarding software environments await future work. We can, and should, version the data objects that form the information content of our digital editions, starting now.</p>
<h2 id="developing-versioning-protocols-for-_piers-plowman-electronic-archive_-data-objects">Developing Versioning Protocols for <em>Piers Plowman Electronic Archive</em> Data Objects</h2>
<p>I will turn now to a case study based on my work in creating a formal versioning policy for the <em>Piers Plowman Electronic Archive</em> (PPEA)<a class="footnote-ref" href="#duggan2019"> [duggan2019] </a>, an open-access online resource that aims to document the complete medieval and early modern textual tradition of the Middle English alliterative poem <em>Piers Plowman</em> through TEI-encoded documentary editions of individual witnesses and critical editions of archetypal texts. This long-running project, which began in 1987, demonstrates both the need for and the challenge of clear versioning practices.<sup id="fnref:39"><a href="#fn:39" class="footnote-ref" role="doc-noteref">39</a></sup></p>
<p>The first seven PPEA editions were published on CD-ROM, from 2000 to 2011, in separate partnerships between the Society for Early English and Norse Electronic Texts (SEENET) and the University of Michigan Press, Boydell and Brewer, and the Medieval Academy of America. The first two CD-ROMs were encoded in SGML presented using the proprietary Multidoc Pro SGML browser; later editions were encoded in XML and published using software that ran within a web browser. In 2014, all texts were made openly available online, in a new web interface created by the Institute for Advanced Technology in the Humanities at the University of Virginia. The new online <em>Archive</em> saw the release of previously unpublished editions; older editions were updated to XML conforming to the P4 version of the TEI guidelines. Since 2014, intermittent changes have been made to the appearance and function of the web editions. Forthcoming updates will create additional versions of existing texts: the <em>Archive</em> is in the process of updating its texts to TEI P5, and the newly launched PPEA in Print series publishes print volumes derived from electronic texts.<sup id="fnref:40"><a href="#fn:40" class="footnote-ref" role="doc-noteref">40</a></sup></p>
<p>In addition to changes in medium, file format, and technical infrastructure, the PPEA, like any project of its age and scope, has had to deal with errors in its materials. The web versions of the texts were updated to correct known errors. These changes were not explicitly recognized on the pages for the text. For texts originally published on CD-ROM, the website used to provide Errata lists recording corrections to the CD-ROM texts. However, these lists are no longer maintained given the age of the CD-ROMs, and Errata lists were never created for texts first published online. The corrections made to files spanned a wide range of types and significances, including changes to the format of line numbers (but not the lineation), minor changes to markup unlikely to affect the output on the screen, and the correction of textual errors.</p>
<p>As part of a CLIR Postdoctoral Fellowship in Data Curation for Medieval Studies at the North Carolina State University Libraries, I set out to create standards for assigning version numbers to texts. My primary goals were (1) to allow users of the <em>Archive</em> to record and cite unambiguously which version of a text they consulted; (2) to permit previously published versions of texts to be archived and retrieved; (3) to make the history of a given text legible; and (4) to allow users with references to two versions of the same text to have a basic understanding of the relationship between them. From the start, I was concerned only with versioning published resources; while we might use prerelease identifiers to track the evolution of unpublished resources internally, what I sought to define was how we would assign version numbers to editions beginning at the moment of their publication and encompassing all successive published changes.<sup id="fnref:41"><a href="#fn:41" class="footnote-ref" role="doc-noteref">41</a></sup></p>
<p>A few fundamental decisions guided my work. One early, crucial question was what resource was actually being versioned. First, guided by Kirschenbaum’s work, I concluded that edition content and the way that content is displayed cannot be described by the same version numbers. While versioning our display software is a long-term desideratum, my immediate goal was to version editions’ informational content. Accordingly, any version numbers we provided would have to refer to the source files for an edition — in this case, the TEI-encoded XML — rather than to its rendered text. The decision to privilege the XML made sense as the XML files can be easily archived, and because it recognizes the markup of an edition as a significant intellectual product. Versioning the XML files also allows us to link them with any derivatives produced from them: derivatives which can include not just electronic renderings but print volumes.<sup id="fnref:42"><a href="#fn:42" class="footnote-ref" role="doc-noteref">42</a></sup> For instance, editions published in the PPEA in Print series carry a statement on the copyright page declaring the version of the XML files to which the print text corresponds.</p>
<p>Choosing XML files as the objects of versioning has additional consequences. The component files of an edition will be versioned separately. Each full edition consists of, at minimum, separate XML files for the introduction and the edited text. If the versioned objects are XML files, a change to the text does not affect the status of the introduction. Even though the PPEA conceives of each edition as a single coherent publication, and they are peer reviewed as integral wholes, they are made up of separate data sources whose version histories must be managed independently. (This is a more practical approach than creating data packages versioned as a single unit because it allows us to include a file’s version number within the file itself without having to modify files that have not otherwise changed.)</p>
<p>One more question concerned what resources must actually be versioned. The PPEA website contains many pages with background information and supplementary resources that are not part of individual editions — some of which, such as site credits, may change frequently. Further, editions include files such as prefaces that are not necessarily advancing the same sort of scholarly claims. At least for the time being, I decided to version only content subject to peer review, meaning the text, apparatus, and introductions of editions.</p>
<p>Establishing some priority of changes that gives a sense of their scope is essential to make version numbers useful. Thus I sought to distinguish changes on the grounds of their scale and significance.<sup id="fnref:43"><a href="#fn:43" class="footnote-ref" role="doc-noteref">43</a></sup> Changes that systematically affected the editorial or markup approaches to a file seemed to constitute a highest level of change. A file’s markup might change completely — it might, indeed, be recreated from the ground up in a new format (for instance in HTML rather than XML) — without any differences being visible to users of the edition. However, given the intellectual significance of the way a text is marked up, these two files would be radically different from each other as data. Accordingly, I reasoned, the conversion of a file from one markup language to another (from SGML to XML, or between major versions of an encoding scheme, like the transition from TEI P4 to P5) would constitute a major release (at the highest level of versioning), because even if the intent is to keep the textual content the same, the different affordances of different file formats and encoding schemes mean that the nature of the file has fundamentally changed.<sup id="fnref:44"><a href="#fn:44" class="footnote-ref" role="doc-noteref">44</a></sup> A file modified in this way is incompatible with previous versions in a concrete sense, because changed elements and structure mean that the files can no longer be compared directly to each other by analytical tools that process the underlying XML, and software that worked with earlier versions may not display it successfully. (However, minor changes to how data is stored or expressed, like a switch between minor versions of TEI or a change in character encoding from ISO-8859-1 to UTF-8, maintain the fundamental identity of the file and do not rise to the level of a major release.) Similarly, systematic editorial revisions to a file, I suggested, would constitute a new major release, because they represent a far-reaching editorial reassessment that disrupts intellectual continuity with the existing version. In its relationship to preceding material, a major release is in some ways comparable to a new edition of a print book (marked by a new setting of type), or to a significant version in the text-critical sense.</p>
<p>Since one of the central goals of a digital edition is to present one or more texts, any changes to readings are necessarily significant. I therefore proposed that individual changes to the text that do not rise to the level of systematic revision might constitute a middle level, less high than systematic changes but greater than other forms of change. The concept of apatch, a change intended only to correct an error and restore expected behavior, does not apply to an edition, because edition contents may have been used as the grounds for scholarly argument and the change from a mistaken reading to a correct one may thus have great scholarly significance. If changes to text are regarded as the more significant form of local change, then changes to paratext, including editorial content, might be at the lowest level. These three levels of change seemed well suited to the common three-level version number format. I therefore initially proposed that version numbers take the following form: [systematic changes to encoding or editing].[changes to text].[changes to paratext]. I outlined the meanings of these segments as follows:</p>
<ul>
<li>The first segment, <em>systematic changes</em> , would increase when we make a large number of changes systematically across the text that have a significant effect on its markup or on the how it is edited as a whole.</li>
<li>The second segment, <em>changes to text</em> , would increase when we make any change to our representation of what is on the page. Most obviously this includes alterations of readings, but it also includes highlighting and other features present in the source document.</li>
<li>The third segment, <em>changes to paratext</em> , would increase when we make changes to paratextual content that is not in the source document, such as editorial notes and apparatus.</li>
</ul>
<p>This proposal sparked discussion with other project leaders. One specific point of debate was the extent to which version numbers should reflect the file history. Following the conventions of software versioning (and of bibliographic classification), I proposed that when any segment of the version number changed, all segments to the right should revert to zero. (So, for instance, version 2.1.3 might be followed by 3.0.0.) Our discussion raised the possibility that this practice hid file history, as after a systematic change it would no longer be clear how many changes to text or paratext had occurred. An alternate proposal was that each segment would increment independently without being reset, so that the number of changes of each type would be permanently visible. That alternative proposal raised its own complications. For one, it deviates from the practices typical of software version numbering, and so would likely prove confusing to users: the practice of zeroing-out later segments is culturally familiar not just from annual demands that we upgrade our phone operating systems, but from its cultural currency in the form of phrases likeweb 2.0. In addition, it creates a false impression of precision, because any number of changes might be bundled into a single update to the file. (For instance, a single update might include three separate alterations to the text and two to the paratext, but the final two segments of the version number would each increase only by one, concealing the actual number of changes.) And there is in any case a hierarchical bibliographical logic to the major version’s resetting the clock on other forms of revision: if a major version compares to a new edition, such a significant change establishes a new baseline against which less significant changes can be measured going forward.</p>
<p>Perhaps the most troubling aspect of three-level version numbers from the perspective of the PPEA, however, was the realization that not all files that need to be versioned can have changes at three levels. Files consisting purely of editorial material, such as introductions, do not have distinct textual and paratextual content in the manner of edited texts, and so version numbers would have to express all changes to these files in either the text or paratext segment, with the other segment remaining permanently at zero (or omitted entirely). The idea of versioning different XML files according to different principles, or of having a version number segment that would always remain zero for some files, seemed too unwieldy. And, of course, for readers interacting with and citing primarily editors’ comments, it is not necessarily the case that changes to text will be the most significant changes.</p>
<p>Accordingly, despite the potential advantages offered by three-part version numbers, we ultimately elected to adopt a simpler, two-part system for version numbers, in the form of [major version].[minor version], where the segments have the following meanings:</p>
<ul>
<li>The first segment, <em>major version</em> , increases when we make a large number of changes systematically across the text that have a significant effect on its markup or on the how it is edited as a whole. Moving from P4 to P5 of the TEI protocols, which requires non-trivial changes in markup across the text, is an example of a change that would increase the number of the major version segment.<sup id="fnref:45"><a href="#fn:45" class="footnote-ref" role="doc-noteref">45</a></sup> The significance of any program of changes must be assessed by the resource’s maintainers in terms of the needs of the community that will use the resource, as the Semantic Versioning debate suggests.</li>
<li>The second segment, <em>minor version</em> , increases when we make any other change. These include corrections to readings, updates to notes or paratexts, modification of markup, or changes of any other kind as long as they do not rise to the systematic, significant status that would constitute a new major version.</li>
</ul>
<p>Users in possession of an XML file should be able to determine the version from that file, so the policy stipulates that wherever possible, the version number should be recorded internally within the file to which it applies. In TEI documents, we record the version number using the <code>@n</code> attribute within the <code>&lt;edition&gt;</code> element in the <code>&lt;editionStmt&gt;</code> section of the header. We also recommend documenting the revision history of the file using <code>&lt;change&gt;</code> elements within the <code>&lt;revisionDesc&gt;</code> section of the header; the version number should be attached to each change newly introduced within a particular version using the @n attribute on the <code>&lt;change&gt;</code> entity. In this way, we can both identity particular states of files and construct a human-readable history of how the file developed. Where version numbers and change histories cannot practically be included in the file itself, we will store them in a supplementary text file to be archived and distributed with the data files.</p>
<h2 id="conclusion">Conclusion</h2>
<p>The practices considered and developed by the PPEA offer a starting point for versioning digital projects, laying out standards for what needs to be versioned and how version numbers can make the status of files and their histories more intelligible. Other projects, with different needs, materials, data formats, and philosophies may need to develop different strategies in order to make their material comprehensible and usable. Development of standard practices would benefit the field of digital editing as a whole. And standards and mechanisms for versioning will have to continue to evolve alongside ongoing developments in the field of digital editing. The versioning protocol developed for the PPEA is based on a document-driven paradigm of the digital edition — that is, on a model in which key informational components of the edition are contained in individual XML files, to which version numbers can be attached. But this model is a notably simple one, even in the context of the TEI. XML documents need not be self-contained; an XML document can virtually include content drawn dynamically from other sources, a design pattern that Alan Liu terms “data pours” and finds characteristic of the modern web<a class="footnote-ref" href="#liu2004"> [liu2004] </a>. Each source file could be versioned individually, but the compound XML that is processed to render the edition may never before have existed as a coherent whole; such hybrid documents threaten a nearly endless proliferation of versions, not to mention challenging technical measures to expose the version number of each element. And more complicated paradigms may become increasingly common in sophisticated digital editions of the future. For instance, in editions developed according to the principles of “stand-off” markup, there may be a “source” document containing a core stream of textual data, designed to work in tandem with various kinds of markup stored outside that document<a class="footnote-ref" href="#TEI2019"> [TEI2019] </a>.<sup id="fnref:46"><a href="#fn:46" class="footnote-ref" role="doc-noteref">46</a></sup> Some editions may even avoid storing data in documents that map to a traditional file system, opting instead for databases or other complex storage structures.<sup id="fnref:47"><a href="#fn:47" class="footnote-ref" role="doc-noteref">47</a></sup> Versioning practices will require ongoing consideration to keep pace with the shifting field. These are conversations the digital editing community needs to begin to have.</p>
<p>Until even an initial community consensus emerges, individual projects will have to develop their own approaches to versioning their data — approaches that will shift in tandem with their needs, infrastructures, material, and scale. Accordingly, instead of a set of rules, I conclude with three principles that can help to guide discussions about versions of texts:<br>
Digital editions must version their underlying data and communicate those versions to users, independent of how that data is displayed. This is not the same as tracking file history in version control; nor is it the same as the bibliographical analysis scholars of future generations may want to perform. It is a declarative act in which digital editors make assertions about the state of their work. Where editorial projects offer reading interfaces or APIs, they should strongly consider versioning their software environments, due to the complicated technological interactions required to display a text. However, versioning the data itself is of paramount importance. Wherever possible, editions should provide direct access to their versioned data (for example, in the form of TEI-encoded XML files) so that users can examine the data directly, apart from its interface.Versioning is social. As debates in the software community have suggested, versioning is not an abstract concept, but is inherently tied to <em>use</em> . Developing versioning principles will requires editorial projects to have a use-model of their resources, one that takes into account what kinds of changes are intellectually and practically significant. This means, for instance, deciding what types of object are fundamental to the resource and at what level they should be versioned. (A single epigraph, or a corpus? Chapter or novel? Poem or volume? The entire archive offered by a large project? Given both creators and users, how should we understand the resource as transforming?)Digital editions must explicitly scope their revisions, delivering version numbers that communicate with users (based on their needs) the scale and significance of the change. It should be possible to understand through version numbers not only what version of a resource is most recent, but howcompatiblethey are, how likely it is that the differences have a significant impact on their intellectual coherence or their probable uses. Both in individual projects and the field of digital editing as a whole, we should develop explicit guidelines that make these versions meaningful.</p>
<p>Kirschenbaum has claimed that despite the significant technical challenges of digital preservation, its greatest challenges are “ultimately — and profoundly — social” <a class="footnote-ref" href="#kirschenbaum2008"> [kirschenbaum2008] </a>. The same, I would argue, is true of the issues surrounding the evolution and internal histories of digital editions. The field must begin to develop standards and practices for managing resource histories — standards and practices that ideally should not be limited to any one file format or encoding scheme, but can help organize data of many forms, for many purposes, now and in the future. And other practices will need to develop around those standards: for instance, support for versioning in emerging APIs for digital scholarly text. Version numbers, I argue, can help meet these needs, and it is time for digital editors to begin discussing and using them.</p>
<h2 id="acknowledgements">Acknowledgements</h2>
<p>My thanks to Timothy Stinson for his feedback on this article, and to Matthew Kirschenbaum for his comments on an earlier version of this work. I am also grateful to my many generous interlocutors at the BH and DH conference where I first presented these ideas, and to anonymous reviewers at DHQ. This research was made possible by the support of a CLIR Postdoctoral Fellowship in Data Curation for Medieval Studies at the North Carolina State University Libraries. Another version of this article will be published as “Versioning and Digital Editions” , in <em>Book History and Digital Humanities</em> , Wacha, H. and Vareschi, M. (eds), Center for the History of Print and Digital Culture and University of Wisconsin Press, Madison, forthcoming.</p>
<h2 id="appendix-1">Appendix 1</h2>
<p>To get an idea of existing versioning practices in existing digital editions, I examined versioning and revision-tracking practices in the thirty “interesting editions/projects” singled out for recommendation in Patrick Sahle’s online <em>Catalogue of Digital Editions</em> <a class="footnote-ref" href="#sahle2019"> [sahle2019] </a>. These resources, comprising projects with start dates ranging from 1995 to 2018 and covering multiple fields, languages, and kinds of material, offer a convenient sampling of available digital editorial work. I examined each resource and attempted to determine whether it provided version numbers and whether it kept granular revision histories. To attempt to learn a project’s practices, I examined its opening page, pages describing the project and its technical and editorial policies, credits pages, citation instructions, and a small sampling of pages displaying texts belonging to the edition. Where a project provided direct access to its underlying data files (typically in the form of TEI-encoded XML), I also examined a few of these files to see if version numbers or revision histories were represented in the data files. Because my examination of the data files of any individual project was limited and manual, it is possible that a project includes version information or revision history in a file I did not examine, or discusses these matters in a portion of the site I did not access; however, my examination suggests broadly whether such information is accessible to site users. My findings are summarized in the table below, followed by brief discussion.<br>
Project Name<sup id="fnref:48"><a href="#fn:48" class="footnote-ref" role="doc-noteref">48</a></sup> Project–wide version numberVersion numbers for individual documentsChange logs in data files<sup id="fnref:49"><a href="#fn:49" class="footnote-ref" role="doc-noteref">49</a></sup> Other detailed change logsProvides date of last update<sup id="fnref:50"><a href="#fn:50" class="footnote-ref" role="doc-noteref">50</a></sup> Jane Austen’s Fiction Manuscripts Digital EditionBayeux-Tapestry Digital Edition<sup id="fnref:51"><a href="#fn:51" class="footnote-ref" role="doc-noteref">51</a></sup> Samuel Beckett - Digital Manuscript Project<sup id="fnref:52"><a href="#fn:52" class="footnote-ref" role="doc-noteref">52</a></sup> XBurckhardt SourceLord Byron and his TimesXThe Canterbury Tales Project: The Miller&rsquo;s Tale on CD-ROMThe Canterbury Tales Project: The Nun&rsquo;s Priest&rsquo;s Tale on CD-ROMDante Alighieri: Commedia - A Digital Edition<sup id="fnref:53"><a href="#fn:53" class="footnote-ref" role="doc-noteref">53</a></sup> Alfred Escher - BriefeditionXFaustedition / Johann Wolfgang Goethe: Faust. Historisch-kritische EditionXIn Transition: Selected Poems by the Baroness Elsa von Freytag-LoringhovenThe Diary of William GodwinXThe Thomas Gray (1716-1771) Interactive Online Commentary<sup id="fnref:54"><a href="#fn:54" class="footnote-ref" role="doc-noteref">54</a></sup> XThe Charles Harpur Critical ArchiveWolfgang Koeppen: Jugend - Textgenetische EditionHugo von Montfort - Das poetische WerkThe Newton ProjectXThe Proceedings of the Old Bailey, London 1674 to 1834XXPetrus Plaoul - Editio Critica Commentarii in libris Sententiarum<sup id="fnref:55"><a href="#fn:55" class="footnote-ref" role="doc-noteref">55</a></sup> XThe Complete Writings and Pictures of Dante Gabriel Rossetti - A Hypermedia ArchiveXArthur Schnitzler - Digitale historisch-kritische Edition (Werke 1905 bis 1931)XXCodex SinaiticusXXBichitra: Online Tagore VariorumXDigital ThoreauVincent van Gogh - The LettersX<sup id="fnref:56"><a href="#fn:56" class="footnote-ref" role="doc-noteref">56</a></sup> Van Nu en Straks. De BrievenXLope de Vega - La Dama Boba - EDICIÓN CRÍTICA Y ARCHIVO DIGITALThe Digital Vercelli Book<sup id="fnref:57"><a href="#fn:57" class="footnote-ref" role="doc-noteref">57</a></sup> X<sup id="fnref:58"><a href="#fn:58" class="footnote-ref" role="doc-noteref">58</a></sup> Carl-Maria-von-Weber-Gesamtausgabe (WeGA) [Digitale Präsentation]XXThe Walt Whitman ArchiveXXTotal Number34943<br>
Of the projects represented, 18 (60%) acknowledge in some form that their resources may change over time. The most common form of acknowledging changes, practiced by thirteen projects (43%), is maintain a change log, which typically records a description of changes, the date on which they were made, and who made them. Four projects provide a list of changes as part of the website (in the case of the <em>Vercelli</em> , I suspect generated from the underlying data files, which are not accessible). These vary in detail; <em>Schnitzler</em> focuses on the addition of new content and features; <em>Old Bailey</em> discusses corrections but often summarizes changes made to many records simultaneously; the <em>Whitman Archive</em> provides a detailed description of revisions in an external blog. Ten projects, all encoded in TEI XML or a derivative format, store revision lists in each data file using the <code>&lt;revisionDesc&gt;</code> element; the <em>Whitman Archive</em> is noteworthy in providing both internal and external change logs.</p>
<p>Supplying version numbers is a rarer approach. Seven projects (23%) employ version numbers in some capacity. This total is split almost evenly between projects that assign a single version number to a given state of the project (3; 10% of the total) and those that version separate texts or data files independently (4; 13% of the total). Surprisingly, only three projects combine version numbers with a detailed listing of changes; in all cases one version number applies to the resources as a whole, though one of the projects records changes to individual files while the other two announce sitewide changes (including new features) in tandem with new versions.</p>
<p>The three projects using sitewide version numbers all use conventional two-segment version numbers.<sup id="fnref:59"><a href="#fn:59" class="footnote-ref" role="doc-noteref">59</a></sup>  <em>Schnitzler</em> is the only project to explicitly explain the meaning of its version numbers, the two segments of which correspond to major and minor releases: major releases are marked by the release of significant new functionality or materials, as anticipated by the phases mapped out by the Release Plan, while minor versions correspond to minor updates<a class="footnote-ref" href="#izb2019"> [izb2019] </a>.<sup id="fnref:60"><a href="#fn:60" class="footnote-ref" role="doc-noteref">60</a></sup> Generally, these project-wide version numbers assign clear version numbers that clearly communicate something about the scope of their changes.</p>
<p>By contrast, the projects that grant version numbers to individual documents or data files use a much wider array of formats. Of the four projects that version individual resources, two conceive of versioning by analogy to print. The <em>Rossetti Archive</em> , on the pages for individual works within the archive, refers not to versions of its materials but to editions. At the bottom of the page for each item in the archive, the site gives anElectronic Archive Editionnumber as an integer; an item might be listed as edition 1 or 2.<sup id="fnref:61"><a href="#fn:61" class="footnote-ref" role="doc-noteref">61</a></sup> Inspecting the XML files reveals that the book analogy is suggested in part by the TEI; the edition number is given in the document header using the <code>&lt;edition&gt;</code> element.<sup id="fnref:62"><a href="#fn:62" class="footnote-ref" role="doc-noteref">62</a></sup> ( <em>Lord Byron</em> similarly attaches an integer edition number to the <code>&lt;edition&gt;</code> element, though it uses the <code>@n</code> attribute, where <em>Rossetti</em> makes the edition number the element’s content; <em>Lord Byron</em> does not display this number in the reading interface.) These numbers provide a mechanism for recording changes, but the number’s low resolution combined with the ambiguity of the <code>&lt;edition&gt;</code> element and the lack of a stated versioning policy means that it is difficult to be certain the edition number will be updated with any change.</p>
<p><em>Petrus Plaoul</em> , as available in the Scholastic Commentaries and Texts Archive, also refers to a state of one of its digital texts as an edition, but the identifiers it assigns suggests a more robust way of thinking about textual state. The identifier for aneditionmight take the form2011.10-dev-master, accompanied by the dateOctober 04, 2011; these appear at the head of every text on the site (for example,<a href="#plaoul2011">Plaoul, 2011</a>). These identifiers offer a form of Calendar Versioning, labeling a state of the text according to when it last changed, combined with what appears to be technical control information. The dot separation between the year and month visually evokes standard formats for version numbers. But of the projects profiled, only <em>Codex Sinaiticus</em> explicitly refers to states of its material as versions. The website detaches this information from the presentation of the text; I found the version number listed only on the XML Download page, where it is also accompanied by a revision date<a class="footnote-ref" href="#XMLD"> [XMLD] </a>. The version number — 1.04 at the time of writing — is stored in the downloadable XML file, attached to the <code>@n</code> attribute of the <code>&lt;edition&gt;</code> element and also labeled “Version 1.04” in the content of that element, where it is accompanied by a date of last update (March 25, 2014). The <code>&lt;revisionDesc&gt;</code> element enumerates changes made to the XML file and parenthetically links each to the version number of the file in which the change was made. Of the projects examined, <em>Codex Sinaiticus</em> alone offers detailed version numbers capable of registering the scope of revisions, and it is also alone in explicitly articulating the link between labeled version and the revision history.</p>
<p>The field of digital editing as a whole shows an understanding of the importance of acknowledging resource change; a majority of the editorial projects surveyed take some steps to show how the current state of the resources differs from earlier forms in which the same resource was available. Based on the prevalence of various approaches to change, it appears that at least a partial consensus has formed within the field about using change logs to describe to human readers the changes that have occurred. By comparison, labeling specific states of the resource through the use of version numbers or other identifiers is much less common, and even among projects that do explicitly version resources, practices are wildly inconsistent. Should whole websites be versioned, or individual texts? What constitutes a version? What form should version numbers take? This article has argued that version numbers are necessary for data management and interoperability among digital editions. The significant disparities in existing practices highlight the need for a field-wide conversation to develop practices around versioning practices.</p>
<ul>
<li id="abilgaard2015">Abildgaard, N.R. (2015) “On Versioning” . 5 February. Available at:<a href="http://blog.hypesystem.dk/on-versioning">http://blog.hypesystem.dk/on-versioning</a>. Archived at:<a href="http://web.archive.org/web/20190803210015/http://blog.hypesystem.dk/on-versioning">http://web.archive.org/web/20190803210015/http://blog.hypesystem.dk/on-versioning</a>.
</li>
<li id="altice2015">Altice, N. (2015) _I am Error: The Nintendo Family Computer / Entertainment System Platform_ . MIT Press, Cambridge, MA.
</li>
<li id="andrews2018">Andrews, T.L. and Zundert, J.J.V. (2018) “What Are You Trying to Say? The Interface as an Integral Element of Argument” . In Bleier, R. et al. (eds), _Digital Scholarly Editions as Interfaces_ , Books on Demand, Norderstedt, pp. 3-33. urn:nbn:de:hbz:38-91064. Available at:<a href="http://kups.ub.uni-koeln.de/id/eprint/9106">http://kups.ub.uni-koeln.de/id/eprint/9106</a>.
</li>
<li id="ashkenas2015">Ashkenas, J. (2015) “Why Semantic Versioning Isn’t” . GitHub Gist. Available at:<a href="https://gist.github.com/jashkenas/cbd2b088e20279ae2c8e/39e281d9bc8f3090dd79347b5271aa9b30c2bb8b">https://gist.github.com/jashkenas/cbd2b088e20279ae2c8e/39e281d9bc8f3090dd79347b5271aa9b30c2bb8b</a>. Archived at:<a href="http://web.archive.org/web/20190803210633/https://gist.github.com/jashkenas/cbd2b088e20279ae2c8e/39e281d9bc8f3090dd79347b5271aa9b30c2bb8b">http://web.archive.org/web/20190803210633/https://gist.github.com/jashkenas/cbd2b088e20279ae2c8e/39e281d9bc8f3090dd79347b5271aa9b30c2bb8b</a>.
</li>
<li id="ANDS">Australian National Data Service (n.d.) “Data Versioning” . Australian National Data Service. Available at:<a href="https://www.ands.org.au/working-with-data/data-management/data-versioning">https://www.ands.org.au/working-with-data/data-management/data-versioning</a>(viewed 1 September 2017). Archived at:<a href="http://web.archive.org/web/20190803211318/https://www.ands.org.au/working-with-data/data-management/data-versioning">http://web.archive.org/web/20190803211318/https://www.ands.org.au/working-with-data/data-management/data-versioning</a>.
</li>
<li id="berners-lee1998">Berners-Lee, T. (1998) “Web Architecture from 50,000 Feet” . W3C. September. Available at:<a href="https://www.w3.org/DesignIssues/Architecture.html">https://www.w3.org/DesignIssues/Architecture.html</a>. Archived at:<a href="http://web.archive.org/web/20190803211651/https://www.w3.org/DesignIssues/Architecture.html">http://web.archive.org/web/20190803211651/https://www.w3.org/DesignIssues/Architecture.html</a>.
</li>
<li id="berrie2006">Berrie, P. et al. (2006) “Authenticating Electronic Editions” . In Burnard et al. 2006, pp. 269-276.
</li>
<li id="boot2011">Boot, P. and Zundert, J. (2011) “The Digital Edition 2.0 and The Digital Library: Services, not Resources” . In Fritze, C. et al (eds), _Digitale Edition und Forschungsbibliothek: Beirtäge der Fachtagung im Philosophicum der Universität Mainz am 13. und 14 Januar 2011_ , Harrassowitz, Wiesbaden, pp. 141-152.
</li>
<li id="bowers2005">Bowers, F. (2005) _Principles of Bibliographical Description_ . Oak Knoll Press, New Castle, DE. First published 1949.
</li>
<li id="bradley2012">Bradley, J. (2012) “No Job for Techies: Technical Contributions to Research in the Digital Humanities” . In Deegan, M. and McCarty, W. (eds), _Collaborative Research in the Digital Humanities_ , Ashgate, Farnham, Surrey, pp. 11-26.
</li>
<li id="bryant2002">Bryant, J. (2002) _The Fluid Text: A Theory of Revision and Editing for Book and Screen_ . University of Michigan Press, Ann Arbor.
</li>
<li id="burnard2006">Burnard, L., O'Brien O'Keefe, K., and Unsworth, J, eds. (2006) _Electronic Textual Editing_ . Modern Language Association, New York.
</li>
<li id="burrow2018">Burrow, J.A. and Turville-Peter, T., eds. (2018) _Piers Plowman: The B-Version Archetype (Bx)_ . Society for Early English and Norse Electronic Texts, Raleigh, NC.
</li>
<li id="cerquiglini1999">Cerquiglini, B. (1999) _In Praise of the Variant: A Critical History of Philology_ . Johns Hopkins University Press, Baltimore.
</li>
<li id="chassanoff2018">Chassanoff, A. et al. (2018) “Software Curation in Research Libraries: Practice and Promise” . _Journal of Librarianship and Scholarly Communication_ , 6 (1).<a href="http://dx.doi.org/10.7710/2162-3309.2239">http://dx.doi.org/10.7710/2162-3309.2239</a>.
</li>
<li id="CM2003"> _The Chicago Manual of Style_ (2003) 15th ed. University Of Chicago Press, Chicago.
</li>
<li id="CM2017"> _The Chicago Manual of Style_ (2017) 17th ed. University of Chicago Press, Chicago.
</li>
<li id="deegan2006">Deegan, M. (2006) “Collection and Preservation of an Electronic Edition” . In Burnard et al. 2006, pp. 358-370.
</li>
<li id="DTS2019"> “Distributed Text Services (DTS)” (2019). Distributed Text Services. Available at:<a href="https://distributed-text-services.github.io/specifications/">https://distributed-text-services.github.io/specifications/</a>(viewed 3 August 2019).
</li>
<li id="duggan2005">Duggan, H.N. and Lyman, E.W. (2005) “A Progress Report on _The Piers Plowman Electronic Archive_ ” . _Digital Medievalist_ , 1.<a href="http://dx.doi.org/10.16995/dm.5">http://dx.doi.org/10.16995/dm.5</a>.
</li>
<li id="duggan2019">Duggan, H.N., Stinson, T.L., and Turville-Petre, T., eds. _Piers Plowman Electronic Archive_ . Society for Early English and Norse Electronic Texts. Available at:<a href="http://piers.chass.ncsu.edu">http://piers.chass.ncsu.edu</a>.
</li>
<li id="eaves2017">Eaves, M., Essick. R.N., and Viscomi, J., eds. (2017). _The William Blake Archive_ . Chapel Hill, NC. Available at:<a href="http://www.blakearchive.org/">http://www.blakearchive.org/</a>.
</li>
<li id="eggert2013">Eggert, P. (2013) “Apparatus, Text, Interface: How to Read a Printed Critical Edition” . In Fraistat, N. and Flanders, J. (eds), _The Cambridge Companion to Textual Scholarship_ , Cambridge University Press, Cambridge, pp. 97-118.
</li>
<li id="escobarvarela2016">Escobar Varela, M. (2016) “The Archive as Repertoire: Transience and Sustainability in Digital Archives” . _Digital Humanities Quarterly_ , 10(4). Available at:<a href="http://digitalhumanities.org/dhq/vol/10/4/000269/000269.html">http://digitalhumanities.org/dhq/vol/10/4/000269/000269.html</a>. Archived at:<a href="http://web.archive.org/web/20190804175354/http://digitalhumanities.org/dhq/vol/10/4/000269/000269.html">http://web.archive.org/web/20190804175354/http://digitalhumanities.org/dhq/vol/10/4/000269/000269.html</a>.
</li>
<li id="fitzpatrick2011">Fitzpatrick, K. (2011) _Planned Obsolescence: Publishing, Technology, and the Future of the Academy_ . New York University Press, New York.
</li>
<li id="folsomprice"> _The Walt Whitman Archive_ . Center for Digital Research in the Humanities, University of Nebraska-Lincoln, Lincoln, NE. Available at:<a href="https://whitmanarchive.org">https://whitmanarchive.org</a>.
</li>
<li id="franzini2019">Franzini, G., Terras, M. and Mahony, S. (2019) “Digital Editions of Text: Surveying User Requirements in the Digital Humanities” . _Journal on Computing and Cultural Heritage_ , 12(1).<a href="http://dx.doi.org/10.1145/3230671">http://dx.doi.org/10.1145/3230671</a>.
</li>
<li id="fyfe2012">Fyfe, P. (2012) “Electronic Errata: Digital Publishing, Open Review, and the Futures of Correction” . In Gold, M.K. (ed), _Debates in the Digital Humanities_ , University of Minnesota Press, Minneapolis, pp. 259-280.
</li>
<li id="gabler2010">Gabler, H.W. (2010) “Theorizing the Digital Scholarly Edition” . _Literature Compass_ , 7, 43-56.<a href="http://dx.doi.org/10.1111/j.1741-4113.2009.00675.x">http://dx.doi.org/10.1111/j.1741-4113.2009.00675.x</a>.
</li>
<li id="galey2010">Galey, A. (2010) “The Human Presence in Digital Artifacts” . In McCarthy, W. (ed), _Text and Genre in Reconstruction: Effects of Digitization on Ideas, Behaviours, Products and Institutions_ , Open Book Publishers, Cambridge, 93-117. Available at:<a href="https://www.openbookpublishers.com/reader/64/#page/104/mode/2up">https://www.openbookpublishers.com/reader/64/#page/104/mode/2up</a>.
</li>
<li id="gants2010">Gants, D.L. (2010) “Descriptive Bibliography and Electronic Publication” . _Essays and Studies_ , 2010, 121-141.
</li>
<li id="gibaldi1995">Gibaldi, J. (1995) _MLA Handbook for Writers of Research Papers_ . Modern Language Association of America, New York.
</li>
<li id="gibaldi1998">Gibaldi, J. (1998) _MLA Style Manual and Guide to Scholarly Publishing_ . Modern Language Association of America, New York.
</li>
<li id="git"> “Git” . (n.d.) Git. Available at:<a href="https://git-scm.com">https://git-scm.com</a>(viewed 22 July 2019). Archived at:<a href="http://web.archive.org/web/20190803215454/https://git-scm.com/">http://web.archive.org/web/20190803215454/https://git-scm.com/</a>.
</li>
<li id="gitum"> _The Git User’s Manual_ . (n.d.) Version 2.22.0. Git. Available at:<a href="https://git-scm.com/docs/user-manual.html">https://git-scm.com/docs/user-manual.html</a>. Archived at:<a href="http://web.archive.org/web/20190803235925/https://git-scm.com/docs/user-manual.html">http://web.archive.org/web/20190803235925/https://git-scm.com/docs/user-manual.html</a>.
</li>
<li id="greetham1992">Greetham, D.C. (1992) _Textual Scholarship: An Introduction_ . Garland, New York.
</li>
<li id="hashemi2019">Hashemi, M. (2019) “Calendar Versioning” . CalVer: Timely Software Versioning. 1 July. Available at:<a href="http://calver.org">http://calver.org</a>. Archived at:<a href="http://web.archive.org/web/20190803215814/http://calver.org/">http://web.archive.org/web/20190803215814/http://calver.org/</a>.
</li>
<li id="huber2019">Huber, A. (2019) “About” . _Thomas Gray Archive_ . 2 July. Available at:<a href="http://www.thomasgray.org/about/index.shtml">http://www.thomasgray.org/about/index.shtml</a>. Archived at:<a href="http://web.archive.org/web/20190803220008/https://www.thomasgray.org/about/index.shtml">http://web.archive.org/web/20190803220008/https://www.thomasgray.org/about/index.shtml</a>.
</li>
<li id="hunt">Hunt, W.H. (n.d.) “Pre-Raphaelitism and the Pre-Raphaelite Brotherhood” . 2nd Archive Edition. In McGann, J.J. (ed), _The Complete Writings and Pictures of Dante Gabriel Rossetti: A Hypermedia Archive_ , Rossetti Archive. Available at:<a href="http://www.rossettiarchive.org/docs/nd467.h9.1914.1.rad.html">http://www.rossettiarchive.org/docs/nd467.h9.1914.1.rad.html</a>. Archived at:<a href="http://web.archive.org/web/20190803221443/http://www.rossettiarchive.org/docs/nd467.h9.1914.1.rad.html">http://web.archive.org/web/20190803221443/http://www.rossettiarchive.org/docs/nd467.h9.1914.1.rad.html</a>.
</li>
<li id="izb2019"> “Informationen zum Beta-release 2.0” . (2019) 17 April. _Arthur Schnitzler digital_ . Beta 2.0. Available at:<a href="https://www.arthur-schnitzler.de/edition/beta">https://www.arthur-schnitzler.de/edition/beta</a>. Archived at:<a href="http://web.archive.org/web/20190803221659/https://www.arthur-schnitzler.de/edition/beta">http://web.archive.org/web/20190803221659/https://www.arthur-schnitzler.de/edition/beta</a>.
</li>
<li id="jones2016">Jones, S.M. et al. (2016) “Scholarly Context Adrift: Three out of Four URI References Lead to Changed Content” . _PLoS One_ , 11(12), e0167475.<a href="https://doi.org/10.1371/journal.pone.0167475">https://doi.org/10.1371/journal.pone.0167475</a>.
</li>
<li id="kalvesmaki2014">Kalvesmaki, J. (2014) “Canonical References in Electronic Texts: Rationale and Best Practices” . _Digital Humanities Quarterly_ , 8(2). Available at:<a href="http://www.digitalhumanities.org/dhq/vol/8/2/000181/000181.html">http://www.digitalhumanities.org/dhq/vol/8/2/000181/000181.html</a>. Archived at:<a href="https://web.archive.org/web/20190803222140/http://www.digitalhumanities.org/dhq/vol/8/2/000181/000181.html">https://web.archive.org/web/20190803222140/http://www.digitalhumanities.org/dhq/vol/8/2/000181/000181.html</a>.
</li>
<li id="kilbride2016">Kilbride, W. (2016) “Saving the Bits: Digital Humanities Forever?” In Schreibman, S., Siemens, R. and Unsworth, J. (eds), _A New Companion to Digital Humanities_ , Wiley Blackwell, Malden, MA, pp. 408-419.
</li>
<li id="kirschenbaum2002">Kirschenbaum, M.G. (2002) “Editing the Interface: Textual Studies and First Generation Electronic Objects” . _TEXT_ , 14, 15-51.
</li>
<li id="kirschenbaum2008">Kirschenbaum, M.G. (2008) _Mechanisms: New Media and the Forensic Imagination_ . MIT Press, Cambridge, MA.
</li>
<li id="kirschenbaum2017">Kirschenbaum, M.G. (2017) “Post Scripts: Graphologies of Bookmaking after Adobe” . Paper presented to BH & DH: Book History and Digital Humanities, Madison, WI, 22 September.
</li>
<li id="klein2014">Klein, M. et al. (2014) “Scholarly Context Not Found: One in Five Articles Suffers from Reference Rot” . _PLoS One_ , 9, e115253.<a href="https://doi.org/10.1371/journal.pone.0115253">https://doi.org/10.1371/journal.pone.0115253</a>.
</li>
<li id="kline2008">Kline, M.J. and Perdue, S.H. (2008) _A Guide to Documentary Editing_ . University of Virginia Press, Charlottesville.
</li>
<li id="knowles2014">Knowles, J. and Stinson, T. (2014) “The Piers Plowman Electronic Archive on the Web: An Introduction” . _The Yearbook of Langland Studies_ , 28, 225-238.
</li>
<li id="kuczera2016">Kuczera, A. (2016) “Digital Editions beyond XML – Graph-based Digital Editions” . _Proceedings of the 3rd Histo–Informatics Workshop, Krakow, Poland, 11 July 2016_ , 37-46. Available at:<a href="http://ceur-ws.org/Vol-1632/paper_5.pdf">http://ceur-ws.org/Vol-1632/paper_5.pdf</a>.
</li>
<li id="leblanc2018">Leblanc, E. (2018) _Design of a Digital Library Interface from User Perspective, and its Consequences for the Design of Digital Scholarly Editions: Findings of the Fonte Gaia Questionnaire_ . In Bleier et al. (eds), _Digital Scholarly Editions as Interfaces_ , Books on Demand, Norderstedt, pp. 287-315. urn:nbn:de:hbz:38-91215 Available at:<a href="https://kups.ub.uni-koeln.de/9121/">https://kups.ub.uni-koeln.de/9121/</a>.
</li>
<li id="liu2004">Liu, A. (2004) “Transcendental Data: Toward a Cultural History and Aesthetics of the New Encoded Discourse” . _Critical Inquiry_ , 31, 49-84.
</li>
<li id="liu2005">Liu, A. et al. (2005) “Born-Again Bits: A Framework for Migrating Electronic Literature” . Version 1.1. 5 August. Electronic Literature Organization, Vancouver. Available at:<a href="https://eliterature.org/pad/bab.html">https://eliterature.org/pad/bab.html</a>. Archived at:<a href="http://web.archive.org/web/20190803223824/https://eliterature.org/pad/bab.html">http://web.archive.org/web/20190803223824/https://eliterature.org/pad/bab.html</a>.
</li>
<li id="lóscio2017">Lóscio, B.F., Burle, C., and Carlegari, N., eds. (2017) “Data on the Web Best Practices: W3C Recommendation” . 31 January. W3C. Available at:<a href="https://www.w3.org/TR/2017/REC-dwbp-20170131/">https://www.w3.org/TR/2017/REC-dwbp-20170131/</a>. Archived at:<a href="http://web.archive.org/web/20190803224048/https://www.w3.org/TR/2017/REC-dwbp-20170131/">http://web.archive.org/web/20190803224048/https://www.w3.org/TR/2017/REC-dwbp-20170131/</a>.
</li>
<li id="MDGR"> _Masterpieces of D. G. Rossetti (1828-1882): Sixty Reproductions of Photographs from the Original Oil-paintings_ . (n.d.) 2nd Archive Edition. In McGann, J.J. (ed), _The Complete Writings and Pictures of Dante Gabriel Rossetti: A Hypermedia Archive_ , Rossetti Archive. Available at:<a href="http://www.rossettiarchive.org/docs/ac-gowans.759.2r735m393.rad.html">http://www.rossettiarchive.org/docs/ac-gowans.759.2r735m393.rad.html</a>.
</li>
<li id="mcdonough2010">McDonough, J. et al. (2010) “Twisty Little Passages Almost All Alike: Applying the FRBR Model to a Classic Computer Game” . _Digital Humanities Quarterly_ , 4(2). Available at:<a href="http://www.digitalhumanities.org/dhq/vol/4/2/000089/000089.html">http://www.digitalhumanities.org/dhq/vol/4/2/000089/000089.html</a>. Archived at:<a href="https://web.archive.org/web/20190803224557/http://www.digitalhumanities.org/dhq/vol/4/2/000089/000089.html">https://web.archive.org/web/20190803224557/http://www.digitalhumanities.org/dhq/vol/4/2/000089/000089.html</a>.
</li>
<li id="mcgann1995">McGann, J. (1995) “The Rationale of Hypertext” . 6 May. IATH WWW Server. Available at:<a href="http://www2.iath.virginia.edu/public/jjm2f/rationale.html">http://www2.iath.virginia.edu/public/jjm2f/rationale.html</a>. Archived at:<a href="http://web.archive.org/web/20190803224726/http://www2.iath.virginia.edu/public/jjm2f/rationale.html">http://web.archive.org/web/20190803224726/http://www2.iath.virginia.edu/public/jjm2f/rationale.html</a>.
</li>
<li id="mcgann1996">McGann, J. (1996) “The Rationale of HyperText” . _TEXT_ , 9, 11-32.
</li>
<li id="mcgann2001">McGann, J. (2001) _Radiant Textuality: Literature after the World Wide Web_ . Palgrave, New York.
</li>
<li id="MLAC2011">MLA Committee on Scholarly Editions (2011) “Guidelines for Editors of Scholarly Editions” . 29 June. MLA, New York. Available at:<a href="https://www.mla.org/Resources/Research/Surveys-Reports-and-Other-Documents/Publishing-and-Scholarship/Reports-from-the-MLA-Committee-on-Scholarly-Editions/Guidelines-for-Editors-of-Scholarly-Editions">https://www.mla.org/Resources/Research/Surveys-Reports-and-Other-Documents/Publishing-and-Scholarship/Reports-from-the-MLA-Committee-on-Scholarly-Editions/Guidelines-for-Editors-of-Scholarly-Editions</a>. Archived at:<a href="http://web.archive.org/web/20190803225011/https://www.mla.org/Resources/Research/Surveys-Reports-and-Other-Documents/Publishing-and-Scholarship/Reports-from-the-MLA-Committee-on-Scholarly-Editions/Guidelines-for-Editors-of-Scholarly-Editions">http://web.archive.org/web/20190803225011/https://www.mla.org/Resources/Research/Surveys-Reports-and-Other-Documents/Publishing-and-Scholarship/Reports-from-the-MLA-Committee-on-Scholarly-Editions/Guidelines-for-Editors-of-Scholarly-Editions</a>
</li>
<li id="MLAC2016">MLA Committee on Scholarly Editions (2016) “MLA Statement on the Scholarly Edition in the Digital Age” . May. Modern Language Association of America, New York. Available at:<a href="https://www.mla.org/content/download/52050/1810116/rptCSE16.pdf">https://www.mla.org/content/download/52050/1810116/rptCSE16.pdf</a>. Archived at:<a href="http://web.archive.org/web/20190803225220/https://www.mla.org/content/download/52050/1810116/rptCSE16.pdf">http://web.archive.org/web/20190803225220/https://www.mla.org/content/download/52050/1810116/rptCSE16.pdf</a>.
</li>
<li id="nielsen2017">Nielsen, L.H. (2017) “Zenodo now supports DOI versioning!” 30 May. Zenodo Blog. Available at:<a href="http://blog.zenodo.org/2017/05/30/doi-versioning-launched/">http://blog.zenodo.org/2017/05/30/doi-versioning-launched/</a>. Archived at:<a href="http://web.archive.org/web/20190803225348/http://blog.zenodo.org/2017/05/30/doi-versioning-launched/">http://web.archive.org/web/20190803225348/http://blog.zenodo.org/2017/05/30/doi-versioning-launched/</a>.
</li>
<li id="odonnell2008">O’Donnell, D.P. (2008) “Resisting The Tyranny of the Screen, or, Must a Digital Edition be Electronic?”  _The Heroic Age_ , 11. Available at:<a href="https://www.heroicage.org/issues/11/em.php">https://www.heroicage.org/issues/11/em.php</a>. Archived at:<a href="http://web.archive.org/web/20190803225437/https://www.heroicage.org/issues/11/em.php">http://web.archive.org/web/20190803225437/https://www.heroicage.org/issues/11/em.php</a>.
</li>
<li id="odonnell2018">O'Donnell, D.P., ed. (2018) _Cædmon’s Hymn: A Multimedia Study, Edition, and Archive_ . Internet Edition [source code]. Version 1.1. 21 April. Zenodo.<a href="https://doi.org/10.5281/zenodo.1226549">https://doi.org/10.5281/zenodo.1226549</a>.
</li>
<li id="OTA">The Oxford Text Archive (n.d.) University of Oxford, Oxford. Available at:<a href="ttps://ota.ox.ac.uk">https://ota.ox.ac.uk</a>.
</li>
<li id="pichler2014">Pichler, A. and Bruvik, T.M. (2014) “Digital Critical Editing: Separating Encoding from Presentation” . In Apollon, D., Bélisle, C. and Régnier, P. (eds), _Digital Critial Editions_ , University of Illinois Press, Urbana, Chicago, and Springfield, pp. 179-199.
</li>
<li id="pierazzo2014">Pierazzo, E. (2014) “Digital Documentary Editions and the Others” . _Scholarly Editing_ , 35. Available at:<a href="http://scholarlyediting.org/2014/essays/essay.pierazzo.html">http://scholarlyediting.org/2014/essays/essay.pierazzo.html</a>. Archived at:<a href="http://web.archive.org/web/20190803225830/http://scholarlyediting.org/2014/essays/essay.pierazzo.html">http://web.archive.org/web/20190803225830/http://scholarlyediting.org/2014/essays/essay.pierazzo.html</a>.
</li>
<li id="pierazzo2015">Pierazzo, E. (2015) _Digital Scholarly Editing: Theories, Models and Methods_ . Routledge, London.<a href="https://doi.org/10.4324/9781315577227">https://doi.org/10.4324/9781315577227</a>.
</li>
<li id="plaoul2011">Plaoul, P. (2011) “Lectio 14, De Fide” . 2011.10-dev-master. 4 October. Witt, J.C. (ed), _Scholastic Commentaries and Texts Archive_ , LombardPress. Available at:<a href="http://scta.lombardpress.org/text/lectio14">http://scta.lombardpress.org/text/lectio14</a>. Archived at:<a href="http://web.archive.org/web/20190803230348/http://scta.lombardpress.org/text/lectio14">http://web.archive.org/web/20190803230348/http://scta.lombardpress.org/text/lectio14</a>.
</li>
<li id="preston-werner">Preston-Werner, T. (n.d.) “Semantic Versioning” . Version 2.0.0. Available at:<a href="https://semver.org/spec/v2.0.0.html">https://semver.org/spec/v2.0.0.html</a>. Archived at:<a href="http://web.archive.org/web/20190803230451/https://semver.org/spec/v2.0.0.html">http://web.archive.org/web/20190803230451/https://semver.org/spec/v2.0.0.html</a>.
</li>
<li id="price2009">Price, K.M. (2009) “Edition, Project, Database, Archive, Thematic Research Collection: What’s in a Name?”  _Digital Humanities Quarterly_ , 3(3). Available at:<a href="http://www.digitalhumanities.org/dhq/vol/3/3/000053/000053.html">http://www.digitalhumanities.org/dhq/vol/3/3/000053/000053.html</a>. Archived at:<a href="http://web.archive.org/web/20190803231004/http://www.digitalhumanities.org/dhq/vol/3/3/000053/000053.html">http://web.archive.org/web/20190803231004/http://www.digitalhumanities.org/dhq/vol/3/3/000053/000053.html</a>.
</li>
<li id="PM2001"> _Publication Manual of the American Psychological Association_ (2001) American Psychological Association, Washington, DC.
</li>
<li id="PM2010"> _Publication Manual of the American Psychological Association_ (2010) American Psychological Association, Washington, DC.
</li>
<li id="rasmussen2016">Rasmussen, K.S.G. (2016) “Reading or Using a Digital Edition? Reader Roles in Scholarly Editions” . In Driscoll, M.J. and Pierazzo, E. (eds), _Digital Scholarly Editing: Theories and Practices_ , Open Book Publishers, Cambridge, pp. 119-133.<a href="http://dx.doi.org/10.11647/OBP.0095.07">http://dx.doi.org/10.11647/OBP.0095.07</a>.
</li>
<li id="régnier2014">Régnier, P. (2014) “Ongoing Challenges for Digital Critical Editions” . In Apollon, D., Bélisle, C. and Régnier, P. (eds), _Digital Critical Editions_ , University of Illinois Press, Urbana, pp. 58-80.
</li>
<li id="reiman1987">Reiman, D.H. (1987) “ Versioning : The Presentation of Multiple Texts” . In _Romantic Texts and Contexts_ , University of Missouri Press, Columbia, pp. 167-180.
</li>
<li id="robinson2013">Robinson, P. (2013) “What Digital Humanists Don’t Know about Scholarly Editing; What Scholarly Editors Don’t Know about the Digital World” . Paper presented to Social, Digital, Scholarly Editing, University of Saskatchewan, 11-13 July. Available at:<a href="https://www.academia.edu/4124828/SDSE_2013_why_digital_humanists_should_get_out_of_textual_scholarship">https://www.academia.edu/4124828/SDSE_2013_why_digital_humanists_should_get_out_of_textual_scholarship</a>.
</li>
<li id="sahle2016">Sahle, P. (2016) “What is a Scholarly Digital Edition?” In Driscoll, M.J. and Pierazzo, E. (eds), _Digital Scholarly Editing: Theories and Practices_ , Open Book Publishers, Cambridge, pp. 19-40.<a href="http://dx.doi.org/10.11647/obp.0095.02">http://dx.doi.org/10.11647/obp.0095.02</a>.
</li>
<li id="sahle2019">Sahle, P. (2019) “Some particularly interesting editions/projects” . In _A Catalog of Digital Scholarly Editions_ . Version 3.0, snapshot 2008ff. 19 February. Available at:<a href="http://www.digitale-edition.de/vlet_interesting.html">http://www.digitale-edition.de/vlet_interesting.html</a>. Archived at:<a href="http://web.archive.org/web/20190803232312/http://www.digitale-edition.de/vlet_interesting.html">http://web.archive.org/web/20190803232312/http://www.digitale-edition.de/vlet_interesting.html</a>.
</li>
<li id="schmidt2014">Schmidt, D. (2014) “Towards an Interoperable Digital Scholarly Edition” . _Journal of the Text Encoding Initiative_ , 7. doi:<a href="10.4000/jtei.979">10.4000/jtei.979</a>.
</li>
<li id="SPR"> “Scholarly Peer Review” . (n.d.) ARC. Available at:<a href="http://ar-c.org/about/peer-review/">http://ar-c.org/about/peer-review/</a>(viewed 15 July 2019). Archived at:<a href="http://web.archive.org/web/20190803233452/http://ar-c.org/about/peer-review/">http://web.archive.org/web/20190803233452/http://ar-c.org/about/peer-review/</a>.
</li>
<li id="schreibman2013">Schreibman, S. (2013) “Digital Scholarly Editing” . In Price, K.M. and Siemens, R. (eds), _Literary Studies in the Digital Age: An Evolving Anthology_ , MLA Commons, New York. Available at:<a href="https://dlsanthology.mla.hcommons.org/digital-scholarly-editing/">https://dlsanthology.mla.hcommons.org/digital-scholarly-editing/</a>. Archived at:<a href="http://web.archive.org/web/20190803233920/https://dlsanthology.mla.hcommons.org/digital-scholarly-editing/">http://web.archive.org/web/20190803233920/https://dlsanthology.mla.hcommons.org/digital-scholarly-editing/</a>.
</li>
<li id="shillingsburg1991">Shillingsburg, P.L. (1991) “Text as Matter, Concept, and Action” . _Studies in Bibliography_ , 44, 31-82.
</li>
<li id="shillingsburg1996">Shillingsburg, P.L. (1996) _Scholarly Editing in the Computer Age_ . University of Michigan Press, Ann Arbor.
</li>
<li id="siemens">Siemens, R. et al., eds. (n.d.) _A Social Edition of the Devonshire MS (BL Add. MS 17492)_ . WikiBooks. Available at:<a href="https://en.wikibooks.org/wiki/The_Devonshire_Manuscript">https://en.wikibooks.org/wiki/The_Devonshire_Manuscript</a>.
</li>
<li id="smith2016">Smith, A.M. et al. (2016) “Software Citation Principles” . _PeerJ Computer Science_ , 2:e86. doi:<a href="10.7717/peerj-cs.86">10.7717/peerj-cs.86</a>.
</li>
<li id="smith2004">Smith, M.N. (2004) “Electronic Scholarly Editing” . In Schreibman, S., Siemens, R. and Unsworth, J. (eds) _A Companion to Digital Humanities_ , Blackwell, Malden, MA, pp. 306-322.
</li>
<li id="sperberg-mcqueen2009">Sperberg-McQueen, C.M. (2009) “How to Teach Your Edition How to Swim” . _Literary and Linguistic Computing_ , 24, 27-39.<a href="http://dx.doi.org/10.1093/llc/fqn034">http://dx.doi.org/10.1093/llc/fqn034</a>.
</li>
<li id="sperberg-mcqueen1999">Sperberg-McQueen, C.M. and Burnard, Lou, eds. (1999) _Guidelines for Electronic Text Encoding and Interchange_ . Revised Reprint. May. TEI P3 Encoding Initiative, Chicago, Oxford. Available at:<a href="https://tei-c.org/Vault/GL/P3/index.htm">https://tei-c.org/Vault/GL/P3/index.htm</a>.
</li>
<li id="tanselle1975">Tanselle, G.T. (1975) “The Bibliographical Concepts of Issue and State” . _Papers of the Bibliographical Society of America_ , 69, 17-66.
</li>
<li id="tanselle1980">Tanselle, G.T. (1980) “The Concept of Ideal Copy” . _Studies in Bibliography_ , 33, 18-53.
</li>
<li id="tanselle1992">Tanselle, G.T. (1992) _A Rationale of Textual Criticism_ . University of Pennsylvania Press, Philadelphia.
</li>
<li id="tanselle1995">Tanselle, G.T. (1995) “Critical Editions, Hypertexts, and Genetic Criticism” . _Romanic Review_ , 86, 582-593.
</li>
<li id="tanselle2001">Tanselle, G.T. (2001) “Thoughts on the Authenticity of Electronic Texts” . _Studies in Bibliography_ , 54, 133-136.
</li>
<li id="tarr">Tarr, D. (n.d.) “Sentimental Versioning, Version One dot Oh, okay then” . Available at:<a href="http://sentimentalversioning.org">http://sentimentalversioning.org</a>(viewed 15 July 2018). Archived at:<a href="http://web.archive.org/web/20190803234954/http://sentimentalversioning.org/">http://web.archive.org/web/20190803234954/http://sentimentalversioning.org/</a>.
</li>
<li id="TEI2019">TEI Consortium (2019) _TEI P5: Guidelines for Electronic Text Encoding and Interachange_ . Text Encoding Initiative. Version 3.6.0, revision daa3cc0b9. 16 July. Available at:<a href="https://www.tei-c.org/release/doc/tei-p5-doc/en/html/index.html">https://www.tei-c.org/release/doc/tei-p5-doc/en/html/index.html</a>.
</li>
<li id="TGC2006">TextGrid Consortium (2006-2014) _TextGrid: A Virtual Research Environment for the Humanities_ . TextGrid Consortium, Göttingen. Available at:<a href="https://textgrid.de">https://textgrid.de</a>.
</li>
<li id="turabian1996">Turabian, K.L., Grossman, J.B. and Bennett, A.B. (1996) _A Manual for Writers of Term Papers, Theses, and Dissertations_ . 6th ed. University of Chicago Press, Chicago.
</li>
<li id="turska2016">Turska, M., Cummings, J. and Rahtz, S. (2016) “Challenging the Myth of Presentation in Digital Editions” . _Journal of the Text Encoding Initiative_ , 9.<a href="http://dx.doi.org/10.4000/jtei.1453">http://dx.doi.org/10.4000/jtei.1453</a>.
</li>
<li id="WWAC2019"> “Walt Whitman Archive Changelog” . (2019) Blogger. Available at:<a href="http://wwa-changelog.blogspot.com">http://wwa-changelog.blogspot.com</a>(viewed 3 August 2019).
</li>
<li id="witt">Witt, J.C., ed. (n.d.) The SCTA Reading Room. Scholastic Commentaries and Texts Archive. LombardPress. Available at:<a href="http://scta.lombardpress.org">http://scta.lombardpress.org</a>(viewed 3 August 2019).
</li>
<li id="witt2018">Witt, J.C. (2018) “Digital Scholarly Editions and API Consuming Applications” . In Bleier, R. et al. (eds) _Digital Scholarly Editions as Interfaces_ , Books on Demand, Norderstedt, pp. 219-247. urn:nbn:de:hbz:38-91182. Available at:<a href="http://kups.ub.uni-koeln.de/id/eprint/9118">http://kups.ub.uni-koeln.de/id/eprint/9118</a>.
</li>
<li id="wittern2013">Wittern, C. (2013) “Beyond TEI: Returning the Text to the Reader” . _Journal of the Text Encoding Initiative_ , 4. doi:<a href="10.4000/jtei.691">10.4000/jtei.691</a>.
</li>
<li id="wittern2009">Wittern, C., Ciula, A. and Tuohy, C. (2009) “The Making of TEI P5” . _Literary and Linguistic Computing_ , 24(3), 281-296.<a href="http://dx.doi.org/10.1093/llc/fqp017">http://dx.doi.org/10.1093/llc/fqp017</a>.
</li>
<li id="XMLD"> “XML Download of the Electronic Transcription of Codex Sinaiticus” (n.d.) _Codex Sinaiticus_ , Codex Sinaiticus Project. Available at:<a href="http://codexsinaiticus.org/en/project/transcription_download.aspx">http://codexsinaiticus.org/en/project/transcription_download.aspx</a>(viewed 1 August 2019). Archived at:<a href="http://web.archive.org/web/20190801161134/http://www.codexsinaiticus.org/en/project/transcription_download.aspx">http://web.archive.org/web/20190801161134/http://www.codexsinaiticus.org/en/project/transcription_download.aspx</a>.
</li>
<li id="zeller1975">Zeller, H. (1975) “A New Approach to the Critical Constitution of Literary Texts” . _Studies in Bibliography_ , 28, 231-264.
</li>
</ul>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p><a href="#tanselle2001">Tanselle (2001)</a>objects to the idea that electronic texts are in any way more fluid than printed texts. Modifying electronic files, Tanselle says, alters outputs no more completely or undetectably than does the resetting of type. But Tanselle undervalues the ways in which digital textuality (especially online) collapses the space between creation and publication: a revised forme is not broadcast into copies already printed, but changes made to a file on a webserver will immediately appear to anyone visiting the website, even if they have previously visited it, unless they have taken pains to archive a copy, and any savvy web user understands that an online resource may not be the same as last time they visited. The print world may be following the electronic: in today’s publishing environment, books areborn digital, designed on computer screens and printed with laser printers or with plates that are designed to be disposable and can be regularly recycled and recreated. I learned about current publishing practices from a talk by Matthew Kirschenbaum<a class="footnote-ref" href="#kirschenbaum2017"> [kirschenbaum2017] </a>, which has considerably influenced my thinking. Today’s printed books thus share in digital instability, and their bibliographers and archivists will need to be concerned with many of the issues of digital revision and versioning that I discuss in this article.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>As is frequently noted — see for example<a href="#schreibman2013">Schreibman (2013), ¶41</a>and<a href="#sahle2016">Sahle (2016), 29-30</a>.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>These might be changes to underlying technologies, such as updating a piece of software on a webserver or upgrading to a version of a web framework designed for newer browsers, but they might also be changes in support of the long-term interoperability and accessibility of underlying data, such as migrating data to a new encoding standard after an old one has become obsolete.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>This definition is broadly similar to that offered in<a href="#sahle2016">Sahle (2016)</a>.Textual objectsis an intentionally expansive term, most obviously encompassing literary and historical works and documents, but potentially describing any materials that could be encoded and edited. I make no distinction betweeneditionandarchive(see<a href="#price2009">Price (2009)</a>), and also refer to the organizations and publishing outlets that create and provide access to edition materials asprojectsand to their outputs astextsor, more generally,resources. The versioning problems with which I am concerned affect editions of all size, from ad-hoc encodings of individual documents to large digital archives encompassing many edited texts. While I am primarily thinking of richly encoded editions such as those based on Text Encoding Initiative standards, the issues and solutions I present would apply equally to plain text files.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>In addition to peer review processes offered by publishers, professional organizations have created mechanisms for peer reviewing digital editions. The Modern Language Association’s Committee on Scholarly Editions (MLA CSE) seal, awarded to Approved Editions, is available to print and digital editions alike. Member organizations of the Advanced Research Consortium — the Medieval Electronic Scholarly Alliance (MESA), 18thConnect, Nineteenth-Century Scholarship Online (NINES), ModNets, and Studies in Radicalism Online (SIRO) — also facilitate peer review processes for digital resources including digital editions. According to the ARC, when a node approves a resource, the node’s director issues a letter “geared toward tenure and promotion committees” that “highlights equivalencies to print publications” <a class="footnote-ref" href="#SPR"> [SPR] </a>.&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>Of the thirty projects surveyed in the Appendix, only The Proceedings of the Old Bailey in my assessment prioritizes access to textual data over the reading of text. Turksa et al. emphasizes the degree to which editors, not to mention funders and potential publics, respond to the presentation of digital editions. The massive body of writing emphasizing the flexible displays and interfaces of digital editions implicitly understands them as resources that users will interact with through reading interfaces<a class="footnote-ref" href="#turska2016"> [turska2016] </a>; see for example<a href="#tanselle1995">Tanselle (1995), 591-2</a>and<a href="#shillingsburg1996">Shillingsburg (1996), 163-6</a>. Shillinsburg suggests that digital editions are not well suited for novice or pleasure readers<a class="footnote-ref" href="#shillingsburg1996"> [shillingsburg1996] </a>, a proposition echoed by<a href="#gabler2010">Gabler (2010)</a>: “we read texts in their native print medium, that is, in books; but we study texts and works in editions – in editions that live in the digital medium.” However, note Krista Stinne Greve Rasmussen’s assertion that the role of reader is the foundation for more involved forms of textual study and knowledge creation<a class="footnote-ref" href="#rasmussen2016"> [rasmussen2016] </a>.&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>That is, while commentators have praised the ways in which digital editions expose the editorial process and invite readers to interrogate the editors’ methods (see for example<a href="#smith2004">Smith (2004), 317-318</a>;<a href="#gabler2010">Gabler (2010), 48</a>), digital editions still typically produce texts (even if those texts are multiple or provisional), and readers may well want to reference those texts as texts, using them as a basis for literary analysis, rather than engaging with them as arguments about text. A study of users of the Font Gaia digital library (which includes digitized content, digital exhibits, and digital editions) found that the most common use for the library was to consult documents online, though users were primarilyscanningand reading selectively rather than reading in full — occupying, the study’s author notes, Rasmussen’suserrole<a class="footnote-ref" href="#leblanc2018"> [leblanc2018] </a>.&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>See<a href="#kalvesmaki2014">Kalvesmaki (2014)</a>for a discussion the Canonical Text Services (CTS) standard for digital cross-references. The in-progress Distributed Text Services specification builds on the work of CTS to develop systems for computers to query and retrieve data from digital editions<a class="footnote-ref" href="#DTS2019"> [DTS2019] </a>.&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>In the<a href="#appendix1">Appendix</a>I present the results of an examination of thirty digital editions, finding that only 23% give their material version numbers, and only 13% version individual data files representing distinct texts. Major works devoted to scholarly editing in the digital age also omit discussion of versioning materials post-publication.<a href="#shillingsburg1996">Shillingsburg (1996), 169</a>and<a href="#kline2008">Kline and Perdue (2008), 288</a>both comment approvingly on the ability of editors to make corrections to published electronic editions, and both devote attention to managing data during the preparation of an edition, but neither offers concrete suggestions for managing changes to materials after publication. The essays in<a href="#burnard2006">Burnard et al. (2006)</a>, offer a good deal of practical insight into data issues of digital editions, and two address head-on the challenges posed by the mutability of digital editions<a class="footnote-ref" href="#berrie2006"> [berrie2006] </a><a class="footnote-ref" href="#deegan2006"> [deegan2006] </a>, but none of the contributions suggests clear practices for publicly versioning materials. The MLA CSE’s Guidelines for Editors of Scholarly Editions ask editors to consider the importance of “permanence or fixity” as well as the benefits of “openness and fluidity” (§1.2.3), and ask those charged with vetting editions in all media to determine whether a correction file will be available (§2, questions 22.3-4), and in the case of digital editions whether edition materials have been deposited in a long-term repository (§2, question 28.4); however, the guidelines offer no standards for how digital editions should make users aware of changes or ensure long-term reference<a class="footnote-ref" href="#MLAC2011"> [MLAC2011] </a>.<a href="#pierazzo2015">Pierazzo (2015), 186-187</a>directly addresses the problem of versioning, perhaps heralding a needed increase of attention toward the issue. Pierazzo’s suggestion is to embed a revision control system within a digital edition; I will discuss the limitations of that approach below.&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>For one statement on the impossibility of a definitive edition, see<a href="#tanselle1992">Tanselle (1992), 74</a>. On an edition as presenting a theory about the work being edited, see<a href="#cerquiglini1999">Cerquiglini (1999), 22</a>.&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>The most recent edition of the APA style guide eliminates the recommendation to provide access dates<a class="footnote-ref" href="#PM2010"> [PM2010] </a>. By contrast, the 2003 edition of <em>The Chicago Manual of Style</em> initiated the still-standing Chicago style recommendation against access dates. It also warned against including revision dates, though that stance has since weakened<a class="footnote-ref" href="#CM2003"> [CM2003] </a>.&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p>The blog also provides descriptions of additions and modifications to the <em>Archive</em> website apart from updates to the XML data, though the blog description notes that minor changes in appearance and events such as server outages are not recorded.&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13">
<p>These systems are also known as version control systems; I use the term revision control systems to emphasize the fact that these systems record project data and changes to it, but do not identify versions of the data unless those versions are explicitly labeled within the RCS. The terms are essentially interchangeable in their typical use; Git, for example, identifies itself as a “version control system” on its homepage but as a “revision control system” in its manual<a class="footnote-ref" href="#git"> [git] </a><a class="footnote-ref" href="#gitum"> [gitum] </a>.&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14">
<p>RCSs can still be useful for presenting and exploring project history even if not embedded within the edition. For one endorsement, see<a href="#escobarvarela2016">Escobar Varela (2016) ¶¶34-35</a>. Release tagging, which Escobar Varela highlights, can be used in tandem with version numbering to label a particular state of the repository as representing a specific version — but this works effectively only where the contents of the repository are versioned together as a unit.&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:15">
<p>It would be possible to automatically embed information regarding RCS revisions into data files so they preserved the information even if removed from the RCS, as one project profiled in the<a href="#appendix1">Appendix</a>did; see<a href="#note56">note 56</a>. However, such measures are workarounds that highlight the extent to which RCS revision numbers differ from version identifiers created specifically for a resource.&#160;<a href="#fnref:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:16">
<p>I collaborated with Daniel Paul O’Donnell to use Zenodo to publish and version the source code for the online republication of his digital edition of <em>Cædmon’s Hymn</em> <a class="footnote-ref" href="#odonnell2018"> [odonnell2018] </a>.&#160;<a href="#fnref:16" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:17">
<p>On the prominence of documentary editing in the digital sphere, see<a href="#pierazzo2014">Pierazzo (2014)</a>.&#160;<a href="#fnref:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:18">
<p>I cite only a few productive samples from these wide-ranging debates. For a concise and helpful, though dated, overview, see<a href="#greetham1992">Greetham (1992), 335-346</a>.&#160;<a href="#fnref:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:19">
<p>This is not, of course, to suggest that the edition exists apart from the history of the work edited. A new edition becomes the latest entry in the textual history of the work it edits — it might even be said to constitute a version of that text — and a future study of the reception or evolution of the work might include the edition as one of its objects of study. But for the purposes of publication and data management, we need to think of the edition as its own unit and manage its evolution.&#160;<a href="#fnref:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:20">
<p>In general, the individual numbers making up each segment are independent integers, so that 3.11.15 is a valid version number with major version 3, minor version 11, revision or patch 15.&#160;<a href="#fnref:20" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:21">
<p>The CalVer proposal was released in 2016, but as the authors note, the practices they describe predate the document. Rather than trying to impose a standard format, the CalVer convention seeks to provide a common vocabulary and expose influential practices.&#160;<a href="#fnref:21" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:22">
<p>Ironically, at the time of writing, the Semantic Versioning specification suffers from its own versioning problems; the version cited differs in two minor details from the version available at<a href="https://semver.org/">https://semver.org/</a>(archived at<a href="http://web.archive.org/web/20190803230500/https://semver.org/">http://web.archive.org/web/20190803230500/https://semver.org/</a>), though both are labeled as version 2.0.0.&#160;<a href="#fnref:22" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:23">
<p>In light of this article’s argument, it is worth pointing out that Ashkenas posted his manifesto to GitHub’s Gist service, which versions files using Git. The document has been revised several times since its creation, and the hash in the URL allows me to link to a particular state of the document, but does not provide a way to signal how the state I cite (the most recent at the time of writing) relates to other states.&#160;<a href="#fnref:23" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:24">
<p>For a satiric presentation ofsentimental versioning, see<a href="#tarr">Tarr (n.d.)</a>.&#160;<a href="#fnref:24" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:25">
<p>Boot and van Zundert stress the importance of versioning the individual resources within the edition-networks they imagine, and suggest that the systems for managing data and services should even handle the versioning of platform infrastructure such as the operating systems on which technical services may depend<a class="footnote-ref" href="#boot2011"> [boot2011] </a>.&#160;<a href="#fnref:25" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:26">
<p><a href="#witt2018">Witt (2018)</a>argues for making APIs the foundational avenues of data access and for constructing user interfaces as applications that consume data through APIs — if adopted at scale, an elegant approach to multiplicity and reuse.&#160;<a href="#fnref:26" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:27">
<p>See<a href="#badley2012">Bradley (2012)</a>for one critique of the marginalization of collaborators with technical expertise astechies, which Bradley argues improperly reduces technical contributions to “support work” <a class="footnote-ref" href="#bradley2012"> [bradley2012] </a>rather than recognizing the important intellectual contributions and innovations that all partners bring to the table. Bradley specifically notes the importance of “blending of the understanding of the materials with which one is working with an understanding of how to exploit the technology to emphasize what is important” as one important area of partnership<a class="footnote-ref" href="#bradley2012"> [bradley2012] </a>. I hope it will be clear that the separation of versioning I propose is not meant to downplay the intellectual importance of the technological components, either from the perspective of labor or from the perspective of scholarly resources.&#160;<a href="#fnref:27" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:28">
<p>A related problem exists in the study of videogames, where many older games are experienced using emulators and researched through ROM files extracted from original media by third parties. For a discussion of the bibliographic description of such objects, see<a href="#altice2015">Altice (2015), 333-341</a>, which argues among other things that videogame scholars should cite even the emulators they use to examine such files.&#160;<a href="#fnref:28" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:29">
<p>For example, in modern web development, the accepted best practice is for the structure of the document to specified in HTML, while formatting is applied using CSS. For a discussion of this principle, see<a href="#berners-lee1998">Berners-Lee (1998)</a>. In the context of digital editing, see<a href="#pichler2014">Pichler and Bruvik (2014)</a>.&#160;<a href="#fnref:29" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:30">
<p>For a critique of the separation from the standpoint of textual studies, see<a href="#galey2010">Galey (2010), 110-114</a>.&#160;<a href="#fnref:30" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:31">
<p>The TEI Guidelines credit this separation as a characteristic of the XML encoding language, which emphasizesdescriptiverather thanproceduralmarkup: that is, the markup categorizes pieces of a document according to what they mean or the structural purpose they serve rather than according to how they should be formatted; the formatting of a published document should be accomplished through other mechanisms<a class="footnote-ref" href="#TEI2019"> [TEI2019] </a>. That is not to say that the TEI Guidelines lack any facilities for describing the appearance of texts, but the Guidelines stress that components related to visual appearance are intended to describe a source document, not its desired output appearance (§1.3.1.1.3), and note that markup describing the visual features of a source document is descriptive markup (§v.2).&#160;<a href="#fnref:31" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:32">
<p><a href="#r%C3%A9gnier2015">Régnier (2014), 76</a>, argues that “philologists can . . . be held responsible for the functional and aesthetic quality of the digital framework to which they entrust their work” and insists that “they have to collaborate on the invention of digitized text standards” like the visual codes that coalesced as standards for print scholarship.&#160;<a href="#fnref:32" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:33">
<p>Sperberg-McQueen declares that the selection and presentation together constitute the interface of an edition. I use the term interface differently, to refer to a mechanism through which the edition exposes its information, whether displaying it visually through a graphical user interface (GUI) or exposing it to other computer programs by means of an application programming interface (API).&#160;<a href="#fnref:33" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:34">
<p>Other platforms might require the edition’s maintainers to perform some action to update the data file’s derivatives, for example running a script to generate new HTML files for web display by applying an XSLT transformation to a source XML file. Only if the edition is exceptionally tightly packaged is it likely that the software of the edition must also be regenerated, and even if it is, the resulting display software will not be materially different.&#160;<a href="#fnref:34" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:35">
<p>For an introductory overview to issues of digital preservation, see<a href="#kilbride2016">Kilbride (2016)</a>. For a discussion focused on digital editions, see<a href="#deegan2006">Deegan (2006)</a>. In combining text with (sometimes custom-built) software interfaces, digital editions present problems closely related to those involving other electronic literature.<a href="#liu2005">Liu et al. (2005)</a>argue for the value of creating an XML-based format that can make content and portions of the experience of such works available even where the full experience cannot be recreated due to the obsolescence of software or hardware.&#160;<a href="#fnref:35" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:36">
<p>See, for example,<a href="#turska2016">Turska et al. (2016)</a>, which argues that encoded data are the most important output of editing projects but suggests that editors are concerned with presentation and so lowering the barriers to publication will help them get down to the business of creating data.&#160;<a href="#fnref:36" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:37">
<p><a href="#gants2010">Gants (2010), 133-134</a>, considering the issues involved in describing a work of interactive fiction that takes the form of a computer program, proposes a similar identity. Using Bowers’s bibliographic framework, Gants compares the source code for the game to a single setting of type; compiling the game into executable code that will run on separate operating systems, he suggests, is analogous to reimposition in other formats.&#160;<a href="#fnref:37" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:38">
<p>For an overview of the importance and challenges of software preservation and curation and a discussion of the role research libraries might play, see<a href="#chassanoff2018">Chassanoff et al. (2018)</a>.&#160;<a href="#fnref:38" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:39">
<p>On the history of the PPEA, see<a href="#knowles2014">Knowles and Stinson (2014)</a>. On its early publication practices, see<a href="#duggan2005">Duggan and Lyman (2005)</a>.&#160;<a href="#fnref:39" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:40">
<p>The first volume of this series was published in 2018:<a href="#burrow2018">Burrow and Turville-Petre (2018)</a>.&#160;<a href="#fnref:40" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:41">
<p>Publication, for our purposes, means the official appearance of an edition on the public pages of the PPEA website. Because in digital scholarship the lines between unpublished and published materials have become increasingly blurred — many editors and projects, including the PPEA, make draft materials available — it may be appropriate to version prerelease materials as well, and similar procedures could apply. However, in drawing a distinction between unpublished and published materials, I emphasize that published materials have been officially recognized as appropriate for reference and citation, so users expect to be able to rely on it. Formal versioning practices support that implicit contract with users.&#160;<a href="#fnref:41" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:42">
<p><a href="#odonnell2008">O&rsquo;Donnell (2008)</a>uses the example of an earlier SEENET publication of his edition of <em>Cædmon’s Hymn</em> to argue that print works can be outputs of digital editing. For the current print series, volumes are produced by transforming the XML source into LaTeX markup (which might be finessed by hand to improve page layout). The LaTeX markup is compiled into a PDF, which is used to print the physical volume. The physical book is thus the product of transformations of the XML source, just like the web display. The copyright page of the print book contains a statement of the version number from which it was printed, asserting the identity between them.&#160;<a href="#fnref:42" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:43">
<p>Inspired in part by the careful and precise distinctions suggested by Semantic Versioning, I at first attempted to theorizebreaking changesfor the digital edition, trying to identify what kinds of change would render two states of the same fileincompatiblewith each other. However, I soon realized that identifyingbreaking changesrequires committing to a particular theory of the digital edited text and the primary form of interface it provides. People using the edition mainly as a documentary text will have different concerns from those most interested in the editors’ arguments; those studying dialect will prioritize different features from those examining scribal decoration and again from scholars interested in markup practices; readers working directly with the XML files will have a very different experience of changes from the probable majority who are reading through the mediation of a web interface. In the context of digital editing, nearly every change is potentially a breaking change for someone (a claim Ashkenas made even about many software packages).&#160;<a href="#fnref:43" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:44">
<p>On the intellectual and technical differences between P4 and P5, see<a href="#wittern2009">Wittern et al. (2009)</a>, which observes that one of the changes it discusses marks “a fundamental change in the relationship between textual content and markup” <a class="footnote-ref" href="#wittern2009"> [wittern2009] </a>. Automated tools were developed to aid the transition from P4 to P5, and for simple files those tools might suffice, but the differences between P4 and P5 are sufficiently significant that at the conversion requires careful assessment and may require manual intervention. Because changes of this nature are interpretative, and distributed throughout the document, they amount to a significant overhaul.&#160;<a href="#fnref:44" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:45">
<p>However, some types of widespread changes do not rise to the level of constituting a new major version, because they are intellectually trivial and do not involve theories of the text or its encoding. One example previously encountered by the project is changing the format in which line numbers are written without changing the numbers themselves.&#160;<a href="#fnref:45" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:46">
<p>Desmond Schmidt argues that stand-off markup is essential to interoperability and should become the dominant approach to digital editing. In stand-off markup, a resource and its markup might reside in different files, and depending on technical and procedural approaches might have to be versioned separately. Stand-off markup also complicates the notion of versioning because stand-off markup is entangled with the text to which it is applied: though the markup may be stored externally, changes to the “source” document are likely to necessitate corresponding changes to the stand-off annotations in order to maintain their relationship. Schmidt’s discussion offers one possible way forward: he describes his stand-off alternative to a conventional document-based edition as a “bundle” of materials in separate files, and notes that this collection of files might be stored in a single container files<a class="footnote-ref" href="#schmidt2014"> [schmidt2014] </a>. An editorial bundle consisting of a “source” document, stand-off files, and metadata might thus be versioned as a single unit, in the same way that multiple research data files may be combined into a data package consisting as multiple files, versioned as one unit.&#160;<a href="#fnref:46" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:47">
<p>See for example<a href="#kuczera2016">Kuczera (2016)</a>.&#160;<a href="#fnref:47" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:48">
<p>I give the short titles provided by Sahle in order to facilitate easy cross-referencing with his list, where fuller citations and hyperlinks are available. On August 4, 2019, I used the Internet Archive Wayback Machine (<a href="https://web.archive.org">https://web.archive.org</a>) to archive a copy of the landing page for each site on Sahle’s list, as well as all the links from that page that the Archive was able to automatically follow. That process does not preserve the sites in full, but it does establish a partial record of how the sites presented themselves when I consulted them. The archived versions may be accessed by entering the URL for each site at the Wayback Machine and navigating to the date in the site history.&#160;<a href="#fnref:48" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:49">
<p>I recorded a project as keeping change logs in data files if I actually found such logs (for instance, in a <revisionDesc> element) or if the project’s technical documentation discusses creating them.&#160;<a href="#fnref:49" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:50">
<p>Resources offering version numbers or change logs often record the dates of the changes; this column notes only those instances where a site records the date of last update in the absence of other change information. With the exception of <em>Van Gogh</em> , the resources listed in this column provide a single date of last update for the site as a whole.&#160;<a href="#fnref:50" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:51">
<p>I accessed the online sample of this resource. The URL button at the top of the screen, which provides a recommended citation, describes this as therevised edition, and gives the publication date as 2011. The Credits page provides only the original date of 2003. I have not considered that statement of a revised edition, which does not seem to be repeated elsewhere, to constitute a version identifier.&#160;<a href="#fnref:51" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:52">
<p>Most of the materials published by this project are available only to subscribers; in addition to the project’s documentation, I consulted the freely available demo versions of a few texts made available on the site.&#160;<a href="#fnref:52" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:53">
<p>I accessed the online sample of this resource.&#160;<a href="#fnref:53" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:54">
<p>A Website History page provides updates on additions to the site materials, and individual pages, such as the Finding Aid resource, contain their own revision histories. The About page claims that “versioned corrections and revisions of the pages take place continuously” <a class="footnote-ref" href="#huber2019"> [huber2019] </a>, but I have not been able to find version identifiers or retrieve old versions — though the same page notes that archived versions of the site are available upon request.&#160;<a href="#fnref:54" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:55">
<p>The URL provided by Sahle directs to the Plaoul Commentary as housed within the Scholastic Commentaries and Texts Archive (<a href="http://scta.lombardpress.org/text/questions/plaoulcommentary">http://scta.lombardpress.org/text/questions/plaoulcommentary</a>). It is this form of the resource that I have examined.&#160;<a href="#fnref:55" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:56">
<p>Each data file includes an XML comment at the beginning of the file that provides both the data and time when the project data was last modified and aSVN Revisionnumber: presumably the revision number in the Subversion repository in which the project is stored. (Apache Subversion is a revision control system predating Git.) This comment is likely created using an automated software tool when changes are committed to the Subversion repository, and all files appear to be labeled with the time and revision number of the latest commit to the project repository as a whole. I do not count these SVN revision numbers as version numbers because, as I discuss in this article, versioning involves intention and judgment. (Moreover, the revision numbers are not displayed outside of the data files.) However, SVN revision numbers do resemble project-wide version numbers more than do Git commit hashes: in SVN, revision numbers take the form of integers and each is one greater than the previous. Accordingly, these revision numbers could be used both to identify a particular state of the project and to understand the sequence among versions of a file.&#160;<a href="#fnref:56" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:57">
<p>The Project Info menu option describes the site release asSecond digital edition (beta 2).However, the six listed revisions suggest a more complicated change history than this numbering expresses, so I do not count it as a meaningful site-wide version number. Thesecond editionappears to refer to the platform rather than to the underlying data or to the site as a whole.&#160;<a href="#fnref:57" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:58">
<p>Does not provide access to underlying TEI files. The list of changes accompanying the edition exists in a format that might have been generated using <change> elements in the <revisionDesc> section of the TEI header, though it is impossible to be sure without the underlying XML.&#160;<a href="#fnref:58" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:59">
<p>At the time of writing, <em>Proceedings of the Old Bailey</em> is on version 8.0, <em>Schnitzler</em> is on beta version 2.0, and <em>WeGA</em> is on version 3.4.&#160;<a href="#fnref:59" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:60">
<p>However, it is ambiguous whether all content changes register in the site’s version history, which currently only notes the addition of a new resource in conjunction with the release of Beta 2.0. Following its explanation of its versioning practices, the site explains, “Kleinere, z.B. von Benutzern gemeldete Fehler (Bugfixing und inhaltliche Fehlerkorrekturen) werden laufend behoben” [Smaller, e.g. user-reported errors (bugfixes and content corrections) are continuously fixed]; it is unclear whether these ongoing corrections are recorded or versioned.&#160;<a href="#fnref:60" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:61">
<p>I was able to locate only two texts in the Archive with a stated Archive Edition number of 2:<a href="#hunt">Hunt (n.d.)</a>;<a href="#MDGR">Masterpieces of D.G. Rossetti (n.d.)</a>. For neither does the publicly available XML source include any metadata on revisions (the <revisionDesc> element is empty in both), so it is impossible to get a sense of what sorts of changes constitute a new edition. Possibly the publication of a new edition was considered to reset the revision state of the document, so that records of changes from the previous edition need not be preserved. It is also possible that the second edition resulted from creating the resources anew a second time. I looked for such materials by conducting a Google search of the rossettiarchive.org domain for the phrase “Electronic Archive Edition: 2” (and higher numbers).&#160;<a href="#fnref:61" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:62">
<p>The <em>Rossetti Archive</em> is not actually encoded in TEI, which its creators found insufficient for the needs of their materials<a class="footnote-ref" href="#mcgann2001"> [mcgann2001] </a>. However, the <em>Archive</em> ’s encoding principles drew upon the standards of the TEI, and the <code>&lt;editionStmt&gt;</code> element of the file header is one component derived from the TEI. Early version of the TEI Guidelines are available at<a href="https://tei-c.org/Vault/Vault-GL.html">https://tei-c.org/Vault/Vault-GL.html</a>. For the earliest version of the <code>&lt;editionStmt&gt;</code> recommendation readily available online, see<a href="#sperberg-mcqueen1999">Sperberg-McQueen and Burnard (1999), §5.2.2</a>.## Bibliography&#160;<a href="#fnref:62" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry><entry><title type="html">Digital Humanities and Natural Language Processing: Je t’aime... Moi non plus</title><link href="https://rlskoeser.github.io/dhqwords/vol/14/2/000454/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://rlskoeser.github.io/dhqwords/vol/14/2/000454/</id><author><name>Barbara McGillivray</name></author><author><name>Thierry Poibeau</name></author><author><name>Pablo Ruiz Fabo</name></author><published>2020-06-19T00:00:00+00:00</published><updated>2023-08-06T15:53:44-04:00</updated><content type="html"><![CDATA[<h2 id="introduction1">Introduction<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></h2>
<p>The recent years have witnessed an increased interest in Digital Humanities (DH) textual datasets within the Natural Language Processing (NLP) community, as several initiatives (such as the Computational Humanities group at Leipzig<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> and the Computational Humanities committee),<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup> workshops (such as Computational Humanities 2014,<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup> Teach4DH,<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup> COMHUM 2018,<sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup> and the various editions of the LaTeCH-CLfL workshops),<sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup> and publications testify to<a class="footnote-ref" href="#biemann2014"> [biemann2014] </a><a class="footnote-ref" href="#nyhan2016"> [nyhan2016] </a><a class="footnote-ref" href="#jenset2017"> [jenset2017] </a><a class="footnote-ref" href="#vanderzwaan2017"> [vanderzwaan2017] </a><a class="footnote-ref" href="#bamman2017"> [bamman2017] </a><a class="footnote-ref" href="#schulz2018"> [schulz2018] </a><a class="footnote-ref" href="#hinrichs2019"> [hinrichs2019] </a>. Research in this area has focussed on developing new computational techniques to analyse and model humanities data. This interest stems from the methodological and technical challenges posed by these datasets, including those related to non-standard textual or multi-modal input, historical languages, multilinguality, and the need for advanced methods to improve the quality of digitization and for semi-automatic annotation tools. A number of successful results have been achieved, especially in the area of handwriting recognition<a class="footnote-ref" href="#christlein2018"> [christlein2018] </a>, computational stylistics<a class="footnote-ref" href="#boukhaled2016"> [boukhaled2016] </a>, historical natural language processing pipelines<a class="footnote-ref" href="#piotrowski2012"> [piotrowski2012] </a>, authorship attribution<a class="footnote-ref" href="#stamatatos2009"> [stamatatos2009] </a>, and semantic change detection<a class="footnote-ref" href="#tang2018"> [tang2018] </a>, to name a few examples.</p>
<p>In spite of this growing activity, there is a real danger that NLP research on DH datasets does not take into account all the complexities of the phenomena and corpora, as others have pointed out<a class="footnote-ref" href="#dubossarsky2017"> [dubossarsky2017] </a><a class="footnote-ref" href="#hellrich2016"> [hellrich2016] </a>. Moreover, NLP work on DH data has often been confined within the limits of the NLP community, which leads to serious methodological limitations for its applicability to DH and humanities research. In spite of the huge potential impact of NLP for DH datasets, NLP activities aimed at applying and adapting NLP research to the needs of the humanities are still marginal. This can be explained by the standard processes that the discipline adopts. Because the emphasis is on developing new computational systems or improving existing ones, it is very important that these are evaluated on standard datasets using reproducible methods. This means that there is an incentive for NLP researchers to work on very restricted sets of datasets and languages, leading to the development of tools which are optimized for those datasets and languages. This drives research towards a very specific direction, away from the idiosyncratic features displayed by historical languages and DH data. Moreover, publication venues dedicated to NLP methods for DH are few and do not set the mainstream agenda of the field. Coupled with the challenges and the effort required to work on DH datasets, this means that engaging with this line of research appears to be a less than attractive option for most scholars.</p>
<p>On the other hand, a large part of humanities research involves analysing and interpreting written texts. Over the past few years large digital text collections have become available to the scholarly community, but where DH scholars confront Big Data to answer humanities questions, they often rely on methodologically un-sophisticated tools such Google Books Ngram Viewer<a class="footnote-ref" href="#greenfield2013"> [greenfield2013] </a>. There is a real danger that these non-scientifically rigorous approaches will become state of the art<a class="footnote-ref" href="#pechenick2015"> [pechenick2015] </a>.</p>
<p>In this article we aim to draw attention to the lack of communication between the communities of NLP and DH. In spite of its damaging effect on the progress of the disciplines, we believe this lack of communication and miscommunication are underestimated. We argue that what is needed is to bridge the gap between the highly technical state of the art in NLP and the needs of the DH community. We also offer a solution to this situation, inviting DH researchers to play a more active role in making NLP tools work for their data in order to give new insights into their questions, while at the same time advocating for a higher profile of NLP research applied to humanities data. Institutions also need to play a role in enabling better communication between the two communities, promoting interdisciplinary work and multi-author publications; publication venues welcoming such NLP/DH collaborative research also have an important role to play.</p>
<h2 id="contexts">Contexts</h2>
<p>An informal definition of the scope of DH was given by Fitzpatrick commenting on the DH 2010 conference, as “a nexus of fields within which scholars use computing technologies to investigate the kinds of questions that are traditional to the humanities [&hellip;] or who ask traditional kinds of humanities-oriented questions about computing technologies” <a class="footnote-ref" href="#fitzpatrick2010"> [fitzpatrick2010] </a>. Though informal, this broad characterization agrees with the variety of work described as DH in overviews of the field<a class="footnote-ref" href="#berry2012"> [berry2012] </a><a class="footnote-ref" href="#schreibman2004"> [schreibman2004] </a>.</p>
<p>More recently, some authors<a class="footnote-ref" href="#biemann2014"> [biemann2014] </a>have observed two types of research in the work described as DH in the overviews just cited. First, what Biemann et al. call DHproper, which in their characterization focuses on digital resource creation and access. Second, research which these authors callComputational Humanities, and which analyzes digital materials with advanced computational techniques, while trying to assess the value of those computational means for addressing humanities questions. They see work in what they termComputational Humanitiesas situated in a continuum between the humanities or the DH (according to their definition of the latter term) and Computer Science. Therefore, should we want to adopt the Digital vs. Computational Humanities terminology sometimes proposed, the work referred to here can be considered within the Computational Humanities. However, in the rest of this paper we will use the more widely adopted term ofDH. In 2019, a heated debate emerged around the role of computational analysis in the study of literature specifically, after the publication of Nan Da&rsquo;s paper, which questions whether the computational analysis of literary texts can bring any additional insight in literary studies<a class="footnote-ref" href="#da2019"> [da2019] </a>. Counterarguments to such claims were offered by computational literary studies researchers<a class="footnote-ref" href="#criticalinquiry2019"> [criticalinquiry2019] </a><a class="footnote-ref" href="#culturalanalytics2020"> [culturalanalytics2020] </a>. We do believe that computational methods can contribute to literary text analysis, and cite some examples related to character identification below. We agree, however, with Da&rsquo;s emphasis on the need to use computational tools optimally<a class="footnote-ref" href="#da2019"> [da2019] </a>. As we argue below, this can involve considerable work in order to adapt to the specifics of a DH-relevant dataset, going beyond the tools&rsquo; default configuration and requiring at times novel evaluation procedures.</p>
<p>Data relevant for social sciences and humanities research often take the shape of large masses of unstructured text, which is impossible to analyze manually. For example, regarding the use of textual evidence in political science, a variety of relevant text types have been identified, such as regulations issued by different organizations, international negotiation documents, and news reports<a class="footnote-ref" href="#grimmer2013"> [grimmer2013] </a>. Grimmer and Brandon conclude that “[t]he primary problem is volume: there are simply too many political texts” . In the case of literary studies, the complete text of thousands of works spanning a literary period<a class="footnote-ref" href="#clement2008"> [clement2008] </a><a class="footnote-ref" href="#moretti2005"> [moretti2005] </a>are beyond a scholar’s reading capacity, and researchers turn to automated analyses that may facilitate the understanding of relevant aspects of those corpora.</p>
<p>Because DH researchers now face volumes of data that cannot be analyzed manually, NLP technologies need to be applied and adapted to specific use cases, integrating them in user interfaces to make the technology more easily usable by domain experts from the humanities and social sciences. Besides, a critical reflection on the computational tools and methods developed must be initiated, based on an evaluation by domain experts who are expected to benefit from those technological means.</p>
<p>We argue that researchers in the social sciences and humanities need ways to gain relevant access to large corpora. NLP can help provide an overview of a corpus, by automatically extracting actors, concepts, and the relations between them. However, NLP tools do not perform equally well on all texts and may require adaptation. Furthermore, the connection between these tools’ outputs and research questions in a domain expert’s field may not be immediately obvious and needs to be made explicit and kept in mind in the development of computational tools. Finally, evaluating the usefulness of an NLP-based tool for a domain expert is not trivial and ways to enable accurate and helpful evaluations need to be devised.</p>
<h2 id="different-datasets">Different Datasets</h2>
<p>Research in NLP aims to build tools and algorithms for the automatic processing of language<a class="footnote-ref" href="#jurafsky2009"> [jurafsky2009] </a>. In NLP, such systems are typically evaluated against baseline and existing systems with the aim to improve various measures of accuracy, coverage, and performance. Because the focus is on developing optimal algorithms, it is common practice to build and evaluate them based on existing, standard corpora. This way, it is possible to compare different approaches in a systematic way.</p>
<p>In the case of DH research, however, the focus is not so much on the algorithms as on the results that they lead to, which help the researchers answer their research questions. In this context, each study tends to focus on a specific and often unique dataset, with an emphasis on achieving satisfactory and insightful results using or adapting existing methods, if possible.</p>
<p>Several differences separate corpora typically used in NLP research from such DH datasets. First of all, size is typically very large in the former case, unless it is a particular aim of the research to optimize algorithms for small datasets. Moreover, with the exception of the cases in which the NLP research is focussed on a specific domain (such as medical, legal, etc.), balanced corpora are typically used. This ensures that the systems developed on such corpora can be generalized to the language as a whole, in line with generally accepted definitions of a corpus as “a collection of pieces of language text in electronic form, selected according to external criteria to represent, as far as possible, a language or language variety as a source of data for linguistic research” <a class="footnote-ref" href="#sinclair2004"> [sinclair2004] </a>. In DH research, on the other hand, if answers to more general questions are sought, they rarely concern linguistic phenomena per se, and the aim of generalization is instead replaced by the need to describe and further explore the datasets at hand. Sometimes these take the form of archives, which, unlike corpora, are not collected with the aim to represent a language variety. Therefore, it may not be an option to modify the size of the corpus without changing the scope of the research, as the research question is bound to a specific set of texts. In the case of historical languages, for example, unlike current languages, it is often not possible to increase the size of the data because the amount of transmitted texts is limited by particular historical circumstances<a class="footnote-ref" href="#mcgillivray2014"> [mcgillivray2014] </a>.</p>
<p>DH studies normally do not involve balanced datasets according to criteria such as genre, style, register, etc. Another important difference concerns the quality and format of the texts, with particular reference to historical texts. Because almost every DH study requires its own dataset, a necessary preliminary step of the research process consists in acquiring the texts, if they are not already available. When Optical Character Recognition (OCR) is chosen, the texts will likely require a significant amount of processing before they reach an acceptable level of quality, otherwise there is a serious risk that standard NLP tools will fail to provide satisfactory results<a class="footnote-ref" href="#piotrowski2012"> [piotrowski2012] </a>. This is sometimes complemented with other challenges inherent to historical texts and which have to do with diachronic variation, including spelling variation and other types of language change. This means that a diachronic dimension needs to be included in the text processing when applying NLP tools developed for current languages or based on synchronic corpora. We argue that DH corpora can usually be used without the privacy concerns of user-generated data. At the same time, the challenges that come with them offer a good test case not only to pursue meaningful DH questions, but also to measure the robustness of state-of-the-art algorithms.</p>
<p>Natively digital texts are not exempt from the caveats above. A case illustrating this is the work on the <em>Earth Negotiations Bulletin</em> (ENB) corpus<a class="footnote-ref" href="#ruiz2016"> [ruiz2016] </a><a class="footnote-ref" href="#ruiz2017"> [ruiz2017] </a>. ENB’s volume 12 consists of reports summarizing participants’ statements at international climate negotiation summits, where global climate agreements are handled. The goal of the study was to automatically identify where participants stand with respect to negotiation issues (support or opposition) as well as regarding other participants, thanks to syntactic and semantic role annotations obtained with an NLP toolkit. The corpus covers a period starting in 1995, and it contains what we might callhistorical HTML varieties, as well as fixed column-width plain-text formats that required normalization before the content can be input to the NLP pipeline. In addition to this type of normalization, language use in the corpus has several non-standard traits, for which it was necessary to adapt the NLP toolchain (see section 4 below). In other respects, this corpus displays some of the typical features of DH datasets that we described above: it does not seek to represent a linguistic phenomenon, instead it is the set of texts required to answer domain-specific research questions. Venturini et al. argued that the corpus is a good choice to study climate negotiations, given that the corpus editors strive to cover participants in a balanced way, using a neutral and controlled language, although its specific linguistic characteristics need not generalize to political reporting texts<a class="footnote-ref" href="#venturini2014"> [venturini2014] </a>.</p>
<p>When the texts are already available to the DH community, they are often encoded according to the Text Encoding Initiative (TEI) markup, which has become a standard in DH. This markup focusses on editorial aspects and structural properties of the texts themselves, and less on their linguistic features. This contrasts with the state of the art in NLP, where corpora typically have shallow metadata information and are often rich in linguistic annotation<a class="footnote-ref" href="#jenset2017"> [jenset2017] </a>. Some linguistic corpora using TEI are however available, e.g. the National Corpus of Polish<a class="footnote-ref" href="#bański2009"> [bański2009] </a><a class="footnote-ref" href="#przepiórkowski2009"> [przepiórkowski2009] </a>. Although traditions have diverged since then, at its inception the TEI was sponsored by both DH-related and NLP-related professional associations<a class="footnote-ref" href="#cummings2007"> [cummings2007] </a>. Also, major cultural works encoded in TEI have been annotated with morphosyntactic information, e.g. Dante’s works<a class="footnote-ref" href="#dante2018"> [dante2018] </a>. Therefore, extra processing and particular attention is required when employing NLP tools on such texts.</p>
<h2 id="nlp-techniques-in-dh-and-their-challenges">NLP Techniques in DH and Their Challenges</h2>
<p>The DH research process can sometimes include an NLP pipeline, such as sentence segmentation, tokenization, lemmatization or stemming, or part-of-speech tagging. These steps are often required in order to conduct further analyses, either because they contribute tocleaningthe texts, or because they help the researchers identify individual linguistic elements of interest such as word tokens, or classes such as parts-of-speech. This is often complemented by the use of corpus linguistics techniques, such as collocation analysis or various quantitative analyses of linguistic elements. In other words, the boundary between computational and corpus linguistics blurs in the interest of the DH research questions<a class="footnote-ref" href="#jenset2017"> [jenset2017] </a>.</p>
<p>Partially related to and building on the previous point, another way in which NLP methods are used in DH research concerns those techniques that extract various types of structured information from texts, including keywords, named entities, and relations. Again, these steps usually precede further analysis, in the form of qualitative or quantitative investigations, and can rest on other levels of linguistic processing, such as syntactic or semantic parsing.</p>
<p>Semantic processing can also support the identification and analysis of more abstract entities, such as semantic fields or concepts, and their linguistic realization in texts. For example, McGregor and McGillivray report on a methodology for extracting explicit occurrences of smell in historical medical records, the Medical Officer of Health Reports collected in London in the 19th and 20th century<a class="footnote-ref" href="#mcgregor2018"> [mcgregor2018] </a>. Given the large size of the dataset, automatic detection of instances of smell-related words by drawing on the distributional semantic properties of a small set of seed words enables medical historians to extract relevant passages in the texts, which can then be further analyzed, for instance geographically or diachronically.</p>
<p>Some types of information are generally useful to understand a corpus. These include actors mentioned in it (e.g. people, organizations, characters), core concepts or notions of specific relevance for the corpus domain, as well as the relations between those actors and those concepts. This is a critical element in the analysis of the ENB, the climate diplomacy corpus mentioned in section 3<a class="footnote-ref" href="#ruiz2017"> [ruiz2017] </a>. As the corpus covers international negotiation reports, in order to understand its content it is essential to know not only which concepts were discussed in the negotiation, but also who discussed them and with which attitude (in a supporting or oppositional manner). Syntactic dependencies, semantic role annotations and coreference chains obtained with an NLP toolkit were exploited to that end. The agent of a reporting predicate was identified as a negotiation actor, and the message tied to that reporting predicate was considered to express negotiation issues addressed by the agent. The predicate itself (a reporting verb or noun) was seen as expressing the actor’s attitude. The corpus has some non-standard linguistic features, and the NLP toolchain had to be adapted to handle them. For instance, personal pronounshe,shecan refer to countries in this corpus, so anaphora resolution had to be modified accordingly. Corpus sentences contain information about several participants at the same time (multiple actors mentioned within the agent role, or in adjunct roles), which also required a specific treatment in order to identify as many relations as possible of the type ⟨actor,predicate,concept⟩. A good coverage of such relations is crucial to analyses on negotiation behaviour relevant to domain experts.</p>
<p>A widespread approach to gain an overview of a corpus relies on network graphs called concept networks, social networks or socio-technical networks, depending on their content<a class="footnote-ref" href="#diesner2012"> [diesner2012] </a>. In such graphs, the nodes represent terms relevant in the corpus (actors and concepts), and the edges represent either a relation between the terms (likesupportoropposition), or a notion of proximity between them, based on overlap between their contexts. Creating networks requires a method to identify nodes, as well as a way to extract relations between nodes or to define node proximity, such as a clustering method. Networks have yielded very useful results for social sciences and humanities research. To cite an example, Baya-Laffite et al. and Venturini et al. created concept networks to describe key issues in 30 years of international climate negotiations described in the ENB corpus, providing new insights regarding the evolution of negotiation topics<a class="footnote-ref" href="#baya-laffite2016"> [baya-laffite2016] </a><a class="footnote-ref" href="#venturini2014"> [venturini2014] </a></p>
<p>Established techniques to extract networks from text exist, and networks offer useful corpus navigation possibilities. However, NLP can complement widespread methods for network creation. Sequence labeling and disambiguation techniques like Entity Linking can be exploited to identify the network’s nodes: actors and concepts. The automatic definition of network edges is usually based on node co-occurrence, while more detailed information about the relation between actors and concepts is not usually automatically identified for defining edges. Nonetheless, such information can also be obtained via NLP methods. As for corpus navigation, networks do not in themselves provide access to the corpus fragments that were used as evidence to create the networks; but they can be complemented with search workflows that allow researchers to access the contexts for network nodes and the textual evidence for the relations between them.</p>
<p>The above-mentioned techniques are, in most cases, considered solved problems in NLP research, and are normally developed and tested on a set of large standard synchronic corpora of current languages. However, when applied tomessy, noisy, and/or historical texts, these tasks prove to be significantly more challenging. This points to the need of more research to bridge the gap between the DH and NLP communities, so that NLP tools can be developed with the requirements of the former in mind. In this context, Perrone et al.<a class="footnote-ref" href="#perrone2019"> [perrone2019] </a>develop further a Bayesian learning model for semantic change detection developed by Frermann and Lapata<a class="footnote-ref" href="#frermann2016"> [frermann2016] </a>. Frermann and Lapata’s original system was built with the aim to model the change in meaning of English words in a general corpus covering the time period 1700-2010. Perrone et al. extend this to the case of Ancient Greek. This extension is not trivial, as the differences between the two corpora are substantial. The Ancient Greek corpus covers 13 centuries and its content is highly skewed, as certain centuries are only represented by one author or very few works, and some genres only appear in certain centuries. This lack of balance means that the models need to account for the differences in the texts’ features and their effect on word semantics. For example, the sense of a word is highly dependent on the genre of the text it appears in. The Ancient Greek wordmuscan meanmuscle,mussel, andmouse, but in medical texts is it more likely to mean ‘muscle’, independently of the era of the text. As a result, Perrone et al. modified the original system to incorporate genre information into the statistical model, thus achieving improved accuracy of the system compared to the English case.</p>
<p>Applying NLP for text analysis in social sciences and humanities poses some specific challenges. First of all, researchers in these domains work on texts displaying a large thematic and formal variety, whereas NLP tools have been trained on a small range of text types, e.g. newswire<a class="footnote-ref" href="#plank2016"> [plank2016] </a>. Second, the experts’ research questions are formulated using constructs relevant to their fields, whereas core tools in an NLP pipeline (e.g. part-of-speech tagging or syntactic parsing) provide information expressed in linguistic terms. Researchers in social sciences, for example, are not interested in automatic syntactic analyses per se, but insofar as they provide evidence relevant for their research questions: e.g. which actors interact with each other in this corpus, or which concepts does an actor mention, and which attitudes are shown towards those concepts? Adapting tools to deal with a large variety of corpora, and exploiting their outputs to make them relevant for the questions of experts in different fields is a challenge in itself.</p>
<h2 id="evaluation-and-usability">Evaluation and Usability</h2>
<p>In this section we first discuss the mismatch between the evaluation carried out in NLP and the needs of DH scholars in terms of tool evaluation and tool performance. Then, we present an example of a DH-relevant evaluation approach.</p>
<p>In the same way that exploiting NLP technologies to make them useful to experts in social sciences and humanities is challenging, evaluating the application of NLP tools to those fields also poses difficulties. A vast literature exists about evaluating NLP technologies using NLP-specific measures. However, these NLP measures do not directly answer questions about the usefulness for a domain expert of a tool that applies NLP technologies. Even less do they answer questions about potential biases induced by the technologies (e.g. focusing only on items with certain corpus frequencies), and how these biases affect potential conclusions to draw from the data<a class="footnote-ref" href="#rieder2012"> [rieder2012] </a><a class="footnote-ref" href="#marciniak2016"> [marciniak2016] </a>. As Meeks et al. state, research is needed with “as much of a focus on what the computational techniques obscure as reveal” <a class="footnote-ref" href="#meeks2012"> [meeks2012] </a>.</p>
<p>Let us take the example of a researcher interested in the analysis of characters in different novels. Named-entity recognition is an interesting application for the task, but existing tools are not always very accurate with fiction texts. Moreover, named entities are not enough: the system must probably be coupled with an anaphora recognition and resolution system, and with other modules able to recognize titles and occupations, for example, as some characters may just be mentioned by the title they have. Indeed, some available NLP-based systems for character detection have taken such difficulties into account<a class="footnote-ref" href="#collardanuy2015"> [collardanuy2015] </a><a class="footnote-ref" href="#bamman2014"> [bamman2014] </a><a class="footnote-ref" href="#vala2015"> [vala2015] </a>. Standard NLP evaluations do not provide enough information to evaluate if a tool will be accurate enough or not on a specific corpus, and evaluating this is a hard and, above all, time-consuming task for computational humanists. The character-detection papers just cited, accordingly, developed task-specific corpora.</p>
<p>To take another example, McGregor and McGillivray describe a computational semantics system for information retrieval from historical texts, the Medical Officer of Health reports from London for the years from 1848 to 1972<a class="footnote-ref" href="#mcgregor2018"> [mcgregor2018] </a>. The ultimate aim of this research was to answer a specific question in medical history, namely the nature of the relationship between smell and disease in 19th-century London. Therefore, the computational system had to be evaluated in the context of the original research question and according to metrics that assessed its suitability for the specific task at hand, which was the retrieval of smell-related sentences in a specific large collection of historical health reports, rather than on standard NLP metrics and approaches.</p>
<p>As we said in section 2, evaluation procedures for NLP tools are typically focussed around the aim to improve the state of the art. In the case of DH research, the objective is connected to a set of research questions, which are typically not methodological or linguistic. A computational system may perform very well according to standard NLP evaluation measures, but it is not usable in DH if it does not help the researchers answer their questions. Moreover, the converse scenario has also been attested: a system that attains low scores in an NLP task may prove useful for a DH application. An example of the latter case is automatic keyphrase extraction. A standard evaluation task took place within the SemEval campaign<a class="footnote-ref" href="#kim2010"> [kim2010] </a>. The best systems reached an F1 measure below 0.3. These scores in themselves may seem unimpressive, the possible range being between 0 and 1. However, keyphrase extraction is routinely applied in order to get an overview of a corpus in DH research<a class="footnote-ref" href="#moretti2016"> [moretti2016] </a><a class="footnote-ref" href="#rule2015"> [rule2015] </a>, which suggests the usefulness of this technology for corpus-based research in the humanities. The way the NLP task was defined at SemEval (a keyphrase extracted by the candidate systems had to exactly match a keyphrase in the reference set annotated by human experts for it to count as correct) does not fully reflect the way the technology is useful for a corpus overview in DH tasks, where inexact matches can still be useful. In other words, a tool’s performance in a standard NLP competition like SemEval and the tool’s performance with a DH-relevant corpus need not be related.</p>
<p>In the same way that we encourage humanities researchers to accept automatic textual analyses as complementary to manual ones, we would also like to insist on the importance of understanding the limits of computational tools. Initiatives in this direction have emerged, like Tool Criticism<a class="footnote-ref" href="#traub2015"> [traub2015] </a>, which seeks to understand biases introduced by tools and digital methods on a task’s results.</p>
<p>We will now review an example of evaluation relevant to a specific DH task<a class="footnote-ref" href="#ruiz2017"> [ruiz2017] </a>. That study evaluates a corpus navigation application for the ENB corpus introduced above. The application relies on relation extraction, based on an NLP pipeline adapted to the corpus. Based on the pipeline’s results, a user interface (UI) allows users to search separately for actors, their statements in climate negotiations, and their attitudes towards negotiation items or other actors (support or opposition). First of all, an NLP intrinsic evaluation for relation extraction was performed, comparing automatic results with human reference annotations. The difference here is that, additionally, a qualitative evaluation was carried out, based on interviews of over one hour with three domain-experts familiar with the corpus: two of them had previously published research on it, and one of them works at the organization that produces the corpus reports. The goal of the evaluation was threefold. First, to assess to what extent the corpus navigation application developed for the ENB corpus helps experts obtain an overview of its content (i.e. an overview of actors&rsquo; behaviour in the negotiations). Second, whether the tool can help experts gain new insights on the corpus: facts unknown to them that emerge from using the tool, or new research ideas made apparent to them by using the tool. Finally, another goal was to see if the quality (in F1 terms) of the NLP-based system was sufficient for this specific application.<sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup> As an evaluation protocol, the experts were first shown the corpus exploration functions available to them on the UI, following the same steps with each expert. The experts were then asked to come up with questions about the corpus, use the interface to obtain information relevant to their questions, and comment on the results. As said above, it was assessed whether the UI helps them gain an overview as well as new insights into the corpus. The assessment was based on verbal evidence (expert comments) or other behavioural evidence (queries and other operations performed on the UI). The sessions were recorded and later transcribed non-verbatim: expert comments were summarized, and a description of operations performed by users on the UI was written up; the reports and session audios are publicly available. Rather than asking experts explicitly for feedback, their spontaneous comments were considered as possible evidence for or against the usefulness of the system in terms of corpus overview and insight gain.</p>
<p>The results that emerged from this qualitative evaluation highlight those aspects of the NLP-based system that are useful or pose problems for domain experts. Dynamic extraction of speakers as agents of reporting predicates, without relying on a predefined speaker list, brought to light statements by lesser-studied negotiation participants, that our experts had no data about, like NGOs and interest groups on climate and gender or climate and indigenous peoples. A perceived shortcoming of the system was its incomplete treatment of complex predicates likeexpress concern, where the informative part of the predicate regarding the speaker&rsquo;s attitude to a negotiation issue is the nounconcernrather than the verbexpress. Such results, obtained not from researcher expectations but from actual user feedback, can help shape improved versions of the system, and even identify generalizable areas of improvement for an NLP technology itself.<sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup></p>
<p>The main point here is that a simple evaluation task as the one we just described can be informative in ways an intrinsic quantitative NLP evaluation would not be. However, note that tasks like the one just described involve several challenges, and the system just described could be improved. A first difficulty is that, as reported before<a class="footnote-ref" href="#khovanskaya2015"> [khovanskaya2015] </a>, study participants often form an opinion about the intended contribution of a study, and they believe that it helps the research if the experiment provides evidence for their expected result. These beliefs can bias participants’ behaviour. The attempt by Ruiz et al. to reduce this bias relied on avoiding to ask participants for explicit feedback, but rather on recording their comments and behaviour, although it is debatable whether such bias decreases this way. A second difficulty is that collecting domain-expert feedback in the way described is time-consuming; even finding suitable experts may be difficult, hence the small number of participants in the study. Finally, as the NLP-based system was exploited via a UI, the task could be complemented by an evaluation based on Human Computer Interaction criteria, like usability or user satisfaction<a class="footnote-ref" href="#al-maskari2010"> [al-maskari2010] </a><a class="footnote-ref" href="#kelly2007"> [kelly2007] </a>. Such an evaluation would involve defining tasks to perform with the UI, employing a larger sample of domain experts. The proportion of tasks completed successfully could be measured, as well as task completion time; a questionnaire could measure user satisfaction. All in all, even the simple small-sample qualitative evaluation task above was informative about concrete aspects of an NLP-based system that helped or were an obstacle to answering specific questions relevant to a DH-research task.</p>
<h2 id="conclusion">Conclusion</h2>
<p>We would like to end this paper with a summary of the status quo and some recommendations for the future.</p>
<p>As we have seen, many DH projects are based on large or even very large textual corpora. Sometimes it is not possible for the experts to know all the texts included in their corpus, and in some other cases the corpus is complex and a specific interface would help and quicken the analysis. In such contexts, it makes sense to use advanced NLP techniques in DH research. NLP tools are now mature and accurate enough to provide, in principle, reliable and extended analyses of various corpora in literature, history, and other text-focussed humanities fields. It is possible to annotate entities (people and locations, companies and institutions), semantic concepts, technical terms and domain-specific expressions. It is also possible to extract links between entities, and derive a network of relations from the information included in the texts in an unstructured form. It is even possible to represent relevant information through navigable maps, so that the end user can navigate the corpus and find relevant pieces of information scattered in different texts.</p>
<p>However, despite the accuracy and robustness of current NLP techniques, these are not yet widely used in DH. And, on the other hand, even if we see a growing interest for social sciences and cultural heritage in the NLP community, this is still quite marginal. The main issue is probably that the two communities are fundamentally driven by opposite goals.</p>
<p>The NLP community is interested in advancing NLP techniques, which means every published experiment must be evaluated and compared to previous work and show some improvement over it. The use of gold standards is thus of paramount importance to perform this comparison. The drawback is a large standardization of the research effort: many researchers explore broadly the same methodological avenues, with the same techniques applied on the same data. In fact, the NLP community tends to focus on one paradigm at a time and to produce a phase of homogeneous research questions and methods, sometimes at the expense of leaving out potentially interesting but less mainstream work. Today it is hard to publish research in an NLP venue that does not use word embeddings, neural networks and deep learning. Deep learning methods have had a huge impact on the field, leading to a clear (and sometimes dramatic) improvement in performances for almost all kinds of NLP tasks. However, we believe diversity of methods and critical approaches to their use can only be beneficial.</p>
<p>On the other hand, the DH community, generally speaking, is only secondarily interested in processing techniques. The goal of DH is to shed new light on humanities problems, taking into account the advantages of digital and computerized methods. Evaluating processing techniques is a challenge, since data are by definition always project-specific and there are no gold standards in the same sense intended in NLP. Tools are thus used as they are (off-the-shelf), sometimes after a brief evaluation and estimation of their quality and adequacy for a given task, sometimes without any evaluation. The complexity of current tools should also not be underestimated: most tools are difficult to use and, even when they are open source, modifying them is hard for most users and even most projects. Adapting tools to DH corpora is not trivial: most tools nowadays are based on machine learning techniques, which means large quantities of annotated data are necessary to be able to train a new model or, in other words, to adapt the system to the corpus under study. This means that even when the team is skilled enough to adapt a system, this adaptation is not always possible in practice.</p>
<p>All this explains the current situation and maybe some of the missed opportunities. At this point, one conclusion could be that the two communities are just different, they have different goals and, even if we can observe some points of contact, these are marginal and not so interesting from a scientific point of view. But in fact we would like to defend the exact opposite view.</p>
<p>DH offers new, original and complex challenges to the experts, and these challenges require new, original and complex techniques to tackle them. DH research also offers real-world problems that are needed to prove that NLP techniques can be applied to different languages than English, to different corpora than newspapers and to different periods than just the 21st century. At the same time, NLP tools and methods are often underutilized and a more accurate choice of the techniques to use can have a strong impact on DH research. In fact, we argue that adapting and tuning NLP techniques to the corpus or domain under study is precisely where some of the most challenging, innovatively interesting and impactful research can be.</p>
<p>The dialogue between disciplines and the constructive and critical use of methods that we advocate for implies that most projects need a multidisciplinary approach. We acknowledge that more and more institutions support this interdisciplinary and multidisciplinary research and even create new job positions in this space, often spanning over several departments. However, more needs to be done to properly support interdisciplinary careers. An academic culture that favours single-author publications does not sufficiently support multidisciplinary (and multi-authored) research. Institutions should acknowledge this situation, which means promoting multi-author publications, and also publications related to different domains, ranging from computer science to traditional areas of the humanities.</p>
<p>There is a growing number of papers mixing DH and NLP presented at conferences such as the annual Digital Humanities conference or in the series of LaTeCH workshops (Language Technology for Cultural Heritage, Social Sciences, and Humanities). This indicates that there is a community, which sometimes calls itself Computational Humanities and includes the community of NLP for DH, but also research around techniques dedicated to image, video, sounds and music, etc. However, the insufficient number of high-profile publication venues where such research can be hosted penalizes scholars active in this space, as they are less likely to be consideredsuccessfulby academic standards. One possible solution to this is that humanities departments recognize publications in computer science venues as equally valuable as publications in venues considered more traditionally humanistic.</p>
<p>In addition to these academic cultural changes, we believe that changes to research practices and publication standards can support the work at the interface between NLP and DH. We have stressed that off-the-shelf algorithms are often blindly applied, but it is hard to benchmark an algorithm’s performance on a DH corpus due to its unique features. We propose that reproducibility and algorithmic robustness checks are added to all NLP/DH publications, to strengthen the methodological foundations of the research results. For example, if a DH paper uses LDA topic modeling, a requirement for publication should be that the authors run the analysis using differing values of the parameter for the number of topics (k) and differing pre-processing steps.</p>
<p>After stressing the challenges (and some possible solutions to them), we would like to end this paper on a positive note, highlighting the interest of the research at the frontier between these domains and the opportunities it brings. The state of the art in both fields is advanced enough to contemplate combined approaches, research opportunities happen at a global scale, ethical considerations are a priority and the potential negative impact of the increasing use of artificial intelligence in propagating fake news, for example, is widely recognized. NLP for DH offers a unique opportunity to explore complex data, and to show how we can deal with complexity to get a better knowledge of our past.</p>
<p>By increasing the recognition and support of computational scholars within DH, NLP scholarship can become an attractive area of DH, thus creating space for scholars who might otherwise hesitate to go into DH due to poor job prospects. Just as the social sciences have successfully created space for these kinds of scholars, which has benefited the social sciences overall, DH could achieve similar results by adopting a similar strategy.</p>
<h2 id="acknowledgements">Acknowledgements</h2>
<p>We would like to thank the anonymous reviewers for their thorough and helpful comments. Thierry Poibeau’s research is partially funded by the PRAIRIE 3IA Institute, part of the “Investissements d&rsquo;avenir” program, reference ANR-19-P3IA-0001. This work has been mainly done while Thierry Poibeau benefited from a Rutherford fellowship at the Turing Institute (London and University of Cambridge). This work was supported by The Alan Turing Institute under the EPSRC grant EP/N510129/1.</p>
<ul>
<li id="al-maskari2010">Al-Maskari, Azzah and Sanderson, Mark. “A review of factors influencing user satisfaction in information retrieval” . _Journal of the American Society for Information Science and Technology_ 61. (2010): 859–868.
</li>
<li id="bamman2014">Bamman, David, Underwood, Ted, Smith, Noah A. “A Bayesian Mixed Effects Model of Literary Character” . _Proceedings of the Association for Computational Linguistics_ , pages. 370–379. (2014)
</li>
<li id="bamman2017">Bamman, David. “Natural Language Processing for the Long Tail” . Digital Humanities 2017 Conference Abstracts, pages 382-384, Montreal, Canada (2017).<a href="https://dh2017.adho.org/abstracts/408/408.pdf">https://dh2017.adho.org/abstracts/408/408.pdf</a>
</li>
<li id="bański2009">Bański, Piotr and Przepiórkowski, Adam. “Stand-off TEI Annotation: The Case of the National Corpus of Polish” .Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP ’09 (2009): 64–67.
</li>
<li id="baya-laffite2016"> “Mapping Topics in International Climate Negotiations: A Computer-Assisted Semantic Network Approach” . In Kubitschko, S., Kaun, A. (eds.), _Innovative Methods in Media and Communication Research_ . Springer International Publishing, Cham, pp. 273–291.<a href="https://doi.org/10.1007/978-3-319-40700-5_14">https://doi.org/10.1007/978-3-319-40700-5_14</a>(2016)
</li>
<li id="berry2012"> _Understanding Digital Humanities_ . Palgrave Macmillan. (2012)
</li>
<li id="biemann2014">Biemann, C., Crane, G., Fellbaum, C., and Mehler, A. (eds) (2014). “Computational Humanities - bridging the gap between Computer Science and Digital Humanities” . Report from Dagstuhl Seminar 14301.
</li>
<li id="bonial2016">Bonial, Claire and Palmer, Martha. “Comprehensive and Consistent PropBank Light Verb Annotation” . _Proceedings of LREC 2016, the 10th Language Resources and Evaluation Conference_ . (2016): 3980–3985.
</li>
<li id="boukhaled2016">Boukhaled, Mohamed Amine. _On Computational Stylistics: mining Literary Texts for the Extraction of Characterizing Stylistic Patterns. Document and Text Processing_ . Université Pierre et Marie Curie - Paris VI (2016).
</li>
<li id="christlein2018">Christlein, V., Nicolaou, A., Schlauwitz, T., Späth, S., Herbers, K. & Maier, A. “Handwritten Text Recognition Error Rate Reduction in Historical Documents using Naive Transcribers” . In Burghardt, M. and Müller-Birn, C. (eds), _INF-DH-2018_ . Bonn (2018).
</li>
<li id="clement2008">Clement, Tanya, Sara Steger, John Unsworth, and Kirsten Uszkalo (2008). “How Not To Read A Million Books” .<a href="http://www.people.virginia.edu/~jmu2m/hownot2read.html">http://www.people.virginia.edu/~jmu2m/hownot2read.html</a>.
</li>
<li id="collardanuy2015">Coll Ardanuy, Mariona, Sporleder, Caroline. “Clustering of Novels Represented as Social Networks” . _LiLT (Linguistic Issues in Language Technology)_ 12. (2015).
</li>
<li id="cummings2007">Cummings, James. “The Text Encoding Initiative and the Study of Literature” . In Siemens, Raymond G., Schreibman, Susan (eds), _A Companion to Digital Literary Studies_ . (2007): 451–476.
</li>
<li id="criticalinquiry2019"> _Computational Literary Studies: A Critical Inquiry Online Forum_ .<a href="https://critinq.wordpress.com/2019/03/31/computational-literary-studies-a-critical-inquiry-online-forum/">https://critinq.wordpress.com/2019/03/31/computational-literary-studies-a-critical-inquiry-online-forum/</a>.
</li>
<li id="culturalanalytics2020"> “Debates” section of the _Journal of Cultural Analytics_ .<a href="https://culturalanalytics.org/section/1580-debates">https://culturalanalytics.org/section/1580-debates</a>.
</li>
<li id="da2019">Da, Nan Z. (2019). “The Computational Case against Computational Literary Studies” . _Critical Inquiry_ , 45(3), 601‑639.<a href="https://doi.org/10.1086/702594">https://doi.org/10.1086/702594</a>.
</li>
<li id="dante2018">Dante Search Project. Dante Search. University of Pisa.<a href="http://www.perunaenciclopediadantescadigitale.eu/">http://www.perunaenciclopediadantescadigitale.eu/</a>.
</li>
<li id="diesner2012">Diesner, J., 2012. _Uncovering and Managing the Impact of Methodological Choices for the Computational Construction of Socio-Technical Networks from Texts_ . Carnegie Mellon, Pittsburgh, PA.
</li>
<li id="dubossarsky2017">Dubossarsky, H., Grossman, E., & Weinshall, D. “Outta Control: Laws of Semantic Change and Inherent Biases in Word Representation Models” . _Proceedings of Empirical Methods in Natural Language Processing (EMNLP)_ , Copenhagen, Denmark, 1147–1156 (2017).
</li>
<li id="fitzpatrick2010">Fitzpatrick, Kathleen. “Reporting from the Digital Humanities 2010 Conference” . _The Chronicle of Higher Education_ .<a href="https://web.archive.org/web/20190829004943/https://www.chronicle.com/blogs/profhacker/reporting-from-the-digital-humanities-2010-conference/25473">https://web.archive.org/web/20190829004943/https://www.chronicle.com/blogs/profhacker/reporting-from-the-digital-humanities-2010-conference/25473</a>(2010).
</li>
<li id="frermann2016">L. and Lapata, M. “A Bayesian Model of Diachronic Meaning Change” . _Transactions of the Association for Computational Linguistics_ , 4 (2016).
</li>
<li id="greenfield2013">Greenfield, Patricia M. “The Changing Psychology of Culture From 1800 Through 2000” . _Psychological Science_ , vol. 24.9: 1722–1731, doi:10.1177/0956797613479387 (2013).
</li>
<li id="grimmer2013">Grimmer, Justin and Stewart, Brandon M., 2013. “Text as Data: The Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts” . _Political Analysis_ 21, 267–297.<a href="doi:10.1093/pan/mps028">doi:10.1093/pan/mps028</a>(2013).
</li>
<li id="hellrich2016">Hellrich, Johannes and Udo Hahn. “Bad Company — Neighborhoods in Neural Embedding Spaces Considered Harmful” . _Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics_ : Technical Papers, pages 2785–2796, Osaka, Japan, December 11-17 (2016).
</li>
<li id="hinrichs2019">Hinrichs, E., Hinrichs, M., Kübler, S. et al. “Language technology for digital humanities: introduction to the special issue” . _Language Resources & Evaluation_ 53, 559–563 (2019).<a href="https://doi.org/10.1007/s10579-019-09482-4">https://doi.org/10.1007/s10579-019-09482-4</a>.
</li>
<li id="jenset2017">Jenset, Gard and McGillivray, Barbara. _Quantitative Historical Linguistics. A corpus framework_ . Oxford Studies in Diachronic and Historical Linguistics. Oxford University Press, Oxford (2017).
</li>
<li id="jurafsky2009">Jurafsky, Daniel, and James H. Martin. _Speech and Language Processing: An Introduction to Natural Language Processing, Speech Recognition, and Computational Linguistics_ . 2nd edition. Prentice-Hall (2009).
</li>
<li id="kelly2007">Kelly, Diane. “Methods for Evaluating Interactive Information Retrieval Systems with Users” . _Foundations and Trends® in Information Retrieval_ 3. (2007): 1–224.<a href="https://doi.org/10.1561/1500000012">https://doi.org/10.1561/1500000012</a>.
</li>
<li id="khovanskaya2015">Khovanskaya, Vera, Baumer, Eric, and Sengers, Phoebe. “Double binds and double blinds: evaluation tactics in critically oriented HCI” . _Proceedings of The Fifth Decennial Aarhus Conference on Critical Alternatives_ . (2015): 53–64.
</li>
<li id="kim2010">Kim, Su Nam, Medelyan, Olena., Kan, Min-Yen and Baldwin, Timothy, 2010. “Semeval-2010 task 5: Automatic keyphrase extraction from scientific articles” . _Proceedings of the 5th International Workshop on Semantic Evaluation_ . (2010): 21–26.
</li>
<li id="marciniak2016">Marciniak, Daniel. “Computational text analysis: Thoughts on the contingencies of an evolving method” . _Big Data & Society_ vol. 3, 1–5 .<a href="doi:/10.1177/2053951716670190">doi:/10.1177/2053951716670190</a>(2016).
</li>
<li id="mcgillivray2014">McGillivray, Barbara. _Methods in Latin Computational Linguistics_ . Brill, 2014.
</li>
<li id="mcgregor2018">McGregor, Stephen and McGillivray, Barbara. “A distributional semantic methodology for detecting implied smell in historical medical records” . In Adrien Barbaresi, Hanno Biber, Friedrich Neubarth, and Rainer Osswald (eds), _Proceedings of the 14th Conference on Natural Language Processing (KONVENS 2018)_ – September 19-21, 2018, pages 1–11, Vienna, Austria, 2018.
</li>
<li id="meeks2012">Meeks, Elijah and Weingart, Scott B. “The Digital Humanities Contribution to Topic Modeling” . _Journal of Digital Humanities_ , vol. 2:1<a href="http://journalofdigitalhumanities.org/2-1/dh-contribution-to-topic-modeling/">http://journalofdigitalhumanities.org/2-1/dh-contribution-to-topic-modeling/</a>.
</li>
<li id="moretti2005">Moretti, Franco. _Graphs, maps, trees: abstract models for a literary history_ . Verso, 2005.
</li>
<li id="moretti2016">Moretti, Giovanni, Sprugnoli, Rachele, Menini, Stefano, Tonelli, Sara, 2016. “ALCIDE: Extracting and visualising content from large document collections to support humanities studies” . _Knowledge-Based Systems_ . (2016) 111: 100–112.<a href="https://doi.org/10.1016/j.knosys.2016.08.003">https://doi.org/10.1016/j.knosys.2016.08.003</a>.
</li>
<li id="nyhan2016">Nyhan, J., Flinn, A. _Computation and the Humanities: Towards an Oral History of Digital Humanities_ . Springer (2016).
</li>
<li id="palmer2005">Palmer, Martha, Gildea, Daniel, Kingsbury, Paul. “The proposition bank: An annotated corpus of semantic roles” . _Computational linguistics_ , 31 (2005): 71–106.
</li>
<li id="pechenick2015">Pechenick, Eitan Adam, Danforth, Christopher M., Dodds, Peter Sheridan “Characterizing the Google Books Corpus: Strong Limits to Inferences of Socio-Cultural and Linguistic Evolution” . _PLOS ONE_ 10(10): e0137041.<a href="https://doi.org/10.1371/journal.pone.0137041">https://doi.org/10.1371/journal.pone.0137041</a>(2015).
</li>
<li id="perrone2019">Perrone, Valerio, Hengchen, Simon, Vatri, Alessandro, Palma, Marco, and McGillivray, Barbara. “GASC: Genre-Aware Semantic Change for Ancient Greek” . _Proceedings of the 1st International Workshop on Computational Approaches to Historical Language Change_ , Florence, Italy, August 2, 2019. Association for Computational Linguistics.
</li>
<li id="piotrowski2012">Piotrowski, Michael. _Natural language processing for historical texts_ . Synthesis Lectures on Human Language Technologies. Morgan & Claypool Publishers, (2012).
</li>
<li id="plank2016">Plank, Barbara. “What to do about non-standard (or non-canonical) language in NLP” . _Proceedings of the 13th Conference on Natural Language Processing (KONVENS 2016)_ – September 19-21, 2016, pages 13–20, Bochum, Germany, 2016.
</li>
<li id="przepiórkowski2009">Przepiórkowski, Adam. “TEI P5 as an XML Standard for Treebank Encoding” . _Proceedings of the Eighth International Workshop on Treebanks and Linguistic Theories (TLT)_ (2009): 149–160.
</li>
<li id="rieder2012">Rieder, Bernhard and Röhle, Theo. “Digital Methods: Five Challenges” . In Berry, David (ed.) _Understanding Digital Humanities_ , pages 67–84 (2012).
</li>
<li id="ruiz2017">Ruiz, Pablo, 2017. _Concept-based and Relation-based Corpus Navigation: Applications of Natural Language Processing in Digital Humanities_ (PhD Thesis). École normale supérieure, PSL Research University, Paris.
</li>
<li id="ruiz2016">Ruiz, Pablo, Plancq, Clément and Poibeau, Thierry. “More than Word Cooccurrence: Exploring Support and Opposition in International Climate Negotiations with Semantic Parsing” . _Proceedings of LREC 2016, the 10th Language Resources and Conference_ (2016): 192–197.
</li>
<li id="rule2015">Rule, Alix, Cointet, Jean-Philippe, and Peter S. Bearman. “Lexical shifts, substantive changes, and continuity in State of the Union discourse, 1790–2014” . _Proceedings of the National Academy of Sciences_ (2015) 112: 10837–10844.<a href="https://doi.org/10.1073/pnas.1512221112">https://doi.org/10.1073/pnas.1512221112</a>.
</li>
<li id="schreibman2004">Schreibman, Susan, Siemens, Ray G., Unsworth, John. (Eds.), _A companion to digital humanities. Blackwell companions to literature and culture_ . Blackwell, Malden, MA. (2004).
</li>
<li id="schulz2018">Schulz, Sarah. “The Taming of the Shrew - Non-Standard Text Processing in the Digital Humanities” . Dissertation, University of Stuttgart (2018).
</li>
<li id="sinclair2004">Sinclair, John. “Corpus and text. basic principles” . Martin Wynne, editor, _Developing linguistic corpora: a guide to good practice_ : 1–16. Oxbow books, Oxford (2004).
</li>
<li id="stamatatos2009">Stamatatos, E. “A survey of modern authorship attribution methods” . _Journal of the American Society for Information Science and Technology_ , 60: 538-556.<a href="doi:10.1002/asi.21001">doi:10.1002/asi.21001</a>(2009).
</li>
<li id="tang2018">Tang, Xuri. “A State-of-the-Art of Semantic Change Computation” . _Natural Language Engineering_ 24: 649-676 (2018).
</li>
<li id="traub2015">Traub, Myriam C. and van Ossenbruggen, Jacco (eds.) Workshop on Tool Criticism in the Digital Humanities. Centrum Wiskunde & Informatica, KNAW eHumanities, and Amsterdam Data Science Center.<a href="http://persistent-identifier.org/?identifier=urn:nbn:nl:ui:18-23500">http://persistent-identifier.org/?identifier=urn:nbn:nl:ui:18-23500</a>(2015).
</li>
<li id="vala2015">Vala, Hardik, Jurgens, David, Piper, Andew, Ruths, Derek. “Mr. Bennet, his coachman, and the Archbishop walk into a bar but only one of them gets recognized: On The Difficulty of Detecting Characters in Literary Texts” . _Proceedings of Empirical Methods in Natural Language Processing_ . (2015).
</li>
<li id="vanderzwaan2017">van der Zwaan, J. M., Smink, W. A. C., Sools, A. M., Westerhof, G. J., Veldkamp, B. P., & Wiegersma, S. “Flexible NLP Pipelines for Digital Humanities Research” . Paper presented at 4th Digital Humanities Benelux Conference 2017, Utrecht, Netherlands (2017).
</li>
<li id="venturini2014">Venturini, Tommaso, Baya Laffite, Nicolas, Cointet, Jean-Philippe, Gray, Ian, Zabban, Vinciane and De Pryck, Kari. “Three maps and three misunderstandings: A digital mapping of climate diplomacy” . _Big Data & Society_ 1 (2014): 1–19<a href="https://doi.org/10.1177/2053951714543804">https://doi.org/10.1177/2053951714543804</a>.
</li>
</ul>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>“Je t&rsquo;aime… moi non plus” (French forI love you… me neither) is a 1967 song written by Serge Gainsbourg for Brigitte Bardot. “The song was banned in several countries due to its overtly sexual content” (Wikipedia), but there is nothing sexual in this paper.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p><a href="https://ch.uni-leipzig.de/about/">https://ch.uni-leipzig.de/about/</a>(Last accessed on 20/05/2020).&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p><a href="https://www.ehumanities.nl/computational-Humanities/">https://www.ehumanities.nl/computational-Humanities/</a>(Last accessed on 20/05/2020).&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p><a href="https://www.dagstuhl.de/en/program/calendar/semhp/?semnr=14301">https://www.dagstuhl.de/en/program/calendar/semhp/?semnr=14301</a>(Last accessed on 20/05/2020).&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p><a href="https://teach4dh.github.io">https://teach4dh.github.io</a>(Last accessed on 20/05/2020).&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p><a href="http://wp.unil.ch/llist/en/event/comhum2018/">http://wp.unil.ch/llist/en/event/comhum2018/</a>(Last accessed on 20/05/2020).&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p><a href="https://sighum.wordpress.com/events/latech-clfl-2018/">https://sighum.wordpress.com/events/latech-clfl-2018/</a>(Last accessed on 20/05/2020).&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>F1 was 0.69 based on an exact match across system and reference results of triples containing a negotiating actor, its statement, and the reporting predicate (verbal or nominal) relating both.&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>Indeed, at the time those evaluations took place, a current concern in computational linguistics<a class="footnote-ref" href="#bonial2016"> [bonial2016] </a>was how to represent certain complex predicates (light verbs) in the lexical knowledge-base against which we automatically annotated the ENB corpus (PropBank<a class="footnote-ref" href="#palmer2005"> [palmer2005] </a>).## Bibliography&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry><entry><title type="html">Geovisualização de dados e ciência aberta e cidadã - a experiência da Plataforma LindaGeo</title><link href="https://rlskoeser.github.io/dhqwords/vol/14/2/000452/?utm_source=atom_feed" rel="alternate" type="text/html"/><link href="https://rlskoeser.github.io/dhqwords/pt/vol/14/2/000452/?utm_source=atom_feed" rel="alternate" type="text/html" hreflang="pt"/><id>https://rlskoeser.github.io/dhqwords/vol/14/2/000452/</id><author><name>Sarita Albagli</name></author><author><name>Hesley Py</name></author><author><name>Allan Yu Iwama</name></author><published>2020-06-19T00:00:00+00:00</published><updated>2023-08-06T15:53:44-04:00</updated><content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>The intensification of data production in large volume and in different areas, which has been characterized as a process of “datification”<a class="footnote-ref" href="#mayer-schönberger2013"> [mayer-schönberger2013] </a>, as well as the various possibilities of use and implications deriving from it, have aroused growing interest in new visualization strategies and tools. Within the scientific environment, it is not only the “hard sciences” that seek to appropriate these new platforms when faced with the expansion and technical requirements ofe-science(the so-calledbig dataof science). This topic is also the object of experimentation and reflection by human and social sciences, fostering enriching synergy and interdisciplinary dialogue in joint applications and mutual learning.</p>
<p>This paper addresses a specific topic in this field, that is, data and information geovisualization as maps, discussing their possibilities and limitations in promoting the co-production of knowledge among different actors as well as in providing tools for social intervention in territorial ordering and development.</p>
<p>In the first part, we systematize, from a critical perspective, a set of key concepts, approaches and issues that guide the debate on the topic. We argue that strategies of visualization and the corresponding transformations in the technologies supporting them cannot be conceived as mere technical tools. They are permeated by power struggles between actors with different visions, knowledge bases and epistemic perspectives, constituting thus, more properly, sociotechnical infrastructures.</p>
<p>Subsequently, we discuss the role of participatory methodologies of social cartography within this setting, presenting a synthesis of the reflections derived from the experimentation with the development of a prototype of a geospatial open data platform as part of an open science action-research project carried out in the municipality of Ubatuba, on the Northern coast of the state of São Paulo, in Southeastern Brazil<a class="footnote-ref" href="#albagli2019"> [albagli2019] </a>. It is noteworthy, in the context of disputes and controversies about the territorial re-ordering of the region, the relevance of the combination of participative methods and open tools in order to incorporate, in the production of the maps that define territorial use, social groups historically excluded from or disregarded by these processes. In the case of the region under study, they are thecaiçara, indigenous people andquilombolacommunities, besides traditional fishermen and workers in general. These social groups hold strategic knowledge about the territory in which they live and about its resources, but do not have access to data or to the skills required to the use of technological tools for processing and geovisualizing this information.</p>
<h2 id="data-and-information-visualization">Data and information visualization</h2>
<p>Data and information visualization constitutes an auxiliary element for the presentation and communication of a set of data turning, for example, a textual or tabular representation of alphanumeric codes into a visual representation (graphs, maps, infographs, etc.). It can thus be defined as the “remapping of other codes into a visual code” <a class="footnote-ref" href="#manovich2010"> [manovich2010] </a>, making them more accessible cognitively or intelligible.</p>
<p>Visualization enables the broadening of conditions for a better understanding of large amounts of heterogeneous data, besides facilitating sharing and using them, contributing towards exposing the information “that would otherwise be hidden (or we could say invisible) in data” <a class="footnote-ref" href="#boechat2015"> [boechat2015] </a>. Compared to textual description which requires specific linguistic abilities for its understanding, given the diversity of languages, visualization through images has a more universal character, broadening the audience of data and information users.</p>
<p>The delimitation between data visualization and information visualization is not rigid: it varies according to the context and the type of interpretation<a class="footnote-ref" href="#boechat2015"> [boechat2015] </a>. Information has been understood asthe mediating elementbetween data (the basic element) and knowledge<a class="footnote-ref" href="#machlup1962"> [machlup1962] </a><a class="footnote-ref" href="#nonaka2008"> [nonaka2008] </a>; or yet as a means of <em>registering</em> or <em>communicating</em> knowledge<a class="footnote-ref" href="#saracevic1992"> [saracevic1992] </a><a class="footnote-ref" href="#wersig1996"> [wersig1996] </a>. Therefore, it has a “relational character” that “only achieves a semantic value through selective and interpretative processes” <a class="footnote-ref" href="#gonzálezdegómez1999"> [gonzálezdegómez1999] </a>. It relates to the idea of flux, of circulation, involving both material and subjective conditions. Thus, both the <em>function</em> and the <em>formatting</em> of data and information are inseparable<a class="footnote-ref" href="#castiglione2009"> [castiglione2009] </a>.<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></p>
<p>Visualization is influenced both by the perspective of the author (who produces it), as by the user or reader (who uses it), as well as by their respective competencies to carry out the visualization and to interpret it. In this sense, it is not neutral; instead, “strategies of visualization or visuality regimes impose a certain outlook; thus, a culturally constructed outlook, a language and a way of knowing that determine our way of being and of living our daily life” <a class="footnote-ref" href="#deaguiar2010"> [deaguiar2010] </a>.</p>
<h2 id="use-of-maps-for-geovisualizing-data-and-information">Use of maps for geovisualizing data and information</h2>
<p>Maps are a mode of geoespatial information,<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> a means ofgeo-graphicvisualization of data and information, providing them with a spatial context and situating them within the territory. The elaboration of maps involves extensive work processes that encompass “from the initial collecting of data, to choices of how data are categorized and displayed, and on through the map´s ultimate dissemination and use” <a class="footnote-ref" href="#bier2017"> [bier2017] </a>, expressing the social relationships and values of the societies where they are produced<a class="footnote-ref" href="#harley1989"> [harley1989] </a><a class="footnote-ref" href="#crampton2001"> [crampton2001] </a><a class="footnote-ref" href="#castiglione2009"> [castiglione2009] </a>.</p>
<p>Their function cannot be reduced to defining a place in space through geographic coordinates (georeferenced data). Maps allow for “a specific way of seizing reality, that of languages aimed at the sight, at the point of view,” a way of apprehending “a world that is too complex for our eyes,” but, above all, “a world that is only available in the map” <a class="footnote-ref" href="#fonseca2014"> [fonseca2014] </a>. While a symbolic-imagistic representation of a certain object or spatial phenomenon, maps are instrumental as cognitive assistants that help us to contemplate the different aspects of the territory.</p>
<p>Starting from the perspective of the social studies of science, Bruno Latour<a class="footnote-ref" href="#latour1990"> [latour1990] </a>considers maps as one among the possible forms of “inscription” that include also pictures, numbers, letters, graphs, among others, all of them to a certain extent consisting in means of visualization. They present to the onlooker, in a synoptic manner, heterogeneous things that are either absent or not perceptible, combining them among themselves in only one place in hybrid forms. To the author, they areimmutable mobiles, as they simultaneously allow for mobility (they facilitate displacement and communication of what is being represented) and immutability (they confer stability to what is being represented).</p>
<p>Latour stresses that different forms of visualization (or of inscription) combine with one another, re-enforce each other and thus turn themselves into long “cascades” , allowing for the “overlapping of several images from different sources and of a different scale” <a class="footnote-ref" href="#latour1990"> [latour1990] </a>. These successive re-combinations are facilitated nowadays by the homogeneous treatment of data made possible by digital technologies, as part of a “trend towards increasingly simpler inscriptions that mobilize increasingly larger number of events within a single point” <a class="footnote-ref" href="#latour1990"> [latour1990] </a>. Therefore, the author proposes that differences in scale are not given or pre-existing, but are part of a production that involves different kinds and tools of visualization, expressive of power relations:</p>
<blockquote>
<p>the globe is not, by definition, global, but is, almost literally, a model of scale [&hellip;] we don&rsquo;t have on one side the scientists benefitting from a globally complete view of the globe and, on the other, the poor ordinary citizen with alimited localview. There are only local views. However, some of us look at connected scale models based on data that has been reformatted by more and more powerful programs run through more and more respected institutions.<br>
<a class="footnote-ref" href="#latour2011"> [latour2011] </a></p>
</blockquote>
<p>Therefore, to Latour, the relationship between visualization and cognition is not limited to its implications to human perception, but lies in its ability to <em>mobilize</em> , especially in situations of controversy and antagonism. According to the author, “inscriptions make recruitment possible!” <a class="footnote-ref" href="#latour1990"> [latour1990] </a>. They intervene in the way we argue, provide evidence and believe, contributing to the convincing and recruiting of allies in the construction of facts<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup> — which “requires a huge effort of measurement, calculation, and definition” <a class="footnote-ref" href="#bier2017"> [bier2017] </a>.</p>
<p>Thus, maps constitute a representation that not only disseminates and shares information, but also contributes to the affirmation and certification of a certain type and conception of knowledge, which, in turn, instrumentalizes a way of territorial intervention. Along these lines, it is argued that maps are highly selective in what they intend to show: “maps never simply convey information in a direct and unmediated manner, but instead they are invested with the ability to incorporate some forms of information while omitting others” <a class="footnote-ref" href="#bier2017"> [bier2017] </a>. They express the point of view of the actors, “a variety of observational frames that cannot be divorced from their unequal positions within the very terrains that they seek to portray” <a class="footnote-ref" href="#bier2017"> [bier2017] </a>. Therefore, they reflect power relations, as integral parts of political disputes about the territory and also exerting influence on public perception about these conflicts<a class="footnote-ref" href="#crampton2001"> [crampton2001] </a><a class="footnote-ref" href="#acselrad2008"> [acselrad2008] </a>.</p>
<p>On the other hand, we would stress that while “it fixates the space of places, it locates, it distributes, it guides” , the map “preserves [for travelers] the way, the route on which they will learn with events the reading of themselves, of others and of their own space” <a class="footnote-ref" href="#deaguiar2010"> [deaguiar2010] </a>. Thus, maps mobilize subjectivities. “The reading of maps follows a movement that produces experiences, practices, meanings besides those already constituted. They also trigger affects and perceptions, differences that traverse the inhabited space” <a class="footnote-ref" href="#deaguiar2010"> [deaguiar2010] </a>.</p>
<h2 id="digitalization-of-geovisualization">Digitalization of geovisualization</h2>
<p>Despite the fact that, in its more rudimentary forms of pictographic representation, maps historically preceded oral language and numerical systems, they only became widely available after the European Renaissance. From the 19th century on, with the complexification of geographical space and urban-industrial expansion, the need expanded for more detailed and precise description and representation with a view to better understanding and ordering of territory. “ Chamber cartography could no longer adequately represent a more sophisticated and more diversified territory upon which the State had to act and intervene in an efficient and realistic manner” <a class="footnote-ref" href="#castiglione2009"> [castiglione2009] </a>. Initially, surveys and efforts of immersion in the territory through field work intensified. However, soon more complex and sophisticated technological solutions became necessary. These solutions would develop and become widespread throughout the 20th and 21st centuries.</p>
<p>The perception of maps as space representation tools and, therefore, as ways of storing geographic data about the territory was then extended to their role as instruments for the presentation and communication of any type of data and information. Digital mapping technologies have also expanded the uses of maps as devices that allow not only representation, but also the construction of new sets of data, information and knowledge<a class="footnote-ref" href="#py2019"> [py2019] </a>. At the same time, the limitations of these uses by non-specialists were evident, as will be seen below. New information and communication technologies represented a fundamental turnaround in instruments and systems for surveying, processing and representing geospatial data and information. Noteworthy are digital cartography (particularly Geographical Information Systems — GIS) and remote sensory that pushed forward thematic cartography,<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup> as well as the different modes of representation and analysis of spatial data originating from this.<a class="footnote-ref" href="#câmara2001"> [câmara2001] </a><a class="footnote-ref" href="#marchezini2017"> [marchezini2017] </a>.</p>
<p>Large scale production of data (datification) was pushed forward by the diffusion of the use of digital technologies<a class="footnote-ref" href="#mayer-schönberger2013"> [mayer-schönberger2013] </a>. On the one hand, it is argued that digital cartography contributed towards the “de-materialization” of means of visualizing spatial data through the extended use of virtual environments<a class="footnote-ref" href="#castiglione2009"> [castiglione2009] </a>. On the other, it is stressed that digital cartography “has rematerialized the whole chain of production — a chain that requires people, skills, energy, software, and institutions that all contribute to the constantly changing quality of the data ” <a class="footnote-ref" href="#bier2017"> [bier2017] </a>.</p>
<p>Thus, new material and cognitive infrastructures are required to provide support for, from field work to the graphic designer as well as to the management of databases, in a continuous process of feedback and updating. The concept of infrastructure must not be naturalized, that is, be conceived as a transparent object with pre-defined characteristics, mere substrate or backdrop against which actions are carried out. “Infrastructure is a fundamentally relational concept. It <em>becomes</em> infrastructure in relation to organized practices” <a class="footnote-ref" href="#star1996"> [star1996] </a>(italics added). It is the case then to reflect upon these technological infrastructures as part of social arrangements, or better, as sociotechnical arrangements, articulated to other components and variables, in which “substrate becomes substance” <a class="footnote-ref" href="#star1996"> [star1996] </a>. Within them, decision processes that express the power game between social groups with different points of view and interests are implicated.</p>
<p>From this derives the ambiguous or contradictory character of new technological platforms that push forward datification processes and open up new possibilities of data and information geovisualization and analysis. They enable the move from bi-dimensional, static and unidirectional visualization to tri-dimensional, dynamic and interactive shapes (considerably more sophisticated and complex than former terrestrial globes). In bi-dimensional and unidirectional visualization, point of view is fixed and information isready, closed, a black box impossible to be explored, intervened with and contested by the user.</p>
<p>On the other hand, in the tridimensional, interactive and dynamic presentation it is possible to have a variation of perspective or of point of view, a “decentering” in relation to the object, allowing also “the observer to walk through the representation of the geographic space in a way similar to the flexibility he/she enjoys to walk about the geographical space of the external world” <a class="footnote-ref" href="#castiglione2009"> [castiglione2009] </a>. It is possible to navigate through different visual levels (zooms) and to combine different sets of data (or themes), adjusting them to specific needs. At the same time that they become more complex and sophisticated, these new platforms of visualization become potentially less abstract, more playful and friendlier. They enable more intelligent and effective connections between the symbolic-imagistic mental representation of space and its formal-conceptual representation in the shape of geospatial data and information which, in principle, broadens the perceptual experience and the resulting interpretative capacity<a class="footnote-ref" href="#castiglione2009"> [castiglione2009] </a>.</p>
<p>New computational technologies for producing interactive digital maps are seen as a means towards a relative democratization of cartography as they enable the availability and the access to spatial data and to software of online visualization, and as they blur the borders between the roles of producers and readers/users of these data and information, mobilizing their experience and their specific knowledge. They facilitate the collaborative production of data and information, as well as the incorporation of different perspectives and points of view, which potentially makes these systems stronger and more democratic – scientifically and politically.</p>
<p>The combination of these tools with recent movements in favor of open data<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup> <a class="footnote-ref" href="#machado2018"> [machado2018] </a>expands their democratic potential. Beyond access, the availability of data in formats and means (through web services) is required, allowing for their reconstruction and re-utilization and hence, for the freedom and greater autonomy to explore and analyze them. Even though new cartographic technologies and geovisualization tools still are privately owned and of high cost, dominated by North-American and European corporations, the open source movement is expanding in the field of cartography and geo-information. This movement has made accessible and popularized (1) the use of Geographic Information Systems (GIS) and (2) applications for the collection and availability of data on web platforms that enable the visualization of spatial data.</p>
<p>In the first case, GIS with functionalities and extensions for manipulating vector and matrix data as well as for spatially referenced data analysis that support the expert user stand out. Some examples are: Quantum GIS (QGis) and gvSig (originated in communities and collaborative associations such as OSGEO and gvSIG); and Brazilian initiatives, such as Spring, TerraView and TerraME (developed by the National Institute for Space Research - INPE).</p>
<p>In the second case, there are applications and platforms that enable collaboration between users in different locations, providing geographical information that will be added and consolidated in a single online environment, where they will be available through web viewers (Web GIS) and services. Among the platforms known as collaborative mapping, the Open Street Map stands out for its active community and worldwide reach. Among the Web GIS, i3Geo (developed by the Brazilian Ministry of the Environment and later maintained by the Ministry of Health), GeoNode and GeoNetwork stand out. The latter two, despite being platforms more focused on data and metadata exchange, which are today strongly linked to the concept of SDI (Spatial Data Infrastructure), have in recent years sought to increase their ability to provide tools for data visualization.</p>
<p>With regard to GIS installed locally on users&rsquo; computers, its ability to perform more complex operations and in-depth analysis of data sets stands out, since it is not subject to network or server processing limitations. In the case of Web GIS, which is available through Web servers, the existing functionalities tend to be limited to the characteristics of a networked system, subject to data traffic issues and concurrent processing. However, as the technology evolves, the differences between the Desktop (local, dedicated) and Web (on the network, shared) environments, their limitations and potential, tend to decrease, as well as the differences between the functionalities found in the Web GIS and those available on Desktop.</p>
<p>Table 1 below presents a synthetic description of these platforms and tools, as well as links to sites with more detailed information.<br>
Platforms/Systems of open geospatial dataPlatformDescriptionType<a href="https://www.qgis.org/pt_BR/site/">QGIS</a>– Quantum GISProject of the Open Source Geospatial Foundation (OSGeo) of a system of free and open geographical information that offers for free functionalities commonly found in proprietary software such as: visualization, editing and analysis of geospatial data and information as well as the creation and printing of maps.Desktop GIS software with online map interface<a href="http://www.dpi.inpe.br/spring/">Spring</a>Geographic Information System developed by Inpe, aimed at incorporating the state-of-the-art in image processing, spatial analysis, numerical modeling of terrain and consultation of spatial databases. It seeks to provide an easy-to-learn, unified GIS geoprocessing and remote sensory environment for urban and environmental applications.Desktop GIS software<a href="http://www.dpi.inpe.br/terralib5/wiki/doku.php">TerraView</a>Application developed by Inpe, built by using the TerraLib library, containing the main functions of a GIS for visualizing matrix and vectorial data, managing a geographic database built upon different managerial devices, map vectorial algebra, image processing, vectorial editing, printing of cartographic products, among others.Desktop GIS software<a href="http://www.dpi.inpe.br/terrama2/doku.php">TerraMA</a>Developed by Inpe with open software, with extensive use of TerraLib geographical library, to meet a growing demand for environmental monitoring applications. It is based on an open architecture of services that provide the necessary technological infrastructure to the development of operational systems aimed at the monitoring of environmental risk alerts.Desktop GIS software<a href="http://www.gvsig.com/pt">GvSIG</a>A Project that calls itself “the largest network of professionals of free geomatics”, consisting basically of a set of applications that guide and facilitate the whole geo cycle of information, from its collection in the field through analysis and editing, up to its dissemination in Spatial Data Infrastructures (SDI).Desktop GIS software with online map interface<a href="https://www.mma.gov.br/governanca-ambiental/geoprocessamento/download-do-i3geo.html">i3Geo</a>Application to access and analyze geographical data using the web, based upon free software, especially MapServer. It was developed by the Ministry of Environment and distributed under the GPL license (General Public License), aimed at disseminating the use of geoprocessing as a technical-scientific tool and at implementing a generic interface for the access of geographical data to be found at public, private or non-governmental institutions.WebGIS platform<a href="https://www.openstreetmap.org/#map=4/-39.27/-73.60">Open Street Map</a>A project of world mapping in a collaborative way, according to the concept of Voluntary Geographic Information (VGI), with a focus on community participation, local knowledge, technology for collecting geographical coordinates using the Global Positioning System (GPS), satellite images etc., and adherence to the open data principlesCollaborative WebGIS platform<a href="http://geonode.org/">GeoNode</a>Web-based open platform and application for the development of geospatial information systems (GIS) and for the implementation of spatial data infrastructures (SDI).WebGIS platform<a href="https://geonetwork-opensource.org/">GeoNetworks</a>Free and open source cataloging application (FOSS) for georeferenced resources. It is a catalog of location-oriented information.WebGIS platform<br>
All these platforms have a common objective with a few variations: to make possible ample access by individuals and communities through the use of functions of image processing, spatial analysis, the consultation of spatial databases to access data and information on the territory as well as interfaces with applications for collecting coordinates and Global Positioning System (GPS) devices. They also make possible the adherence to principles of open data, encompassing analysis (spatial or otherwise) and editing, to their dissemination through Spatial Data Infrastructures - SDI<sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup></p>
<p>On the other hand, we would like to stress that the availability of data in open access and format does not ensure by itself their democratization or social appropriation; usability is also crucial, requiring the development of the competencies of users: “too much emphasis is given to the data supplier and only limited attention for the user […] the main challenge is that open data has no value in itself; it only becomes valuable when used” <a class="footnote-ref" href="#janssen2012"> [janssen2012] </a>.</p>
<p>Besides, it is evident that the new systems of tridimensional and dynamic geoprocessing and geovisualization of data and information may also contribute, in reverse, to strengthen the unilateral dominance of the perspective of the author of the data and of the information, as well as of visions and discourses expressing particular and dominant interests over the space and territory. By facilitating and emphasizing statistical and quantitative analyses on the territory, attributing value to them as a means of objective representation of the materiality of space<a class="footnote-ref" href="#bier2017"> [bier2017] </a>, these computational platforms also contribute to the disregarding and abating of other more qualitative and subjective aspects. Therefore, they can strengthen reductionist conceptions that equate knowledge to the capacity of generating and organizing a significative volume of data and information in a structured manner. In summary: “virtualized geographic knowledge of the territory in these potentialized systems of geographical information acquires such great authority that their presentation ends up by becoming the evidence of reality, despite the fact that it is only a hypothesis of knowledge, built within the scope of the modelling of an information system” <a class="footnote-ref" href="#castiglione2009"> [castiglione2009] </a>.</p>
<p>Thus, the concern arises that the new computational resources for surveying and visualizing territorial data might excessively strengthen the role of specialists of these tools in determining “our image of the world”, decreasing the importance of field work<a class="footnote-ref" href="#harley1989"> [harley1989] </a>. On the other hand, it is argued that “despite the proliferation of drones and satellites, [&hellip;] ground data collection is still central to digital cartography” <a class="footnote-ref" href="#bier2017"> [bier2017] </a>, as a means of both validation and of interpretation of what is seen from a distance.</p>
<h2 id="social-cartography-and-the-open-science-experience-of-the-lindageo-platform">Social cartography and the open science experience of the LindaGeo platform</h2>
<p>Social cartography approaches have attempted to establish a counterpoint to the utilization of maps as instruments of power affirmation of hegemonic social groups. Social cartography consists of carrying out the process of mapping according to the point of view of the participant or social group on their own world, their surroundings and corresponding social contexts. At a first instance, its purpose is to represent the daily life of a community according to its own vision, incorporating, in a second instance, the concepts and techniques of cartographical mapping, such as the role and scope of scale, detailed spatial location and legend of what is being represented/ mapped<a class="footnote-ref" href="#acselrad2008"> [acselrad2008] </a>. To this end, one resorts increasingly to the new digital tools and platforms of geovisualization.</p>
<p>In Brazil, social cartography has been most frequently used in the Amazon region<a class="footnote-ref" href="#almeida2005"> [almeida2005] </a><a class="footnote-ref" href="#acselrad2008"> [acselrad2008] </a><a class="footnote-ref" href="#acselrad2013"> [acselrad2013] </a>, expanding to other regions<a class="footnote-ref" href="#carpi2011"> [carpi2011] </a><a class="footnote-ref" href="#gorayeb2015"> [gorayeb2015] </a><a class="footnote-ref" href="#simões2016"> [simões2016] </a><a class="footnote-ref" href="#fujii2017"> [fujii2017] </a><a class="footnote-ref" href="#marchezini2017"> [marchezini2017] </a>, as a strategy towards the affirmation of territorial rights and the transformation of social demands into public policies, through the use of different processes of participative mapping.<sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup> One aspect that has been stressed is the centrality of the role the communities themselves can and must play in directing the mapping of their territories in order to strengthen their points of view and to ensure protagonism in claiming their rights<a class="footnote-ref" href="#knapp2007"> [knapp2007] </a><a class="footnote-ref" href="#acselrad2013"> [acselrad2013] </a><a class="footnote-ref" href="#gorayeb2015"> [gorayeb2015] </a>.</p>
<p>The action research project Ubatuba Open Science<sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup> has developed a prototype to test the participative production of a geovisualization platform in that territory, as part of the investigation of the role of open and citizen science<sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup> in local development strategies. The notion of open science adopted by the project referred not only to the opening within the restricted field of scientists, but also to the increased porosity and interlocution of science with other types of knowledge<a class="footnote-ref" href="#albagli2019"> [albagli2019] </a>. Along the same lines, a less instrumental type of citizen science was adopted with regard to the contribution of non-specialists and more horizontal or democratic in the perspective of the co-production of knowledge involving both scientists and non-scientists<a class="footnote-ref" href="#albagli2015"> [albagli2015] </a>.</p>
<p>Named LindaGeo - Northern Coast Geospatial Open Data (see<a href="#albagli2019">Albagli et al. 2019</a>), the geovisualization prototype had, as a focus of experimentation, the discussions started in 2014 around the revision of the Economic-Ecological Zoning of the Northern Coas of São Paulo (EEZ-NC),<sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup> established in 2004 in order to promote territorial ordering and to discipline the use of the region’s natural resources.</p>
<p>Along the revision of the EEZ-NC, a consultation process was conducted by a Working Group constituted by the Municipal Council of the Environment, with the purpose of obtaining subsidies for its revision as well as to enlighten local communities, particularly the most affected ones and with little information about the process. Meetings and public audiences were held, both at the regional and local levels in different neighborhoods, revealing the existence of local groups highly mobilized to claim their rights of territorial use.<sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup></p>
<p>On the one hand, public audiences brought to light the existence of divergences and conflicts of interest of and perspectives among the different actors – such as representatives from both the municipal and state governments, Federal and State attorneys, representatives from businesses, traditional communities, environmental organizations, among others – about the different uses of the territory, its natural resources and its local cultures. On the other hand, it also became evident that the local groups’ lack of qualified information concerning the procedures related to the revision of the EEZ, the significance of each delimited zone (terrestrial, marine and stretches between tides), as well as of the allowed and forbidden uses<a class="footnote-ref" href="#iwama2017"> [iwama2017] </a><a class="footnote-ref" href="#iwama2018"> [iwama2018] </a>. This fact had direct repercussion on the quality – and consequently on the reach – of the claims presented by these groups.</p>
<p>Along this process, local communities expressed a constant discomfort, particularly the traditional populations (indigenous peoples, quilombolas and caiçaras), because they felt that the way consultations were conducted was not sufficiently participative and informative, privileging and legitimizing points of view based on what was considered “reasoned technical criteria”. Local communities also questioned the truthfulness of the information expressed in official maps presented by the EEZ revisions<a class="footnote-ref" href="#iwama2017"> [iwama2017] </a>. Traditional communities expressed a clear interest in taking part in the mapping of their own territory, according to the report of a representative from the Traditional Communities Forum, which encompasses the municipalities of Angra dos Reis, Paraty and Ubatuba:</p>
<blockquote>
<p>We have to sit down together in order to draw the map and to be acknowledged by the map, and not only allow the government to do it&hellip;<br>
(leadership of the Traditional Communities Forum - FCT)<a class="footnote-ref" href="#iwama2017"> [iwama2017] </a></p>
</blockquote>
<p>Thus, when developing LindaGeo, different local groups were mobilized, involved and exchanged experiences, such as, the Northern Coast Hydrographic Basin Committee, the Protected Area of the Marine Environment, local schools, University researchers, besides members of the Traditional Communities Forum and of the Bocaina Observatory of Healthy and Sustainable Territories (OTSS).<sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup></p>
<p>Tests and workshops were carried out on an online platform, developed with free software – Geonode, Geoserver and Geonetwork – in order to produce collaboratively and to share geospatial data<sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup> . Geonetwork and Geoserver were chosen because they are free software adhering to the standards of the Open Geo Consortium (OGC)<sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup> and because they are present in the majority of the Spatial Data Infrastructure built in Brazil. Geonetwork is a catalogue of metadata that allowed the identification, the disclosure and the access to a set of data identified in the context of the project. Geoserver was used for the dissemination of data and information that, up to then, were available only in files stored in local devices, through web services meeting standards defined by OGC. As to Geonode, it was used as a geovisualizer of data and information on the territory.</p>
<p>It was established that the selected software met the objective of offering visibility, knowledge and use to available data and information to those local agents. However, for the use of these software to be effective, considering the diversity of participants and the different demands of the communities involved, it was necessary to devise the structure of a task that involved training and tutoring in order to make possible the fulfilment of the multiple use of these platforms by local groups. Other visualization platforms available were also tried out together with Geonode, such as the Quantum GIS platform, besides proprietary platforms such as Google Earth<sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup> and ArcGIS/ESRI.<sup id="fnref:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup></p>




























<figure ><img loading="lazy" alt="" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Visualization of the territory of the Northern coast of São Paulo using different platforms. (a) Conservation units in ArcGIS online; (b) overlapping of EEZ-NC and precarious settlements in QGIS; (c) marine EEZ-NC in Geonode; (d) Ilhabela State Park on Google Earth. Source: Records from the Ubatuba Open Science Project in meetings of the LindaGeo group during the period of 2016-2017[^36] .
        </p>
    </figcaption>
</figure>
<p>It was also possible to establish the functionalities and limits of the adopted software as well as the need to collectively construct certain protocols. The experience showed that, with the tools and resources then available, even trying to make them more ludic and friendlier, it was difficult to carry out a process of joint construction and representation of their own reality by the participants.</p>
<p>Still under experimentation, a strategy the group has sought is to draw another prototype, involving schools and/ or universities in the co-construction of a Geotechnology Laboratory (LabUbaGeo), with High School and post-High school students, in such a way that the project become part of a continuous pedagogic program in the region.</p>




























<figure ><img loading="lazy" alt="" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Activities carried out in the computer lab (LabUbaGeo) of the Tancredo Neves state school in Ubatuba. (a) QGIS workshop offered by INPE researchers; (b) visualizing printed maps of the currently in use EEZ-NC (2017) and Ubatuba directory plan; (c) computer network and High school students.
        </p>
    </figcaption>
</figure>
<p>Finally, it is worth pointing out that the very discussion about the maps involving different interested local groups and communities is, in itself, a result of the project, encouraging collective thinking about territory on the basis of existing data and encouraging a critical view of what is represented and of the geovisualization tools themselves. The development of a critical perspective about the processes of elaboration and uses of maps by local groups was undoubtedly an important positive outcome of the experience. After all, how to enable different local groups to make use of the different sets of data available remains a key issue to be solved.</p>
<p>From the perspective of this paper, among the various challenges identified towards the implementation of the platform, the following aspects stand out: the importance of setting up a governance system among groups and institutions, according to their different levels and types of participation; the need to structure a multidisciplinary team, involving from information technologists to researchers in the areas of sociology, geography, remote detection and cartography; the definition of a protocol for using data, according to the specificity of each one of the institutions involved; carrying out activities and training in methodologies of social cartography.</p>
<h2 id="conclusions-and-future-projects">Conclusions and Future Projects</h2>
<p>By intending to produce objective scientific facts about territory and as a means of inscription that seeks to consider as truthful the information and data they represent, maps act directly upon the modes of <em>producing</em> space, as well as on how it is appropriated, as a consequence historically becoming a tool in the exercise of power. In this sense, cartography must be understood “as a practice both political as scientific” <a class="footnote-ref" href="#bier2017"> [bier2017] </a>, a characterization that must be extended to the new tools and infrastructure of geovisualization that open up new opportunities, but also create obstacles to the broadening the social basis of knowledge and intervention on territory.</p>
<p>AParticipative approaches and methodologies seek to confer value to and to practice more varied ways of representing territory in order to integrate mapping, (self-) organization and the knowledge of local populations<a class="footnote-ref" href="#eades2014"> [eades2014] </a><a class="footnote-ref" href="#monteiro2015"> [monteiro2015] </a><a class="footnote-ref" href="#dávila2017"> [dávila2017] </a>. To this end, they have adopted new digital tools and platforms of visualization. However, it is necessary to open the black box of these platforms, particularly by democratizing the role played by the author or group of authors of the data when they impose their own perspective, which represents a limited and particular view of the territory.</p>
<p>Strategies of social cartography associated with approaches of open and citizen science can potentially offer a significant contribution towards this end. Beyond the problem of access, it is the case of bringing into question who, how and what type of knowledge is produced – and frequently imposed on territory.</p>
<p>Throughout the development of the prototype of the LindaGeo Platform, it became evident how inequality in access to information and associated technologies is reflected on unequal conditions of participation and intervention in decision processes as indicated in other studies<a class="footnote-ref" href="#craig2002"> [craig2002] </a><a class="footnote-ref" href="#sheppard2008"> [sheppard2008] </a><a class="footnote-ref" href="#acselrad2008"> [acselrad2008] </a>. At the same time, the limits that the access to these infrastructures can generate in the democratization of public policies and interventions in the territory are demonstrated. It became evident that the participative production of the map does not ensure that the opinions expressed there be considered by official governance systems.</p>
<p>On the other hand, within a short period of experimentation, it was noticed that the region has great potential to follow up the initiative, articulating collaborative actions of geovisualization with efforts of sharing and opening data about the territory. The synergy due to this work resulted in the development of an experience of common interest, with the expansion of the working party, the identification of new partners<sup id="fnref:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup> and the acknowledgment of the relevant scientific production.<sup id="fnref:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup> In 2018, the group succeeded in gathering together at least 12 institutions from different sectors and around 60 members interested in unfolding LindaGeo as a structured, long-term action (<a href="http://wiki.ubatuba.cc/doku.php?id=linda:linda">http://wiki.ubatuba.cc/doku.php?id=linda:linda</a>), including managing councils of hydrographic basins and Conservation Units, non-governmental organizations as well as teaching and scientific research institutions.</p>
<p>Thus, as important as the creation in that region of an open data platform of spatial information, the LindaGeo must become an infrastructure for a socially engaged continuous process of dialogue and reflection about power relation, knowledge and participative management of territory.</p>
<h2 id="acknowledgments">Acknowledgments</h2>
<p>The research project resulting in this paper was made possible by the financial support of the National Council for Scientific and Technological Development (CNPq), of the Carlos Chagas Foundation for the Support of Research in the Rio de Janeiro State (Faperj, Project E-26/ 202.413/2017) and of the Open and Collaborative Science in Development Network (OCSDNet), with resources from the International Development Research Center (IDRC), Canada.</p>
<ul>
<li id="acselrad2008">Acselrad, H. (org.) (2008). _Cartografias Sociais e Território_ . Rio de Janeiro, IPPUR/UFRJ. 168p.
</li>
<li id="acselrad2013">Acselrad, H. (org.). (2013) _Cartografia social, terra e território_ . Rio de Janeiro, IPPUR/UFRJ., 318p.
</li>
<li id="albagli2015"> “Albagli, S. Ciência aberta em questão” (2015). In: S.Albagli, M.L. Maciel, A.H. Abdo. _Open Science, Open Issues_ . Brasília: Ibict, Rio de Janeiro: Unirio. Disponível em:<a href="http://livroaberto.ibict.br/handle/1/1061">http://livroaberto.ibict.br/handle/1/1061</a>. Acesso em: 4 maio 2018.
</li>
<li id="albagli2019">Albagli, S., Parra, H., Fonseca, F., & Maciel, M.L. (2019). “Open Science and Social Change: A Case Study in Brazil.” In: Leslie Chan, Angela Okune, Becky Hillyer, Denise Albornoz, Alejandro Posada (Orgs.). Contextualizing Openness: Situating Open Science . Ottawa: University of Ottawa Press. Disponível em:<a href="https://www.idrc.ca/en/book/contextualizing-openness-situating-open-science">https://www.idrc.ca/en/book/contextualizing-openness-situating-open-science</a>.
</li>
<li id="almeida2005">Almeida, A.W.B., Shiraishi Neto, J., Martins, C.C. (2005) “Quebradeiras de Coco Babaçu - Piauí” . São Luís, MA: Projeto Nova Cartografia Social da Amazônia / Editora: UFAM. Disponível em:<a href="http://novacartografiasocial.com.br/fasciculos/">http://novacartografiasocial.com.br/fasciculos/</a>. Acesso em 4 maio 2018.
</li>
<li id="bier2017">Bier, J. (2017). _Mapping Israel, Mapping Palestine: how occupied landscapes shape scientific knowledge._ 
</li>
<li id="boechat2015">Boechat, M.P. (2015). “Visualizar, descobrir e compartilhar: sobre os usos da visualização de informação para construir espaços compartilhados para o debate, os casos do jornalismo de dados e da cartografia de controvérsias.” Tese de Doutorado. Escola de Comunicação, Universidade Federal do Rio de Janeiro, Rio de Janeiro.
</li>
<li id="carpi2011">Carpi Jr., S., Leal, A. (2011) _Mapping environmental risks as tool of participatory plan in hydrographic basins_ . In: Bilibio, C., Hensel, O., Selbach, J. (Org.). Sustainable water management in the tropics and subtropics - and case studies in Brazil. Jaguarão/Kassel: Fundação Universidade Federal do Pampa UNIKASSEL PGCUlt-UFMA, v. 2, pp. 225-248. Disponível em:<a href="http://goo.gl/NxcKwx">http://goo.gl/NxcKwx</a>. Acesso em 4 maio 2018.
</li>
<li id="câmara2001">Câmara, G., Davis.C., Monteiro, A.M., D'Alge, J.C. (2001). _Introdução à Ciência da Geoinformação_ . São José dos Campos, INPE, (on-line, 2a. edição, revista e ampliada).
</li>
<li id="castiglione2009">Castiglione, L. H. G. (2009). “Epistemologia da geoinformação: uma análise histórico-crítica” . Tese de Doutorado. Programa de Pós-Graduação em Ciência da Informação, Niterói, Universidade Federal Fluminense.
</li>
<li id="concar2010">Concar - Comissão Nacional de Cartografia (2010). _Plano de Ação para Implantação da Infraestrutura Nacional de Dados Espaciais_ . Brasília: Ministério do Planejamento, Orçamento e Gestão. 205 p. Disponível em:<a href="http://www.concar.gov.br/pdf/PlanoDeAcaoINDE.pdf">http://www.concar.gov.br/pdf/PlanoDeAcaoINDE.pdf</a>. Acesso em 4 maio 2018.
</li>
<li id="craig2002">Craig, W.J., T. Harris and D. Weiner, eds. 2002. _Community Participation and Geographic Information Systems_ , New York: Taylor Et Francis.
</li>
<li id="crampton2001">Crampton, J.W. (2001). “Maps as Social Constructions: Power, Communication and Visualization” . _Progress in Human Geography_ , v. 25, n.2, pp. 235-252.
</li>
<li id="dávila2017">Dávila, P. (2017). Visualization as assemblage: Exploring critical visualization practice. Information Design Journal, v. 23, n.1, pp. 19–31.
</li>
<li id="deaguiar2010">De Aguiar, L.M. B.(2010).. “Para que serve a educação geográfica?” Disponível em<a href="http://www.ufsj.edu.br/portal2-repositorio/File/vertentes/v.%2019%20n.%201/Ligia_Aguiar.pdf">http://www.ufsj.edu.br/portal2-repositorio/File/vertentes/v.%2019%20n.%201/Ligia_Aguiar.pdf</a>, acesso em 15 de agosto de 2017.
</li>
<li id="eades2014">Eades, G., Zheng, Y. (2014). “Counter-mapping as assemblage.” In B. Doolin, E. Lamprou, N. Mitev & L. McLeod (Eds.). _Information Systems and Global Assemblages: (Re)Configuring Actors, Artefacts, Organizations_ . pp. 79–94. Berlin: Springer.
</li>
<li id="fonseca2014">Fonseca, F.P. (2014). _A Cartografia no Ensino: Os Desafios do Mapa da Globalização_ . Revista do Departamento de Geografia, n. spe, pp. 141-154.
</li>
<li id="fujii2017">Fujii, L.C., Pereira, B.F., Simões, G.S., Takahashi, E.H., Ribeiro, V.C., Cardoso, B.T., Iwama, A.Y., Medeiros, L.C.C. (2017). “Projeto Cachoeiras 2.0 - Uma metodologia participativa ao longo do Rio Paraíba do Sul” . In: _Brahve - 1st Brazilian Workshop on Assessment of Hazards, Vulnerability, Exposure and Disaster Risk Reduction_ . Anais…, pp.1-5. Disponível em:<a href="http://www.cemaden.gov.br/wp-content/uploads/2017/08/Fujii_eixo4_poster.pdf">http://www.cemaden.gov.br/wp-content/uploads/2017/08/Fujii_eixo4_poster.pdf</a>. Acesso em 4 maio 2018.
</li>
<li id="gonzálezdegómez1999">González de Gómez, M.N. (1999). _Da política de informação ao papel da informação na política contemporânea_ . Revista Internacional de Estudos Políticos, Rio de Janeiro, ano 1, n. 1, UERJ/NUSEG, pp. 67-93.
</li>
<li id="gorayeb2015">Gorayeb, A., Meireles, A.J.A, Silva, E.V. (2015) “Princípios básicos de Cartografia e Construção de Mapas Sociais” . In: Gorayeb, A., Meireles, A. J. A., Silva, E. V. (Org.). _Cartografia Social e Cidadania: experiências de mapeamento participativo dos territórios de comunidades urbanas e tradicionais_ . Fortaleza: Expressão Gráfica Editora, pp.9-24.
</li>
<li id="harley1989">Harley, J.B. (1989). _Desconstructing the map_ . Cartographica, v. 26, n. 2, pp. 1-20.
</li>
<li id="janssen2012">Janssen, M., Charalabidis, Y. & Zuiderwijk, A. (2012). “Benefits, Adoption Barriers and Myths of Open Data and Open Government” . _Information Systems Management (ISM)_ , vol. 29, no.4. pp. 258-268.
</li>
<li id="iwama2017">Iwama, A.Y., Silva, D.S., Ballabio, T.A., Fonseca, F.S.(2017). “A Participação no Zoneamento Ecológico-Econômico no Litoral Norte de São Paulo: como estamos nesta discussão?” . _Informar Ubatuba_ . pp.1-6, Disponível em:<a href="http://informarubatuba.com.br/participacao-no-zoneamento-ecologico-economico-no-litoral-norte-de-sao-paulo-como-estamos-nesta-discussao/">http://informarubatuba.com.br/participacao-no-zoneamento-ecologico-economico-no-litoral-norte-de-sao-paulo-como-estamos-nesta-discussao/</a>. Acesso em 30 abril 2018.
</li>
<li id="iwama2018">Iwama, A.Y, Delgado, L.E. “Participación Comunitaria en Procesos de Decisión en la Conservación del Territorio, Cuadernos del Pensamiento Crítico Latinoamericano” . _Clacso_ . pp.1-5, 2018. Disponível em:<a href="https://www.clacso.org.ar/libreria-latinoamericana/libro_por_programa_detalle.php?id_libro=1372&campo=programa&texto=19">https://www.clacso.org.ar/libreria-latinoamericana/libro_por_programa_detalle.php?id_libro=1372&campo=programa&texto=19</a>.
</li>
<li id="">
</li>
<li id="knapp2007">Knapp, F.L. (2007). “Making Maps that Make A Difference: A Citizens’ Guide to Making and Using Maps for Advocacy Work” . International Rivers (Org.): Oakland, USA. Disponível em:<a href="http://www.internationalrivers.org/resources/making-maps-that-make-a-difference-4000">http://www.internationalrivers.org/resources/making-maps-that-make-a-difference-4000</a>. Acesso em 4 maio 2018.
</li>
<li id="latour1990">Latour, B. (1990). _Drawing things together_ . In: Lynch, Michael; Woolgar, Steve (Ed.). Representation in scientific practice. Cambridge: MIT Press. pp.19-68.
</li>
<li id="latour2000">Latour, B. (2000). _Ciência em Ação: como seguir cientistas e engenheiros sociedade afora_ . São Paulo: Editora Unesp.
</li>
<li id="latour2011">Latour, B. (2011). “Waiting for Gaia. Composing the common world through arts and politics.” A lecture at the French Institute, London, November 2011 for the launching of SPEAP (the Sciences Po program in arts & politics) Bruno Latour, Science Po Disponível em:<a href="http://www.bruno-latour.fr/sites/default/files/124-GAIA-LONDON-SPEAP_0.pdf">http://www.bruno-latour.fr/sites/default/files/124-GAIA-LONDON-SPEAP_0.pdf</a>. Acesso em 28 abril 2018.
</li>
<li id="machado2018">Machado, J. “Open data and open science.” In: S.Albagli, M.L. Maciel, A.H. Abdo. _Open Science, Open Issues_ . Brasília: Ibict, Rio de Janeiro: Unirio. pp. 189-214 Disponível em:<a href="http://livroaberto.ibict.br/handle/1/1061">http://livroaberto.ibict.br/handle/1/1061</a>. Acesso em: 4 maio 2018.
</li>
<li id="machlup1962">Machlup, F. (1962). _The Production and Distribution of Knowledge in the United States_ . Princeton, NJ: Princeton University Press.
</li>
<li id="manovich2010">Manovich, L. (2010). “What is visualization?” paj 2.1: _The Journal of the Initiative for Digital Humanities, Media and Culture._ , v. 2, n. 1. Disponível em:<a href="https://journals.tdl.org/paj/index.php/paj/article/view/19">https://journals.tdl.org/paj/index.php/paj/article/view/19</a>. Acesso em 1 abril 2018.
</li>
<li id="marchezini2017">Marchezini, V., Iwama, A.Y., Andrade, M. R. M., Trajber, R.; Rocha, I., Olivato, D. (2017). “Geotecnologias para prevenção de riscos de desastres: usos e potencialidades dos mapeamentos participativos” . _RBC. Revista Brasileira de Cartografia_ , v. 69, pp. 107-128.
</li>
<li id="mayer-schönberger2013">Mayer-Schönberger, V., Cukier, K.. (2013). _Big Data: a revolution that will transform how we live, work, and think_ . Boston: Houghton Mifflin Harcourt.
</li>
<li id="mma1997">MMA – Ministério do Meio Ambiente (1997). “Detalhamento da Metodologia para Execução do Zoneamento Ecológico-Econômico pelos Estados da Amazônia Legal” . Brasília: MMA, Secretaria de Coordenação da Amazônia – SCA, Secretaria de Assuntos Estratégicos da Presidência da República – SAE/PR.
</li>
<li id="monteiro2015">Monteiro, M. (2015). “Construindo imagens e territórios: pensando a visualidade e a materialidade do sensoriamento remoto” . _História, Ciências, Saúde_ – Manguinhos, Rio de Janeiro, v.22, n.2, abr.-jun., pp.577-591.
</li>
<li id="nonaka2008">Nonaka, I., Takeuchi, H. (2008). _Gestão do conhecimento_ . Porto Alegre: Bookman.
</li>
<li id="py2019">Py, H.S. (2019). “Visualização de dados por meio de mapas e participação como recursos para a semocratização dos dados e para a apropriação social da informação.” Tese de Doutorado em Ciência da Informação. Rio de Janeiro: PPGCI/IBICT-UFRJ.
</li>
<li id="saracevic1992">Saracevic, T. (1992). “Information Science: origin, evolution and relations” . In: Vakkari, P. & Cronin, B. (Ed.). _Conceptions of Library and Information Science_ . London: Taylor Graham. pp. 5-27.
</li>
<li id="simões2016">Simões, E., Navarro, F.C.S., Bussolotti, J., Alves Junior, J.I. (2016). _Planejamento Ambiental da Bacia Hidrográfica do Ubatumirim - instrumento de justiça socioambiental_ . 1ª. ed. São Paulo: Páginas & Letras, 114 p.
</li>
<li id="star1996">Star, S.L., Ruhleder, K. (1996). “Steps toward an ecology of infrastructure: design and access for large information spaces” . _Information Systems Research_ . v. 7, n. 1. pp. 111-134.
</li>
<li id="sheppard2008">Sheppard, E. (2008). “Produção de conhecimento através do Sistema de Informações Geográficas Crítico: genealogia e perspectivas.” In: Acselrad, H. (Org.). _Cartografias Sociais e Território_ . p.113-151.
</li>
<li id="wersig1996">Wersig, G. (1996). _The information service of the 21st Century_ . SungKyunKwan University, Seoul, Korea. Disponível em:<a href="http://www.kommwiss.fuberlin.de/439.html">http://www.kommwiss.fuberlin.de/439.html</a>. Acesso em 24 jul. 2007.
</li>
</ul>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p><a href="#castiglione2009">Castiglione (2009)</a>also mentions<a href="#ara%C3%BAjo1994">Araújo (1994)</a>who reminds us that “the word information comes from the Latin word informare: to form, to shape or to create, but also to represent, to present, to create an idea or notion – something that is ordered.”&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>According to<a href="#castiglione2009">Castiglione (2009, p. 158)</a>, “the terms geoinformation, geographical information, georeferenced information and geospatial information are generally employed as different signifiers which, nevertheless, refer to the same signified, even though some currents of thought on the subject matter consider that the term geospatial information refers to a broader, almost generic concept of georeferenced information” <a class="footnote-ref" href="#castiglione2009"> [castiglione2009] </a>.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>To Latour, scientific facts result from collective constructions that involve the “recruitment” of different allies, from peers in the scientific community to organizations for policies and incentive of research<a class="footnote-ref" href="#latour2000"> [latour2000] </a>.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>A type of cartography based on the elaboration and use of thematic mapping (ground maps, environmental zoning maps) involving the collection, analysis, interpretation and representation of information in a base map, making use of graphic symbology (size, form, value, grain, color) for its representation.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>he expression open data has been commonly used to refer to the transparency of government data. In the field of science, it refers to the publicization of the primary data of a research project, which is considered a fundamental step towards its reproducibility and re-use, besides allowing for broad scrutiny. It can also contribute towards exposing inconsistencies, low quality, plagiarism or fraud.&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>“The term Spatial Data Infrastructure is frequently used to denote a basic set of technologies, policies and institutional arrangements that facilitate the availability and access to spatial data” <a class="footnote-ref" href="#concar2010"> [concar2010] </a>.&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>he focus has been on attributing the main role to the perceptions of local communities, such as thequilombolas, fishermen, extractivists and periphery groups, acknowledging and strengthening their points of view regarding topics such as protected areas, disasters, hydric resources and hydrographic basins, among others.&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>This is a Project coordinated by the Brazilian Institute of Information in Science and Technology (IBICT) from 2015 to 2017, as part of the Open and Collaborative Science in Development Network – OCSDNet <a href="https://www.ocsdnet.org">www.ocsdnet.org</a> ), with the financial support of IDRC/Canada. Project documentation can be found at<a href="http://cienciaaberta.ubatuba.cc/">http://cienciaaberta.ubatuba.cc/</a>and<a href="https://pt.wikiversity.org/w/index.php?title=Pesquisa:Ci%C3%AAncia_Aberta_Ubatuba">https://pt.wikiversity.org/w/index.php?title=Pesquisa:Ci%C3%AAncia_Aberta_Ubatuba</a>&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>Open Science means that scientific knowledge should be used, re-used and distributed freely by people with no legal, technological or social restrictions. The open Science movements began around open access to scientific publications and expanded in different directions, among which: open research data, open hardware, open research tools and citizen science. Citizen science encompasses the contribution and participation of non-scientists to scientific research.&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>Ecological- Economic Zoning (EEZ) consists in a “political and technical instrument for planning whose ultimate goal is to optimize the use of space and public policies” <a class="footnote-ref" href="#mma1997"> [mma1997] </a>. EEZ presupposes four fundamental political axes: 1) territorial knowledge and understanding; 2) ecological and economic sustainability; 3) democratic participation and 4) institutional articulation.&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>Discussions taken place during these meetings were accompanied and registered by the Project team; their results were incorporated into reports and publications. During this process, around 60 proposals were received, 35 from traditional communities (indigenous communities, caiçaras, quilombolas), through which 156 requests for modification or continuity of inclusion within specific zones<a class="footnote-ref" href="#iwama2017"> [iwama2017] </a>.&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p>OTSS is a local initiative in partnership with the Oswaldo Cruz Foundation (Fiocruz) which has sought to develop actions to secure the rights of indigenous communities, quilombolas and caiçaras existing in the region, relating to the uses of territory, culture and to quality of life. It developed strategies to confer visibility to these communities, including a Google Maps platform —<a href="http://otss.org.br/mapas/">http://otss.org.br/mapas/</a>.&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13">
<p><a href="http://wiki.ubatuba.cc/doku.php?id=cienciaaberta:encontro160808">http://wiki.ubatuba.cc/doku.php?id=cienciaaberta:encontro160808</a>&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14">
<p>The Open Geospatial Consortium (OGC) is a non-profit international organization oriented towards the creation of quality standards open to the global geospatial community. The consortium currently encompasses more than 525-member organizations and its proposed standards are used in a variety of areas, including geoscience and the environment; defense and intelligence; intelligent cities; the Internet of Things (IoT) and Sensors, mobile technologies; emergency responses and disaster management; aviation; energy and utilities, among others. Adherence to OGC standards favors interoperability between different technologies including proprietary ones. Available at:<a href="http://www.opengeospatial.org/">http://www.opengeospatial.org/</a>. Accessed on 29 Jan., 2019.&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:15">
<p>The Google Earth platform, developed by Google, allows users to visualize and to use different contents, including map and terrain data, images, information on businesses, traffic, evaluations as well as other information provided by Google -<a href="https://www.google.com.br/earth/download/gep/agree.html">https://www.google.com.br/earth/download/gep/agree.html</a>&#160;<a href="#fnref:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:16">
<p>ArcGIS Online is an online collaborative web SIG, developed by ESRI that allows users to utilize, create and share maps, scenes, applications, layers, analytics and data -<a href="https://doc.arcgis.com/pt-br/arcgis-online/reference/what-is-agol.htm">https://doc.arcgis.com/pt-br/arcgis-online/reference/what-is-agol.htm</a>.&#160;<a href="#fnref:16" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:17">
<p>The group set up committees that have been working towards the identification of which governmental institutions produce geospatial data in the region, their lineage, scale and metadata, discussing their use for recurrent issues in the region such as EEZ revisions, Conservation Units management plans, directory plans and, more recently, the urban mobility plan. See in Working Groups at<a href="http://wiki.ubatuba.cc/doku.php?id=linda:lindageo">http://wiki.ubatuba.cc/doku.php?id=linda:lindageo</a>.&#160;<a href="#fnref:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:18">
<p>The strategy is to identify and request that scientific results be placed on a platform that is accessible to the whole community. View at scientific publications on the Northern Coast of São Paulo.<a href="http://wiki.ubatuba.cc/doku.php?id=linda:trabalhos_cientificos_lnsp">http://wiki.ubatuba.cc/doku.php?id=linda:trabalhos_cientificos_lnsp</a>.## Bibliography&#160;<a href="#fnref:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry><entry><title type="html">Ler a prosa do mundo hoje</title><link href="https://rlskoeser.github.io/dhqwords/vol/14/2/000457/?utm_source=atom_feed" rel="alternate" type="text/html"/><link href="https://rlskoeser.github.io/dhqwords/pt/vol/14/2/000457/?utm_source=atom_feed" rel="alternate" type="text/html" hreflang="pt"/><id>https://rlskoeser.github.io/dhqwords/vol/14/2/000457/</id><author><name>Maria Clara Paixão de Sousa</name></author><published>2020-06-19T00:00:00+00:00</published><updated>2023-08-06T15:53:44-04:00</updated><content type="html"><![CDATA[<h2 id="note-on-translation">Note on Translation</h2>
<p>For articles in languages other than English, DHQ provides an English-language abstract to support searching and discovery, and to enable those not fluent in the article&rsquo;s original language to get a basic understanding of its contents. In many cases, machine translation may be helpful for those seeking more detailed access. While DHQ does not typically have the resources to translate articles in full, we welcome contributions of effort from readers. If you are interested in translating any article into another language, please contact us at <a href="mailto:editors@digitalhumanities.org">editors@digitalhumanities.org</a> and we will be happy to work with you.</p>
<h2 id="bibliography">Bibliography</h2>
<ul>
<li id="artières2010">Artières, Philippe; Bert, Jean-François. “La Bibliothèque Foucaldienne” . [internet]. Centre Michel Foucault, École normale supérieure de Lyon; 2010. Disponível em<a href="http://lbf-ehess.ens-lyon.fr/pages/index.html">http://lbf-ehess.ens-lyon.fr/pages/index.html</a>.
</li>
<li id="baumann2010">Bamman, D. and Crane, G. “Corpus linguistics, treebanks and the reinvention of philology” . _INFORMATIK 2010. Service Science–Neue Perspektiven für die Informatik_ . Band 2, 2010
</li>
<li id="belon1555">Belon, Pierre.  _Histoire de la nature des oiseaux_ . Paris, 1555. Imagens destacadas, Bibliothèque Interuniversitaire de Santé: <a href="http://www2.biusante.parisdescartes.fr/img/?refphot=09330">http://www2.biusante.parisdescartes.fr/img/?refphot=09330</a>,<a href="http://www2.biusante.parisdescartes.fr/img/?refphot=09331">http://www2.biusante.parisdescartes.fr/img/?refphot=09331</a>.
</li>
<li id="berry2011">Berry, David M. “The computational turn. Culture Machine:” thinking about the Digital Humanities. 2011, 12. Disponível em<a href="http://culturemacne.net/index.php/cm/article/view/440/470">http://culturemacne.net/index.php/cm/article/view/440/470</a>.
</li>
<li id="betts2009">Betts, Russell. _Colloquium on Digital Humanities and Computer Science_ , Chicago, 2009.
</li>
<li id="busa2004">Busa, R.A., 2004. “Foreword: Perspectives on the digital humanities” . _A companion to digital humanities_ , pp.xvi-xxi.
</li>
<li id="chaudiron2008">Chaudiron, S., Ihadjadene, M, Maredj, A. “La fragmentation et lunité documentaire em question” . In: Actes … CONGRÈS DE LA SFSIC, 16. Compiègne, 2008.
</li>
<li id="clement2008">Clement, T., Steger, S., Unsworth, J., & Uszkalo, K. (2008) “How Not to Read a Million Books” , accessed 21 June 2010. Disponível em<a href="http://www3.isrl.illinois.edu/~unsworth/hownot2read.html#sdendnote4sym">http://www3.isrl.illinois.edu/~unsworth/hownot2read.html#sdendnote4sym</a>.
</li>
<li id="crane2008">Crane, Gregory; Bamman, David; Babeu, Alison. _ePhilology: when the books talk to their readers_ . Blackwell Companion to Digital Literary Studies, R. Siemens; S. Schreibman (eds). Oxford: Blackwell, 2008. <a href="http://www.digitalhumanities.org/companion">http://www.digitalhumanities.org/companion</a>.
</li>
<li id="dacos2013">Dacos, Marin. “La stratégie du Sauna finlandais. Les frontières de Digital Humanities. Essai de Géographie politique dune communauté scientifique” , 2013. <a href="http://bn.hypotheses.org/11138">http://bn.hypotheses.org/11138</a>.
</li>
<li id="dewey2015">Dewey, Caitlin. “If you could print out the whole Internet, how many pages would it be?” The Washington Post, 2105. Disponível em<a href="https://www.wasngtonpost.com/news/the-intersect/wp/2015/05/18/if-you-could-print-out-the-whole-internet-how-many-pages-would-it-be/?utm_term=.1cb813d38f45">https://www.wasngtonpost.com/news/the-intersect/wp/2015/05/18/if-you-could-print-out-the-whole-internet-how-many-pages-would-it-be/?utm_term=.1cb813d38f45</a>.
</li>
<li id="eggert2010">Eggert, P. “Text as Algorithm and Process.” In _Text and Genre in Reconstruction: Effects of Digitalization on Ideas, Behaviours, Products and Institutions_ . McCarty, Willard (eds). United Kingdom,  Open Book Publishers, 2010.
</li>
<li id="fiormonte2014">Fiormonte, D. “A quando i Brics della conoscenza?” Infolet: Cultura e critica dei media digitali. Número 6, 2014.i Disponível em<a href="https://infolet.it/?issue=numero-6">https://infolet.it/?issue=numero-6</a>.
</li>
<li id="fiormonte2012">Fiormonte, Domenico. “Towards a Cultural Critique of the Digital Humanities” . Historical Social Research / Historische Sozialforschung, vol. 37, no. 3 (141), 2012, pp. 59–76. JSTOR,<a href="www.jstor.org/stable/41636597">www.jstor.org/stable/41636597</a>.
</li>
<li id="foucault2000">Foucault, Michel. _[As palavras e as coisas:_ uma arqueologia das ciências humanas]. Tradução de Salma Tannus Muchail para Les mots et les choses: une archéologie des sciences humaines, 1966. São Paulo: Martins Fontes; 2000.
</li>
<li id="gradmann2008">Gradmann, Stefan; Meister, Jan Christoph. “Digital document and interpretation: re-thinking text and scholarship in electronic settings” . Poiesis Prax (2008) 5:139–153. DOI 10.1007/s10202-007-0042-y.<a href="http://link.springer.com/content/pdf/10.1007%2Fs10202-007-0042-y.pdf">http://link.springer.com/content/pdf/10.1007%2Fs10202-007-0042-y.pdf</a>.
</li>
<li id="kermode2016">Kermode F. _Forms of attention_ . The University of Chicago Press, Chicago; 2016 [1985].
</li>
<li id="khabsa2014">Khabsa M, Giles CL (2014) “The Number of Scholarly Documents on the Public Web” . PLoS ONE 9(5): e93949.<a href="https://doi.org/10.1371/journal.pone.0093949">https://doi.org/10.1371/journal.pone.0093949</a>.
</li>
<li id="kirschenbaum2010">Kirschenbaum, Matthew G. “What is Digital Humanities and what is it doing in your English department?” ,  ADE BullEtin, 150, 2010. <a href="https://humanidadesdigitais.files.wordpress.com/2011/09/kirschenbaum_whatisdigitalhumanities.pdf">https://humanidadesdigitais.files.wordpress.com/2011/09/kirschenbaum_whatisdigitalhumanities.pdf</a>.
</li>
<li id="kunder2018">Kunder, Maurice. “The size of the World Wide Web” . Disponível em<a href="http://www.worldwidewebsize.com">http://www.worldwidewebsize.com</a>. Acesso em: 01 mar. 2018.
</li>
<li id="marche2013">Marche, Stephen. “Literature is not Data: Against Digital Humanities” , 2013.<a href="http://lareviewofbooks.org/essay/literature-is-not-data-against-digital-humanities">http://lareviewofbooks.org/essay/literature-is-not-data-against-digital-humanities</a>.
</li>
<li id="meister2012">Meister, Jan Christoph. “DH is us or on the unbearable lightness of a shared methodology” . In: Historical Social Research 37 (2012), 3, pp. 77-85. URN: <a href="http://nbn-resolving.de/urn:nbn:de:0168-ssoar-378413">http://nbn-resolving.de/urn:nbn:de:0168-ssoar-378413</a>.
</li>
<li id="paixão2015">Paixão de Sousa, M. C.  “As Humanidades Digitais Globais?”  Ciclo de Conferências: Congresso Humanidades Digitais em Portugal (Universidade Nova de Lisboa, 8/10/2015), CIDEHUS (Universidade de Évora, 6/10/2015), Programa Materialidades da Literatura (Universidade de Coimbra, 12/10/2015). Disponível em <a href="http://humanidadesdigitais.org/hd2015/">http://humanidadesdigitais.org/hd2015/</a>.
</li>
<li id="paixão2014">Paixão de Sousa, Maria Clara. “O Corpus Tycho Brahe: contribuições para as humanidades digitais no Brasil” . Filologia e Linguística Portuguesa, v. 16, p. 53-93, 2014. <a href="http://www.revistas.usp.br/flp/article/view/88404">http://www.revistas.usp.br/flp/article/view/88404</a>.
</li>
<li id="paixão2013">Paixão de Sousa, Maria Clara. “Texto digital: Uma perspectiva material” . Revista ANPOLL (Associação Nacional de Pós–Graduação e Pesquisa em Letras e Lingüística). Volume 1, Número 35, 2013. ISSN: 1982-7830. Disponível em<a href="https://revistadaanpoll.emnuvens.com.br/revista/article/view/643/712">https://revistadaanpoll.emnuvens.com.br/revista/article/view/643/712</a>.
</li>
<li id="paixão2010">Paixão de Sousa, Maria Clara.   “O Leitor na biblioteca digital” . II Seminário Mindlin: O Futuro das bibliotecas, São Paulo, 2010. Disponível em <a href="http://oleitornabibliotecadigital.wordpress.com">http://oleitornabibliotecadigital.wordpress.com</a>.
</li>
<li id="pêcheux1982">Pêcheux, Michel. “[Ler o arquivo hoje]” . Tradução de Maria das Graças Lopes Morin do Amaral para Lire larchive aujourdhui, 1982. In E. Orlandi et al. (Orgs.), _Gestos de leitura:_ da história no discurso. Tradução: Bethânia S. C. Mariani [et. al]. Campinas: Editora da Unicamp, 1994, p.55-66 (Coleção Repertórios).
</li>
<li id="peckel2012">Peckel, Joris. “The State of Digitisation” . [Internet]. Open Knowledge Foundation / DM2E, 2012. Disponível em<a href="https://openglam.org/2012/08/03/the-state-of-digitisation">https://openglam.org/2012/08/03/the-state-of-digitisation</a>; acesso em 12/12/2012.
</li>
<li id="pédauque2004">Pédauque, Roger T. “Document et texte: Permanence et transformations” . In: HAL-SHS- Sciences de lHomme et de la Société. Version du 15-06-2004. Disponível em<a href="http://halshs.arcves-ouvertes.fr/sic_00001003">http://halshs.arcves-ouvertes.fr/sic_00001003</a>. Acesso em: 03 de jul. 2013.
</li>
<li id="pédauque2007">Pédauque, Roger T. _La redocumentarisation du monde_ . Toulouse: Cépaduès, 2007.
</li>
<li id="pédauque2006">Pédauque, Roger T. _Le document à la lumière du numérique_ . Caen, France: C & F, 2006.
</li>
<li id="porta1655">Porta G. _La physionomie humaine de Iean Baptiste Porta neapolitain, divisée en quatre liures_ : enrichie de quantité de figures tirées au naturel, ou par les signes exterieurs du corps, on voit si clairement la complexion, les mœurs, & les desseins des hommes, qu'on semble penetrer iusques au plus profond de leurs ames. A Rouen: Chez Iean & David Berthelin; 1655. Disponível em<a href="https://arcve.org/details/gri_33125008642148">https://arcve.org/details/gri_33125008642148</a>.
</li>
<li id="priani2014">Priani Saisó, Ernesto, et. al. “Las humanidades digitales en español y portugués. Un estudio de caso: DíaHD/DiaHD” . ANUARIO AMERICANISTA EUROPEO, 2221-3872, N° 12, 2014, Sección Tema Central p. 5-18 5. <a href="http://www.red-redial.net/revista/anuario-americanista-europeo/article/view/267">http://www.red-redial.net/revista/anuario-americanista-europeo/article/view/267</a>.
</li>
<li id="schreibman2004">Schreibman,  Susan; Siemens, Ray;  Unsworth, John (eds.).  _A Companion to Digital Humanities_ .  Oxford: Blackwell, 2004.  <a href="http://www.digitalhumanities.org/companion">http://www.digitalhumanities.org/companion</a>.
</li>
<li id="unsworth2006">Unsworth, John. “Forms of Attention: Digital Humanities Beyond Representation.” Paper delivered at "The Face of Text: Computer-Assisted Text Analysis in the Humanities," III Conference of the Canadian Symposium on Text Analysis (CaSTA), McMaster University, November 19-21, 2006. <a href="http://people.brandeis.edu/~unsworth/FOA/">http://people.brandeis.edu/~unsworth/FOA/</a>.
</li>
<li id="unsworth2001">Unsworth, John. “Knowledge Representation in Humanities Computing” . Lecture I in the  _eHumanities_  NEH Lecture Series on Technology & the Humanities, Washington, DC, April 3, 2001.Disponível em:<a href="http://www.people.virginia.edu/~jmu2m/FOA/">http://www.people.virginia.edu/~jmu2m/FOA/</a>.
</li>
</ul>
]]></content></entry><entry><title type="html">Open Data in Cultural Heritage Institutions: Can We Be Better Than Data Brokers?</title><link href="https://rlskoeser.github.io/dhqwords/vol/14/2/000462/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://rlskoeser.github.io/dhqwords/vol/14/2/000462/</id><author><name>S.L. Ziegler</name></author><published>2020-06-19T00:00:00+00:00</published><updated>2023-08-06T15:53:44-04:00</updated><content type="html"><![CDATA[<h2 id="introduction-collections-as-data-in-cultural-institutions">Introduction: Collections as Data in Cultural Institutions</h2>
<p>Recently, increased attention has been paid to data in cultural institutions.<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> In both 2016 and 2017, the Library of Congress hosted conferences on the use of library collections as data<a class="footnote-ref" href="#loc2016"> [loc2016] </a><a class="footnote-ref" href="#loc2017"> [loc2017] </a>. Also in 2016, the Institute of Museums and Library Services (IMLS) funded a two-year project, Always Already Computational (AAC), “lead to the creation of a framework to support library collections as data, the identification of methods for making computationally-amenable library collections more discoverable, use cases and user stories for such collections, and guidance for future technical development” <a class="footnote-ref" href="#imls2016"> [imls2016] </a>. In addition to workshops and meetings, the AAC<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> team compiles information on like-minded projects, and has released “The Santa Barbara Statement on Collections as Data” , a document of guiding principles for treating collections as data<a class="footnote-ref" href="#aac2018b"> [aac2018b] </a>.</p>
<p>The AAC project has done a great deal of important work in bringing together a wide variety of practitioners and examples and for this reason situating an exploration of data in cultural heritage institutions within the framework of the collections as data conversation is beneficial. As a catalyst within a wider world of data-oriented endeavors in cultural institutions, the AAC project has opened new avenues of investigation and has amplified the need for collaboration among institutions and practitioners. It is becoming increasingly common to see issues related to data in special collections libraries appearing in syllabi, library strategic goals, and position papers<a class="footnote-ref" href="#liedlibrary"> [liedlibrary] </a>. “The growing interest in collections as data,” writes Chela Scott Weber in a recent OCLC Research Position Paper, “means we must collaborate with colleagues in scholarly communications, data services, and elsewhere across the library to grapple with what computational access to our collections might look like” <a class="footnote-ref" href="#weber2017"> [weber2017] </a>.</p>
<p>Collections as data explores an expansive definition of data. “To see collections as data begins with reframing all digital objects as data,” Thomas Padilla writes. “Data are defined as ordered information, stored digitally, that are amenable to computation. Wax cylinders, reel to reel tape, vellum manuscripts, websites, masterworks, musical scores, social media, code and software in digital collections are brought onto the same field of consideration” <a class="footnote-ref" href="#padilla2017"> [padilla2017] </a>. In addition to digital collections being reconceptualized as data, the metadata — such as titles, descriptions, dates — can also be rethought as data. “Data as well as the data that describe those data,” explains the Santa Barbara Statement, “are considered in scope. For example, images and the metadata, finding aids, and/or catalogs that describe them are equally in scope. Data resulting from the analysis of those data are also in scope” <a class="footnote-ref" href="#aac2018b"> [aac2018b] </a>.</p>
<p>In many ways treating collections as data eases some barriers to sharing data. However, collections as data is not the same as open data. Open data has few, if any restrictions on use and reuse<a class="footnote-ref" href="#oki"> [oki] </a>. “Accessibility and reusability,” write Koster and Woutersen-Windhouwer, “do not require collections and objects to be freely available, modifiable and shareable with free tools,” as the open definition requires. “Some metadata or objects will be copyright protected, have privacy issues or local law issues” <a class="footnote-ref" href="#koster2018"> [koster2018] </a>. When we rethink collections as data, collections are usually easier to share, however, there are still many reasons that the data might not be open.<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup></p>
<p>In November 2019 the AAC grant project came to an end and was succeeded by a new phase, Collections as Data: Part to Whole (“Part to Whole” n.d.). Collections as Data: Part to Whole fosters “the development of broadly viable models that support implementation and use of collections as data” by funding project teams that “will develop models that support collections as data implementation and holistic reconceptualization of services and roles that support scholarly use” (“Part to Whole” n.d.). Even beyond the AAC project the number and scale of cultural heritage collections available as data continues to increase. In 2019, the Library of Congress, with funding from the Andrew W. Mellon Foundation, launched the Computing Cultural Heritage in the Cloud (CCHC) project, to “pilot ways to combine cutting edge technology and the collections of the largest library in the world, to support digital research at scale” <a class="footnote-ref" href="#loc2019"> [loc2019] </a>. Also in 2019, the National Library of Scotland launched the Data Foundry Website to present, “collections as data in a machine-readable format, widening the scope for digital research and analysis” <a class="footnote-ref" href="#nls2019"> [nls2019] </a>.</p>
<h2 id="data-in-cultural-heritage-institutions">Data in Cultural Heritage Institutions</h2>
<p>The AAC team has compiled a number of Facets, or case studies that draw attention to many ways cultural institutions are creating and using data.<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup> These examples include the use metadata, digital facsimiles, and structured transcriptions. The metadata examples show new forms of access and engagement with collections, “allowing people to creatively re-imagine and re-engineer our collection in the digital space” <a class="footnote-ref" href="#newbury"> [newbury] </a>. The Carnegie Museum of Art, for example, makes available “data on approximately 28,269 objects across all departments of the museum[:] fine arts, decorative arts, photography, contemporary art, and the Heinz Architectural Center” <a class="footnote-ref" href="#carnegie"> [carnegie] </a>. Released as part of the 120th anniversary celebration of the museum, the data promotes the central mission of the museum. “The case to provide the public increased access to museum data was not a difficult one at the Carnegie Museum of Art,” explain the authors of the data, “the museum considers engagement and education to be a core part of its mission, and firmly believes in Open Access as essential to museum practice” <a class="footnote-ref" href="#newbury"> [newbury] </a>.</p>
<p>In addition to sharing metadata, many libraries and museums are allowing full access to digital facsimiles. The Getty, for example, “makes available, without charge, all available digital images to which the Getty holds the rights or that are in the public domain to be used for any purpose. No permission is required” <a class="footnote-ref" href="#getty"> [getty] </a>. As another example, OPenn, at the University of Pennsylvania, “contains complete sets of high-resolution archival images of manuscripts … along with machine-readable TEI P5 descriptions and technical metadata. All materials on this site are in the public domain or released under Creative Commons licenses as Free Cultural Works” <a class="footnote-ref" href="#upenn"> [upenn] </a>.</p>
<p>Beyond metadata and digital facsimiles, collections reformatted as structured data is also a growing trend in cultural institutions. This often involves transcribing text and applying some type of structure. Haverford College Libraries reformatted collections into TEI-encoded text for the projects “Beyond Penn’s Treaty” <a class="footnote-ref" href="#zarafonetis"> [zarafonetis] </a>and “Ticha: A Digital Text Explorer for Colonial Zapotec” <a class="footnote-ref" href="#lillehaugen"> [lillehaugen] </a>. Reformatting the collections as structured data allows for enhanced linkages between items as well as customized interfaces. The Museum of Modern Art (MOMA) structured exhibition data in a database, and shared as a CSV (Lill n.d.). This project has resulted in a rich history of past MOMA exhibitions.</p>
<p>A fuller exploration of a particular open data project will help highlight both the process and possible pitfalls of collections as data work in cultural heritage institutions. The American Philosophical Society Library (APS) digitized, reformatted as data, and opened historic prison records. The APS holds three admission books created by the Eastern State Penitentiary.<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup> The books, covering the years 1830-1850 (with a gap in the 1840s), hold information on each prisoner, including name, age, the crime(s) for which they had been found guilty, the sentence, and often a note on when they were freed (or died). Also included, though less consistently, is gender, race, and religious affiliation. Additionally, notes from the moral instructor appear for each record; similar to contemporary prison parsons, moral instructors were non-denominational religious authorities and they recorded a paragraph-length note on each inmate which details the religious education of the prisoner as well as other biographical elements.</p>
<p>In 2015, the team<sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup> digitized the admission books. At this point, the scanned images of the pages of the admission books could be treated as data. As data, they are open to computational analysis. Just as digital representations of paintings allow the application of computer models to extract information and recombine it to find new patterns and propose linkages between pieces,<sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup> the pages of the prison admission books can be treated as data. For example, by analyzing the pages as data, one could ask if the handwriting changes based on the positive or negative attributes ascribed to the prisoner.</p>




























<figure ><img loading="lazy" alt="" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     >
</figure>
<p>Taking the process one step further, in 2016 and 2017 the team transcribed the content into spreadsheets.<sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup> The library published the data as CSV files in a variety of outlets, including the APS Github repository<a class="footnote-ref" href="#aps-git"> [aps-git] </a>and the Magazine for Early American Datasets<a class="footnote-ref" href="#ziegler2016"> [ziegler2016] </a>. The workflow was simple: each transcriber had two computer monitors, one to see the digitized page and the other to type into a spreadsheet. The information on the pages are mostly consistent. For example, the first line of each entry usually includes the prisoner’s name, age, crime for which he or she is incarcerated, race (usually if non-White), and location of birth (if outside of US). The repetition and consistency enabled the information to be captured with few issues.</p>
<p>However, the process is not always obvious, and choices do matter. For example, the handwriting can often be difficult to read, and conventions about how to depict abbreviations, spelling errors and indecipherable words had to be established.<sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup> Sometimes racial categories are mentioned, such asblack, orlight black. Sometimes, on the same place on the page, religions are mentioned, such asCatholic. Sometimes the birth location is listed near the same place on the page, such as “Irish”. It is easy to imagine that the categories of race, religion, and birth location were doing interpretive work for both the creator of the records and the intended audience. Jacksonian America was a time of great anxiety of immigration and saw a large number of race riots<a class="footnote-ref" href="#feldberg1980"> [feldberg1980] </a>. The transcription team needed to make decisions about how to group these pieces of information. Records are not neutral; the process of transcription is not neutral.<sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup></p>




























<figure ><img loading="lazy" alt="" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     >
</figure>
<p>From the point of view of the APS Library data initiatives team, the CSV files themselves are the finished product. The Open Data Initiative of the APS Library aims to increase access to computational data by identifying material in the collection that “is conducive to being reconfigured as datasets” <a class="footnote-ref" href="#aps-data"> [aps-data] </a>. In the case of this project, however, we built example visualization tools as a means of promoting the use of the data. In late 2017, the library launched a series of visualization tools grouped together under the title “Eastern Apps: Visualizing Historic Prison Data” <a class="footnote-ref" href="#aps-apps"> [aps-apps] </a>. A suite of three visualization apps, “Eastern Apps” serves as an example of the use of collections as data for digital humanities projects, and has served to help the APS Digital Scholarship Center promote the use of other datasets to digital humanists<a class="footnote-ref" href="#ziegler2019"> [ziegler2019] </a>.<sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup></p>
<p>As the examples in this section highlight, collections as data in cultural institutions can take many forms. Metadata enables the re-imagining of art collections, as in the case with the Carnegie Museum of Art. Digitized objects allow use and reuse for both scholarly and imaginative purposes. Collection content can be transcribed as structured open data allowing for analysis, visualizations, access through innovative interfaces, and new types of research. However, there is always interpretive work that needs to be done when creating datasets from cultural heritage material.</p>
<h2 id="data-brokers">Data Brokers</h2>
<p>In June 2016, HBO’s <em>Last Week Tonight With John Oliver</em> purchased nearly 15 million dollars of debt for 60 thousand dollars. The purchase was part of an exposé on the debt buying business, what John Oliver called “a grimy business.” A large part of the griminess of the business is the ability to buy and sell personal information about individuals. Names, addresses, social security numbers and other information is passed from one buyer to the next, often emailed in spreadsheets. The buyers hope to pressure the named individuals into paying the debts, creating a profit for the debt buyer. While the <em>Last Week Tonight</em> exposé focused on debt buying. The business of buying and selling personal information extends much further.</p>
<p>In March 2014, CBS’ <em>60-Minutes</em> aired an episode on data brokers. “Every piece of data about us now seems to be worth something to somebody,” said Tim Sparapani during the show. “And lots more people are giving up information about people they do business with, from state Departments of Motor Vehicles, to pizza parlors” <a class="footnote-ref" href="#cbs2018"> [cbs2018] </a>. The data from different sources are gathered together to form dossiers. “The dossiers are about individuals,” one interviewee continued. “That&rsquo;s the whole point of these dossiers. It is information that is individually identified to an individual or linked to an individual” <a class="footnote-ref" href="#cbs2018"> [cbs2018] </a>. This information can then be used to identify individuals with health issues, significant debt, and individuals who suffer from addictions. This data is then sold to potential employers, other data brokers and increasingly to credit monitoring companies and law enforcement.</p>
<p>In 2014, the FTC released a report on data brokers that highlighted how little we know about the data being collected, and how much data there is about us. Because most of the data gathered about us do not come directly from us, most people do not fully grasp the amount of information data brokers collect and sell. “Some of the information data brokers collect, like bankruptcy information, voting history, consumer purchase data, web browsing activities and warranty registrations” are gathered from other sources<a class="footnote-ref" href="#ftc2014"> [ftc2014] </a>. This data is used to put us in categories that make the data easier to market to other companies. “Potentially sensitive categories include those that primarily focus on ethnicity and income-levels, a consumer’s age, or health-related conditions like Expectant Parent, Diabetes Interest, and Cholesterol Focus ” <a class="footnote-ref" href="#ftc2014"> [ftc2014] </a>.</p>
<p>The practice of data brokerage is secretive, and there is often no way to appeal incorrect information. The profiles these companies assign to us are often incorrect. “In the world of data brokers, you have no idea who all has bought, acquired or harvested information about you, what they do with it, who they provide it to, whether it is right or wrong or how much money is being made on your digital identity,” writes Kalev Leetaru, of his efforts to determine who is making money from his information. “Nor do you have the right to demand that they delete their profile on you” <a class="footnote-ref" href="#leetaru2018"> [leetaru2018] </a>. In the case of Leetaru, the companies got many things wrong, including his age. In 2017 writer Caitlyn Renee Miller bought information about herself from a data broker only to find that “nearly 50 percent of the data in the report about me was incorrect” <a class="footnote-ref" href="#miller2017"> [miller2017] </a>.</p>
<p>In 2017, Equifax announced a data breach that allowed the personal information of 143 million people to be stolen. In 2018, Facebook announced that the data analysis company Cambridge Analytica used personal data in ways that easily match John Oliver’s definition of grimy. The use of personal data by companies large and small to profit is an important backdrop against which to evaluate open data in cultural institutions. Equifax and Cambridge Analytica are not, technically, data brokers. The former is a credit monitoring company and the latter, before declaring bankruptcy<a class="footnote-ref" href="#confessore2018"> [confessore2018] </a>, was a data analysis firm. However, the role of data brokers, companies that buy, combine, and sell personal information to other companies, is instructive for our purposes, and the shades of differences can be grouped together for our purposes.</p>
<p>Data brokers, along with credit monitoring companies and data analytic companies, benefit from the information of other people. This information often harms individuals through the categories they create. Examples such as “ single mom struggling in an urban setting or people who did not speak English and felt more comfortable speaking in Spanish or gamblers ” <a class="footnote-ref" href="#naylor2016"> [naylor2016] </a>. Categories such as these, which are created, populated, and shared by data brokers, make life harder for individuals.<sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup> The data are also used to construct systems that target and harm individuals. Advances in predictive policing, civilian surveillance, and backlashes against activists are all the outcomes of systems built on data shared by and among companies representing us in categories we cannot appeal<a class="footnote-ref" href="#winston2018"> [winston2018] </a><a class="footnote-ref" href="#feldman2018"> [feldman2018] </a><a class="footnote-ref" href="#waldman2018"> [waldman2018] </a><a class="footnote-ref" href="#levin2018"> [levin2018] </a>.</p>
<p>Data brokers offer an important example of one way of interacting with data. This is an example against which we should compare ourselves when we release data in cultural institutions and use data for digital humanities. We should always ask ourselves, are we better than data brokers?</p>
<h2 id="are-we-better-than-data-brokers">Are We Better Than Data Brokers?</h2>
<p>Data brokers profit from other people’s information. Those described in their datasets often have no way of knowing how they are being represented, and have no way of questioning or correcting this representation. As data becomes more prevalent in cultural institutions, and many of us — through publishing papers and presenting on our work — benefit from data about other people, now is good time to evaluate ourselves in relation to data brokers. This section explores examples of harm done by institutions as they represent individuals and groups.</p>
<p>Identifying specific cases of harm can be difficult. For this reason, it is common to focus on groups that are historically marginalized, and who have reason to be suspicious of their representation in mainstream culture. Anyone who has ever had a reason to fear categorization by a dominant culture can more easily understand the power of data. Many groups have reason to be suspicious. For the purposes of this paper, however, examples will focus on African American representations. This decision is meant to both draw attention to the unique position of the African American community as a marginalized group and to honor the important work of generations of scholars who struggle to educate the dominant culture about these and related dangers.</p>
<p>Are people harmed by the data that we have and share? It is not standard practice for cultural institutions to share social security numbers, credit card numbers, or other sensitive personal information in either physical records, digitized facsimiles, or datasets.<sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup> As such, there is a significant difference between us and the work done by data brokers. Even with standardized practices in place to protect individuals, cultural institutions, historically, have done a form of harm to groups though representation. If we do not take this seriously now, we are likely to compound the problem through our open data.</p>
<p>Writing about the role of the media in enforcing negative representations of African Americans, and thus the media’s culpability in historic lynchings, Sherrilyn Ifill writes, “[t]he failure to report on the ordinariness of the black people&rsquo;s lives … undermined the ability of whites to see their black neighbors, servants, and laborers as human beings” <a class="footnote-ref" href="#ifill2007"> [ifill2007] </a>. She continues:</p>
<blockquote>
<p>Whites could at best ignore the conditions in which most blacks lived and at worst develop a sense that blacks did not lead normal lives in which education, work and family were paramount and central. Instead, blacks could be seen as ‘other,’ ‘different,’ not possessed of the same humanity as whites &hellip; The complicity of ordinary whites, who stood and watched a lynching without interfering, was made possible by the dehumanizing choices the media made in their coverage of blacks.<br>
<a class="footnote-ref" href="#ifill2007"> [ifill2007] </a>And the representations of one group of another, in this case the representation of African Americans by predominantly Caucasian media institutions, continues to affect our society. “The lingering remnants of these dehumanizing portrayals of blacks in the media,” writes Ifill, bringing the issue to the present, “have modern currency,” including over-incarceration of African American males, hyper policing of black communities, and police brutality<a class="footnote-ref" href="#ifill2007"> [ifill2007] </a>. Many of these issues are often ignored by white communities because of a history of media representation of African Americans.</p>
</blockquote>
<p>The representation of one group by another group can range from obvious fiction to pretense of objective truth. “All groups tell stories,” writes David Pilgrim, founding curator of the Jim Crow Museum, “but some groups have the power to impose their stories on others, to label others, stigmatize others, paint others as undesirables, and to have these social labels presented as scientific fact, God’s will or wholesome entertainment” <a class="footnote-ref" href="#pilgrim2017"> [pilgrim2017] </a>. The types of stories matter. It also matters which type of institution is telling the story. “When we watch movies or read novels,” continues Pilgrim, “we know that they are stories; we identify the characters, follow the plot and anticipate the conclusion. But there are other stories that are not so easily identified — sometimes they masquerade as object, race-neutral truth” <a class="footnote-ref" href="#pilgrim2017"> [pilgrim2017] </a>. To illustrate this point, Pilgrim investigates the use of pseudo-science to justify racist beliefs and actions. Scientific institutions were used, during slavery and Jim Crow, to promote and legitimize racism.</p>
<p>In what way are cultural institutions doing the same? The collecting practices of cultural institutions have long been marred by the racial bias of the archivists and curators who build collections. The decisions made about what is collected are colored by the opinions of those doing the collecting and this has tipped the scales on how African Americans are represented in archives. An overt example from the author&rsquo;s own institution is a case in point. In 1945, the LSU Department of Archives and Manuscripts (a pre-curser to the current Special Collections) was offered the opportunity to acquire the collection of African American bibliophile and book collector, Henry P. Slaughter. The quality of the collection was endorsed by archivist Herbert Kellar and book dealer Forest Sweet who wrote, “it is no sense a collection to be filed away - it is rather a collection to be worn out with legitimate use for what it can offer as a basis of study of the negro problem” <a class="footnote-ref" href="#sweet1945"> [sweet1945] </a>.</p>
<p>Despite this endorsement, the University eventually passed on the acquisition on the advice of Archivist and History Professor, Edwin Davis:<sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup>  “the collection has been selected with the plan to emphasize the Negro’s point of view of the race problem. If this is true it is my opinion that the collection will have a considerable amount, perhaps an appreciable percentage, of what might be termed weak material. I have however, no evidence for this opinion. It might prove to be a very valuable collection” <a class="footnote-ref" href="#davis1946"> [davis1946] </a>. Davis, based only on his assumption that any collection that centers an African American perspective has minimal value, declined the collection.<sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup> Rarely does such documentation, but similar decisions are common throughout collecting institutions.</p>
<p>Decisions about what material cultural institutions collect have long term repercussions that are felt for generations. In March 2019, Tamara Lanier filed a lawsuit against Harvard University over ownership of a daguerreotype photograph of an enslaved ancestor named Renty and his daughter, Delia. The photographs were commissioned by Professor Louis Agassiz and utilized as evidence of inferiority of the African American race. The photographs were rediscovered in 1976 hidden away in the attic of a campus museum. Since then the University has loaned the photographs to other museums but also limited the use of the images by researchers due to their “sensitive” nature<a class="footnote-ref" href="#hartocollis2019"> [hartocollis2019] </a><a class="footnote-ref" href="#schuessler2019"> [schuessler2019] </a>.</p>
<p>What is collected matters, and so does its description. The role of the library catalog in reinforcing dominant points-of-view has been explored many times.<sup id="fnref:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup> Melissa Adler, for example, draws attention to the “ways that sections of library classifications were constructed based on ideas about African Americans” <a class="footnote-ref" href="#adler2017"> [adler2017] </a>. Adler traces the creation of the library classifications as they map to racist ideas active in the late 19th and early 20th centuries. The library catalog freezes these ideas in place, Adler claims, and makes them look natural and obvious, similar to the manner of the pseudo-science Pilgrim describes.</p>
<p>The business of gathering, combining, and selling data is not new. However, the scope of surveillance and reach of the systems created with the data is unprecedented due to new tools and methods. In the same way, library catalogs have long represented groups of people in problematic ways. What is different is the new tools and methods we are using to promote the use and reuse of these descriptions and collections. The collections as data framework in cultural institutions carries with it the possibility for our descriptions of people to be shared, combined with other data, and used to negatively affect groups. The ability to exert “control over group and personal identity and memory,” writes Noble, “must become a matter of concern for archivists, librarians, and information workers” <a class="footnote-ref" href="#noble2018"> [noble2018] </a>. This is all the more urgent as we look toward a future of increased share-ability and data-oriented services. Now is the time to ask how we can be better than data brokers.</p>
<h2 id="how-can-we-be-better">How Can We Be Better?</h2>
<p>This section posits three possible directions that are still in the early phases of exploration. These possibilities are listed as sincere efforts to investigate possible steps toward a future in which my work with data feels less grimy. As white librarians, we work in a field that has long struggled to be inclusive to historically marginalized communities.<sup id="fnref:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup> We hope to implement these steps as experiments at our current institution, and to join a community of practitioners exploring these topics in different settings. These experiments are meant to inform the practice of collections as data — as it relates to metadata, digital facsimiles, and structured data — as well as the practice of digital humanities. These approaches are informed by literature on empathy, data science, and critical librarianship.</p>
<h2 id="empathy-and-description">Empathy and Description</h2>
<p>The grimy business of data brokers is legal.<sup id="fnref:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup> While it is important that cultural institutions follow laws when we rethink our collections as data, and when we use this data to build digital humanities projects, this alone will not be enough. For example, the Health Insurance Portability and Accountability Act of 1996 (HIPPA) provides privacy protection for health-related data (Health and Human Services 2015), and, as we saw above, it is established practice in cultural institutions not to share sensitive financial or personal information about people represented in the collection. However this is not enough. A thin legalistic understanding of what should and should not be shared will never be robust enough to ensure we do not harm people we represent. There are no laws, of course, against harmful library subject headers or terminology.</p>
<p>Arguing for a move away from thin legalistic frameworks, Michelle Caswell and Marika Cifor explore the role of feminist ethics in reconceptualizing the role of archives and the people represented in our collections. This approach is applicable for all cultural institutions, collections as data work, and digital humanities projects that use this data. “In a feminist ethics approach,” they write, “archivists are seen as caregivers, bound to records creators, subjects, users, and communities through a web of mutual affective responsibility” <a class="footnote-ref" href="#caswell2016"> [caswell2016] </a>. By caring about the subjects of records — the people represented in the records, who are “counted, classified, studied, enslaved, traded as property, and/or murdered” <a class="footnote-ref" href="#caswell2016"> [caswell2016] </a>— we can make decisions about the records based on our empathy with the people described. Caswell and Cifor write:</p>
<blockquote>
<p>[A] feminist approach guides the archivist to an affective responsibility to empathize with the subjects of the records and, in so doing, to consider their perspectives in making archival decisions. This is in contrast to the dominant West mode of archival practice, in which archivists solely consider the legal rights of records creators … In the feminist approach, the archivist cares about and for and with subjects; she empathizes with them.<br>
<a class="footnote-ref" href="#caswell2016"> [caswell2016] </a>Even in situations where no laws exist to stop the sharing of information, we can ask ourselves: What would the people described in the records think of this project or representation? If the people described in our data could see our project, would we be as eager to work on this project?</p>
</blockquote>
<h2 id="asking-for-help">Asking For Help</h2>
<p>If we would work differently when those represented in our work can see it, how could we ensure that this happens? The particular individuals described in datasets by cultural institutions are likely to be deceased, as is the case for the historic prison records described above. However, we can work with members of affected communities.</p>
<p>Safiya Noble, writing about the shortcomings of Google and other tech companies, calls for the combination of technology and critical studies. “We need people designing technologies for society to have training and an education on the histories of marginalized people, at a minimum,” Nobel writes, “and we need them working alongside people with rigorous training and preparation from the social sciences and humanities” <a class="footnote-ref" href="#noble2018"> [noble2018] </a>. Applying this idea to the cultural institutions creating and releasing data, and to digital humanist using the data, we could bring in experts in African American Studies, for example, if our data represents the African American community, or Women and Gender Studies, if applicable. The expertise from their subject could be brought to bear within the cultural institution creating the data and within the digital humanities group working with the data. In short, we can ask for help.<sup id="fnref:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup></p>
<p>Many cultural institutions are unlikely to be able to create new professional positions specifically for individuals trained in the histories of marginalized communities. However, it is critical that we find ways to pay the scholars whose help we seek. In academic libraries and archives this might take the form of graduate assistantships. Digital humanists will likely be similarly restricted. However, including these roles in project plans and grant applications is one way to normalize the process of asking for help. Money is not the only incentive; this could be a valuable means of exposing scholars to possible careers in cultural institutions and digital humanities. However, monetary compensation is an important part of letting scholars know we take their expertise seriously.</p>
<h2 id="taking-suggestions">Taking Suggestions</h2>
<p>As we have seen, it can be very difficult to know what information data brokers have about us, and to correct what is incorrect.<sup id="fnref:20"><a href="#fn:20" class="footnote-ref" role="doc-noteref">20</a></sup> Is there any reason why cultural institutions cannot do better? We could include feedback channels for the systems we create, and we could make our decisions as transparent as possible. We can bring in experts in the history of marginalized communities, as discussed above. We can also take a step further by ensuring that anyone — those we identify as experts and invite into the process, and those we do not — has a chance to ask us about our decisions and suggest alternate practices. We could ensure that every data project includes feedback options for people to ask questions and make suggestions. After all, we will not be able to invite everyone to work with us, but we could try to include everyone who is affected by our representations.</p>
<h2 id="explaining-ourselves">Explaining Ourselves</h2>
<p>We can also include context in downloadable data. For example, the CSV files that constitute the core of the historic prison data project, described above, have the transcriptions of the admission books. No metadata is included in the download, and no context. Instead of simple files containing structured transcriptions, we could use data packets to bundle contextualizing information together with the transcriptions.<sup id="fnref:21"><a href="#fn:21" class="footnote-ref" role="doc-noteref">21</a></sup> As we release collections as data we could standardize the inclusion of context; as we promote data from cultural heritage institutions, we could normalize the process of incorporating the context supplied within data packages.</p>
<p>Part of the context we could add would be explanations about decisions that we make while creating data. A danger of institutionally-generated collections as data is the perception of objectivity. Devon Mordell, writing about this danger, proposes we frame collections data work in cultural heritage institutions within a new paradigm, a collections-as-data paradigm, that considers both the conceptual and practical concerns related to the use of data in archives<a class="footnote-ref" href="#mordell2019"> [mordell2019] </a>. The benefits of framing a paradigm around collections-as-data, Mordell argues, is to “ensure that a social justice critique is maintained within” the emerging work related to collections as data<a class="footnote-ref" href="#mordell2019"> [mordell2019] </a>. In using archival material as data, there is a risk that the archival holdings and descriptions of the holdings will look objective and natural, and the work of archivists and others to show how archival collections are never neutral and natural will be obscured. Mordell argues that “active participation and critical discourse” around the tools and practices is needed to ensure that new technologies reinscribe a false since neutrality<a class="footnote-ref" href="#mordell2019"> [mordell2019] </a>. Following Mordell, we can include context that encourages critical discourse around our data.</p>
<p>These four directions are presented as a means to begin conversations about practical implementation toward the goal of ensuring that the work we do is better than data brokering. There is still much to learn, and the implementation of any of these proposed solutions will certainly open new possibilities, of both success and failure. Many of the issues we hope to address have taken decades, and sometimes centuries, to create. There will be no quick solution.</p>
<h2 id="conclusion">Conclusion</h2>
<p>The examination of data brokers is meant to offer a warning to those of us working in cultural heritage institutions. However, it is not just one example taken at random. Rather it is a generative lens to view our work because we are always already existing as data in the world of data brokers. The power that data brokers have over us to collect, analyze, describe, and sell our data should bring into sharp focus the power that we have over those with whose data we work.</p>
<p>Thinking about collections as data framework creates an ideal moment for a reflection on the creation and use of data in cultural institutions and digital humanities. This reconceptionalization enables unprecedented access and interaction with collection material in libraries, archives, and museums, i “ncluding but not limited to text mining, data visualization, mapping, image analysis, audio analysis, and network analysis” <a class="footnote-ref" href="#aac2018b"> [aac2018b] </a>.</p>
<p>This is also a moment to consider how we do not want to interact with data. Having a ready example against which to judge our behavior is useful, and data brokers provide a perfect use case. Companies that buy, combine, and resell personal data to the detriment of individuals and groups can be the example we need. Defining ourselves in contrast to data brokers also grants us the opportunity to reflect on the historically problematic aspects of cultural institutions’ descriptive practices.</p>
<ul>
<li id="adler2017">Adler, Melissa. “Classification Along the Color Line: Excavating Racism in the Stacks” . _Journal of Critical Library and Information Studies_ 1, no. 1 (January 29, 2017).<a href="https://doi.org/10.24242/jclis.v1i1.17">https://doi.org/10.24242/jclis.v1i1.17</a>.
</li>
<li id="aac2018a">Always Already Computational - Collections as Data. 2018. “Always Already Computational” . Accessed June 5, 2018.<a href="https://collectionsasdata.github.io/">https://collectionsasdata.github.io/</a>
</li>
<li id="aac2018b">Always Already Computational - Collections as Data. 2018. “The Santa Barbara Statement on Collections as Data Version 2” . Accessed June 5, 2018.<a href="https://collectionsasdata.github.io/statement/">https://collectionsasdata.github.io/statement/</a>
</li>
<li id="aps-git">American Philosophical Society. n.d. “Historic Prison Data” .<a href="https://github.com/AmericanPhilosophicalSociety/Historic-Prison-Data">https://github.com/AmericanPhilosophicalSociety/Historic-Prison-Data</a>
</li>
<li id="aps-data">American Philosophical Society. n.d. “Open Data” . Accessed June 5, 2018.<a href="http://diglib.amphilsoc.org/data">http://diglib.amphilsoc.org/data</a>
</li>
<li id="aps-apps">American Philosophical Society. n.d. “Eastern Apps: Exploring Historic Prison Data” . Accessed June 5, 2018.<a href="https://diglib.amphilsoc.org/labs/eastern-apps/">https://diglib.amphilsoc.org/labs/eastern-apps/</a>
</li>
<li id="carnegie">Carnegie Museum of Art. n.d. “Carnegie Museum of Art’s Collection Dataset” . Accessed June 5, 2018.<a href="https://github.com/cmoa/collection">https://github.com/cmoa/collection</a>
</li>
<li id="caswell2016">Caswell, Michelle and Marika Cifor. 2016. “From Human Rights to Feminist Ethics: Radical Empathy in the Archives” , _Archival Science_ 81 (Spring 2016): 23-43.
</li>
<li id="cbs2018">CBS News. 2018. “The Data Brokers: Selling Your Personal Information” . Accessed June 5, 2018.<a href="https://www.cbsnews.com/news/the-data-brokers-selling-your-personal-information/">https://www.cbsnews.com/news/the-data-brokers-selling-your-personal-information/</a>.
</li>
<li id="confessore2018">Confessore, Nicholas, and Matthew Rosenberg. 2018. “Cambridge Analytica to File for Bankruptcy After Misuse of Facebook Data” . _The New York Times_ , May 3, 2018, sec. U.S.<a href="https://www.nytimes.com/2018/05/02/us/politics/cambridge-analytica-shut-down.html">https://www.nytimes.com/2018/05/02/us/politics/cambridge-analytica-shut-down.html</a>.
</li>
<li id="davis1946">Davis, Edwin. Letter to Guy Lyle. 12 January 1946. LSU Department of Archives and Manuscripts records, A1506, University Archives, LSU Libraries Special Collections, Baton Rouge, La.
</li>
<li id="ftc2014">Federal Trade Commission. 2014. “FTC Report Examines Data Brokers” . Accessed June 5, 2018.<a href="https://www.consumer.ftc.gov/blog/2014/05/ftc-report-examines-data-brokers">https://www.consumer.ftc.gov/blog/2014/05/ftc-report-examines-data-brokers</a>.
</li>
<li id="feldberg1980">Feldberg, Michael. 1980. _The Turbulent Era: Riot & Disorder in Jacksonian America_ . New York: Oxford University Press.
</li>
<li id="feldman2018">Feldman, Noah. 2018. “The Future of Policing Is Being Hashed Out in Secret” . _Bloomberg_ , February 28, 2018.<a href="https://www.bloomberg.com/view/articles/2018-02-28/artificial-intelligence-in-policing-advice-for-new-orleans-and-palantir">https://www.bloomberg.com/view/articles/2018-02-28/artificial-intelligence-in-policing-advice-for-new-orleans-and-palantir</a>.
</li>
<li id="hartocollis2019">Hartocollis, Anemona. “Taking On Harvard Over Rights to Slave Photos” . New York Times, 21 Mar. 2019, p. A1(L). Gale In Context: Biography,<a href="https://link.gale.com/apps/doc/A579490887/BIC?u=lln_alsu&sid=BIC&xid=c62f7942">https://link.gale.com/apps/doc/A579490887/BIC?u=lln_alsu&sid=BIC&xid=c62f7942</a>. Accessed 5 Dec. 2019.
</li>
<li id="hhs2015">Health and Human Services. 2015. “Health Information Privacy” . Accessed June 5, 2018.<a href="https://www.hhs.gov/hipaa/index.html">https://www.hhs.gov/hipaa/index.html</a>
</li>
<li id="ifill2007">Ifill, Sherrilyn A. 2007. _On the Courthouse Lawn: Confronting the Legacy of Lynching in the Twenty-First Century_ . Boston: Beacon Press.
</li>
<li id="imls2016">Institute of Museum and Library Services. 2016. “LG-73-16-0096-16” . Accessed June 6, 2018.<a href="https://www.imls.gov/grants/awarded/lg-73-16-0096-16">https://www.imls.gov/grants/awarded/lg-73-16-0096-16</a>.
</li>
<li id="getty">J. Paul Getty Trust. n.d. “Open Content” . Accessed June 5, 2018.<a href="https://www.getty.edu/about/whatwedo/opencontent.html">https://www.getty.edu/about/whatwedo/opencontent.html</a>.
</li>
<li id="leetaru2018">Leetaru, Kalev. 2018. “The Data Brokers So Powerful Even Facebook Bought Their Data - But They Got Me Wildly Wrong” . _Forbes_ , April 5, 2018.<a href="https://www.forbes.com/sites/kalevleetaru/2018/04/05/the-data-brokers-so-powerful-even-facebook-bought-their-data-but-they-got-me-wildly-wrong/#92a30923107a">https://www.forbes.com/sites/kalevleetaru/2018/04/05/the-data-brokers-so-powerful-even-facebook-bought-their-data-but-they-got-me-wildly-wrong/#92a30923107a</a>.
</li>
<li id="levin2018">Levin, Sam. 2018. “Black Activist Jailed for His Facebook Posts Speaks out about Secret FBI Surveillance” . _The Guardian_ , May 11, 2018, sec. World news.<a href="http://www.theguardian.com/world/2018/may/11/rakem-balogun-interview-black-identity-extremists-fbi-surveillance">http://www.theguardian.com/world/2018/may/11/rakem-balogun-interview-black-identity-extremists-fbi-surveillance</a>.
</li>
<li id="loc2016">Library of Congress. 2016. “Collections as Data 2016” . Accessed June 5, 2018.<a href="http://digitalpreservation.gov/meetings/dcs16.html">http://digitalpreservation.gov/meetings/dcs16.html</a>.
</li>
<li id="loc2017">Library of Congress. 2017. “Collections as Data IMPACT 2017” . Accessed June 5, 2018.<a href="http://digitalpreservation.gov/meetings/asdata/impact.html">http://digitalpreservation.gov/meetings/asdata/impact.html</a>.
</li>
<li id="loc2019">Library of Congress. 2019. “Library Receives $1M Mellon Grant to Experiment with Digital Collections as Big Data” . Accessed 7 Nov. 2019.<a href="https://www.loc.gov/item/prn-19-098/library-receives-1m-mellon-grant-to-experiment-with-digital-collections-as-big-data/2019-10-04/">https://www.loc.gov/item/prn-19-098/library-receives-1m-mellon-grant-to-experiment-with-digital-collections-as-big-data/2019-10-04/</a>.
</li>
<li id="lill">Lill, Jonathan. “The Museum of Modern Art Exhibition Index” . Accessed June 5, 2018.<a href="https://collectionsasdata.github.io/facet12/">https://collectionsasdata.github.io/facet12/</a>.
</li>
<li id="lillehaugen">Lillehaugen, Brook and Michael Zarafonetis. “Ticha: A Digital Text Explorer for Colonial Zapotec” . Accessed June 5, 2018.<a href="https://collectionsasdata.github.io/facet12/">https://collectionsasdata.github.io/facet12/</a>.
</li>
<li id="liedlibrary">Lied Library. “Collections as Data National Forum 2” . Filmed May 7-8 at the University of Nevada Las Vegas. Accessed June 5, 2018.<a href="https://www.youtube.com/watch?v=ENaPV2XmO9I">https://www.youtube.com/watch?v=ENaPV2XmO9I</a>.
</li>
<li id="koster2018">Koster, Lukas and Saskia Woutersen-Windhouwer. “FAIR Principles for Library, Archive and Museum Collections: A Proposal for Standards for Reusable Collections” . _The Code4Lib Journal_ , May 7, 2018.<a href="http://journal.code4lib.org/articles/13427">http://journal.code4lib.org/articles/13427</a>.
</li>
<li id="miller2017">Miller, Caitlyn Renee. 2017. “I Bought a Report on Everything That’s Known About Me Online” . _The Atlantic_ , June 6, 2017.<a href="https://www.theatlantic.com/technology/archive/2017/06/online-data-brokers/529281">https://www.theatlantic.com/technology/archive/2017/06/online-data-brokers/529281</a>/.
</li>
<li id="mordell2019">Mordell, Devon. 2019. “Critical Questions for Archives as (Big) Data” . _Archivaria_ , vol. 87, no. 0, May 2019, pp. 140–61.
</li>
<li id="nls2019">National Library of Scotland. 2019. “Launch of Data Foundry Website” .<a href="https://www.nls.uk/news/archive/2019/09/data-foundry">https://www.nls.uk/news/archive/2019/09/data-foundry</a>. Accessed 7 Nov. 2019.
</li>
<li id="naylor2016">Naylor, Brian. 2016. “The Data Brokers: Selling Your Personal Information” . CBS News, July 11, 2016.<a href="https://www.cbsnews.com/news/the-data-brokers-selling-your-personal-information/">https://www.cbsnews.com/news/the-data-brokers-selling-your-personal-information/</a>.
</li>
<li id="newbury">Newbury, David, and Daniel Fowler. n.d. “Carnegie Museum of Art Collection Data” . Accessed June 5, 2018.<a href="https://collectionsasdata.github.io/facet2/">https://collectionsasdata.github.io/facet2/</a>.
</li>
<li id="noble2018">Noble, Safiya Umoja. 2018. _Algorithms of Oppression: How Search Engines Reinforce Racism_ . New York: New York University Press.
</li>
<li id="oki">Open Knowledge International. n.d. “The Open Definition - Open Definition - Defining Open in Open Data, Open Content and Open Knowledge” . Accessed June 5, 2018.<a href="https://opendefinition.org/">https://opendefinition.org/</a>.
</li>
<li id="padilla2017">Padilla, Thomas. 2017. “On a Collections as Data Imperative” . UC Santa Barbara. Retrieved from<a href="https://escholarship.org/uc/item/9881c8sv">https://escholarship.org/uc/item/9881c8sv</a>.
</li>
<li id="pilgrim2017">Pilgrim, David. 2017. _Watermelons, Nooses, and Straight Razors: Stories from the Jim Crow Museum_ . Oakland, CA: PM Press.
</li>
<li id="schuessler2019">Schuessler, Jennifer. “Portraits Of Slaves Require Moral Lens” . _New York Times_ , 23 Mar. 2019, p. C1(L). Gale In Context: Biography,<a href="https://link.gale.com/apps/doc/A579778374/BIC?u=lln_alsu&sid=BIC&xid=613dc968">https://link.gale.com/apps/doc/A579778374/BIC?u=lln_alsu&sid=BIC&xid=613dc968</a>. Accessed 5 Dec. 2019.
</li>
<li id="sweet1945">Sweet, Forest. Letter to Edwin Davis. 7 January 1945. LSU Department of Archives and Manuscripts Records, #A1506, University Archives, LSU Libraries Special Collections, Baton Rouge, La.
</li>
<li id="upenn">University of Pennsylvania. n.d. “OPenn: Read Me” . Accessed June 5, 2018.<a href="http://openn.library.upenn.edu/ReadMe.html">http://openn.library.upenn.edu/ReadMe.html</a>.
</li>
<li id="waldman2018">Waldman, Peter, Lizette Chapman, and Jordan Robertson. 2018. “Palantir Knows Everything About You” . _Bloomberg_ , April 19, 2018.<a href="https://www.bloomberg.com/features/2018-palantir-peter-thiel/">https://www.bloomberg.com/features/2018-palantir-peter-thiel/</a>.
</li>
<li id="weber2017">Weber, Chela Scott. “Research and Learning Agenda for Archives, Special, and Distinctive Collections and Research Libraries” . OCLC Research, 2017.<a href="https://doi.org/10.25333/c3c34f">https://doi.org/10.25333/c3c34f</a>.
</li>
<li id="winston2018">Winston, Ali. 2018. “A Pioneer in Predictive Policing Is Starting a Troubling New Project” . _The Verge_ , April 26, 2018.<a href="https://www.theverge.com/2018/4/26/17285058/predictive-policing-predpol-pentagon-ai-racial-bias">https://www.theverge.com/2018/4/26/17285058/predictive-policing-predpol-pentagon-ai-racial-bias</a>.
</li>
<li id="zarafonetis">Zarafonetis, Michael and Sarah M. Horowitz. “Beyond Penn’s Treaty” . Accessed May 5, 2018.<a href="https://collectionsasdata.github.io/facet11/">https://collectionsasdata.github.io/facet11/</a>.
</li>
<li id="ziegler">Ziegler, Scott. “American Philosophical Society Open Data Projects” . Accessed June 5, 2018.<a href="https://collectionsasdata.github.io/facet4/">https://collectionsasdata.github.io/facet4/</a>.
</li>
<li id="ziegler2019">Ziegler, S. L. and Steve Marti. 2019. “A Hidden Gem Become a Fertile Mining Ground: Historic Prison Admission Books and Data-Driven Digital Projects” . _The Pennsylvania Magazine of History and Biography_ . Vol. 143: 3. Pp. 363-373.
</li>
<li id="ziegler2016">Ziegler, Scott and Michelle Ziogas. 2016. “Eastern State Penitentiary Admission Book B” , - . 21. Philadelphia, PA: McNeil Center for Early American Studies [distributor], 2016.<a href="https://repository.upenn.edu">https://repository.upenn.edu</a>
</li>
</ul>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Cultural institutions, for the purpose of this paper, are libraries, archives and museums. These institutions collect, describe, and make available material of cultural significance such as works of art, historical documents, and published material.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Because the phrase collections as data is used to refer to both the conceptual practice of rethinking the role of collection material and the team active in the IMLS grant, the name AAC will be used for the latter.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>The author is thankful to Thomas Padilla for reviewing an early draft of this section for accuracy; any remaining errors or misrepresentations are the fault of the author.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>“A facet documents a collections as data implementation. An implementation consists of the people, services, practices, technologies, and infrastructure that aim to encourage computational use of cultural heritage collections.” see “A Release and a Call - Collections as Data Facets” . Always Already Computational - Collections as Data. Accessed May 17, 2018.<a href="https://collectionsasdata.github.io/facets/">https://collectionsasdata.github.io/facets/</a>.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>Historically important for many reasons, the Eastern State Penitentiary championed the standardized use ofcellular isolationin which each inmate spends all of his or her time in a cell by themselves. For more information, see the APS finding aid:<a href="https://search.amphilsoc.org/collections/view?docId=ead/Mss.365.P381p-ead.xml">https://search.amphilsoc.org/collections/view?docId=ead/Mss.365.P381p-ead.xml</a>, and the website of the Eastern State Historic Site:<a href="https://www.easternstate.org">https://www.easternstate.org</a>.&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>The digitization team for this project included Grace DiAgostino and Bayard Miller.&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>On the uses of digitized art as data, see: “The Next Rembrandt” . The Next Rembrandt. Accessed May 29, 2018.<a href="https://www.nextrembrandt.com">https://www.nextrembrandt.com</a>. Hristova, Stefka. “Images as Data: Cultural Analytics and Aby Warburg’s Mnemosyne” . <em>International Journal for Digital Art History</em> 0, no. 2 (October 18, 2016). <a href="https://doi.org/10.11588/dah.2016.2.23489">https://doi.org/10.11588/dah.2016.2.23489</a>.&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>The transcription team consisted of Michelle Ziogas, Bayard Miller, and Kristina Frey&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>See, for example, the “Conventions Used” section of the Github Read Me file. “Historic Prison Data” American Philosophical Society Github Repository,<a href="https://github.com/AmericanPhilosophicalSociety/Historic-Prison-Data#conventions-used">https://github.com/AmericanPhilosophicalSociety/Historic-Prison-Data#conventions-used</a>&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>For concerns about relying on transcriptions, see James H. Merrell, “ Exactly as they appear : Another Look at the Notes of a 1766 Treason Trial in Poughkeepsie, New York, with Some Musings on the Documentary Foundations of Early American History” , <em>Early American Studies</em> 12, no. 1 (Winter 2014): 202-237. See also Jacqueline Wernimont’s thoughts on the role of tabular data and the loss of understanding:<a href="https://digital-frontiers.org/conference/2017/texas-digital-library-closing-keynote-address-counting-dead-quantum-media-and-how-we">https://digital-frontiers.org/conference/2017/texas-digital-library-closing-keynote-address-counting-dead-quantum-media-and-how-we</a>, as well as Foreman, P. Gabrielle, and Labanya Mookerjee, “Computing in the Dark: Spreadsheets, Data Collection and DH’s Racist Inheritance” in <em>Always Already Computational Position Statements</em> ,<a href="https://collectionsasdata.github.io/resources/">https://collectionsasdata.github.io/resources/</a>, accessed 5/18/2018&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>A growing selection of open data sets can be found on the APS Library Open Data page, “Open Data | APS Digital Library” . Accessed June 5, 2018.<a href="http://diglib.amphilsoc.org/data">http://diglib.amphilsoc.org/data</a>.&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p>Quantifying the harm is difficult due to the secretive nature of data transfer and use. The United Kingdom-based non-profit Privacy International is one of many drawing attention to this harm. “Video: What Is Data Exploitation?” Privacy International. Accessed May 29, 2018.<a href="http://privacyinternational.org/video/1626/video-what-data-exploitation">http://privacyinternational.org/video/1626/video-what-data-exploitation</a>.&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13">
<p>See, for example, “Questions and Answers on Privacy and Confidentiality” . <em>Text. Advocacy, Legislation &amp; Issues</em> , May 29, 2007.<a href="http://www.ala.org/advocacy/privacy/FAQ">http://www.ala.org/advocacy/privacy/FAQ</a>. Software for digital preservation systems, such as ePadd for email, are also built with these issues in mind, see “EPADD User Guide 5.0” . Google Docs, May 2017.<a href="https://docs.google.com/document/d/1ZMuWU0z-IVsk80_lUEYMfVrwfCsS1bp0sjL28GBGcMU">https://docs.google.com/document/d/1ZMuWU0z-IVsk80_lUEYMfVrwfCsS1bp0sjL28GBGcMU</a>&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14">
<p>Herbert A. Kellar was the Director of the McCormick Historical Association and a founding member of the Society of American Archivists; Edwin Davis was the first archivist hired to manage the LSU Department of Archives and Manuscripts&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:15">
<p>The collection is currently housed at the Atlanta University Center Robert W. Woodruff Library Archives Research Center. See:<a href="http://findingaids.auctr.edu/repositories/2/resources/62">http://findingaids.auctr.edu/repositories/2/resources/62</a>. The author wishes to thank Jenny Mitchell, Head of Manuscript Processing at LSU Libraries, for this example.&#160;<a href="#fnref:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:16">
<p>See, for example, Berman, Sanford.Prejudices and Antipathies : A Tract on the LC Subject Heads Concerning People. McFarland &amp; Co., 1993. Olson, Hope. “Mapping Beyond Dewey’s Boundaries: Constructing Classificatory Space for’ Marginalized Knowledge Domains” . <em>Library Trends</em> 47, no. 2 (Fall 1998): 233–54. Berman, Sanford. <em>Prejudices and Antipathies: A Tract on the LC Subject Heads Concerning People</em> . Adler, Melissa. “Classification Along the Color Line: Excavating Racism in the Stacks” . <em>Journal of Critical Library and Information Studies</em> 1, no. 1 (January 29, 2017). <a href="https://doi.org/10.24242/jclis.v1i1.17">https://doi.org/10.24242/jclis.v1i1.17</a>. “View of Engaging an Author in a Critical Reading of Subject Headings” . Accessed May 22, 2018.<a href="http://libraryjuicepress.com/journals/index.php/jclis/article/view/20/12">http://libraryjuicepress.com/journals/index.php/jclis/article/view/20/12</a>.&#160;<a href="#fnref:16" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:17">
<p>See, by way of introduction, Hudson, David James. “On Diversity as Anti-Racism in Library and Information Studies: A Critique” . <em>Journal of Critical Library and Information Studies</em> 1, no.1 (2017). DOI: 10.24242/jclis.v1i1.6. jesus, nina de. “Locating the Library in Institutional Oppression – In the Library with the Lead Pipe” . Accessed May 30, 2018. /2014/locating-the-library-in-institutional-oppression/. Galvan, Angela. “Soliciting Performance, Hiding Bias: Whiteness and Librarianship – In the Library with the Lead Pipe” . Accessed May 30, 2018. /2015/soliciting-performance-hiding-bias-whiteness-and-librarianship/.&#160;<a href="#fnref:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:18">
<p>At the time of this writing, the European Union’s General Data Protection Regulations (GDPR) is just coming into effect, and while companies large and small that collect data are adjusting some behaviors the outcome on the core practice of data brokerage in the United States is yet to be determined. Ong, Thuy. “Microsoft Expands Data Privacy Tools Ahead of GDPR” . The Verge, May 24, 2018.<a href="https://www.theverge.com/2018/5/24/17388206/microsoft-expand-data-privacy-tools-gdpr-eu">https://www.theverge.com/2018/5/24/17388206/microsoft-expand-data-privacy-tools-gdpr-eu</a>. Tiku, Nitasha. “How Europe’s New Privacy Law Will Change the Web, and More” . WIRED, March 19, 2018.<a href="https://www.wired.com/story/europes-new-privacy-law-will-change-the-web-and-more/">https://www.wired.com/story/europes-new-privacy-law-will-change-the-web-and-more/</a>.&#160;<a href="#fnref:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:19">
<p>It’s worth noting here that this can easily go awry. There’s a history of white people asking black people to explain why racism. For example, soon after the 2016 presidential election,Slate held a forum for African American writers to reflect on an increased demand of this type.Bouie, Jamelle, Gene Demby, Aisha Harris, Tressie McMillan Cottom, and Chau Tu. “I’m Not Your Racial Confessor” . <em>Slate</em> , December 6, 2016.<a href="http://www.slate.com/articles/news_and_politics/politics/2016/12/the_black_person_s_burden_of_managing_white_emotions_in_the_age_of_trump.html">http://www.slate.com/articles/news_and_politics/politics/2016/12/the_black_person_s_burden_of_managing_white_emotions_in_the_age_of_trump.html</a>. See also: Johnson, Theodore R. “How Black Writers Can Help White Readers” . <em>The New Republic</em> , December 29, 2016.<a href="https://newrepublic.com/article/139541/black-writers-can-help-white-readers">https://newrepublic.com/article/139541/black-writers-can-help-white-readers</a>.&#160;<a href="#fnref:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:20">
<p>Even attempts at transparency by data brokers often prove to be misleading. See: Insider, Business. “The Site That Shows You All The Personal Data Advertisers Have On You Isn’t Entirely Accurate” . <em>Business Insider</em> . Accessed May 25, 2018.<a href="http://www.businessinsider.com/acxiom-about-the-data-problems-2013-9">http://www.businessinsider.com/acxiom-about-the-data-problems-2013-9</a>. Singer, Natasha. “Acxiom Lets Consumers See Data It Collects” . <em>The New York Times</em> , September 4, 2013, sec. Technology.<a href="https://www.nytimes.com/2013/09/05/technology/acxiom-lets-consumers-see-data-it-collects.html">https://www.nytimes.com/2013/09/05/technology/acxiom-lets-consumers-see-data-it-collects.html</a>. Breen, Bant. “AboutTheData: Data Collection Gone Wrong” . <em>Huffington Post</em> (blog), September 26, 2013.<a href="https://www.huffingtonpost.com/bant-breen/aboutthedata-data-collect_b_3998252.html">https://www.huffingtonpost.com/bant-breen/aboutthedata-data-collect_b_3998252.html</a>.&#160;<a href="#fnref:20" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:21">
<p>See, for example, Walsh, Paul, and Rufus Pollock. “Data Package” . <em>Frictionless Data</em> . Accessed May 29, 2018.<a href="https://frictionlessdata.io/specs/data-package/">https://frictionlessdata.io/specs/data-package/</a>. For a discussion on using data packets in cultural institutions, Averkamp, Shawn. “Data Packaging Guide” , May 14, 2018.<a href="https://github.com/saverkamp/beyond-open-data">https://github.com/saverkamp/beyond-open-data</a>.## Bibliography&#160;<a href="#fnref:21" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry><entry><title type="html">Reading Chicago Reading: Quantitative Analysis of a Repeating Literary Program</title><link href="https://rlskoeser.github.io/dhqwords/vol/14/2/000461/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://rlskoeser.github.io/dhqwords/vol/14/2/000461/</id><author><name>John Shanahan</name></author><author><name>Robin Burke</name></author><author><name>Ana Lučić</name></author><published>2020-06-19T00:00:00+00:00</published><updated>2023-08-06T15:53:44-04:00</updated><content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>In <em>The Library Beyond the Book</em> , Jeffrey T. Schnapp and Matthew Battles note the variety of data generated in contemporary libraries this way: “Every time a book is taken off the shelf, a file is downloaded, or a computer workstation is booted up, a story is told, and cataloged, and filed away in a database. In this way, each act of reading in the library broadcasts a handful of seeds, from which new growths of data will either spring — or disappear into a forest of statistical noise” <a class="footnote-ref" href="#schnapp2014"> [schnapp2014] </a>. Schnapp and Battles highlight how public libraries, like many institutions, are data-rich but information-poor; they remind us that much of the content in library databases is left fallow from lack of tools or budget or both; in turn, they challenge us to imagine new architectures and algorithms for libraries when, in the age of search, we find both literally and figuratively that “there is no shelf” <a class="footnote-ref" href="#shirky2005"> [shirky2005] </a>.<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></p>
<p>What follows is a report on one attempt to describe, and remedy, some aspects of this condition. This essay is about one of the largest and longest running mass reading programs of the past two decades, “One Book One Chicago” (OBOC), sponsored by the 80-branch Chicago Public Library (CPL) system since the fall of 2001. Our project works with anonymized CPL circulation data from this program and data of several other types to ask with tools of data science, can we capture and model some of the salient relationships of texts, readers, and their environment, particularly when a city assigns itself mass-mediated public interactions with literary works? The Chicago Public Library’s goals with OBOC are civic as well as cultural, and capturing the effects of the program at metropolitan scale might even show how DH tools and methods might inform social policy. “Can cities save the world?” political scientist Benjamin R. Barber asked (and answered affirmatively) in his 2013 study of city-scale planning experiments, <em>If Mayors Ruled the World</em> . Because of their more manageable scale and integrated governance structures, Barber argues that in both developed and developing nations cities have become “democracy&rsquo;s best hope” <a class="footnote-ref" href="#barber2013"> [barber2013] </a>. OBOC is a complex city-scale cultural phenomenon and study of it offers a means of capturing and quantifying how broad-spectrum civic programs have differential impact upon a heterogeneous population.</p>
<p>A major goal of the Reading Chicago Reading project (RCR) is to create open source tools so that public librarians and digital humanists can experiment with the kinds of predictive insight presently being generated by proprietary software in the film and music industries and for mass market book sales<a class="footnote-ref" href="#hss"> [hss] </a><a class="footnote-ref" href="#archer2016"> [archer2016] </a><a class="footnote-ref" href="#alharthi2018"> [alharthi2018] </a><a class="footnote-ref" href="#piper2016"> [piper2016] </a><a class="footnote-ref" href="#alter2016"> [alter2016] </a>.<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> We have created the first instance of such a predictive model, and in what follows we will report about how it works. We also discuss findings and limitations of our data and methods to date. The first half of the essay explains why CPL’s OBOC program is notable in itself and conducive for modeling purposes, and the second half documents creation of our models, their underlying data, and the results. To anticipate a bit, we chose Chicago Public Library’s “One Book One Chicago” program because its prior seasons constitute useful training data and its current and future programs (i.e. different book selections) provide new data for the model which can be tested for the same areas in the same city. Our model predicts the annual circulation of a chosen book for every CPL branch; it is, as far as we know, the first attempt to combine tools of literary sociology and data science for city-scale prediction of this kind. Matthew Jockers noted with regret just a few years ago that “[t]he conclusions we reach as literary scholars are rarely testable in the way that scientific conclusions are testable. And the conclusions we reach as literary scholars are rarely repeatable in the way that scientific experiments are repeatable” <a class="footnote-ref" href="#jockers2013"> [jockers2013] </a>. But the Reading Chicago Reading project captures the repeating traces of literary readership across a city by means of public library data and several other forms of information. In this way, our project offers an opportunity to formulate repeatable, testable, hypotheses about real-world reading culture at city-scale.</p>
<p>The Reading Chicago Reading project is also creating a growing archive about a notable large-scale program of elective reading and related social media, reminding us at the same time of the importance of documenting internet-mediated cultural programs. The stakes are high: if born-digital and mixed digital/analog phenomena such as the “One Book One Chicago” program are not captured in a timely manner they are likely to pass beyond any practical means of recovery by future researchers. Since reconstruction of internet-mediated culture requires hardware and software beyond the means of all but the most attentive and well-resourced archivers, “[t]he first few decades of the online revolution are already set to be a dark ages of sorts” <a class="footnote-ref" href="#weber2016"> [weber2016] </a>. This essay is one of several forthcoming from project researchers and presents initial results of our practice of combining multiple forms of investigation — time-series analysis, social media analytics, location extraction from texts and maps, sentiment analysis, and text measures — to analyze an ongoing program in one of the United States’ largest public library systems. While “One Book One Chicago” forms an opportune starting point, we expect that our models and techniques can be applied to library holdings generally.<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup></p>
<p>And finally, as a pioneeringcity of big data, Chicago offers particular advantages for the study of mass-mediated uptake of cultural forms. A rich history of quantitative sociological work on its highly-stratified urban fabric has long made Chicago “an excellent laboratory for testing theoretically-derived hypotheses” <a class="footnote-ref" href="#sampson2012"> [sampson2012] </a>.<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup> Our study of some CPL book checkout data is, admittedly, small by the standards of Robert J. Sampson’s Project on Human Development in Chicago Neighborhoods (PHCDN), but it is informed by a similar desire to understand how individual actions and perceptions are embedded in “neighborhood effects” that structure people’s understanding of their place and possibilities in the city and the world<a class="footnote-ref" href="#sampson2012"> [sampson2012] </a>.</p>
<h2 id="research-questions">Research questions</h2>
<p>The “Reading Chicago Reading” project is animated by a number of research questions:</p>
<ul>
<li>
<p><em>When a large public library system sponsors a city-wide collective reading event — and makes the book selection part of a larger ensemble of multimedia programming — what can we learn about the different processes that shape cultural perception across a metropolitan region?</em></p>
</li>
<li>
<p><em>How does one live in a city differently after reading about that same city, and perhaps in association with others?</em> Cultural programming often serves as a form of imagined community, and we can use network effects to understand these communities in the city.</p>
</li>
<li>
<p><em>What are the measurable effects of sponsored book culture where spillover effects of library programming (neighborhood to neighborhood, branch to branch, and online) may be identified?</em> Because in addition to reading and group discussion, Chicago’s “One Book” programming has also featured, for example, guided city tours,makerevents, film screenings, dances, and community gardening, novel forms of belonging and citizenship are presumably created by means of recommended reading.</p>
</li>
<li>
<p>And importantly, <em>how does the above vary by neighborhood, and library branch, in a city so identified by neighborhoods?</em></p>
</li>
</ul>
<p>While we cannot in a single paper answer all of these questions, given our circulation data from CPL, census and other publicly-accessible city-wide data (demographics in this study, but police reports, shared bike usage, FOIA requests, and the like are also possible), social media data (gathered via API), and quantitative measures from selected books, it is possible by bundling them to document changes in Chicago’s attention economy, and we will do so in part in what follows. Our project’s aim is to capture and, to a degree quantify, how Chicago’s public script about itself changed during year-long library programming around books on the theme of, for example,Music: The Beat of the City(the 2017-18 OBOC season) orFood(the 2016-17 season). Can the cultural pulse of a large city be captured quantitatively and modeled by way of public library circulation data? Our project proceeds from the premise that it can.</p>
<h2 id="project-description">Project description</h2>
<p>The “Reading Chicago Reading” project began in discussions about the possibility of capturing and predicting library-sponsored readership across Chicago by means of the OBOC program. It was apparent to the project’s founders that the program resembled a repeating and repeatable experiment – that is, each chosen OBOC book season represented a data probe into library usage and, by extension, a window onto the elective reading behavior of the patrons of a major American library system. The project’s motivating hypothesis was that book checkouts per branch, combined with library branch demographics, promotional activities, and chosen text characteristics would constitute variables that could be used to predict patron response to future OBOC books, and one of our key tasks has been to encode these variables into a predictive model and use the modeling process to discover relationships between them. Aone bookprogram is a repeating mixture of personal and collective experience with some slow-changing variables (e.g. number of library branches and general neighborhood demographics) as well as periodically-changing input such as text features of the annual book selection (e.g. reading difficulty and total word count) and publicity around the book, all of which drives patron interest and potentially library branch checkout numbers.</p>
<p>The “One Book One Chicago” (OBOC) program is also a useful optic for observing how readers engage with texts in different ways across quite variegated social space. Reading of course happens in a number of modes, from silent reading of printed books in armchairs (a diminishing practice, we suspect) to partially-solitary reading via electronic devices on public transit or in coffee shops, to fully public reading and associatedengagementvia blogging, posting on Goodreads and Amazon, and participation in book clubs. People who read also like to talk about what and how and why they read; they document books both good and bad, next choices, reflections, recommendations, and the like.<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup> And heterogeneous social spaces bleed over into complex media spaces. For example, uptake of narrative through audio rather than sight is a rapidly growing phenomenon, and the explosion of transmedia fanfiction and other forms ofpost-pressliterature extend the reading and writing process ever farther.<sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>  “Texts,” Matthew Kirschenbaum notes, “are increasingly networks of transmedia properties. … [T]he bigger the book, the more extended its network of transmedia relations becomes” <a class="footnote-ref" href="#kirschenbaum2010"> [kirschenbaum2010] </a>.<sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup> As reading interfaces continue their shift from documents to performances (as Lev Manovich has nicely summarized it), those of us who seek to understand reading in the present and future will benefit from the tools of data science<a class="footnote-ref" href="#manovich2013"> [manovich2013] </a>. Social media platforms in particular facilitate new kinds of capture and visualization of cultural phenomena at the scale of the city, and we have been inspired by many exciting projects and studies.<sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup></p>
<h2 id="the-book-is-just-the-beginning">The Book is Just the Beginning</h2>
<p>Community-based mass reading events like “One Book One Chicago” have exploded in popularity around the United States so quickly that we ought to take a moment to grasp the implications of the mainstreaming of such programs. Book clubs have existed for decades, of course, with roots in the United States reaching back to the first circulating libraries and to public lecture circuits.<sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup> The contemporary mass-mediated book club, however, marked its public success with Oprah Winfrey’s TV “Book Club” which began in the fall of 1996 and ran for fifteen years.<sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup> Book club culture has been the subject of many studies, each noting the different forms of intellectual and social bonds created, and sometimes challenged, when people meet to talk about fiction and nonfiction with others like and unlike themselves.<sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup></p>
<p>Study of a large contemporary metropolitanone bookprogram is useful given growing attention to the scope and effects of social media filter bubbles from micro-targeted advertising and ad-supported news<a class="footnote-ref" href="#pariser2011"> [pariser2011] </a><a class="footnote-ref" href="#helmond2013"> [helmond2013] </a><a class="footnote-ref" href="#tufekci2017"> [tufekci2017] </a><a class="footnote-ref" href="#singer2018"> [singer2018] </a><a class="footnote-ref" href="#mina2019"> [mina2019] </a>.<sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup> In asking residents to read and discuss a literary work as a public, a book program seeks to create a virtual, temporary, community for mass, but not similar, responses among people who might not otherwise become entangled together in real (or virtual) life. Many CPL patrons might read Saul Bellow’s <em>Adventures of Augie March</em> (fall 2011 OBOC season) or Jane Austen’s <em>Pride and Prejudice</em> (spring 2005 OBOC season); perhaps a notable fraction of the city’s population will take up the selected work in some manner over the city’s season. But the result of this programming is not, and indeed cannot be, the diffusion of a single opinion or the creation of consensus as if participants were linked by a newsfeed or social media account pointed to the same content. A city-scale book club, this is to say, fosters a different brand of imagined community: a sponsored, and collective, experience moving together in shared time but in the service not of homogeneity of intellectual outcome but rather an improvisatory and unpredictable detachment from parochialism. In this way, the community fostered by a library system’s one book program both preserves, and departs from, what Anderson described as the normative uses of print capitalism<a class="footnote-ref" href="#anderson1991"> [anderson1991] </a>.</p>
<p>Mass reading events like OBOC are also one more component of a broad trend since the early 1990s in which civic leaders commission iconic buildings, programs, and public artworks to signal status as aglobal cityto local residents and to the world.<sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup> Community book clubs are open-ended in format and have ranged from print-only to all-digital; to date, they have focused largely on literary fiction and nonfiction, but there are other possible reading assignments that might not even be reading in a traditional sense — witness “Open Data Book Clubs” in Canada, for example, for which city data sets are assigned monthly; still other kinds of collective library experience are indeed possible.<sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup> Perhaps not surprisingly, literary fiction has formed the bulk of reading selections for programs such as OBOC. Literary fiction was the standard against which other forms of reading was measured in several NEA studies since the early 2000s<a class="footnote-ref" href="#nea2004"> [nea2004] </a><a class="footnote-ref" href="#nea2007"> [nea2007] </a><a class="footnote-ref" href="#nea2009"> [nea2009] </a><a class="footnote-ref" href="#nea2017"> [nea2017] </a>. A much-cited 2013 article in <em>Science</em> found that literary fiction, with its requirement of attentive engagement with subjective states, unpredictable events, and complex characters, revealed statistically significant benefits in measures of “theory of mind” (ToM), arguing that because “readers take an active writerly role to form representations of characters’ subjective states, literary fiction recruits ToM” <a class="footnote-ref" href="#kidd2013"> [kidd2013] </a>. In addition, literary fiction requires readers “to expand our knowledge of others’ lives, helping us recognize our similarity to them” <a class="footnote-ref" href="#kidd2013"> [kidd2013] </a>.<sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup> The predominance of literary fiction in One Book One Community programs around the U.S. bears this out. Helpfully, the Library of Congress has documented hundreds of programs and their book choices across the U.S. as part of its “Big Read” initiative. While the data is incomplete in at least one major case (stopping at the 2007 season for Chicago’s OBOC, for example), it lists over 2000 community programs and just over 860 book choices stretching from Seattle in 1998 to Santa Monica in 2016. The most popular book choices include predictable popular titles such as <em>To Kill a Mockingbird</em> (90 times), Bradbury’s <em>Fahrenheit 451</em> (50 times), Hosseini’s <em>The Kite Runner</em> (48 times), and Homer Hickam’s <em>Rocket Boys</em> (38 times). A second tier of popular choices includes canonical classics such as <em>Moby Dick</em> , <em>Pride and Prejudice</em> , <em>A Doll’s House</em> , <em>A Long Day’s Journey into Night</em> , and <em>Frankenstein</em> , but also a great deal of middlebrow fiction and nonfiction by the likes of Barbara Kingsolver, Mitch Albom, Mark Haddon, Eric Schlosser, and Barbara Ehrenreich. There is a marked presence of genre fiction, particularly science fiction and thrillers; some unexpected choices might include Loudon County Virginia’s selection of Nikki Giovanni’s poetry (2010), Richard Dawkin’s <em>The Selfish Gene</em> (by Kansas City, MO in 2008), and Hannah Crafts’ <em>The Bondwoman’s Narrative</em> (Indianapolis in 2004).<sup id="fnref:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup></p>
<p>We should also note, however, that the forms of difference readers may encounter in the OBOC’s selected books’ represented worlds — social, ethnic, racial, sexual, and the like — do not of course guarantee additional or positive encounters with such difference in life. Chicago is a highly segregated city, as it has been for generations, and this is reflected in the hard facts of branch-area demography around the city. We are also aware of the notable fact, quantified in Robert J. Sampson’s PHDCN research, that Chicago neighborhoods with a higher density of community organizations tend to maintain that density over time, in effect enabling residents in some parts of the city to wield outsize influence in cultural impact (though spillover effects into contiguous neighborhoods were also notable) [see<a href="#sampson2012">Sampson 2012, 179-233</a>]. And reading for pleasure has a history; it is not a practice evenly distributed across spaces and publics. As Elizabeth Long noted in her study of book clubs in Houston, Texas, “American popular culture marks leisure reading by both class and gender” <a class="footnote-ref" href="#long2003"> [long2003] </a>. In a number of important studies, Wendy Griswold has shown how the U.S. has been fragmenting into a differentially empowered “reading class” and other, less print-centered, parts of the population who are, paradoxically, literate but not readers<a class="footnote-ref" href="#griswold2008"> [griswold2008] </a><a class="footnote-ref" href="#griswold2014"> [griswold2014] </a><a class="footnote-ref" href="#griswold2015"> [griswold2015] </a>.</p>
<h2 id="why-chicago">Why Chicago?</h2>
<p>The origin of the city-wideone bookprogram appears to be in 1998, when librarians Nancy Pearl and Chris Higashi at the Seattle Public Library sought ways harness the collective energy of the many small book clubs scattered around the city. They formulated their initial program as a question —What if all Seattle read the same book?— and chose Russell Banks’ <em>The Sweet Hereafter</em> as the object of citywide discussion groups in libraries and private homes. Pearl and Higashi reported the idea at a meeting of the American Library Association, and by 2002 the ALA distributedhow topackets for library systems willing to try the new idea. Nan Alleman at the Chicago Public Library read a <em>Chicago Tribune</em> article about Seattle’s program and quickly launched “One Book One Chicago” in the fall of 2001 with the city assigned to read together Harper Lee’s novel <em>To Kill A Mockingbird</em> . CPL branches stocked hundreds of additional copies of the book and discussions were scheduled throughout the fall in branches across the city. The Chicago Public Library received extended press coverage that enabled the idea to scale quickly.<sup id="fnref:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup></p>
<p>Chicago city government was immediately committed to the program. Initiated with fanfare by Mayor Richard M. Daley in the fall of 2001, and evolving in concert with other major public projects like Millenium Park (completed in 2004), the OBOC program is now a central node in cultural programming in Chicago. In the wake of the first season, Mayor Richard M. Daley called Chicago’s public library system a “community anchor” and the “heartbeat” of its neighborhoods<a class="footnote-ref" href="#putnam2003"> [putnam2003] </a>. Daley’s comments in the program guide for Willa Cather&rsquo;s <em>My Antonia</em> (the OBOC choice for fall 2002) announced lofty civic goals: “One Book, One Chicago cultivates a culture of reading and discussion by bringing our diverse city together around one great book. Reading great literature inspires us to think about ourselves, our environment and our relationships. Talking about great literature with friends, family and neighbors can add richness and depth to the experience of reading.” OBOC seasons created a dramatic impact in the city. In 2001-2, Harper Lee’s <em>To Kill a Mockingbird</em>  “was checked out of branch libraries more than eight thousand times over the course of a few months. Bookstores sold thousands more copies — <em>To Kill a Mockingbird</em> was on the Barnes &amp; Noble top ten list for two months” <a class="footnote-ref" href="#putnam2003"> [putnam2003] </a>.</p>
<p>The program seemed to fulfill its mission of bringing new engagement into metropolitan cultural events in the aftermath of the 9/11/01 terrorist attacks and the inauguration of new conversations about citizenship and cultural difference. Taking part in a shared narrative took on new meaning and amplified a sense of collective purpose. When asked about the “One Book” program, a 15-year-old African-American student in Chicago told interviewers “[i]t makes me feel good to be part of a city that is all reading the same book” <a class="footnote-ref" href="#fuller2013"> [fuller2013] </a>. Chicago’s program has been successful close to twenty years now and continues, at the time of writing, with Elizabeth Kolbert’s nonfiction book <em>The Sixth Extinction</em> and a city-wide theme ofClimate Change.One Book programs have many benefits and few liabilities for all involved: book publishers sell more books (since not all city residents will wait to check it out from a branch library); public libraries receive additional foot traffic and community relevance; citizens become active in a civic experience with cultural capital. In a publicity statement for the 2016-17 program centered on Barbara Kingsolver’s <em>Animal Vegetable Miracle: A Year of Food Life</em> , Mayor Rahm Emanuel and CPL Commissioner Brian Bannon noted that the choice was intended as a catalyst for city-wide conversations about food, politics, heritage, and the environment. “From October through May 2017, One Book, One Chicago will explore a central theme — Eat Think Grow — with citywide programming focused on cultural cuisine, cooking, eating, sustainability and urban farming. From seed, to grocery, to cookbook, to table — we’re discussing all the ways we relate to and celebrate food. … Branch libraries host highlighted programs such as culinary walking tours, urban gardening discussions, and food talks about beer, bees, coffee and the Slow Food Chicago movement.”</p>
<p>As elective literary reading and related events, OBOC participation is cast as a means toward better historical and personal insight into the lives of others. The ideal of a program like OBOC is collective practices of learning and world-widening through mediated discussion of that reading as well as participation in live events. The OBOC’s annual repetition makes it resemble a city art biennial, and its overall experience is not unlike a fusion of book clubs, continuing ed classes, social media updates, and TED talks. People participate in a number of ways, not all centered on reading. In fact, the CPL slogan that “the book is just the beginning” is apt. OBOC programming itself has changed over the past 15-plus years, shifting from three-month programs centered on print book discussionclubsat physical CPL branches to nine-month seasons for which the assigned book is just one element, a teaser orloss leaderas it were, for other civic initiatives, many of them digital. Figure 1 shows just some of the dozens of events around the city created in association with the 2018/19 OBOC book choice, Philip K. Dick’s <em>Do Androids Dream of Electric Sheep?</em> and season themeImagine the Future.</p>




























<figure ><img loading="lazy" alt="" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Screen capture of a segment of CPL&rsquo;s OBOC programming. Accessed 3 Jan. 2019.
        </p>
    </figcaption>
</figure>
<p>The variety of activities and media shown in Figure 1 also reveals why a large city-wideOne Bookprogram requires dedicated sponsors and culture workers. In <em>Making Literature Now</em> , Amy Hungerford calls for attention to the non-profit organization employees and public library staff who plan and implement experiences with contemporary literature. These “evangelists of culture” , as<a class="footnote-ref" href="#griswold2015"> [griswold2015] </a>describe them, include Jennifer Lizak, CPL’s OBOC director, and around her the wider set of Chicago Public Library community outreach staff who create and facilitate programming across the city. Hungerford writes that these “ neglected agents of cultural formation not only play a crucial role in the cultural field but also constitute a set of actors for whom literary or artistic production matters beyond the moment of ordinary consumption” <a class="footnote-ref" href="#hungerford2016"> [hungerford2016] </a>. Culture brokers play a shaping role in the formation of the imagined community of a One Book program; but their choices of texts, as Griswold and Wohl show, tend to be based on hunches about demographic representativity (of authors, of branch behavior, of book topics) and reports about past book group success elsewhere. Tools of data science have not been an option. The decision-making process to make a Chicago Public Library OBOC selection requires several months of meetings, involves multiple parts of the library and the city bureaucracy, and is opaque to outsiders. As might be suspected, choice of a One Book One Chicago text is not based on generalizable criteria of sales, text length, genre, canonicity, or publication date. As we saw above (and see<a href="#appendix01">Appendix 1</a>), CPL OBOC choices since 2001 have been quite varied — and yet a few tendencies can be discerned in most One Book programs nationwide: a tendency to favor living authors, especially those able to present their work at public readings, and a tendency, when possible, to select books with a tie to Chicago. Given the importance of social media outreach, authors with social media savvy are particularly appealing in the selection process.<sup id="fnref:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup></p>
<h2 id="data-sources">Data Sources</h2>
<p>One of the challenges of digital humanities research is the assembly of appropriate data sources. “Reading Chicago Reading” combines data about people, about books, and about their interactions through a public library system and associated social media. We bring together a diverse set of data sources, each with its own history, complexities, and caveats. While we can be sensitive to these nuances, algorithms by their nature cannot, and consequently treat all data as equally valid and reflective of some real-world quantity or property. Such limitations of our CPL data are therefore limitations of our conclusions as well and need to be fully unpacked. We will be using the terminology in Table 1 in this discussion and in the analysis below.<br>
TerminologySeasonThe time period between the announcement of one OBOC selection and the next.Book/TextAn OBOC selection, a particular literary work.VolumeA physical manifestation of a book, an item in the library’s inventory.TransactionAny action related to a volume recorded in the CPL system.CirculationTotal count of “checkout” transactions of physical volumes from a given branch (or branches) over a particular period.</p>
<h2 id="oboc-selections-2011-2016">OBOC selections (2011-2016)</h2>
<p>Since fall 2001, the One Book One Chicago program has selected over two dozen books. (See the full list of titles in<a href="#appendix01">Appendix 1</a>.) Roughly half are novels, but the set also includes several short story collections (O’Brien, Lahiri, Li), two novellas (Solzhenitsyn, Cisneros) and two plays (Hansberry, Miller). Non-fiction and historical works, especially those with a connection to Chicago, play a notable part in the list (Dybek, Smith, Dyja, Wilkerson, and Kot). Most of the works date from after World War II, and just under half from the past two decades. Several OBOC book choices reflect the multiple communities in Chicago, White, Black, Latino, and Asian; 18 of 29 (62%) authors are male. Our data set encompasses books from seasons running 2011 to 2016 — the years for which we have CPL circulation data.<sup id="fnref:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup> From the fall 2011 to the 2016-17 OBOC season we have seven central, but heterogeneous, texts: Saul Bellow’s sprawling 1953bildungsroman(and National Book Award winner) <em>The Adventures of Augie March</em> ; a 2010 short story collection by Chinese-American author Yiyun Li, <em>Gold Boy, Emerald Girl</em> ; Markus Zusak’s popular novel set in Hitler’s Germany, <em>The Book Thief</em> (2005); Isabel Wilkerson’s <em>The Warmth of Other Suns</em> (2010) about African American migration from the southern U.S. over the 20th century; Michael Chabon’s comic epic <em>The Amazing Adventures of Kavalier and Clay</em> (2000); a 2013 study of post-World War II Chicago ( <em>The Third Coast</em> by Thomas Dyja), and Barbara Kingsolver’s 2007 meditation on food and ecology, <em>Animal Vegetable Miracle: A Year of Food Life</em> . Table 2 lists these books and the abbreviations that we will use to refer to them throughout the rest of this discussion.<br>
Seasons and selections covered by our data setYearTitleAbbreviation2011 (Fall)Saul Bellow, <em>The Adventures of Augie March</em> AM2012 (Spring)Yiyun Li, <em>Gold Boy, Emerald Girl</em> GB2012 (Fall)Markus Zusak, <em>The Book Thief</em> BT2013Isabel Wilkerson, <em>The Warmth of Other Suns</em> WS2014Michael Chabon, <em>The Amazing Adventures of Kavalier and Clay</em> KC2015Thomas Dyja, <em>The Third Coast</em> TC2016Barbara Kingsolver, <em>Animal, Vegetable, Miracle</em> AV</p>
<h2 id="definition-of-an-oboc-season">Definition of an OBOC season</h2>
<p>As noted earlier, CPL’s management of OBOC events has evolved over its nearly 20-year history. For the purposes of our project, the biggest discontinuity appears at the very beginning of our data set. 2012 was the last year that two seasons were held in one calendar year (i.e. fall and spring). <em>Gold Boy, Emerald Girl</em> in spring 2012 followed directly on the heels of the <em>Augie March</em> fall 2011 season. <em>The Book Thief</em> was then featured in the fall of 2012. The next selection, <em>The Warmth of Other Suns</em> , was launched one full year later in the fall of 2013 as CPL moved to a one-book-per-year schedule. In this case, at least, we were presented with a problem of how to define an OBOC “season.” Our data showed, for example, that the fall 2011 selection <em>Augie March</em> was still being checked out at a significantly higher rate at many branches more than one year after its selection compared to the months prior to its announcement as an OBOC selection — and all despite the fact that another OBOC season and book had been launched in the meantime. This finding suggested that a 12-month span was a reasonable choice for a season length, even for the books chosen bi-annually. We therefore use library transaction data over the 12 months following a book’s selection. As a baseline for the circulation of a book prior to its OBOC selection, we use the six months of city-wide transactions prior to the book&rsquo;s launch. The reason for this choice is again contingent on circumstances beyond our control: due to CPL’s data migration in mid-2011, we only have data for six months prior to the launch of <em>Augie March</em> . For consistency, we define prior circulation for all the books in the same manner.</p>
<p>The next question is how to define the season duration for each book. For almost all seasons, the identity of the book remained secret until a definedlaunchday, where a public announcement would be made by the Chicago Public Library and (sometimes) the Mayor’s office. However, the identity of the fall 2012 selection, <em>The Book Thief</em> , was leaked in newspaper reporting and then subsequently confirmed by the Chicago Public Library in the spring of that year, making the choice known for many months prior to the official launch. In this case, we were confronted with a number of choices, none entirely satisfactory. <em>The Book Thief</em> data could have been discarded. However, we were reluctant to do this as our set of seasons was already limited by CPL data availability – losing 1/7th of the total circulation data would have made our analytic tasks and modeling more difficult and uncertain. We could have stuck with theofficiallaunch date, and in fact, some of our early analyses were conducted in this mode. Doing so, however, creates an uncharacteristic earlybumpin what would be considered prior circulation for this title. In Chicago, <em>The Book Thief</em> became a popular summer reading title well before its fall launch, a phenomenon that denied us a clean demarcation of “before” and “after launch” circulation. By elimination, then, we have chosen to use April 30, 2012 (the date the <em>Chicago Sun-Times</em> inadvertently “announced” the next book) although its official launch by CPL was in September. A consequence of this complexity is that this book has remained a stubborn outlier in many of our analyses. One can see in Figure 2 below how <em>The Book Thief</em> ’s circulation (label BT, blue line) differs from the other six in its delay from our x-axis zerolaunchpoint:</p>




























<figure ><img loading="lazy" alt="" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Book circulation (all branches) for seven seasons of OBOC (2011-2017) superimposed with the official launch date set to zero on the x axis.
        </p>
    </figcaption>
</figure>
<p>The time series shown in Figure 2 allows us to grasp the effects of promotion by CPL. One can see that in most cases after an initial burst of interest (i.e. checkouts) created by the launch date, the checkout totals decline sharply, for instance see <em>Augie March</em> (green line) and <em>Animal Vegetable Miracle</em> (orange line). Other books present different results in their city-wide checkout totals: Dyja’s <em>Third Coast</em> (yellow line) has notable swings up and down, as does <em>The Book Thief</em> . Isabel Wilkerson’s <em>Warmth of Other Suns</em> (beige line) has no major launch-date “bump,” but intriguingly keeps a steady background level of checkouts while also increasing in circulation over the following year. (By contrast, OBOC books typically witness overall declines in circulation after the launch date.)</p>
<p>In future work we will examine in detail the meaning of these checkout numbers, here totaled for all branches but capable of being disaggregated and plotted for each CPL branch. We are interested in the differences in checkouts over time per branch, of course, but one can see even with city-wide totals that after official CPL launches, different OBOC titles have different spikes of secondary or tertiary interest. What drives the later increases in checkouts of these titles? Live programming and social media, we suspect, are crucial factors: an author reads the book at a library event or posts about it (Kingsolver for instance has a large social media footprint), or other City of Chicago tie-in events note the book and/or the season’s theme. In a forthcoming paper about the 2015-16 season (Dyja’s <em>The Third Coast</em> ), we have correlated records of CPL-sponsored events, checkout data, and social media about the OBOC program (via Twitter API) to show in detail and at branch-level how the social word of reading culture does and does not drive book circulation totals throughout the city.</p>
<h2 id="library-checkout-data">Library checkout data</h2>
<p>We obtained two types of library data from CPL.<sup id="fnref:20"><a href="#fn:20" class="footnote-ref" role="doc-noteref">20</a></sup> As indicated above, we were able to obtain CPL transaction data — a history of how individual volumes were processed in the library system. Because of inconsistencies in how the data was queried, and a second data migration occurring more recently, we have all transactions data for some books and only checkouts for others. (The larger set of CPL transaction types includes, in addition to checkouts and returns, inter-branch transfers, renewals, book losses, and holds.) In our preliminary examination of this data we found that all types of branch-level transactions were heavily correlated with branch checkouts, and therefore we have used only checkouts in our analysis below. Our circulation modeling is limited to paperback, hardcover, audio CD copies as well asbook club in a bagkits (see explanation below), and these constitute the vast majority of patron transactions. Although we did have e-book checkouts available, given that we are interested in analyzing differences in checkouts between branches and given that virtual checkouts cannot be associated with one branch, the analysis of e-book transactions is outside of the scope of this paper. Data that would have been helpful to have available is the information about readers participating in OBOC events after purchasing their own copies of the books. We do know from early coverage of the program<sup id="fnref:21"><a href="#fn:21" class="footnote-ref" role="doc-noteref">21</a></sup> that OBOC selection does drive metro area bookstore sales of the title to some extent, but a study of book purchasing behavior is beyond the scope of the present study.</p>
<h2 id="library-holdings-data">Library holdings data</h2>
<p>The second type of data is holdings data recording the number of OBOC books available at each branch library. This data is a series of transactions indicating when each OBOC volume was added to the inventory of a branch library, or accessioned: volume ID, branch, and date. In theory, from this data we would know how many books were on the shelf at each branch at the moment that the OBOC book was launched. However, the holdings data that we have indicates only when a book was added to the collection at a particular branch. In some but not all OBOC events, branches assembledbook club in a bagkits containing eight copies of the chosen text for convenient single checkout. In some branches, these were treated as a single unit and not accessioned until broken out of the bags at the end of the OBOC season. In other cases, the books in the bag were available for separate checkout. The inconsistent treatment of these club bags meant thatholdings at the date of launchis not a consistent representation of how many books a patron might have seen at a given branch during the season. Our compromise has been to count every copy accessioned by a CPL branch, regardless of date, under the assumption that the moment of the “One Book” event would by necessity be the moment of peak holdings for the chosen title. This way, the “broken out” books are treated uniformly for purposes of holdings calculations.</p>
<h2 id="demographic-data">Demographic data</h2>
<p>The Chicago Public Library has eighty branches scattered throughout a large and diverse metropolis well known for its history of segregation. To understand how different groups are participating in OBOC events, we need to associate demographics with the branch transactions that form our core data. For important privacy reasons, CPL – like other public libraries – does not retain patron information associated with book circulation after a book has been returned. The transaction data is therefore fully anonymous and there are not even “pseudonymous identifiers” that could be used, for example, to identify accounts that have participated in multiple OBOC events. In the absence of more detailed information about branch library users, we use data from the U.S. Census American Community Survey (ACS) for the area surrounding each branch. Census data is aggregated by tracts containing approximately 4,000 individuals. We used a simple geographical rule to associate tracts with branch libraries.</p>
<p>Each of Chicago&rsquo;s 866 census tract was associated with the branch library in closest geographical proximity, recognizing that this simple approach is only an approximation of complex patron usage patterns. Technically, this rule was realized through the construction of Voronoi polygons defined by the locations of each branch. A Voronoi polygon is defined by n points {p1, .. p n} and consists of n polygons {P1, …, Pn} where the points interior to polygon Pk are closer to point pk than to any other point. Census tracts fully contained in a particular polygon were assigned to the closest branch. Those that spanned multiple polygons were assigned to all intersecting polygons on the assumption that users equidistant from multiple branches might spread their visits across these branches.</p>
<p>Figure 3 shows these branches and their associated polygons. (Branch codes and names are given in<a href="#appendix02">Appendix 2</a>.)</p>




























<figure ><img loading="lazy" alt="" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Chicago Public Library branches and associated Voronoi polygon regions.The shaded region at the top left is O’Hare Airport, which has no associated library branch.
        </p>
    </figcaption>
</figure>
<p>Our analysis was complicated by the special status of three branches. One is CPL’s iconic central downtown Harold Washington Library Center (branch code H0), which is intended to serve the entire city and not just a single local neighborhood (i.e. the Loop). Harold Washington Library Center is also host to dozens of OBOC events every season, and always the highest profile ones (i.e. author readings). The CPL system also has tworegionallibrary centers: Woodson (branch code W1) and Sulzer (branch code S1), located on the city’s south and north sides, respectively. As with Harold Washington, these regional branches are intended to serve large segments of the city and not just an immediate neighborhood area. We considered various methods of handling these larger branches, including treating them on par with neighborhood branches, thereby ignoring their special status and greater geographic reach. In the end, however, we chose to treat these branches as having service areas overlapping with the neighborhood system. Harold Washington is represented by city-wide aggregate demographics and the two regional branches are represented as a separatesystemof two polygons for the halves of the city that they cover.</p>
<p>With the assignment of tracts to branches complete, we could then calculate aggregate demographics for each branch. The American Community Survey contains approximately 180 variables reflecting a wide variety of social indicators, including race, age, type of employment, average rent paid, commuting patterns, and many others.<sup id="fnref:22"><a href="#fn:22" class="footnote-ref" role="doc-noteref">22</a></sup> To capture large-scale demographic variation across branches, we used principal component analysis (PCA) to create a set of projected dimensions capturing the largest proportion of variance in the ACS data over our polygonal regions. The core concept behind PCA is that a high-dimensional data set may have internal regularities that make some dimensions essentially redundant. For example, it may be that property values and rent are highly correlated. The data set needs only to retain one of these features to capture information about the relative wealth of different regions. PCA takes this concept one step further by creating new dimensions that are linear combinations of the existing ones. Such projected dimensions can be difficult to interpret, but they have the benefit of being mutually orthogonal, meaning that they have no overlap in the aspects of the data that they represent. From our PCA decomposition of the census, we identified seven factors that account for 85% of the variation in the demographic data. Of these, the first four account for 75%, meaning that if a neighborhood is represented by just these dimensions, rather than the original 180, the demographic information will differ from the true value by no more than 25%.</p>
<p>To gain an understanding of the consequences of these features for the library regions, we performed unsupervised clustering of the branches based on their demographic characteristics. We used the Partitioning Around Medioids (PAM) algorithm [Kaufman 1990], which is known to be more robust to noise and outliers compared to the more widely-used k-means algorithm. PAM is more computationally-intensive than k-means, but for our small data set, this was not a significant drawback. We explored various combinations of cluster counts and selected features, using the silhouette metric to discriminate between the different choices. Our final set of five clusters was created using the top eight principal components and had an average silhouette width of 0.3. Figure 4 shows these clusters, and includes patterns perhaps familiar to students of the segregated history of Chicago. A number of “near-north” neighborhoods with higher property values are found in cluster 5. Surrounding them in cluster 2 are diverse neighborhoods with many rental property units. Majority African-American areas on the south and west sides of the city are grouped in cluster 4. Hispanic areas are found mostly in cluster 3. Cluster 1 is the so-calledbungalow beltof historic ethnic neighborhoods, now occupied by a diverse mix of residents, distinguished from some of the other areas by a higher rate of home ownership — note the inclusion of Chicago’s Chinatown (CN branch) in this group. The three regional libraries are treated as a single separate cluster for analysis.</p>




























<figure ><img loading="lazy" alt="" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Branch library regions colored by cluster.
        </p>
    </figcaption>
</figure>
<h2 id="textual-measures-data">Textual measures data</h2>
<p>Our last key data source is textual measures of the books themselves. In this, our project differs from much prior work using quantitative methods of full-text analysis in several ways. One major difference is the great heterogeneity of the texts in our corpus: our text set contains both fiction and non-fiction, and ranges from a modernist novel to short stories to a young adult title and (in the non-fiction) from a food memoir replete with actual recipes documenting a single year to the scholarly works of Wilkerson and Dyja covering decades of U.S. history. The books in our set also vary widely in length, from 70,000 words ( <em>Gold Boy, Emerald Girl</em> ) to about 260,000 words ( <em>Augie March</em> ), and in other stylistic measures such as type-token ratio and sentence length, as will be shown below.</p>
<p>We should also note that all of the recent OBOC works are in copyright — indeed, only Bellow’s <em>Augie March</em> is more than 20 years old. Because of the small size of our collection, it was possible to use text extracted from ePub files. However, this would not be feasible at larger scale, and the only option for full-text processing at scale is the Data Capsule feature of the HathiTrust Research Center<sup id="fnref:23"><a href="#fn:23" class="footnote-ref" role="doc-noteref">23</a></sup> which offers computational access to in-copyright texts. When we began our analysis, only four of our seven selections were available in the HathiTrust digital library data capsule: <em>Augie March</em> (AM), <em>Book Thief</em> (BT), <em>Kavalier and Clay</em> (KC), and <em>Gold Boy Emerald Girl</em> (GB). We added additional files to the data capsule to complete the set: <em>Warmth of Other Suns</em> (WS), <em>Third Coast</em> (TC), and <em>Animal, Vegetable, Miracle</em> (AV). With these additions, it was possible to apply non-consumptive analysis to all seven of works in Table 1 within the HathiTrust data capsule environment.</p>
<h2 id="analysis">Analysis</h2>
<p>There are many kinds of analysis we might undertake with these various data sources. In this article, we discuss our work to model normalized branch-level circulation. Such a model will ultimately enable predictive insight into correlations of city-wide book checkouts and city-wide OBOC book promotion.<sup id="fnref:24"><a href="#fn:24" class="footnote-ref" role="doc-noteref">24</a></sup> Total circulation would seem an obvious, and straightforward choice as a measure of popularity. However, we noted that CPL OBOC book holdings varied widely across branches and a circulation value of 100 would have a very different meaning for a branch with 10 copies as opposed to a branch with 50 copies. Checkouts per holding would have been a logical alternative, but we found that some branches (typically smaller ones) had no copies of some OBOC titles, rendering this statistic meaningless. Our alternative normalization is to calculate circulation per thousand visitors. Because we found visitor count to be very closely correlated with OBOC holdings, this statistic closely tracks circulation per copy.<sup id="fnref:25"><a href="#fn:25" class="footnote-ref" role="doc-noteref">25</a></sup> From the City of Chicago Data Portal we obtained the visitorgate countsfor all CPL branches for the year containing the book’s launch. Visitor counts typically change less than 10% from year to year, except when there is a major change to the physical plant such as new construction.</p>
<p>One plausible hypothesis regarding OBOC circulation might be that the book choice is irrelevant to patron participation. One could imagine a relatively-stable cadre of devoted readers with the time and inclination to pick up whatever text CPL chose to promote for each season. This would lead branches to have relatively stable OBOC checkouts over the seasons studied. We analyzed circulation patterns to determine whether this hypothesis might hold. Figure 5 shows a visualization of normalized circulation at each branch for each book. The seven columns are seven seasons of OBOC 2011-17 with the book title abbreviation at the foot of the column. The colors given to the two-letter branch codes in each column are the colors of the six clusters we described above, based on demography of neighborhoods. Note that the y axis is a logarithmic scale – so for example the lowest scoring branch (DO: Douglass, just west of the Loop) for the AV text has 1/100th the circulation per 1k visitors of the RO (Roden, in Norwood Park) branch at the top end of the scale. We see that the branches vary widely in their OBOC circulation. We also see that relative interest in the OBOC book is not consistent for a particular branch from season to season. For example, the DO branch is at the bottom for <em>Animal Vegetable Miracle</em> but closer to the middle of checkout totals for <em>Augie March</em> and <em>Warmth of Other Suns</em> . The AL branch (Albany Park, in the northwest of the city) has the most normalized circulation for <em>Kavalier and Clay</em> but is near the bottom for <em>Book Thief</em> . Clearly, the book choice matters, and different books appeal to different audiences at different branches. This is perhaps not surprising, but was important to establish at this phase of the study.</p>




























<figure ><img loading="lazy" alt="" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Normalized branch circulation by book, colored by cluster.
        </p>
    </figcaption>
</figure>
<p>To understand the interaction between demography and circulation in greater detail, let’s examine a single book: Thomas Dyja’s <em>The Third Coast: When Chicago Built the American Dream</em> (2013) [TC, and the selection for the 2015/16 season] in relation to the first three components of our principal components decomposition. Figure 6 shows a scatter plot with these results. PC1 corresponds (roughly) to property value, with lower (more negative) values corresponding to wealthier neighborhoods. We see a linear trend with some substantial outliers: for example, CH (Chinatown branch) below the trend line — i.e. wealthier (and abutting the rapidly gentrifying south loop) but with lower circulation, and ED (Edgebrook) above the trend, i.e. median wealth but with higher circulation. Readers familiar with Chicago neighborhoods might hazard guesses about this output: despite similar wealth figures, Chinatown residents near the center of Chicago were less likely to read <em>The Third Coast</em> — a book centered primarily on north and south areas of the city — than far-north residents in Edgebrook. Because of the limited amount of data available we cannot trim these outliers, but instead must recognize that any kind of linear model will be an inexact fit. A similar observation can be made for PC3, which corresponds to a combination of the renter/home-owner axis and age, with lower (more negative) values corresponding to more owner-occupied housing and more residents above age 35.</p>




























<figure ><img loading="lazy" alt="" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Trends for circulation vs branch demographics
        </p>
    </figcaption>
</figure>
<p>However, PC2 is different. With this feature, we do not see an overall linear trend. Rather the extremes of negative and positive are lower, and peak circulation occurs at values around zero. This component corresponds to a race/ethnicity spectrum with Latinx communities on the negative end of the scale and African-American ones at the positive end. We experimented with various transformations of the PC2 variable and ended up with the following square transform:</p>
<p>C2X=(C2*C2)/5</p>
<p>The ⅕ factor was added to scale the value to a range similar to the other principal components. A comparison between the original PC2 and the transformed PC2X variables is shown in Figure 7.</p>




























<figure ><img loading="lazy" alt="" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Circulation trend with principal component 2 and transformed version.
        </p>
    </figcaption>
</figure>
<p>Although the analysis above examines only the data for a single book, other OBOC seasons show similar patterns. A complete analysis of the circulation data for all of the seasons can be found in the “Results” page of our project website (see endnote 3).</p>
<h2 id="circulation-modeling-part-i">Circulation Modeling, Part I</h2>
<p>To understand the magnitude of the impact attributable to book choice, we constructed a multi-level linear regression model of the aggregate branch-level circulation. A regression model has the form:y=β0+β1x1+β2x2+&hellip;+βkxkwhere thexivalues are features being used for prediction (for example, a demographic characteristic of a neighborhood) and theβivalues are coefficients fit to the model to maximize its predictive accuracy. A multi-level model adds an additional set of terms to this predictor that are book-specific. For example, the model for the circulation of Saul Bellow’s <em>Augie March</em> (AM) has the following form:</p>
<p>yAM=β0+β1x1+β2x2+&hellip;+βkxk+γ0AM+γ1AMx1+γ2AMx2+&hellip;+γkAMxk</p>
<p>where theγiAMare book-specific coefficients. In this type of model, theβicoefficients are known as the fixed effects and theγiAMcoefficients as the random effects. A different set ofγvalues is fit for each book, enabling us to determine how the relationship between neighborhood demographics and circulation varies by book.</p>
<p>A range of different models was constructed with different subsets of the demographic principal components. These models were evaluated on their ability to account for the existing patterns in the data, yielding the closest fit to the observed data. The best fitting model is described here and included four independent variables: the branch-level holdings (“Holds” in the figures), principal components 1 and 3 (PC1 and PC3) and the transformed version of principal component 2 as described above (PC2X).<sup id="fnref:26"><a href="#fn:26" class="footnote-ref" role="doc-noteref">26</a></sup> Coefficients in the fitted model provide a quantitative measure of the relationship between the independent variables and the circulation, as impacted by the choice of book.</p>
<p>What we learn from this model fits well with intuition gleaned from the analyses above, but sharpens it to identify specific aspects of the data. Figure 8 shows the coefficients learned for the demographic attributes for each OBOC book — both theβiand theγivalues from the equation above where i &gt; 0. Because the coefficient values are small, they are scaled here by 1000, in effect undoing the per-thousand-visitors transformation applied to the circulation data. The fixed effect (βi) coefficients for the three demographic parameters show a very large general effect associated with dimensions 1 and 3 and much smaller one for the (transformed) second component. In general, then, greater wealth and greater homeownership and age are correlated with OBOC participation and the particular book has a smaller effect. Interestingly, this wealth effect is enhanced most substantially for TC, but diluted for GB and to a lesser extent, BT.</p>
<p>The second demographic component, which in its transformed state corresponds more or less to a white+other (at zero) vs black+hispanic axis, shows more dramatic variation. The overall impact of this PC2X axis is not large compared to the other components (7 checkouts per season vs 30-40), but the variation relative to the book choice is more significant. Note Figure 8, and the PC2X results (middle third of the figure): the between-book variation follows patterns that we anticipated. As might be expected for a book about the Great Migration in which Chicago’s African-American community plays a large part, WS reverses the otherwise largely downward trend of the axis: that is, more copies of <em>Warmth of Other Suns</em> were checked out in libraries with larger black+hispanic patrons. (To a lesser extent, <em>The Book Thief</em> also shows this phenomenon.)</p>




























<figure ><img loading="lazy" alt="" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Fitted coefficients demographic variables in the multi-level model.
        </p>
    </figcaption>
</figure>
<p>Figure 9 looks at the coefficients for the CPL Holdings variable — that is, the number of books assigned to a particular library branch. As expected, there is a strong general effect of approximately 3 checkouts per book: it is a more involved process for a patron to check out a book not currently on the shelf at the branch. Some books, however, show greatershelf appeal, and for some, like WS, this effect is negative, suggesting that patrons were sufficiently interested in this title that lack of branch copies was not much of a deterrent.</p>
<p>Finally, Figure 10 shows theinterceptsfor the model, that is theβ0andγ0values from the equation. These are not multiplied by any of the model features and therefore serve as a baseline level of popularity for the program overall and for each OBOC title. We see here that the baseline effect is strong. Choosing a book for the OBOC program will typically add more than 300 checkouts for a typical CPL branch over the season. Different books have, of course, different levels of general interest as shown in the book-specific intercepts: BT, TC, and WS were more popular than average; GB had the lowest general appeal.</p>




























<figure ><img loading="lazy" alt="" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Fitted coefficients for branch library holdings
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Fitted intercepts
        </p>
    </figcaption>
</figure>
<h2 id="circulation-modeling-part-ii">Circulation Modeling, Part II</h2>
<p>This first stage of modeling highlighted the heterogeneity of the book choices and of the OBOC patron audience. Books have clear differences in their overall appeal and in their specific appeal to different patron groups. However, this finding raises the question of what features of books cause them to appeal to different audiences within the city. From the start, a motive of the project has been to capture quantitatively, branch by branch and over time, the differential appeal and impact of literary works. We considered various types of textual features that might be extracted from our texts. However, we were confronted repeatedly with the challenge of the differences between the texts such as length, reading level, genre, and still other traits. We needed to identify features sufficiently generic that they could be extracted from any text in our set — but also for any future book — in order to begin modelingunannouncedbooks for city-wide checkout predictions. Based on our discussions with CPL staff, we hypothesized that readingdifficultyand text length would be important indicators, with some patrons less likely to pick up OBOC selections that overtly present greater reading challenges.</p>
<h2 id="text-characteristics">Text characteristics</h2>
<p>There are many ways to calculate areading levelor degree of difficulty for a text or set of texts. Conventional indices of difficulty cannot tell the whole story, however, as they do not take into account the subject matter or the organization of the text but rely on the surface characteristics such as the occurrence or absence ofdifficultwords or measures of sentence length<a class="footnote-ref" href="#dawkins1956"> [dawkins1956] </a>. Numbers do not automatically align with the actual reading experience. For example, in our first efforts in modeling we noticed that the dense and allusive <em>Adventures of Augie March</em> scored lower on some measures than the relatively simple prose of Li’s <em>Gold Boy Emerald Girl</em> . Such numerical findings in isolation did not, we felt, make sense of the quite different reading experiences involved. To address this, rather than rely on any single text measure we created a combined measure using the following four text attributes:</p>
<ul>
<li>Average sentence length</li>
<li>Dale-Chall reading difficulty<sup id="fnref:27"><a href="#fn:27" class="footnote-ref" role="doc-noteref">27</a></sup></li>
<li>Type-token ratio</li>
<li>Total number of words</li>
</ul>
<p>Our text measures were obtained by running a Python-based readability program on scanned volumes of OBOC selections in the HathiTrust secure data capsule.<sup id="fnref:28"><a href="#fn:28" class="footnote-ref" role="doc-noteref">28</a></sup> We extracted the main text of each work and then tokenized it into sentences. We then extracted ten samples containing approximately 10,000 words each and computed the reading measures above (except for total number of words) on each sample. Then we averaged over the samples to produce the reading measures shown in Table 3 and Figure 11.<br>
OBOC text measuresTitle abbreviationNumber of words (punctuation excluded)Average sentence length (punctuation excluded)Dale-Chall IndexType-token ratioCombined difficultyAM263,42716.178.038.610.43GB71,13821.408.279.900.38BT127,83810.588.087.340.08WS212,61319.028.539.600.53KC240,21616.898.899.200.54TC150,16624.1010.4711.500.85AV125,84918.679.5011.110.60<br>




























<figure ><img loading="lazy" alt="" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>OBOC text measures visualization
        </p>
    </figcaption>
</figure></p>
<p>The final column of Figure 11,Combined difficulty, is a normalized aggregate of the other four measures. We first normalized each measure so that the highest-scoring book has a value of 1 and the lowest a value of 0. Although still based on surface level analysis of the text, we believe that this combined measure captures more accurately a book’s difficulty. Figure 11 indicates that <em>The Third Coast</em> shows the highest average sentence length, Dale-Chall Index score, and Type-token ratio, and has the highest combined difficulty score. Both non-fiction works in our set, <em>The Third Coast</em> (TC) <em>and Animal, Vegetable, Miracle</em> (AV), score higher in reading difficulty and therefore stand apart from novels such as <em>The Book Thief</em> .</p>
<h2 id="locality-features">Locality features</h2>
<p>The nature of place and locality in narrative has generated a lot of exciting research in (digital) humanities, from renewed attention to the concept ofchronotopesto feature extraction and mapping work for corpora large and small.<sup id="fnref:29"><a href="#fn:29" class="footnote-ref" role="doc-noteref">29</a></sup> For our purposes, locality can serve as a way to identify theChicago-nessof a title and its geographical connection to the city and its readers. Early analyses suggested that some clusters of residents had stronger interest in local titles than others, and this suggested to us the value of this feature. Forthcoming papers will document our mapping work. In the meantime, some visualizations can be found at the project website.<sup id="fnref:30"><a href="#fn:30" class="footnote-ref" role="doc-noteref">30</a></sup></p>
<p>Because of the aforementioned findings about the importance of location in book selection decisions, we were most interested in extracting places names from the texts and establishing each book’s geographic locus. But disambiguating named entities of any type, including place name references, is challenging, as<a class="footnote-ref" href="#evans2018"> [evans2018] </a>and others have noted. Our approach was to use the output of the Stanford Named Entity Recognizer (NER) 3.9.1 version<a class="footnote-ref" href="#finkel2005"> [finkel2005] </a>, using the default model for location extraction included in the parser. Although we were working with a scanned version of the text included in a digital library, we found the accuracy of this parser to be acceptable for this purpose<a class="footnote-ref" href="#rodriquez2012"> [rodriquez2012] </a><a class="footnote-ref" href="#atdag2013"> [atdag2013] </a>. Problems arose, however, when we tried to associate toponyms extracted from the text with their geocoordinates. For example,OdessaandPariscan be cities in Texas or in Europe. Likewise,Gold Coasthas a specific local meaning in Chicago, but is also the name of several other places around the world. Our workflow made use of the Google Maps API to resolve toponyms to latitude and longitude coordinates and, as might be expected, without knowledge of the literary context of each toponym, the system could not resolve such ambiguities correctly in many cases. A particularly evocative example we encountered was the numerous references in <em>The Third Coast</em> to the once-famousMeccaapartment building on the south side of Chicago (immortalized in Gwendolyn Brooks’s 1968 book of poetry <em>In the Mecca</em> ). The Mecca building was demolished decades ago to make room for the expanding campus of Illinois Tech on Chicago’s south side, but Google Maps unsurprisingly locates the place name in Saudi Arabia. Ultimately, this problem was solved by the manual examination and correction of each extracted toponym in its textual context together with the computed geolocation.<sup id="fnref:31"><a href="#fn:31" class="footnote-ref" role="doc-noteref">31</a></sup></p>
<p>A further problem with toponyms is how they occur in a hierarchy of specificity, with higher levels of the hierarchy resolved by the mapping API as centroids of a region. For example, the place nameRussiayields a latitude and longitude pair placed in the middle of Siberia, which is geographically correct in an abstract sense, but unlikely to be the part of Russia to which any particular author is referring — and certainly not Chabon in <em>Kavalier and Clay</em> [KC], from which this example is taken. This problem does not admit of an easy all-purpose solution. Our simple expedient was to eliminate all place names with greater thancityextent. Our rationale here is that a text gains its geographical purchase from the accumulated mention of specific locations, not from references to large abstract entities. To produce our locality measures, we computed the distance from each location to a zero-point centered in the Loop in downtown Chicago and averaged these distances. So that local national distances are not swamped by transcontinental ones, we took the logarithm of the value and used this as our measure of thedistanceof the text from the city. We also experimented with a version of the model in which this continuous distance value was replaced with a simpler binary distinction between local (that is, Chicago-centered) texts (AM, TC, WS) and non-local ones (AV, BT, GB, KC), also computed using the same toponyms.</p>
<h2 id="predictive-model">Predictive Model</h2>
<p>Historical circulation models offer a great deal of insight into the patterns of OBOC participation, as we have seen. To askwhat ifquestions about books that might be chosen in the future, however, we need a different kind of model. This predictive modeling task builds on the work above, which allowed us to identify the most important demographic variables and get a sense of their predictive utility. In effect, we are seeking to replace the discrete individual books in the prior model with descriptive variables that capture some aspects of the books&rsquo; contents as discussed above.</p>
<p>As with our prior methodology, the model serves multiple purposes. The ability to predict with some degree of precision indicates that the variables we have chosen for the model do, in fact, capture regularities associated with branch circulation outcomes. This helps us have confidence that the model is in the right form and has the right variables in it. The predictions coming from such a model may be useful to library staff in anticipating how different book choices might engage different patrons across the city. Finally, and most importantly for a digital humanities audience, the importance that the model assigns to different features gives us a sense of the impact of different variables on the final outcome.</p>
<p>As above, the dependent feature of our predictive circulation models is normalized circulation value: that is, checkouts at a given branch per 1000 visitors. As we have described above, the new independent variables are: combined reading difficulty score, degree of promotion (i.e. number of events at branches), and locality. The values are combined with the variables from the earlier analyses: three demographic variables and the number of holdings. For the first model, MPrior, we added the book’s prior circulation (previous 90 days) at that branch. With prior circulation as an input variable, we are enabling MPrior to predict the impact of the library’s selection of a particular book. What, in other words, is the change in circulation pattern induced by a book’s selection? This is interesting to the library, since it may make less sense to pick as a “One Book” selection a text that many people would have read anyway. But it is also interesting for our purposes since it reflects the impact of thecivicmotivation of the act of book selection. The model without prior circulation (MCirc) is interesting to us for a different reason: with this model, we predictde novowhat the match between demographic and book characteristics might say about a book’s popularity at a given branch given a particular level of investment by the library system (in promotions and book holdings).</p>
<p>The model type we chose is a boosted regression tree, an appropriate model for learning the relationship between a numeric output variable and a diverse set of input variables.<sup id="fnref:32"><a href="#fn:32" class="footnote-ref" role="doc-noteref">32</a></sup> Regression tree learning is a form of decision tree learning, where the system builds a complex set of decision rules, each of which is a test against the values of a particular feature. For example, the top level of a decision tree might ask if the value of the PC1 variable is greater than 0.5, and if so, the rules on one side of the tree are applied, otherwise a different set. The power of the gradient boosting algorithm is that the system iteratively determines which cases in the input data generate the most error and focuses additional learning on getting these predictions correct. A typical boosted regression tree model might contain hundreds of such rules, knitted together in a complex pattern of choices.</p>
<p>Our model was trained and evaluated using a cross-validation technique. In each step of evaluation, one OBOC season was omitted and the model was trained on the other six seasons. Then the model was used to predict the missing year and the error calculated. This was repeated across all seven years, and averages computed across all years. The average mean absolute error for MPrior was 0.017. (Recall that all values were normalized between 0 and 1, including the circulation.) This average of 1.7% corresponds to about 7-8 checkouts in a given branch. The corresponding value for MCirc is 6%, or about 3.5 times as high. This is not surprising as this model has much less information to work with. However, it is still within 10% of the actual checkout total.</p>
<p>As noted above, our primary interest in this paper is not in predictions per se, but rather what the model tells us about the features under study. Figure 12 below shows the feature importance for different input features in MPrior. As noted above, the tree itself consists of hundreds of choices in complex combination; the feature importance is a statistic that reflects the overall utility of a particular variable across all the predictions the system makes. It does not offer any insight into the structure of the tree itself. The feature importance values shown here are averages across all seven learned models, and were relatively consistent across the different model runs.</p>
<p>Figure 12 also shows that the importance of prior circulation is very high, which is not surprising since it reflects the pre-existing interest of a particular set of patrons in a particular text. The other variables in descending order of importance are Holdings, Difficulty, PC1, PC3, PC2X, Promotion, and Proximity. (Note that feature importance values are not probabilities and do not sum to 100%.) Theshelf effectnoted above (see Figure 9) is substantiated here. The number of books allocated to a branch does indeed have an impact on patron behavior over and above the prior interest indicated by patrons. Also, we see that our combined reading difficulty measure contributes to the predictions. Feature importance does not indicate the direction of influence, but from other data we know that this influence is negative — i.e. the more difficult the book by this measure, the fewer patrons will check it out. It is also worth noting that, in general, reading difficulty measures track the fiction/non-fiction genre divide, with non-fiction books generally scoring higher. At this point, we do not have enough data to firmly disentangle the effects of reading difficulty for this fiction/nonfiction distinction. Finally, we have the demographic variables, especially the PC1 (property value) and PC3 (owner/renter) axes. At lower levels of importance are the number of OBOC outreach events held and the locality measure (proximity) of the text.</p>




























<figure ><img loading="lazy" alt="" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Average feature importance for MPrior circulation models.
        </p>
    </figcaption>
</figure>
<p>Figure 13 contains similar importance values for MCirc, but now prior CPL circulation is not considered. Here we see that the roles are reversed between the set of demographic variables and the holdings/reading difficulty measure. The prior circulation variable in MPrior is, to some extent, building in the baseline appeal of the book to the patrons of a particular branch, and when this variable is removed the demographic factors become a stronger element. In some ways, this brings us back to one of our original research questions – namely, the relationship between demography and OBOC participation. Here we see effects of PC1, PC3 and PC2X similar to those found in the multi-level model. Once we get past these effects, we have a consistent pattern relating holdings, reading difficulty, promotional events, and proximity.</p>




























<figure ><img loading="lazy" alt="" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Average feature importance for MCirc models
        </p>
    </figcaption>
</figure>
<p>Our final model examined the impact of using a binary Chicago/non-Chicago feature as our representation of a text’s locality rather than the quantitative proximity values used in the other models. We call this model MBinary. Interestingly, the average mean absolute error on this model was approximately 0.018, which is slightly but not significantly improved over the MPrior model. Figure 14 shows the average error for the three models expressed in terms of normalized circulation.</p>




























<figure ><img loading="lazy" alt="" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Average model MAE
        </p>
    </figcaption>
</figure>
<p>Figure 15 shows the average feature importance for the MBinary model. The values are very similar to those found in Figure 12, with the binary feature showing a small, but non-negligible contribution to the model performance. The results for MBinary are quite important as they demonstrate that high accuracy in toponym attribution is not essential to making use of locality in circulation modeling. It is sufficient to label a book simply as Chicago-connected or not, and we expect that this will be possible without the manual effort required to achieve high accuracy for each geographic label.</p>




























<figure ><img loading="lazy" alt="" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Average feature importance for MBinary models
        </p>
    </figcaption>
</figure>
<h2 id="limitations">Limitations</h2>
<p>The analyses above have some gaps that we hope to address in future research. For instance, although the period in question has seen greater use of e-books by CPL patrons, we have not yet included e-book checkouts in our analysis. (Because e-book checkouts are not associated with particular branches, we would not have been able to build them into branch-level circulation models.) However, e-book usage is almost certainly correlated with neighborhood demographics and therefore representsmissingcheckouts in certain areas. A similar point could be made about book purchases. Chicagoans with the resources to purchase a copy of the selected OBOC text rather than read a library copy are obviously not represented in the CPL circulation data. Thus, we expect that our models underestimate total OBOC participation, particularly in more affluent areas of the city.</p>
<p>A limitation in our analysis is the limited number of texts we were able to use. 80 branches and seven books constitute less than 600 circulation data points. The texture of the model can be enhanced in several ways, however. One way would be to obtain year-by-year demographic data for each branch rather than using a single year’s census data to stand in for the whole time period. As it is now, only 80 combinations of the demographic factors can appear, and therefore interrelations between the factors are difficult to discern. In general, we would not expect this to change the results much, but in neighborhoods experiencing rapid or marked demographic change it would provide a more accurate representation.</p>
<p>Richer data can also be obtained by looking at additional books. We have obtained the branch-level circulation data for 309 books chosen by CPL as recommendations associated with each of the seven recent OBOC seasons (theif you liked this book, you might also like &hellip;feature that appears regularly on the CPL website, on program flyers, and in branch libraries). These books were selected by CPL for thematic or other similarities to the OBOC choices and therefore provide an interesting, if idiosyncratic, control group: books that did not receive the full promotional boost of the official OBOC selected texts but were brought to the attention of patrons nonetheless. With these 300+ additional texts, it would be possible to extend MCirc by adding a binary variable distinguishing theselectedvsrecommendedtexts.</p>
<p>Another limitation is working exclusively with in-copyright texts. Both the set of seven recent OBOC selections and the extended corpus of recommended texts contains many in-copyright works and therefore our text processing can only occur, painstakingly, through the non-consumptive text processing capabilities of the HathiTrust Digital Library. We have also found that many of the texts in this extended data set are not held in the HathiTrust collection. For the works that are present, working from scanned OCR-ed text entails significant difficulties in calculating some of the text properties that we have relied on in this analysis to date. We have discovered significant biases in the calculation of reading difficulty. Most challenging is the determination of the location/proximity variable, which in the end required manual checking — a process that does not scale up to a larger corpus. This is one reason that the MBinary model is of interest: we expect that assessing a simple Chicago/non-Chicago placement of a text will be less error-prone than the calculation of distances for each toponym, although the case of theMeccabuilding is an important cautionary tale.</p>
<p>As in other disciplinary contexts, merging data from heterogeneous sources is a difficult task with no easy or standardized solutions. Each type of data — branch demographics, library circulation (both checkouts and book holdings), and book content — poses its own challenge for data curation, normalization, and integration, many of which have been discussed above. One distinguishing feature of this work has been our reliance largely on a relatively small number of heterogeneous in-copyright texts. Thus, we do not have the advantage of large-scaledistanttext processing where small errors in the transformation pipeline can be expected to cancel each other out.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Our project has identified several challenges that will be of interest to scholars in the digital humanities, particularly those working at the intersection of text analysis, geography, and public data sets. Our original goal to capture and predict mass literary events has largely been met. Ascapture, we have created an archive of nearly a decade of multiple media forms (and metrics for them) associated with a cultural program that has engaged many thousands of people across a major American city for years. In pursuit ofprediction, we have produced a novel predictive model integrating demographics, book content, and branch data to produce branch-level predictions of book circulation. With this tool, we plan to generate branch-level circulation predictions for book titles (OBOC program or not) beginning in summer 2020. This will be reported in a future paper.</p>
<p>While we expect this predictive model to be of use to CPL staff, it is important to note that it has not been our intent (nor theirs) to optimize against such a model in choosing books. One does not need data-intensive modeling to identify books that circulate highly; for example, a good bet at any time would be current best-sellers by authors with name recognition who have been highly promoted by publishers. <em>But maximizing circulation alone has never been the primary goal of “One Book, One Chicago” .</em> We expect that library staff will continue to make OBOC text and theme choices as they always have, through an in-depth process that considers an entire constellation of cultural and socio-political factors. However, they will now be able to do so with the help of an additional data source: for any given book, they will also be able to calculatewhat-ifscenarios for all CPL branches and consider different levels and kinds of promotional activity.<sup id="fnref:33"><a href="#fn:33" class="footnote-ref" role="doc-noteref">33</a></sup></p>
<p>One of the key findings of our predictive model is that prior circulation makes the largest predictive contribution for the circulation of OBOC selected works. This is a measure, however, that will be unavailable for new books and little-known or first-time authors. It is possible, as we have shown, to do similar types of predictions without prior circulation data, but with significantly lower accuracy. This is not a surprising finding, but our ability to quantify the effect will enable library staff to reason about the tradeoffs inherent in choosing works already circulating well in the local library-system as opposed toimportingchoices from outside the system in the name of expanding readers’ horizons.</p>
<p>Additional work remains. One key question of interest to us is the interaction between kinds of promotional events surrounding a given OBOC season, the sharing and re-sharing of these events via social media, and subsequent specific branch circulation outcomes. As shown in Figure 2, the time course of each book’s circulation shows a variety of different patterns and temporal structures. Preliminary analysis suggests that, as might be expected, there is a close association between bumps in checkout numbers and branch events and social media activity hosted by the library — but there are notable differences by branch and type of event. However, analysis of this association for each of the eighty branches remains to be done. We do not know, for example, how different types of events impact circulation or whether an event at one library branch impacts circulation at others. We are interested in linking the circulation and event time-series data to the timeline of OBOC Twitter posts, many but not all of which are related to specific OBOC events. We have already collected associated Twitter and Goodreads data for several OBOC seasons and have developed supervised text classification algorithms to isolate the OBOC-specific tweets from unrelated content. This relationship between social media activity and circulation is yet to be quantified, but will be the subject of a future paper.</p>
<h2 id="acknowledgements">Acknowledgements</h2>
<p>Research has been supported by DePaul University, the NEH Office of Digital Humanities (grant HD 248600-16), Microsoft Azure, the Lyrasis Catalyst Fund, and the HathiTrust Advanced Computing Support program. The project has benefited from the contributions of numerous talented students at DePaul: the multi-level linear circulation model was originally designed and executed by Zach Budde and the predictive circulation model was developed by He (Vicky) Yang. Mihaela Stoica created event logs for OBOC seasons, wrote for the project blog, and made manual corrections to data files and maps. Maps and demographic data were compiled by Nandhini Gulasingam of DePaul’s Faculty Scholarship Support Center. Thanks to Jessica Bishop-Royse (FSSC) and Megan Bernal (Library) for ideas and contributions, and to our other student research assistants. Special thanks to the Chicago Public Library’s Jennifer Lizak, Coordinator of Special Projects for Cultural and Civic Engagement, and Michelle Frisque, CPL’s Chief of Technology, Content, and Innovation.</p>
<h2 id="appendix-1">Appendix 1</h2>
<h1 id="one-book-one-chicago-program-book-selections-2001-2019">“One Book One Chicago” Program book selections, 2001-2019</h1>
<p>2019/20 Elizabeth Kolbert, <em>The Sixth Extinction: An Unnatural History</em> 2018/19 Philip K. Dick, <em>Do Androids Dream of Electric Sheep?</em> 2017/18 Greg Kot, <em>I’ll Take You There</em> 2016/17 Barbara Kingsolver, <em>Animal, Vegetable, Miracle</em> 2015/16 Thomas Dyja, <em>The Third Coast</em> 2014/15 Michael Chabon, <em>The Amazing Adventures of Kavalier and Clay</em> 2013/14 Isabel Wilkerson, <em>The Warmth of Other Suns</em> 2012 (Fall) Marcus Zusak, <em>The Book Thief</em> 2012 (Spring) Yiyun Li, <em>Gold Boy, Emerald Girl</em> 2011 (Fall) Saul Bellow, <em>The Adventures of Augie March</em> 2011 (Spring) Neil Gaiman, <em>Neverwhere</em> 2010 (Fall) Toni Morrison, <em>A Mercy</em> 2010 (Spring) Colm Toibin, <em>Brooklyn</em> 2009 (Fall) Carl S. Smith, <em>The Plan of Chicago</em> 2009 (Spring) Sandra Cisneros, <em>The House on Mango Street</em> 2008 (Fall) Tobias Wolfe, <em>The Right Stuff</em> 2008 (Spring) Raymond Chandler, <em>The Long Goodbye</em> 2007 (Fall) Arthur Miller, <em>The Crucible</em> 2007 (Spring) James Baldwin, <em>Go Tell It on the Mountain</em> 2006 (Fall) Jhumpa Lahiri, <em>The Interpreter of Maladies</em> 2006 (Spring) Aleksandr Solzhenitsyn, <em>A Day in the Life of Ivan Denisovitch</em> 2005 (Fall) Jane Austen, <em>Pride and Prejudice</em> 2005 (Spring) Walter Van Tilburg Clark, <em>The Ox-Bow Incident</em> 2004 (Fall) Julia Alvarez, <em>In the Time of Butterflies</em> 2004 (Spring) Stuart Dybek, <em>The Coast of Chicago</em> 2003 (Fall) Tim O’Brien, <em>The Things They Carried</em> 2003 (Spring) Lorraine Hansberry, <em>A Raisin in the Sun</em> 2002 (Fall) Willa Cather, <em>My Antonia</em> 2002 (Spring) Elie Wiesel, <em>Night</em> 2001 (Fall) Harper Lee, <em>To Kill a Mockingbird</em></p>
<h2 id="appendix-2">Appendix 2</h2>
<p>Chicago Public Library Branches and Branch CodesBranch Code and Branch NameAI - Austin-IrvingAL - Albany ParkAR - Archer HeightsAT - AltgeldAU - AustinAV - AvalonBA - Back of the YardsBE - BeverlyBL - BlackstoneBP - Brighton ParkBR - BrainerdBT - Bucktown-Wicker ParkBU - Budlong WoodsBZ - BezazianCB - Chicago BeeCG - Chicago LawnCH - ChinatownCL - ClearingCN - CanaryvilleCO - ColemanDA - Daley, Richard P.-BridgeportDO - DouglassDU - DunningED - EdgebrookEG - EdgewaterGA - Gage ParkGG - Greater Grand CrossingGM - Galewood-Mont ClareGR - Garfield RidgeH0 - Harold Washington Library Center (Main)HA - HallHE - HegewischHU - Humboldt ParkIN - IndependenceJE - Jefferson ParkJM - Jeffrey ManorKE - KellyKI - KingLB - Lincoln BelmontLE - LeglerLG - Lincoln SquareLI - Lincoln ParkLO - LozanoLV - Little VillageMA - MayfairMC - McKinley ParkME - MerloMM - ManningMT - Mount GreenwoodNA - North AustinNN - Near NorthNO - NorthtownNP - North PulaskiOR - Oriole ParkPO - Portage-CraginPU - PullmanPW - Water WorksRG - Rogers ParkRM - Daley, Richard M-West HumboldtRO - RodenRS - RooseveltS1 - Sulzer (Regional)SC- ScottsdaleSH - Sherman ParkSO - South ChicagoSS - South ShoreTH - Thurgood MarshallTO - TomanUP - UptownVO - Vodak-East SideW1 - Woodson (Regional)WA - WalkerWB - West BelmontWC - West Chicago AvenueWE - West lawnWP - West PullmanWR - Wrightwood-AshburnWT - West TownWW - West EnglewoodWY - Whitney M. Young, Jr.</p>
<p>See<a href="#chall1995">Chall and Dale 1995</a>.</p>
<ul>
<li id="alter2016">Alter, Alexandra, and Karl Russell. “Moneyball for Book Publishers: A Detailed Look at How We Read” . _New York Times_ (March 15, 2016).
</li>
<li id="alharthi2018">Alharthi, Haifa, Diana Inkpen, and Stan Szpakowicz. “A Survey of Book Recommender Systems” . _Journal of Intelligent Information Systems_ 51 (2018): 139-160.
</li>
<li id="amory2007">Amory, Hugh. et al, gen eds. _A History of the Book in America_ . 5 vols. Chapel Hill: UNC Press, 2007-2014.
</li>
<li id="anderson1991">Anderson, Benedict. _Imagined Communities: Reflections on the Origin and Spread of Nationalism_ . London: Verso, 1991.
</li>
<li id="archer2016">Archer, Jodie, and Matthew L. Jockers. _The Bestseller Code: Anatomy of the Blockbuster Novel_ . NY: St Martin’s, 2016.
</li>
<li id="ard2013">Ard, BJ. “Confidentiality and the Problem of Third Parties” . _Yale Journal of Law and Technology_ (2013/14).
</li>
<li id="atdag2013">Samet Atdağ, and Vincent Labatut. “A Comparison of Named Entity Recognition Tools Applied to Biographical Texts” . _2nd International Conference on Systems and Computer Science_ 2013: 228–233.
</li>
<li id="bakhtin1983">Bakhtin, Mikhail. “Forms of Time and Chronotope in the Novel” . _The Dialogic Imagination: Four Essays_ . Trans. Michael Holmquist. Austin: U of Texas Press (1981): 84-258.
</li>
<li id="barber2013">Barber, Benjamin R. _If Mayors Ruled the World: Dysfunctional Nations, Rising Cities_ . New Haven: Yale UP, 2013.
</li>
<li id="bishop2012">Bishop, Claire. _Artificial Hells: Participatory Art and the Politics of Spectatorship_ . London: Verso, 2012.
</li>
<li id="boy2016">Boy, John D., and Justus Uitermark. “How to Study the City on Instagram” . _PLoS ONE_ 11(6) 2016: e0158161.doi:10.1371/journal.pone.0158161
</li>
<li id="bratton2015">Bratton, Benjamin H. _The Stack: On Software and Sovereignty._ Cambridge: MIT Press, 2015.
</li>
<li id="chall1995">Chall, J. S., & Dale, E. (1995). _Readability Revisited: The new Dale-Chall Readability Formula_ . Cambridge: Brookline Books, 1995.
</li>
<li id="cohen2019">Cohen, Kris. “Literally, Ourselves” . _Critical Inquiry_ 46 (Autumn 2019): 167-192.
</li>
<li id="collins2010">Collins, Jim. _Bring On the Books For Everybody: How Literary Culture Became Popular Culture_ . Durham: Duke UP, 2010.
</li>
<li id="collins2013">Collins, Jim. “The Use Values of Narrativity in Digital Cultures” . _New Literary History_ 44:4 (Autumn 2013): 639-60.
</li>
<li id="cordell2015">Ryan Cordell. 2015. “Reprinting, Circulation, and the Network Author in Antebellum Newspapers” . _American Literary History_ 27.3 (September 2015): 417–445.
</li>
<li id="cranshaw2012">Cranshaw, Justin, and Raz Schwartz, Jason Hong, and Norman Sadeh. “The Livelihoods Project: Utilizing Social Media to Understand the Dynamics of a City” . _ICWSM 2012: Proceedings of the 6th International AAAI Conference on Weblogs and Social Media_ .
</li>
<li id="davidson1989">Davidson, Cathy N. ed. _Reading in America: Literature and Social History_ . Baltimore: Johns Hopkins UP, 1989.
</li>
<li id="davis2008">Davis, Kimberly Chabot. “White Book Clubs and African American Literature: The Promise and Limitation of Cross-Racial Empathy” . _LIT: Literature Interpretation Theory_ 19 (2008): 155-186.
</li>
<li id="dawkins1956">Dawkins, John. “A Reconsideration of the Dale-Chall Formula [with Reply]” . _Elementary English_ 33.8 (1956): 515–522.
</li>
<li id="dimitrov2015">Dimitrov, Stefan, Faiyaz Zamal, Andrew Piper, and Derek Ruths. “Goodreads vs Amazon: The Effect of Decoupling Book Reviewing and Book Selling” . _Association for the Advancement of Artificial Intelligence_ 2015. Available at<a href="http://piperlab.mcgill.ca/pdfs/Goodreads_ICWSM_2015.pdf">http://piperlab.mcgill.ca/pdfs/Goodreads_ICWSM_2015.pdf</a>
</li>
<li id="dukmasova2018">Dukmasova, Maya. “Chicago Inside Out” . _Places Journal_ (October 2018). Online at<a href="https://placesjournal.org/article/chicago-inside-out/">https://placesjournal.org/article/chicago-inside-out/</a>).
</li>
<li id="english2010">English, James F. “Everywhere and Nowhere: The Sociology of Literature After the Sociology of Literature ” . _New Literary History_ 41:2 (Spring 2010): v-xxiii.
</li>
<li id="evans2018">Elizabeth F. Evans and Matthew Wilkens. “Nation, Ethnicity, and the Geography of British Fiction, 1880-1940” . _CA: Journal of Cultural Analytics_ . 13 July 2018.
</li>
<li id="finkel2005">Jenny Rose Finkel, Trond Grenager, and Christopher Manning. “Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling” . _Proceedings of the 43nd Annual Meeting of the Association for Computational Linguistics (ACL 2005)_ : 363-370.<a href="http://nlp.stanford.edu/~manning/papers/gibbscrf3.pdf">http://nlp.stanford.edu/~manning/papers/gibbscrf3.pdf</a>.
</li>
<li id="fuller2013">Fuller, Danielle, and DeNel Rehberg Sedo. _Reading Beyond the Book: The Social Practices of Contemporary Literary Culture_ . NY: Routledge, 2013.
</li>
<li id="graham2019">Graham, Mark, Rob Kitchin, Shannon Mattern, and Joe Shaw. Eds. _How to Run a City like Amazon, and Other Fables._ Meatspace Press, 2019.
</li>
<li id="grams2008">Grams, Diane. “Creative Reinvention: From One Book to Animals on Parade : How Good Ideas Spread Like Wildfire” . _Entering Cultural Communities: Diversity and Change in the Nonprofit Arts_ , ed. D Carroll Joynes, David Karraker, Morris, Fred, Wendy Norris, Diane Grams, and Betty Farrell. New Brunswick: Rutgers UP, 2008, 194-220.
</li>
<li id="griswold2008">Griswold, Wendy. _Regionalism and the Reading Class_ . Chicago: U of Chicago Press, 2008.
</li>
<li id="griswold2014">Griswold, Wendy, Elizabeth Lenaghan, and Michelle Naffziger. “Readers as Audiences” . _Handbook of Media Audiences_ , ed. Virginia Nightingale. Malden, MA: Wiley Blakcwell (2014): 19-40.
</li>
<li id="griswold2015">Griswold, Wendy, and Hannah Wohl. “Evangelists of Culture: One Book Programs and the Agents who Define Literature, Shape Tastes, and Reproduce Regionalism” . _Poetics_ 50 (June 2015): 96-109.
</li>
<li id="hellekson2014">Hellekson, Karen, and Kristina Busse, eds. _The Fan Fiction Studies Reader_ . Iowa City: U of Iowa Press, 2014.
</li>
<li id="helmond2013">Helmond, Anne. “The Algorithmization of the Hyperlink” . _Computational Culture_ , Issue 3 (2013)<a href="http://computationalculture.net/the-algorithmization-of-the-hyperlink/">http://computationalculture.net/the-algorithmization-of-the-hyperlink/</a>
</li>
<li id="hss">Hit Song Science.<a href="https://en.wikipedia.org/wiki/Hit_Song_Science">https://en.wikipedia.org/wiki/Hit_Song_Science</a>
</li>
<li id="hungerford2016">Hungerford, Amy. _Making Literature Now_ . Palo Alto: Stanford UP, 2016.
</li>
<li id="jenkins2007">Jenkins, Henry. “Transmedia Storytelling 101” . Blog post (21 March 2007).<a href="http://henryjenkins.org/blog/2007/03/transmedia_storytelling_101.html">http://henryjenkins.org/blog/2007/03/transmedia_storytelling_101.html</a>
</li>
<li id="jockers2013">Jockers, Matthew L. _Macroanalysis: Digital Methods and Literary History_ . Urbana-Champaign: U of Illinois Press, 2013.
</li>
<li id="kaser2011">Kaser, James A. _The Chicago of Fiction: A Resource Guide_ . Lanham, MD: Scarecrow Press, 2011.
</li>
<li id="kaufman1990">Kaufman, Leonard, and Peter J. Rousseeuw. “Partitioning around medoids (program pam)” . _Finding Groups in Data: An Introduction to Cluster Analysis_ (1990): 68-125.
</li>
<li id="kidd2013">Kidd, David Comer, and Emanuele Castano. “Reading Literary Fiction Improves Theory of Mind” . _Science_ 342 (18 Oct 2013): 377-80.
</li>
<li id="kirschenbaum2010">Kirschenbaum, Matthew, and Adrian Johns. “Why Books? Session 1 - Storage and Retrieval” . Radcliffe Center for Advanced Studies, Harvard University, 28 October 2010.<a href="https://www.youtube.com/watch?v=os8No9XHjh8">https://www.youtube.com/watch?v=os8No9XHjh8</a>
</li>
<li id="kitchin2017">Kitchin, Rob. “The Timescape of Smart Cities” . The Programmable City Working Paper 35 (27 Nov 2017), 26.<a href="http://progcity.maynoothuniversity.ie/">http://progcity.maynoothuniversity.ie/</a>
</li>
<li id="kozlowski2018">Kozlowski, Michael. “Global Audiobook Trends and Statistics for 2018” . GoodEReader.com (17 Dec 2017)<a href="https://goodereader.com/blog/audiobooks/global-audiobook-trends-and-statistics-for-2018">https://goodereader.com/blog/audiobooks/global-audiobook-trends-and-statistics-for-2018</a>
</li>
<li id="laquintano2016">Laquintano, Timothy. _Mass Authorship and the Rise of Self-Publishing_ . Iowa City: U of Iowa, 2016.
</li>
<li id="levey2016">Levey, Nick. “Post-Press Literature: Self-Published Authors in the Literary Field” . _Post45_ (Feb. 2013).<a href="http://post45.research.yale.edu/2016/02/post-press-literature-self-published-authors-in-the-literary-field-3/">http://post45.research.yale.edu/2016/02/post-press-literature-self-published-authors-in-the-literary-field-3/</a>
</li>
<li id="liu2004">Alan Liu, _The Laws of Cool: Knowledge Work and the Culture of Information_ . Chicago: U of Chicago, 2004.
</li>
<li id="long2003">Long, Elizabeth. _Book Clubs: Women and the Uses of Reading in Everyday Life_ . Chicago: U of Chicago Press, 2003.
</li>
<li id="lynch2017">Lynch, Clifford. “The Rise of Reading Analytics and the Emerging Calculus of Reader Privacy in the Digital World” . _First Monday_ 22.4 (3 April 2017). DOI:<a href="http://dx.doi.org/10.5210/fm.v22i4.7414">http://dx.doi.org/10.5210/fm.v22i4.7414</a>.
</li>
<li id="manovich2013">Manovich, Lev. _Software Takes Command_ . NY: Bloomsbury, 2013.
</li>
<li id="mattern2015">Mattern, Shannon. “Deep Time of Media Infrastructure” . in Lisa Parks and Nicole Starosielski, eds. _Signal Traffic: Critical Studies of Media Infrastructures._ Urbana-Champaign: U of Illinois Press, 2015, 94-112.
</li>
<li id="mattern2016">Mattern, Shannon. “Public In/formation” . _Places Journal_ (November 2016).
</li>
<li id="mattern2017">Mattern, Shannon. “A City Is Not a Computer” , _Places Journal_ (February 2017).
</li>
<li id="mcgrath2019">McGrath, Laura B. “Comping White” . _Los Angeles Review of Books_ (21 January 2019).<a href="https://lareviewofbooks.org/article/comping-white/">https://lareviewofbooks.org/article/comping-white/</a>
</li>
<li id="mcgurl2016">McGurl, Mark. “Everything and Less: Fiction in the Age of Amazon” . _Modern Language Quarterly_ 77.3 (September 2016): 447-71
</li>
<li id="mina2019">Mina, An Xiao. _Memes to Movements: How the World’s Most Viral Media is Changing Social Protest and Power_ . Boston: Beacon Press (2019).
</li>
<li id="moore2018">Moore, Rowan. “The Bilbao Effect: How Frank Gehry’s Guggenheim Started a Global Craze” . _The Guardian_ (01 October 2017). Accessed 19 Dec 2018, and available at:<a href="https://www.theguardian.com/artanddesign/2017/oct/01/bilbao-effect-frank-gehry-guggenheim-global-craze">https://www.theguardian.com/artanddesign/2017/oct/01/bilbao-effect-frank-gehry-guggenheim-global-craze</a>
</li>
<li id="moretti1997">Moretti, Franco. _Atlas of the European Novel 1800-1900_ . New York: Verso, 1997.
</li>
<li id="mumper2019">Mumper, Micah L., and Richard J. Gerrig. “How Does Leisure Reading Affect Social Cognitive Abilities?”  _Poetics Today_ 40.3 (September 2019): 453-73.
</li>
<li id="nakamura2013">Nakamura, Lisa. “ Words With Friends : Socially Networked Reading on Goodreads” . _PMLA_ 128.1 (2013): 238-43.
</li>
<li id="nea2004">National Endowment for the Arts (NEA). _Reading at Risk: A Survey of Literary Reading in America._ Washington, DC: NEA, 2004.
</li>
<li id="nea2007">National Endowment for the Arts (NEA). _To Read or Not to Read: A Question of National Consequence._ Washington, DC: NEA, 2007.
</li>
<li id="nea2009">National Endowment for the Arts (NEA). _Reading on the Rise: A New Chapter in American Literacy._ Washington, DC: NEA, 2009.
</li>
<li id="nea2017">National Endowment for the Arts (NEA). “2017 Survey of Public Participation in the Arts: Reading” . Available at:<a href="https://www.arts.gov/artistic-fields/research-analysis/arts-data-profiles/arts-data-profile-18">https://www.arts.gov/artistic-fields/research-analysis/arts-data-profiles/arts-data-profile-18</a>
</li>
<li id="nunberg2013">Nunberg, Geoffrey. “Noted” , _Chronicle of Higher Education_ , 7 Jan 2013.
</li>
<li id="olmstead2019">Olmstead, Nathan A. “Data and Temporality in the Spectral City” . _Philosophy and Technology_ (pre-print, December 2019): n.p.<a href="https://doi.org/10.1007/s13347-019-00381-8">https://doi.org/10.1007/s13347-019-00381-8</a>
</li>
<li id="palfrey2016">Palfrey, John. “Design Choices for Libraries in the Digital-Plus Era” . _Daedalus_ 145.1 (Winter 2016): 79-86.
</li>
<li id="pariser2011">Pariser, Eli. _The Filter Bubble: What the Internet is Hiding From You_ . NY: Penguin, 2011.
</li>
<li id="pera2010">Pera, Maria Soledad, Nicole Condie, and Yiu-Kai Ng. “Personalized Book Recommendations Created by Using Social Media Data” . In D.K.W. Chiu et al. eds. _WISE: Web Information Systems Engineering_ 2010 Workshops, LNCS 6724, Springer, 2011, 390–403.
</li>
<li id="pinder2012">Pinder, Julian. “Online Literary Communities: A Case Study of Library Thing” . _From Codex to Hypertext: Reading at the Turn of the Twenty-First Century_ , ed. Anouk Lang. Amherst: U of Massachusetts Press, 2012, 68-87.
</li>
<li id="piper2016">Piper, Andrew, and Eva Portelance. “How Cultural Capital Works: Prizewinning Novels, Bestsellers, and the Time of Reading” . _Post45_ (10 May 2016).<a href="http://post45.research.yale.edu/2016/05/how-cultural-capital-works-prizewinning-novels-bestsellers-and-the-time-of-reading/">http://post45.research.yale.edu/2016/05/how-cultural-capital-works-prizewinning-novels-bestsellers-and-the-time-of-reading/</a>
</li>
<li id="porter2018">Porter, J.D. “Popularity/Prestige” . Stanford Literary Lab Pamphlet 17 (September 2018).<a href="https://litlab.stanford.edu/LiteraryLabPamphlet17.pdf">https://litlab.stanford.edu/LiteraryLabPamphlet17.pdf</a>
</li>
<li id="putnam2003">Putnam, Robert D., and Lewis M. Feldstein, with Dan Cohen: _Better Together: Restoring the American Community_ . New York: Simon and Schuster, 2003.
</li>
<li id="radway1997">Radway, Janice. _A Feeling for Books: The Book-of-the-Month Club, Literary Taste, and Middle-Class Desire_ . Chapel Hill: U of North Carolina Press, 1997.
</li>
<li id="geekwire2015">Risley, James. “Seattle Public Library's Free WiFi Hotspot Program Receives Its Funding, 450 More Devices.”  _GeekWire.com_ (2 December 2015).
</li>
<li id="rodriquez2012">Rodriquez, Kepa Joseba, Mike Bryant, Tobias Blanke, and Magdalena Luszczynska. “Comparison of Named Entity Recognition Tools for Raw OCR Text” . _Proceedings of KONVENS 2012_ : 410-414.<a href="http://www.oegai.at/konvens2012/proceedings/60_rodriquez12w/60_rodriquez12w.pdf">http://www.oegai.at/konvens2012/proceedings/60_rodriquez12w/60_rodriquez12w.pdf</a>
</li>
<li id="rooney2005">Rooney, Kathleen. _Reading With Oprah: The Book Club That Changed America_ . Fayetteville: U of Arkansas Press, 2005.
</li>
<li id="rosen2018">Rosen, Jeremy. “Literary Fiction and the Genres of Genre Fiction” . _Post45_ (August 2018)<a href="http://post45.research.yale.edu/2018/08/literary-fiction-and-the-genres-of-genre-fiction/">http://post45.research.yale.edu/2018/08/literary-fiction-and-the-genres-of-genre-fiction/</a>
</li>
<li id="rubery2016">Rubery, Matthew. _The Untold Story of the Talking Book_ . Cambridge: Harvard UP, 2016.
</li>
<li id="sampson2012">Sampson, Robert J. _Great American City: Chicago and the Enduring Neighborhood Effect_ . Chicago: U of Chicago Press, 2012.
</li>
<li id="schnapp2014">Schnapp, Jeffrey T., and Matthew Battles. _The Library Beyond the Book_ . Cambridge: Harvard UP, 2014.
</li>
<li id="shirky2005">Shirky, Clay. “Ontology is Overrated: Categories, Links, and Tags” . Available at:<a href="http://shirky.com/writings/herecomeseverybody/ontology_overrated.html#parable_of_the_ontologist">http://shirky.com/writings/herecomeseverybody/ontology_overrated.html#parable_of_the_ontologist</a>
</li>
<li id="singer2018">P.W. Singer and Emerson T. Brooking. _LikeWar: The Weaponization of Social Media_ . NY: Houghton Mifflin, 2018.
</li>
<li id="sinykin2017">Sinykin, Dan N. “The Conglomerate Era: Publishing, Authorship, and Literary Form, 1965-2007” . _Contemporary Literature_ 58.4 (2017): 462-90.
</li>
<li id="sll2016">Stanford Literary Lab. “Mapping London’s Emotions” . _New Left Review_ (Sept/Oct 2016): 63-91.
</li>
<li id="striphas2009">Striphas, Ted. _The Late Age of Print: Everyday Book Culture from Consumerism to Control._ New York: Columbia UP, 2009.
</li>
<li id="taylor2012">Taylor, Joan Bessman. “Producing Meaning Through Interaction: Book Groups and the Social Context of Reading” . in Anouk Lang, ed. _From Codex to Hypertext: Reading at the Turn of the Twenty-First Century_ . Amherst: U of Massachusetts Press, 2012, 142-58.
</li>
<li id="thomas2011">Thomas, Bronwen. “What is Fanfiction and Why Are People Saying Such Nice Things About It?”  _StoryWorlds: A Journal of Narrative Studies_ 3 (2011): 1-24.
</li>
<li id="tufekci2017">Tufekci, Zeynep. _Twitter and Teargas: The Ecstatic, Fragile Politics of Networked Protest in the 21st Century_ . New Haven: Yale UP, 2017.
</li>
<li id="vadde2017">Vadde, Aarthi. “Amateur Creativity: Contemporary Literature and the Digital Publishing Scene” . _New Literary History_ 48 (2017): 27-51.
</li>
<li id="vlieghe2016">Vlieghe, Joachim, Jal Muls, and Kris Rutten. “Everybody Reads: Reader Engagement with Literature in Social Media Environments” . _Poetics_ 54 (2016): 25-37.
</li>
<li id="weber2016">Weber, Marc. “Self-Fulfilling History: How Narrative Shapes Preservation of the Online World” . _Information & Culture_ 51.1 (2016): 54-80.
</li>
<li id="weinberger2012">Weinberger, David. “Library As Platform” . _Library Journal_ , 4 Sep 2012.
</li>
<li id="wilkens2016">Wilkens, Matthew. “Genre, Computation, and the Varieties of Twentieth-Century U.S. Fiction” . _CA: Journal of Cultural Analytics_ (Nov. 1, 2016). DOI:<a href="https://dx.doi.org/10.31235/osf.io/e7wy6">10.31235/osf.io/e7wy6</a>.
</li>
</ul>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>The phrase grounds Clay Shirky’s influential account of the distinction between browsing and search and the latter’s success: “One reason Google was adopted so quickly when it came along is that Google understood there is no shelf, and that there is no file system” <a class="footnote-ref" href="#shirky2005"> [shirky2005] </a>. On the role of data in design decisions for libraries, in addition to<a class="footnote-ref" href="#schnapp2014"> [schnapp2014] </a>see<a class="footnote-ref" href="#palfrey2016"> [palfrey2016] </a>.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>For historically-informed analysis of the data of contemporary literary production and reception, see<a class="footnote-ref" href="#mcgurl2016"> [mcgurl2016] </a><a class="footnote-ref" href="#hungerford2016"> [hungerford2016] </a><a class="footnote-ref" href="#lynch2017"> [lynch2017] </a>. On the importance ofcomps, a marketing tool still largely unknown to literary scholars, see<a class="footnote-ref" href="#mcgrath2019"> [mcgrath2019] </a>. On the privacy implications of increased for-profit reading platform use, even in public library environments, see<a class="footnote-ref" href="#ard2013"> [ard2013] </a>.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Visualizations, code notebooks, and some data sets are available at the RCR project website:<a href="https://dh.depaul.press/reading-chicago/">https://dh.depaul.press/reading-chicago/</a>&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>The City of Chicago Data Portal, containing downloadable data sets ranging from shared bike journeys to crime statistics, makes “Cook County &hellip; one of the best places in the nation for thinking creatively about the role of government in people’s lives” <a class="footnote-ref" href="#dukmasova2018"> [dukmasova2018] </a>. However, an important examination of smart city hype that imagines “cities as spreadsheets waiting for the right formulas” can be found in<a class="footnote-ref" href="#bratton2015"> [bratton2015] </a>. On the temporality of data ofsmartcities, see<a class="footnote-ref" href="#olmstead2019"> [olmstead2019] </a>. Trenchant critiques of plans to “imagine cities from the internet up” can be found in<a class="footnote-ref" href="#mattern2017"> [mattern2017] </a>and<a class="footnote-ref" href="#graham2019"> [graham2019] </a>.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>And importantly, on Goodreads, LibraryThing, and shared Zotero groups, people publicize taste in ways that make their reading visible to social media analysis methods. As Nakamura points out, “Goodreads invites readers to navigate not in books but in its catalog, to create new catalogs, and to enjoy other people’s collections” since the “site’s main purpose [is] to provide users with familiar tools that encourage them to perform their identities as readers in a public and networked forum” <a class="footnote-ref" href="#nakamura2013"> [nakamura2013] </a>. An early attempt at using LibraryThing data for book recommendations is<a class="footnote-ref" href="#pera2010"> [pera2010] </a>. For quantitative analysis of differences in reviews on Goodreads and Amazon, see<a class="footnote-ref" href="#dimitrov2015"> [dimitrov2015] </a>; for a more recent use of Goodreads for literary sociology, see<a class="footnote-ref" href="#porter2018"> [porter2018] </a>.&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>On audiobooks, see<a class="footnote-ref" href="#rubery2016"> [rubery2016] </a>and<a class="footnote-ref" href="#kozlowski2018"> [kozlowski2018] </a>. On fanfiction,<a class="footnote-ref" href="#thomas2011"> [thomas2011] </a>,<a class="footnote-ref" href="#hellekson2014"> [hellekson2014] </a>, and<a class="footnote-ref" href="#vadde2017"> [vadde2017] </a>. Onpost-press literature, see<a class="footnote-ref" href="#levey2016"> [levey2016] </a>and<a class="footnote-ref" href="#laquintano2016"> [laquintano2016] </a>. An influential account of transmedia storytelling is<a class="footnote-ref" href="#jenkins2007"> [jenkins2007] </a>. The contemporary literary field is dominated by a few large publishers and Amazon.com — the latter on its own facilitating one-half of all U.S. print book purchases and 70% of all e-book purchases<a class="footnote-ref" href="#mcgurl2016"> [mcgurl2016] </a>. See also<a class="footnote-ref" href="#striphas2009"> [striphas2009] </a>and<a class="footnote-ref" href="#sinykin2017"> [sinykin2017] </a>.&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>See also<a class="footnote-ref" href="#collins2013"> [collins2013] </a>. Bob Stein, founder of the Institute for the Future of the Book claims: “in the future … we&rsquo;ll think of a book less as a physical object than as a place to congregate ” (qtd. in<a href="#nunberg2013">Nunberg 2013</a>).&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>In addition to work mentioned above (note 7), see Lev Manovich’s “SelfieCity” (<a href="http://selfiecity.net/">http://selfiecity.net/</a>) and “On Broadway” (<a href="http://www.on-broadway.nyc/">http://www.on-broadway.nyc/</a>). See also<a class="footnote-ref" href="#cranshaw2012"> [cranshaw2012] </a><a class="footnote-ref" href="#boy2016"> [boy2016] </a><a class="footnote-ref" href="#pinder2012"> [pinder2012] </a>.&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>For overviews, see<a class="footnote-ref" href="#davidson1989"> [davidson1989] </a>and<a class="footnote-ref" href="#amory2007"> [amory2007] </a>.&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>On Oprah’s book club, see<a class="footnote-ref" href="#rooney2005"> [rooney2005] </a>and<a class="footnote-ref" href="#striphas2009"> [striphas2009] </a>.&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p><a class="footnote-ref" href="#long2003"> [long2003] </a><a class="footnote-ref" href="#taylor2012"> [taylor2012] </a><a class="footnote-ref" href="#radway1997"> [radway1997] </a>. On demographic difference in book clubs, see<a class="footnote-ref" href="#davis2008"> [davis2008] </a>. Of book group reading choices, Burwell notes that encounters with demographic differences are “more likely to occur through textual engagement than through encounters with other members” (qtd. in<a href="#griswold2014">Griswold et al. 2014, 27</a>).&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p>And see this trenchant reminder thatfilter effectshave a long, non-digital, history: “[Eli] Pariser is certainly right that personalization disguises one of today’s key processes for substituting a narrower world for the world tout court. But in this, the personalization of Web 2.0 may simply be a subset of a larger and longer recommender system that has gone by many names: race, gender, sex, sexuality, ethnicity, class, ability — the original filter bubbles” <a class="footnote-ref" href="#cohen2019"> [cohen2019] </a>.&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13">
<p>On the so-called “Bilbao effect” and its endurance, see<a class="footnote-ref" href="#moore2018"> [moore2018] </a>. On the growing use of people as a distinct medium in contemporary art, see<a class="footnote-ref" href="#bishop2012"> [bishop2012] </a>; see also for Chicago specifically<a class="footnote-ref" href="#grams2008"> [grams2008] </a>. On people as alast miletechnology in city infrastructure, see<a class="footnote-ref" href="#mattern2015"> [mattern2015] </a>and<a class="footnote-ref" href="#barber2013"> [barber2013] </a>.&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14">
<p><a href="http://opendatabook.club/">http://opendatabook.club/</a>. We owe this reference to<a class="footnote-ref" href="#mattern2016"> [mattern2016] </a>. As public libraries increasingly think of themselves as “platforms” , in David Weinberger’s influential phrase<a class="footnote-ref" href="#weinberger2012"> [weinberger2012] </a>, we should also note the growing variety of materials circulated to patrons. Seattle Public Library, for example, created a wifi hotspot checkout program in 2014<a class="footnote-ref" href="#geekwire2015"> [geekwire2015] </a>.&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:15">
<p>But note caveats about<a class="footnote-ref" href="#kidd2013"> [kidd2013] </a>and others in<a class="footnote-ref" href="#mumper2019"> [mumper2019] </a>.&#160;<a href="#fnref:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:16">
<p>Library of Congress Read.gov list:<a href="http://www.read.gov/resources/index.php">http://www.read.gov/resources/index.php</a>.<a class="footnote-ref" href="#griswold2015"> [griswold2015] </a>note creation of their own private database of One Book programs nationwide. They list 567 programs in all 50 states, covering the years 2000 to 2012, numbering 3110 book selections (1506 unique books and 1193 unique authors).&#160;<a href="#fnref:16" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:17">
<p><a class="footnote-ref" href="#fuller2013"> [fuller2013] </a><a class="footnote-ref" href="#grams2008"> [grams2008] </a>quotes Seattle Public Library’s Higashi: “it wasn’t until Chicago chose <em>To Kill a Mockingbird</em> for their One Book One City project that they got national press, a big article in the <em>New York Times</em> . That’s the point at which the project really went boom all over the country and indeed the world.”&#160;<a href="#fnref:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:18">
<p>See<a class="footnote-ref" href="#griswold2015"> [griswold2015] </a>. The social media circuit is changing literary production. Embedding herself at <em>McSweeney’s</em> and other publishing houses, Hungerford finds the cutting edge of publishing now is housed at presses and companies that embed interactive sociality at the core of the reading experience. English departments and establishment publishing houses are improvising ways to respond to these “productionist” tendencies; in the twenty-first century, she asks, “What if literary culture is a culture of making rather than a culture of reading?” <a class="footnote-ref" href="#hungerford2016"> [hungerford2016] </a>. See also<a class="footnote-ref" href="#mcgurl2016"> [mcgurl2016] </a>.&#160;<a href="#fnref:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:19">
<p>Circulation data prior to 2011 was lost in a CPL software migration and is unrecoverable.&#160;<a href="#fnref:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:20">
<p>Note that these data sets are subject to non-disclosure agreements, and we are not able to distribute them.&#160;<a href="#fnref:20" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:21">
<p>See<a class="footnote-ref" href="#putnam2003"> [putnam2003] </a>.&#160;<a href="#fnref:21" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:22">
<p>Our OBOC seasons span seven years in the history of Chicago, 2011-2017, and the demographics of the city were certainly not constant during that time. The ACS data we used is from 2015. Yearly fluctuations in demographic composition of different neighborhoods before and after that time are not reflected in our analysis.&#160;<a href="#fnref:22" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:23">
<p><a href="https://www.hathitrust.org/htrc">https://www.hathitrust.org/htrc</a>.&#160;<a href="#fnref:23" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:24">
<p>We have conducted some analysis of other aspects of our data such as the time-varying patterns of circulation and the interaction between promotion events and circulation, but we will not discuss these results here.&#160;<a href="#fnref:24" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:25">
<p>Indeed, library staff confirmed that branch activity was a key factor in the distribution of OBOC volumes across branches.&#160;<a href="#fnref:25" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:26">
<p>The model was fit using the lmer method from the lme4 package in R. Full model output can be found on the project website at<a href="http://cwi.cdm.depaul.edu/~rburke/circ/multi-level-norm.html">http://cwi.cdm.depaul.edu/~rburke/circ/multi-level-norm.html</a>&#160;<a href="#fnref:26" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:27">
&#160;<a href="#fnref:27" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></li>
<li id="fn:28">
<p><a href="https://pypi.org/project/readability/">https://pypi.org/project/readability/</a>&#160;<a href="#fnref:28" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:29">
<p>See for instance<a class="footnote-ref" href="#moretti1997"> [moretti1997] </a><a class="footnote-ref" href="#bakhtin1983"> [bakhtin1983] </a><a class="footnote-ref" href="#sll2016"> [sll2016] </a><a class="footnote-ref" href="#cordell2015"> [cordell2015] </a><a class="footnote-ref" href="#evans2018"> [evans2018] </a>.&#160;<a href="#fnref:29" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:30">
<p>See our sample “geographical centers” for recent OBOC selections:<a href="http://cwi.cdm.depaul.edu/~rburke/location/geo_center.html">http://cwi.cdm.depaul.edu/~rburke/location/geo_center.html</a>.&#160;<a href="#fnref:30" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:31">
<p>We have begun work on processes to automate Chicago location checking. Reference works such as<a class="footnote-ref" href="#kaser2011"> [kaser2011] </a>makes this, for our subject city at least, a bit easier.&#160;<a href="#fnref:31" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:32">
<p>The GradientBoostingRegressor class in Python’s scikit.learn package was used with parameters optimized by grid search.&#160;<a href="#fnref:32" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:33">
<p>We are in the process of constructing a dashboard with interactive visualizations to support such exploration.## Bibliography&#160;<a href="#fnref:33" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry><entry><title type="html">Reconstruir histórias da conservação da natureza na Califórnia: 1850 – 2010</title><link href="https://rlskoeser.github.io/dhqwords/vol/14/2/000467/?utm_source=atom_feed" rel="alternate" type="text/html"/><link href="https://rlskoeser.github.io/dhqwords/pt/vol/14/2/000467/?utm_source=atom_feed" rel="alternate" type="text/html" hreflang="pt"/><id>https://rlskoeser.github.io/dhqwords/vol/14/2/000467/</id><author><name>Maria J.Ferreira dos Santos</name></author><published>2020-06-19T00:00:00+00:00</published><updated>2023-08-06T15:53:44-04:00</updated><content type="html"><![CDATA[<h2 id="note-on-translation">Note on Translation</h2>
<p>For articles in languages other than English, DHQ provides an English-language abstract to support searching and discovery, and to enable those not fluent in the article&rsquo;s original language to get a basic understanding of its contents. In many cases, machine translation may be helpful for those seeking more detailed access. While DHQ does not typically have the resources to translate articles in full, we welcome contributions of effort from readers. If you are interested in translating any article into another language, please contact us at <a href="mailto:editors@digitalhumanities.org">editors@digitalhumanities.org</a> and we will be happy to work with you.</p>
<h2 id="bibliography">Bibliography</h2>
<ul>
<li id="barth1975">Barth, G.P. _Instant Cities: Urbanization and the Rise of San Francisco and Denver_ . New York: Oxford University Press, 1975.
</li>
<li id="olmstead1929">Olmsted, F.L. “Report of State Parks Survey of California” . California State Park Commission (1929) 106pp
</li>
<li id="santos2014a">Santos, M.J., T. Watt, S. Pincetl. _The push and pull of land use policy: reconstructing 150 years of development and conservation land acquisition_ . PlosONE (2014).
</li>
<li id="santos2014b">Santos, M.J., A. Peers, A. Avery, E. Francis, E. Steiner and J. Coolidge. 2014. “Conservation histories of California: The San Francisco Bay Area” . _Spatial History Project_ . Originally published as part of the 2013 CESTA Anthology,<a href="https://web.stanford.edu/group/spatialhistory/cgi-bin/site/pub.php?id=125&project_id=">https://web.stanford.edu/group/spatialhistory/cgi-bin/site/pub.php?id=125&project_id=</a>.
</li>
<li id="sellers1997">Sellers 1997 Sellers, R.W. _Preserving nature in the National Parks: a history_ . Yale University Press. (1997)
</li>
</ul>
]]></content></entry><entry><title type="html">Tracking the Consumption Junction: Temporal Dependencies between Articles and Advertisements in Dutch Newspapers</title><link href="https://rlskoeser.github.io/dhqwords/vol/14/2/000445/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://rlskoeser.github.io/dhqwords/vol/14/2/000445/</id><author><name>Melvin Wevers</name></author><author><name>Jianbo Gao</name></author><author><name>Kirstoffer L. Nielbo</name></author><published>2020-06-19T00:00:00+00:00</published><updated>2023-08-06T15:53:44-04:00</updated><content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>
<h2 id="advertisements-as-a-lens-on-the-past">Advertisements as a Lens on the Past</h2>
<p>Over the course of the twentieth century, branded consumer goods turned into an integral part of society<a class="footnote-ref" href="#cross2000"> [cross2000] </a><a class="footnote-ref" href="#degrazia2005"> [degrazia2005] </a>. Consequently, researchers turned to consumer goods as an object of research. In addition to writing histories of particular consumer goods, researchers have also conceptualized consumer goods as entry points into broader cultural phenomena. Scholars have, for instance, studied how advertisements represented consumerism, gender identities, and the globalization of food cultures (see<a href="#lears1994">Lears, 1994</a>;<a href="#parkin2007">Parkin, 2007</a>;<a href="#sivulka2012">Sivulka, 2012</a>). In these studies, advertisements functioned as a lens on the past. Roland Marchand argues that advertisements provide an insight into the ideals and aspirations of past realities. He argues that they show the state of technology, the social functions of products, and that they provide information on the society in which a product was sold. Furthermore, Marchand poses that advertisements contributed to the shaping of a “community of discourse” . He claims that advertisements infused public discourse with a particular type of language<a class="footnote-ref" href="#marchand1985"> [marchand1985] </a>.</p>
<p>At the same time, scholars have debated to what extent adverts actually offer a meaningful depiction of the past. In their effort to sell more products, producers, and ad makers amplified or distorted certain social and cultural aspects to make products more appealing to consumers<a class="footnote-ref" href="#brandt2004"> [brandt2004] </a>. Erving Goffman points out that ads shaped consumer’s lived experience by prescribed certain conceptions of identity<a class="footnote-ref" href="#goffman1985"> [goffman1985] </a>. Put differently, ad makers used adverts toshapesociety by tweaking the desires of consumers<a class="footnote-ref" href="#fox1997"> [fox1997] </a>. This form of social engineering was rooted in psychological and sociological theories of the day. Especially the tobacco industry was actively involved in developing new ways of advertising that could entice people to turn to smoking cigarettes<a class="footnote-ref" href="#brandt2004"> [brandt2004] </a>.</p>
<p>These theoretical debates raise the question whether we can actually use advertisements to study the past, or are we merely studying a version of the past constructed by ad makers? Theories on the relationship between advertisements and society can be summarized by three positions. The first position contends that advertisements reflected the desires and aspirations of consumers. The second argues that advertisements merely represented the interest of advertisers and the companies that produced the commodities. This approach attributes more agency to the advertisers. The third approach proposes the existence of a more complexconsumption junction, in which producers, distributors, consumers, and advertisers collectively negotiated the meaning and success of a consumer product<a class="footnote-ref" href="#cowan1987"> [cowan1987] </a>.</p>
<p>This article examines the validity of these three positions in a specific historical context (i.e. twentieth-century Netherlands) and sets out to answer to what extent did advertisements reflect or shape society. We studied this interplay between advertisers and society by analyzing advertisements and articles in newspapers. Newspapers are a well-read rich historical source that contains “conscious representations of conditions and events” as well as “unconscious reflection[s] of the tastes, the interests, the desires, and the spirit of its day” <a class="footnote-ref" href="#smail2008"> [smail2008] </a>. As such, newspapers function as a proxy for public discourse<a class="footnote-ref" href="#marshall1995"> [marshall1995] </a><a class="footnote-ref" href="#schudson1982"> [schudson1982] </a>. Newspapers contained a large number of advertisements. Even though advertisements were a major source of revenue for newspaper publishers, the editors writing the news were generally separated from the people selecting the advertisements<a class="footnote-ref" href="#rooij1974"> [rooij1974] </a><a class="footnote-ref" href="#schreurs2001"> [schreurs2001] </a>. For this reason, we argue that we can view the articles and advertisements in the same source as separate sources of information. The possible relationships between the two are, therefore, more likely to be reflective of relations existing in the <em>real</em> world rather than reflective of choices made by editors. The availability of digitized newspapers enables researchers to use computation to explore the archive, locate particular instances of language use, or extract specific linguistic patterns<a class="footnote-ref" href="#wevers2017"> [wevers2017] </a>.</p>
<p>This study combines techniques from econometrics and complexity science to examine the dynamics of word use in articles and advertisements. Complexity science has been used successfully in the past to model large-scale dynamics of human language, increasing our understanding of social systems and collective dynamics on the macro-scale<a class="footnote-ref" href="#helbing2015"> [helbing2015] </a><a class="footnote-ref" href="#petersen2012"> [petersen2012] </a>. We set out to answer the following three questions. First, did advertisements shape or reflect newspaper discourse? Second, did word use in ads differ in dynamics from articles? And finally, were these characteristics more pronounced for particular product groups? Answering these questions will further our understanding of the dynamics underpinning a complex cultural-historical phenomenon, such as advertising.</p>
<h2 id="beyond-counting-word-frequencies">Beyond Counting Word Frequencies</h2>
<p>With the proliferation of large databases that hold temporally-dispersed text content, time series plots of word frequencies have become a valuable source of exploration and validation of cultural trends<a class="footnote-ref" href="#gao2012"> [gao2012] </a><a class="footnote-ref" href="#michel2011"> [michel2011] </a>. Google has popularized this approach through their web-based n-gram viewer based on its digitized book collection.<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> The popularity of the Google Ngram viewer has sparked other digital archives to also develop n-gram viewers based on their collection.<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup></p>
<p>N-gram viewers can help researchers to determine when words appeared and how they evolved. Plots of word frequencies, however, only offer an overview of particular trends in discourse. To gain more insight into the trends in and between advertisements and articles, we applied two techniques from econometrics and complexity science to the (relative) frequencies of words. First, we applied the Granger causality test to determine whether trends in word use in ads followed trends in articles or vice versa. Second, we analyzed the persistence of words using fractal analysis to identify whether ads differed from articles in terms of dynamics related to word use. Put differently, we determined whether advertising discourse was distinct in its behavior from discourse in articles. Also, we set out to identify specific words that <em>stuck</em> with people? This stickiness can be indicative of social dynamics on a macro-scale. Can we detect behavior of words that transcends individual uses and is more reflective of the existence ofmemoryin communication?</p>
<p>Memory in a time series is modeled as the presence of self-similarity, or more precisely persistent correlation, between the values of these features at various time steps. Merely glancing at visualizations produced by n-gram viewers might show sudden peaks or slow decays in word use, which might suggest dynamic processes reflective of memory. In this paper, however, we quantify such behavior and show the extent to which word use exhibits particular memory functions exhibited in cultural expressions. Jan Assman has described cultural memory as “a collective concept for all knowledge that directs behavior and experience in the interactive framework of a society and one that obtains through generations in repeated societal practice and initiation” <a class="footnote-ref" href="#assman2011"> [assman2011] </a><a class="footnote-ref" href="#assman1995"> [assman1995] </a><a class="footnote-ref" href="#donald2002"> [donald2002] </a>. One of these repeated societal practices is language use, which also shapes our collective understanding of a shared culture. According to Assman, cultural memory is formed over large periods of time whereas communicative memory represents memories shaped over shorter time spans (80-100 years). In a way, communicative memory can be viewed as the short-term memory of a society<a class="footnote-ref" href="#assman2008"> [assman2008] </a><a class="footnote-ref" href="#assman1995"> [assman1995] </a>. Can we use time series of word frequencies in newspapers to detect communicative memory?</p>
<h2 id="methods">Methods</h2>
<h2 id="data">Data</h2>
<p>The National Library of the Netherlands (KB) has digitized thousands of Dutch historical newspapers using optical character recognition (OCR) software.<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup> This software turns scans of physical pages into machine-readable data. Unfortunately, the text extracted from the digital scans is often flawed due to imperfections in the original material or limitations of the recognition software. These material blemishes cause the software to not recognize and transcribe every word correctly, which has resulted in conjoined words, complete gibberish, or words in which certain characters were replaced. The age and quality of the original material are important determinants of the software’s ability to correctly recognize the text; hence, older newspapers contain many more errors than more recent papers. For this reason, we focused on twentieth-century newspapers. Also, the KB does not provide suitable metrics on the quality of the OCR’ed text<a class="footnote-ref" href="#traub2015"> [traub2015] </a>. The study, therefore, assumes that OCR errors are uniformly distributed over the period.</p>
<p>For analysis, we relied on two subsets of the digitized newspaper data. The first subset consisted of the entire set of advertisements (n1=18,564,411) in the KB’s digitized newspaper archive. The second set held newspaper articles (n2=11,465,220) from two national newspapers: <em>De Tijd</em> (1890-1974) and <em>De Telegraaf</em> 1893-1989. During digitization, the OCR software separated articles from advertisements and stored the document type in the metadata, allowing us to select these two types of documents. Because advertisements made up a smaller portion of the newspapers, we selected the entire set of advertisements to make it more comparable to the corpus of articles in terms of size. Also, we narrowed our focus to two national newspapers because these are more likely to represent wider public discourse than regional newspapers.</p>
<p>We calculated keyword frequencies, more specifically, normalized relative daily term frequency per document for these two subsets. We explicitly looked at 265 words (singular and plural forms collapsed) that denoted consumer products. Based on exploratory data analysis using an n-gram viewer, we selected words that appeared in both advertisements and articles throughout greater periods of time with considerable frequency. Brand names were excluded for two reasons. First, brand names often appeared as part of logos in advertisements, making it more difficult to convert these images to machine-readable text. Second, the techniques used in this paper necessitate the existence of the same words over longer periods of time. More often than not, there existed multiple brand names for the same products, which were also not used over longer periods of time.</p>
<h2 id="causal-dependencies">Causal Dependencies</h2>
<p>Several techniques can be used to compare lagged values of time seriesXwith values of a second time seriesYto model variation in their correlation coefficient as a function of temporal displacement. The most widely used technique is cross-correlation, which is simply used to detect the variation in the correlation between two time series as a function of lag. The Granger causality test goes beyond mere correlation and tests for the existence of causal-like dependencies between temporally disjunctive time series of, for instance, words from two sources<a class="footnote-ref" href="#granger1969"> [granger1969] </a>. The test, which originated in econometrics, is based on the assumption that causality is more than temporal disjunction, it involves directionality between time series. The relation tested by the Granger causality test is often characterized as predictive causality and represented asX Granger cause Yto distinguish it from more direct causality<a class="footnote-ref" href="#sugihara2012"> [sugihara2012] </a>. At its core, Granger causality, which is related to correlation, expresses if values of time seriesXcontain information that is uniquely predictive of subsequent values in time seriesY.</p>
<p>For our study, we used the Granger causality test as follows. To identify a <em>shaping</em> relation, we test if variation in a specific word frequency for newspaper discourse (Y) at timetis predicted by variation in the frequency for the same word in advertisement discourse (X) at earlier time stepst-1…t-k. We test forX Granger cause Y, by comparing the performance of the ‘newspaper discourse only’ model:</p>
<p>yt=β0+β1yt-1+…+βkyt-k+ϵ</p>
<p>with the full <em>newspaper and advertisement discourses</em> model:</p>
<p>yt=β0+β1yt-1+…+βkyt-k+α1xt-1+…+αmxt-m+ϵ</p>
<p>to identify which one does the better job at explaining word frequency (yt) based on the residuals. The zero-model for the hypothesis isH0:αi=0for eachiof the element[1,m]with the alternative hypothesis beingH1:αi≠0for at least oneiof the element[1,m]. We applied the test bi-directionally such that a shaping relation finds support if we can confirm that ‘X Granger cause Y’, and in the case of a reflecting relationship we can reject ‘Y Granger cause X’. Finally, if both ‘X Granger cause Y’ and ‘Y Granger cause X’ find support, we viewed this as indicative of a more complex relationship between the two time series.</p>
<h2 id="long-range-dependencies">Long-range Dependencies</h2>
<p>In addition to the Granger Causality test, we used fractal analysis to identify if words exhibited long-range dependencies.<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup> Fractal Analysis is a method to assess the complexity of data. In our case, we look at long-range dependencies, which indicate a rate of decay between two points with increasing time intervals that is slower than exponential decay. Analysis of time-dependent change in complex systems — systems composed of many interacting elements — is an important application of Fractal Analysis.</p>
<p>Some random processes in complex systems are self-affine, that is, fluctuation patterns at shorter time scales are (statistically) similar to fluctuations at longer time scales. In the case of reading, for instance, fluctuations in reading speed are self-affine across multiple time scales, because both reading fluency and comprehension are affected by elements at short time scales (e.g., words and sentences) and longer times scales (e.g., paragraphs and chapters)<a class="footnote-ref" href="#obrien2014"> [obrien2014] </a>. Such fractal behavior is also found in a range of processes related to psychology<a class="footnote-ref" href="#chater1999"> [chater1999] </a>, economy<a class="footnote-ref" href="#marchant2008"> [marchant2008] </a>, sociology<a class="footnote-ref" href="#gao2017"> [gao2017] </a>, health<a class="footnote-ref" href="#eke2002"> [eke2002] </a>, language<a class="footnote-ref" href="#gao2012"> [gao2012] </a>and music<a class="footnote-ref" href="#voss1975"> [voss1975] </a>. We argue, therefore, that fractal analysis has great potential for the study historical trends in cultural expressions<a class="footnote-ref" href="#nielbo2019"> [nielbo2019] </a>. This is particularly the case when we are dealing withbig data, consisting of large sets of mostly unknown parameters<a class="footnote-ref" href="#gao2012"> [gao2012] </a>.</p>
<p>We are interested in a particular kind of fractal processes called1/f2H+1processes, in whichHrefers to the Hurst exponent. The Hurst exponent quantifies the degree of long-range dependencies in a time series (Figure<a href="#figure01">1</a>), such that when0&lt;H&lt;0.5, the time series is an anti-persistent process (i.e., a jump up is followed by a jump down, or vice versa, in the incremental process). WhenH=0.5, the time series only has short-range dependencies, and when0.5&lt;H&lt;1, the time series is characterized by long-range dependencies (i.e. a jump up is followed by another jump up, or vice versa, in the incremental process). It is possible theH&gt;1indicates a non-stationary process. In this study, persistence represents whether a word ‘stuck’ with people and it is in that manner analogous to how scholars have viewed communicative memory.</p>




























<figure ><img loading="lazy" alt="Left: Time series that exhibit anti-persistent (top), short-range (middle), and long-range (bottom) dependencies. Anti-persistent time series oscillate rapidly around its average, which is sometimes referred to as mean-reverting or rigid behavior. Short-range dependencies are indicated by the short cycles, while long-range dependencies show repetitive cycles at multiple time scales. Right: Estimation of the Hurst exponent as the slope of the residual fit F(w) on the time window w for the matching time series in the left column. Anti-persistent time series have a slope &lt; 0.5, the slope for short-range dependencies is 0.5, and long-range dependencies &gt;0.5." src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Figure 1: Left: Time series that exhibit anti-persistent (top), short-range (middle), and long-range (bottom) dependencies. Anti-persistent time series oscillate rapidly around its average, which is sometimes referred to as mean-reverting or rigid behavior. Short-range dependencies are indicated by the short cycles, while long-range dependencies show repetitive cycles at multiple time scales. Right: Estimation of the Hurst exponent as the slope of the residual fit <code>F(w)</code> on the time window <code>w</code> for the matching time series in the left column. Anti-persistent time series have a slope &lt;0.5, the slope for short-range dependencies is 0.5, and long-range dependencies &gt;0.5.
        </p>
    </figcaption>
</figure>
<h2 id="adaptive-fractal-analysis">Adaptive Fractal Analysis</h2>
<p>Adaptive Fractal Analysis (AFA) is a relatively new technique for determining the Hurst exponent of a time series<a class="footnote-ref" href="#gao2011"> [gao2011] </a><a class="footnote-ref" href="#riley2012"> [riley2012] </a>. AFA improves the popular detrended fluctuation analysis<a class="footnote-ref" href="#peng1994"> [peng1994] </a>by identifying a global smoothed trend that can automatically deal with arbitrary, strong nonlinear trends (Gao et al., 2011). The technique is based on a nonlinear adaptive multi-scale decomposition algorithm<a class="footnote-ref" href="#gao2011"> [gao2011] </a>.</p>
<p>After constructing a random walk process from the time series, the initial step of AFA involves partitioning the time series into overlapping segments of lengthw=2n+1, in which neighboring segments overlap byn+1points. In each segment, the time series is fitted with the best polynomial of orderM, obtained using standard least-squares regression. The fitted polynomials in overlapped regions are then combined to yield a single global smoothed trend. Denoting the fitted polynomials for thei-thand(i+1)-thsegments byyi(l1)andy(i+1)(l2), respectively, wherel1,l2=1,⋯,2n+1, we define the fitting for the overlapped region as</p>
<p>y(c)(l)=w1y(i)(l+n)+w2y(i+1)(l),  l=1,2,⋯,n+1,wherew1=(1-l-1n)andw2=l-1ncan be written as(1-dj/n)forj=1,2, and wheredjdenotes the distances between the point and the centers ofy(i)andy(i+1), respectively. Note that the weights decrease linearly with the distance between the point and the center of the segment. Such a weighting is used to ensure symmetry and to effectively eliminate any jumps or discontinuities around the boundaries of neighboring segments. As a result, the global trend is smooth at the non-boundary points and it has the right and left derivatives at the boundary<a class="footnote-ref" href="#riley2012"> [riley2012] </a>.</p>
<p>The global trend can be used to maximally suppress the effect of complex nonlinear trends on the scaling analysis. The parameters of each local fit are determined by maximizing the goodness of fit in each segment. The different polynomials in the overlapped part of each segment are combined such that the global fit will be the smoothest fit of the overall time series. Even if the local fits are linear,M=1, the global trend signal will still be nonlinear. AFA then can be described accordingly: for an arbitrary window sizew, we determine, for the random walk processu(i), a global trendv(i),i=1,2,⋯,N, whereNis the length of the walk. The residual of the fit,u(i)-v(i), characterizes fluctuations around the global trend, and its variance yields the Hurst exponentHaccording to the following scaling equation:</p>
<p>F(w)=[1N∑i=1N(u(i)-v(i))2]1/2∼wH.</p>
<p>By computing the global fits, the residual, and the variance between original random walk process and the fitted trend for each window sizew, we can plotlog2F(w)as a function oflog2w. The presence of fractal scaling amounts to a linear relation in the plot, with the slope of the relation providing an estimate ofH(see Figure<a href="#figure01">1</a>).</p>
<h2 id="design">Design</h2>
<p>To determine whether advertisements reflected or shaped public discourse, we first applied Granger causality tests to each of the 265 keywords, comparing time series from newspaper and advertisement discourse. We hypothesize the existence of the following three Granger causal-like patterns:</p>
<p>Advertisementsshapednewspaper articles as expressed by Granger causality directed exclusively from advertisements to articles;Advertisementsreflectednewspaper articles as expressed by Granger causality directed exclusively from articles to advertisements;A complex, possibly externally-driven, causal pattern as evidenced by cases where Granger causality goes from articles to advertisements <em>and</em> vice versa.</p>
<p>For the second step of the analysis, we used AFA to model the persistence for each keyword in both types of discourse. This enabled us to identify possible dynamic properties of either advertisements and articles as a whole, and possibly the dynamic properties of particular words. Similar to Granger causal-like patterns, the Hurst exponent has three possible patterns of persistence: anti-persistent processes, short-term correlation processes, and persistent processes. Each keyword’s behavior can thus be described by one of nine possible combinations of causality (Granger causality test) and persistence (AFA). Insights into these dynamic properties alongside the causal patterns can help to increase our understanding of the relationship between advertisements and articles, and by extension, between advertisements and society.</p>
<h2 id="data-analysis">Data Analysis</h2>
<p>Statistical tests were conducted with anαlevel of .005<a class="footnote-ref" href="#benjamin2017"> [benjamin2017] </a>. Pearson’s correlation coefficientRwas used to measure the non-lagged association strength between the time series. We converted Pearson’sRusing Fisher’s Z-transformation to normally distributed z-values to permit averaging. Before applying the Granger causality test for comparison of discourses, lag-1 differencing was used to obtain a stationary keyword time series.</p>
<p>For the analysis of the Hurst exponent for each keyword time series per discourse, we used a simple linear regression and compared this with the constant model. This allowed us to test differences in long-range dependencies between the two different discourses. The Shapiro-WilkWtest confirmed that the distribution of the Hurst exponent did not deviate significantly from normality<a class="footnote-ref" href="#shapiro1965"> [shapiro1965] </a>.<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup></p>
<h2 id="results">Results</h2>
<h2 id="directionality">Directionality</h2>
<p>On average, the variance in correlation for each keyword in all the time series was similar between advertisement and articles. The mean correlation coefficient,R¯, between advertisements and the articles in the newspapers <em>De Tijd</em> and <em>De Telegraaf</em> wasR¯=.25andR¯=.27respectively. Sixty-two percent (62%) of these correlations were statistically reliable. The within-newspaper correlation, that is, the correlation between <em>De Tijd</em> and <em>De Telegraaf</em> , was considerably stronger,R¯=.42. Seventy-three percent (73%) of these correlations were significant, suggesting that word use over time in articles between these two newspapers was more similar than between the articles and advertisements.</p>




























<figure ><img loading="lazy" alt="Six keyword frequencies plotted at bi-annual intervals for the two newspapers (News 1: De Tijd; News 2: De Telegraaf) and the advertisements (Ads). The line is smoothed using a simple moving average filter (window size of five years) and gray bands represent confidence intervals at 95%. Notice how Cinema shows a distinct shaping Granger causal pattern, where fluctuations in Ads antecede fluctuations in News. In contrast Potato displays a complex Granger causal pattern, where fluctuations in Ads both seem to antecede and succeed News fluctuations. The Potato system’s spiky behavior during the Dutch famine (1944-45) indicates that war is part of the external cause." src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Figure 2: Six keyword frequencies plotted at bi-annual intervals for the two newspapers (News 1: <em>De Tijd</em> ; News 2: <em>De Telegraaf</em> ) and the advertisements (Ads). The line is smoothed using a simple moving average filter (window size of five years) and gray bands represent confidence intervals at 95%. Notice how ‘Cinema’ shows a distinct shaping Granger causal pattern, where fluctuations in Ads antecede fluctuations in News. In contrast ‘Potato’ displays a complex Granger causal pattern, where fluctuations in Ads both seem to antecede and succeed News fluctuations. The ‘Potato’ system’s spiky behavior during the Dutch famine (1944-45) indicates that war is part of the external cause.
        </p>
    </figcaption>
</figure>
<p>Analysis showed that there was no overarching causal pattern, but rather multiple Granger causal patterns that were keyword-dependent. Twenty percent (20%) of the product terms show evidence of ashapingcausality, in which discursive trends in advertising discourse uniquely predict those in articles. For 17% of the product terms, we found the causal pattern in which advertisementsreflectarticles. Almost half of the product terms (49%) belong to the complex externally-driven category, that is, fluctuations in both advertisements and articles seem to be predicted by extraneous events that perturb the reflecting-shaping dynamic between the time series. Finally, 14% of the terms show no indication of predictive causality.</p>
<p>Almost half of the product terms exhibit complex behavior, pointing towards an external source driving changes in discourse in advertisements and newspapers. These could include economic developments or possibly the invention of new products or technologies. In a way, this underpins that the relationship between advertisements and articles was one of negotiation with external developments. Noteworthy keywords in this category were related to produce (apple,cauliflower,lettuce), energy (stove,cokes,furnace,gasoline) and audiovisual technology (tape recorder,gramophone,radio,television).</p>
<p>There are slightly more keywords for which advertisements were shaping articles than ads reflecting articles. In case of shaping, we detected that behavior in word use in advertisements was related to behavior in articles. Words that exhibited this behavior referred quite generally to fashion and clothing (men’s clothing,sweater,fur,wool,flannel,jeans,heels), interior design (living room,dining room,bedroom) and movies (cinema,film). The keywords that exhibited reflecting behavior were more difficult to categorize into particular categories. They ranged from words such asteapot,dictionary, tocheese. Generally, the words with reflecting behavior seemed to be more specific than the words in the shaping category. For instance, the keywordschair cushionsandwinter coatare specific types of cushions and coats.</p>
<h2 id="detection-of-persistence-in-word-use">Detection of Persistence in Word Use</h2>
<p>The Hurst exponentHfor both articles (t1058=32.8,p&lt;.00001) and advertisements (t538=38.5,p&lt;.00001) was significantly higher when compared to a no-memory baseline (H:M=0.5, SD=0.18). In terms of persistence, discourse in articles and advertisements was different from processes that only showed short-range correlations. AFA found an average difference (ΔH) between articles of 0.21. The dynamics related to word use in articles thus differed from advertisements. Word use in articles was more likely to be a persistent process (H:M=0.89, SD=0.19) than it was in advertisements (H:M=1.1, SD=0.17). Notice that advertisements display non-stationary dynamics withH&gt;1. To test whether the difference between ads and articles was significant, we ran linear regression to predict <em>H</em> as a function of advertisement and articles (advertisement as baseline). Compared to the constant model, a statistically significant regression model was found (χ12=149.1,p&lt;.00001) showing that <em>H</em> was, indeed, reliably lower for articles than for advertisements (articles:β= -0.18 ,SE±0.01,F1,793=163.9p&lt;.00001).</p>
<p>On the whole, word use in articles more clearly expressed persistent trends, while word use in advertisements tended to be more irregular, displaying bursts of high activity followed by little or no activity. This indicates that articles more closely express behavior that could be interpreted as communicative memory, while ads seem more haphazard and perhaps catalytic to the existence of memory in wider public discourse, as represented through articles.</p>
<p>Our findings indicate three distinct types of persistent trends (see<a href="#table01">Table 1</a>). Persistence in only articles, persistence in advertisements and articles, and a lack of persistence in either source. Words that only exhibited persistence in articles included products related to interior design (living room,couch,lamp,bedroom). This suggests that discourse about these products was part of a shared language but that it was not clearly part of an advertising discourse. Conversely, words that showed persistence in both discourses included those related to cigarettes but not to cigars and tobacco (cigarettes), fashion (fur,jeans), energy (cokes,furnace,gasoline) and produce (apple,cauliflower,lettuce). This suggests that advertising discourse for these products was much more persistent and relied on an established frame of reference. Interestingly, cigarettes and fashion are often presented as typical examples of strongly branded products<a class="footnote-ref" href="#blaszczyk2008"> [blaszczyk2008] </a><a class="footnote-ref" href="#brandt2004"> [brandt2004] </a><a class="footnote-ref" href="#hill2002"> [hill2002] </a><a class="footnote-ref" href="#white2012"> [white2012] </a>. Advertisements for produce, on the other hand, might exhibit persistent processes due to its highly seasonal and reoccurring nature (<a href="#gao2012">cf. Gao et al., 2012</a>). Keywords that showed no persistence were related to technology (cinema,tape recorder,gramophone,radio).<br>
Table 1: Example of keywords grouped on type of persistent trend (persistence in articles only, persis- tence in articles and advertisements, and no persistencePersistence in articlesPersistence in ads &amp; articlesNo persistenceLiving roomCigarettesCinemaDining roomFurFilmBedroomWoolTape recorderChairFlannelGramophoneCouchJeansRadioCupboardHeelsTelevisionSeatAppleLampCauliflowerLettuceCokesFurnacegasoline</p>
<h2 id="discussion">Discussion</h2>
<p>Using AFA, we found a significant difference in persistent behavior between word use in advertisements and articles. The latter exhibited long-term dependencies whereas advertisements displayed more non-stationary and irregular behavior. In general, advertisements introduced terms, but many of these terms did not persist and their decay was rapid. For articles, on the other hand, keywords that denoted products showed more persistent behavior and were either mentioned recurrently in a self-reinforcing manner or decayed much slower than advertisements. We speculate that this reflects an overarching media dynamic in which ads introduced keywords and articles represented how these products became part of public discourse. However, this dynamic does not hold for all products as evidenced by Table 1, and, at least partially, by keywords that exhibit areflectingcausal pattern.</p>
<p>The time series of keywords between the two newspapers were more clearly correlated than the time series between the newspapers and advertisements. This shows that word use in newspapers more closely followed each other than word use between advertisements and newspapers. Along with the found difference inHbetween articles and advertisements, the dissimilarity in correlation adds evidence to the hypothesis that the dynamics of discourse in ads are different from articles. This demonstrates that advertisements are not merely a lens on the past, but more clearly adistorted mirrorthat is shaped to a certain degree by advertisers and its own dynamics.</p>
<p>In terms of directionality, we did not find one dominant pattern. For 20% percent of the keywords, advertisements reflected articles, and for 17% of the keywords, advertisements shaped articles. But for almost half of the keywords, there was a more complex causal relationship, indicative of external forces. This lends support to Cowan’s argument for a complex interaction pattern in which advertisers, distributors, producers, and consumers negotiated the meaning of a product<a class="footnote-ref" href="#cowan1997"> [cowan1997] </a>.</p>
<h2 id="product-groups">Product Groups</h2>
<p>The causal direction and type of persistence seems to be, to some extent, related to product type. We were not able to identify specific categories of keywords in thereflectingcausal category. However, the complex relationship andshapingcategory offered interesting groupings of words. The groupings made on the basis of the existence of memory and causal directionality leads to the following four points of discussion.</p>
<p>First, products with a shaping dynamic <em>and</em> long-term dependencies in articles might point towards products that are not constantly advertised — expressed by the lack of persistence in ads — but that nevertheless are part of the cultural life of Dutch consumers throughout the twentieth century, such as bikes, pets, interiors, and clothes. The shaping dynamic reveals that ad makers might have pushed the popularity of these products, which can be described as lifestyle products. One could argue that advertisers might have been able to affect the longevity of these products, effectively installing them within a sharedcommunity of discourse.</p>
<p>Second, one of the most noteworthy behaviors is associated with the cigarette. This product exhibits persistence in advertisements <em>and</em> in articles, and it shows a shaping causal behavior. This suggests that in advertising discourse and in newspaper discourse, cigarettes were a recurring topic that built upon earlier discourse. Moreover, advertisers seemed to be able to shape newspaper discourse on cigarettes. This finding is in line with scholars that view advertisements for cigarettes as the prime example of social engineering<a class="footnote-ref" href="#brandt2004"> [brandt2004] </a>. Our study finds that, at least for the Dutch context, cigarette advertisements were a steadily successful form of advertising. The unique behavior of cigarettes was underlined by the fact that related products such as tobacco and cigars behaved dissimilarly. Tobacco and cigars exhibit no persistence and are driven by a complex causal relationship, underscoring different advertising dynamic than found for cigarettes.</p>
<p>Third, some products revealed persistence in both advertisements and articles without displaying a uniform causal relationship. These products include produce, energy sources, and computer systems. One interpretation might be that produce was of prolonged importance, indicated by the existence of long-range dependencies, yet its importance was not driven by advertisers but by an external source. In the case of produce, this external source could be seasonal or economic shifts. The other two product groups (energy sources and computer systems) were also quite instrumental in society, albeit not for prolonged periods during the twentieth century. The keywords associated with energy were most dominant in the first half of the century, whereas, the words associated with computers only appeared in the latter quarter of the century. Nonetheless, they both still exhibited persistence in newspaper articles.</p>
<p>Finally, keywords associated with technological innovations showed two distinct types of behavior. First, keywords such ascinema,tape recorderandtelevisiondid not exhibit any persistence, which could have resulted from the constant innovation and disruptions in the field of audiovisual technology. Another explanation could be the use of different keywords to refer to similar technologies. Further research is needed to explore this behavior related to technology. Second, we found a distinction in the causal relationship between types of technology.Cinemaandfilmshowed a clear causal relationship between ads and articles. The causal relationship might have resulted from the fact that advertisements played an important role in pushing these innovations to a wider audience. Keywords associated with household technology (radioandtelevision), on the other hand, displayed the complex type of causality. These technological products might be more closely related to particular economic, seasonal, or innovative cycles. Again, further research is needed to untangle these dynamics.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Through two data experiments, we have found evidence of a fundamental difference between the dynamic behavior of word use related to consumer products in articles and advertisements published in newspapers. Articles — taken as a proxy for public discourse — exhibit persistent trends that are likely to be reflective of communicative memory. Contrary to this, advertisements have a more irregular behavior characterized by short bursts and fast decay, which, in part, mirrors the dynamic through which advertisers introduced terms into public discourse. On the issue of whether advertisements shaped or reflected society, we found particular product types that seemed to be collectively driven by a causality going from advertisements to articles. Generally, we found support for a complex interaction pattern that Cowan dubbed the “consumption junction” . Finally, we discovered noteworthy patterns in terms of causality and long-range dependencies for specific product groups.</p>
<p>These findings resonate with Marchand’s claim that advertisements contributed to the “shaping of a community of discourse, an integrative common language shared by an otherwise diverse audience” <a class="footnote-ref" href="#marchand1985"> [marchand1985] </a>. In other words, ads seem to behave as a driver of processes in newspaper articles. Their behavior clearly differs from general discourse, which might stem from the fact that ads are to a large extent driven by ad makers and particular technological innovations.</p>
<p>This study shows how methods from fields of econometrics and complexity science can be applied to improve our understanding of complex cultural-historical phenomena. Further research that employs more extensive keyword lists that also includes brand names and cross-cultural comparisons will make it possible to propose a more detailed and general account of the mechanics that underpin the aforementioned consumption junction.</p>
<h2 id="acknowledgements">Acknowledgements</h2>
<p>Part of this research was performed while the authors were visiting the Institute for Pure and Applied Mathematics (IPAM), which is supported by the National Science Foundation. The newspaper data was provided by the National Library of the Netherlands (KB).</p>
<ul>
<li id="assman2008">Assmann, J. (2008), “Communicative and Cultural Memory” , in Erll, A., Nünning, A. and Young, S.B. (Eds.), _Cultural Memory Studies: An International and Interdisciplinary Handbook_ , Walter de Gruyter, New York, pp. 109–119.
</li>
<li id="assman2011">Assmann, J. (2011), _Cultural Memory and Early Civilization: Writing, Remembrance, and Political Imagination_ , Cambridge University Press, New York.
</li>
<li id="assman1995">Assmann, J. and Czaplicka, J. (1995), “Collective Memory and Cultural Identity” , _New German Critique_ , No. 65, p. 125.
</li>
<li id="benjamin2017">Benjamin, D.J., Berger, J.O., Berk, R., Bollen, K.A., Brembs, B., Brown, L., Camerer, C., et al. (2017), “Redefine statistical significance” , _Nature Human Behaviour_ , available at:<a href="https://doi.org/10.1038/s41562-017-0189-z">https://doi.org/10.1038/s41562-017-0189-z</a>.
</li>
<li id="blaszczyk2008">Blaszczyk, R.L. (Ed.). (2008), _Producing Fashion: Commerce, Culture, and Consumers_ , University of Pennsylvania Press, Philadelphia.
</li>
<li id="brandt2004">Brandt, A. (2004), “Engineering Consumer Confidence in the Twentieth Century” , in Zhou, X. and Gilman, S. (Eds.), _Smoke: A Global History of Smoking_ , Reaktion Books, London.
</li>
<li id="chater1999">Chater, N. and Brown, G.D. (1999), “Scale-invariance as a unifying psychological principle” , _Cognition_ , Vol. 69 No. 3, pp. B17–B24.
</li>
<li id="cowan1987">Cowan, R.S. (1987), “The consumption junction: A proposal for research strategies in the sociology of technology” , in Bijker, W., Hughes, T. and Pinch, T. (Eds.), _The Social Construction of Technological Systems: New Directions in the Sociology and History of Technology_ , MIT Press, Cambridge, pp. 261–80.
</li>
<li id="cowan1997">Cowan, R.S. (1997), “A Proposal for Research Strategies in the Sociology of Technology” , in Bijker, W.E., Hughes, T. and Pinch, T. (Eds.), _The Social Construction of Technological Systems: Papers of a Workshop Held at the University of Twente, The Netherlands, in July 1984_ , MIT Press, Cambridge, pp. 261–280.
</li>
<li id="cross2000">Cross, G. (2000), _An All-Consuming Century: Why Commercialism Won in Modern America_ , Columbia University Press, New York.
</li>
<li id="donald2002">Donald, M. (2002), _A Mind So Rare: The Evolution of Human Consciousness_ , Norton, New York.
</li>
<li id="eke2002">Eke, A., Herman, P., Kocsis, L. and Kozak, L.R. (2002), “Fractal characterization of complexity in temporal physiological signals” , _Physiological Measurement_ , Vol. 23 No. 1, pp. R1–R38.
</li>
<li id="fox1997">Fox, S.R. (1997), _The Mirror Makers: A History of American Advertising and Its Creators_ , University of Illinois Press, Urbana.
</li>
<li id="gao2017">Gao, J., Fang, P. and Liu, F. (2017), “Empirical scaling law connecting persistence and severity of global terrorism” , _Physica A: Statistical Mechanics and Its Applications_ , Vol. 482, pp. 74–86.
</li>
<li id="gao2012">Gao, J., Hu, J., Mao, X. and Perc, M. (2012), “Culturomics meets random fractal theory: insights into long-range correlations of social and natural phenomena over the past two centuries” , _Journal of The Royal Society Interface_ , Vol. 9 No. 73, pp. 1956–1964.
</li>
<li id="gao2011">Gao, J., Hu, J. and Tung, W. (2011), “Facilitating Joint Chaos and Fractal Analysis of Biosignals through Nonlinear Adaptive Filtering” , _PLOS ONE_ , Vol. 6 No. 9, p. e24331.
</li>
<li id="goffman1985">Goffman, E. (1985), _Gender Advertisements_ , Macmillan, London.
</li>
<li id="granger1969">Granger, C.W.J. (1969), “Investigating Causal Relations by Econometric Models and Cross-spectral Methods” , _Econometrica_ , Vol. 37 No. 3, pp. 424–438.
</li>
<li id="degrazia2005">de Grazia, V. (2005), _Irresistible Empire: America’s Advance Through Twentieth-Century Europe_ , Kindle Edition., Belknap Press of Harvard University Press, Cambridge.
</li>
<li id="helbing2015">Helbing, D., Brockmann, D., Chadefaux, T., Donnay, K., Blanke, U., Woolley-Meza, O., Moussaid, M., et al. (2015), “Saving human lives: What complexity science and information systems can contribute” , _Journal of Statistical Physics_ , Vol. 158 No. 3, pp. 735–781.
</li>
<li id="hill2002">Hill, D.D. (2002), _Advertising to the American Woman, 1900-1999_ , Ohio State University Press, Columbus.
</li>
<li id="lears1994">Lears, T.J. (1994), _Fables of Abundance: A Cultural History of Advertising in America_ , Basic Books, New York.
</li>
<li id="marchand1985">Marchand, R. (1985), _Advertising the American Dream: Making Way for Modernity, 1920-1940_ , University of California Press, Berkeley.
</li>
<li id="marchant2008">Marchant, T. (2008), “Scale invariance and similar invariance conditions for bankruptcy problems” , _Social Choice and Welfare_ , Vol. 31 No. 4, pp. 693–707.
</li>
<li id="marshall1995">Marshall, M. (1995), _Contesting Cultural Rhetorics: Public Discourse and Education, 1890-1900_ , University of Michigan Press, Ann Arbor.
</li>
<li id="michel2011">Michel, J.-B., Shen, Y.K., Aiden, A.P., Veres, A., Gray, M.K., The Google Books Team, Pickett, J.P., et al. (2011), “Quantitative Analysis of Culture Using Millions of Digitized Books” , _Science_ , Vol. 331 No. 6014, pp. 176–182.
</li>
<li id="nielbo2019">Nielbo, K.L., Baunvig, K.F., Liu, B. and Gao, J. (2019), “A curious case of entropic decay: Persistent complexity in textual cultural heritage” , _Digital Scholarship in the Humanities_ , Vol. 34 No. 3, pp. 542–557.
</li>
<li id="obrien2014">O’Brien, B.A., Wallot, S., Haussmann, A. and Kloos, H. (2014), “Using Complexity Metrics to Assess Silent Reading Fluency: A Cross-Sectional Study Comparing Oral and Silent Reading” , _Scientific Studies of Reading_ , Vol. 18 No. 4, pp. 235–254.
</li>
<li id="parkin2007">Parkin, K. (2007), _Food Is Love: Advertising and Gender Roles in Modern America_ , University of Pennsylvania Press, Philadelphia.
</li>
<li id="peng1994">Peng, C.-K., Buldyrev, S.V., Havlin, S., Simons, M., Stanley, H.E. and Goldberger, A.L. (1994), “Mosaic organization of DNA nucleotides” , _Physical Review_ , Vol. 49 No. 2, p. 1685.
</li>
<li id="petersen2012">Petersen, A.M., Tenenbaum, J.N., Havlin, S., Stanley, H.E. and Perc, M. (2012), “Languages cool as they expand: Allometric scaling and the decreasing need for new words” , _Scientific Reports_ , Vol. 2, p. 943.
</li>
<li id="riley2012">Riley, M.A., Bonnette, S., Kuznetsov, N., Wallot, S. and Gao, J. (2012), “A tutorial introduction to adaptive fractal analysis” , _Frontiers in Physiology_ , Vol. 3, available at:<a href="https://doi.org/10.3389/fphys.2012.00371">https://doi.org/10.3389/fphys.2012.00371</a>.
</li>
<li id="rooij1974">Rooij, M. (1974), _Kranten: dagbladpers en maatschappij_ , Wetenschappelijke Uitgeverij, Amsterdam.
</li>
<li id="schreurs2001">Schreurs, W. (2001), _Geschiedenis van de Reclame in Nederland_ , Het Spectrum, Utrecht.
</li>
<li id="schudson1982">Schudson, M. (1982), _The Power of News_ , Harvard University Press, Cambridge.
</li>
<li id="shapiro1965">Shapiro, S.S. and Wilk, M.B. (1965), “An Analysis of Variance Test for Normality (Complete Samples)” , _Biometrika_ , Vol. 52 No. 3/4, pp. 591–611.
</li>
<li id="sivulka2012">Sivulka, J. (2012), _Soap, Sex, and Cigarettes: A Cultural History of American Advertising_ , 2nd ed., Wadsworth, Boston.
</li>
<li id="smail2008">Smail, D.L. (2008), _On Deep History and the Brain_ , University of California Press, Berkeley.
</li>
<li id="sugihara2012">Sugihara, G., May, R., Ye, H., Hsieh, C., Deyle, E., Fogarty, M. and Munch, S. (2012), “Detecting causality in complex ecosystems” , _Science_ , Vol. 338 No. 6106, pp. 496–500.
</li>
<li id="traub2015">Traub, M.C., van Ossenbruggen, J. and Hardman, L. (2015), “Impact analysis of OCR quality on research tasks in digital archives” , _International Conference on Theory and Practice of Digital Libraries_ , Springer, pp. 252–263.
</li>
<li id="voss1975">Voss, R.F. and Clarke, J. (1975), “ 1/ f noise in music and speech” , _Nature_ , Vol. 258 No. 5533, pp. 317–318.
</li>
<li id="wevers2017">Wevers, M. and Verhoef, J. (2017), “Coca-Cola: An Icon of the American Way of Life. An Iterative Text Mining Workflow for Analyzing Advertisements in Dutch Twentieth-Century Newspapers” , _Digital Humanities Quarterly_ , Vol. 11 No. 4.
</li>
<li id="white2012">White, C., Oliffe, J.L. and Bottorff, J.L. (2012), “From the Physician to the Marlboro Man: Masculinity, Health, and Cigarette Advertising in America, 1946–1964” , _Men and Masculinities_ , Vol. 15 No. 5, pp. 526–547.
</li>
</ul>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Google Books Ngram Viewer:<a href="https://books.google.com/ngrams">https://books.google.com/ngrams</a>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Examples of ngram viewers: The British Library:<a href="https://www.webarchive.org.uk/ukwa/ngram/">https://www.webarchive.org.uk/ukwa/ngram/</a>, Danish Royal Library:<a href="http://labs.statsbiblioteket.dk/smurf/">http://labs.statsbiblioteket.dk/smurf/</a>, and National Library of the Netherlands<a href="http://kbkranten.politicalmashup.nl">http://kbkranten.politicalmashup.nl</a>&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>These newspapers can be accessed through Delpher:<a href="https://www.delpher.nl">https://www.delpher.nl</a>&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Long-range dependency is also called persistent behavior or long-memory in time series. The terms will be used interchangeably.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>While some keyword time series did show indications of multifractal structure (i.e. local fluctuations with either small or large variation), this information was discarded from the final analysis for the purpose of simplification.## Bibliography&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry></feed>