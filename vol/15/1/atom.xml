<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://gohugo.io/" version="0.116.0">Hugo</generator><link href="https://rlskoeser.github.io/dhqwords/vol/15/1/" rel="alternate" type="text/html" title="html"/><link href="https://rlskoeser.github.io/dhqwords/vol/15/1/index.xml" rel="alternate" type="application/rss+xml" title="rss"/><link href="https://rlskoeser.github.io/dhqwords/vol/15/1/atom.xml" rel="self" type="application/atom+xml" title="Atom"/><updated>2023-10-07T18:26:26+00:00</updated><rights>This work is licensed under a Creative Commons Attribution-NoDerivatives 4.0 International License.</rights><id>https://rlskoeser.github.io/dhqwords/vol/15/1/</id><entry><title type="html">Annotating ritual in ancient Greek tragedy: a bottom-up approach in action</title><link href="https://rlskoeser.github.io/dhqwords/vol/15/1/000538/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://rlskoeser.github.io/dhqwords/vol/15/1/000538/</id><author><name>Gloria Mugelli</name></author><author><name>Federico Boschetti</name></author><author><name>Andrea Bellandi</name></author><author><name>Riccardo Del Gratta</name></author><author><name>Anas Fahad Khan</name></author><author><name>Andrea Taddei</name></author><published>2021-05-21T00:00:00+00:00</published><updated>2021-05-21T00:00:00+00:00</updated><content type="html"><![CDATA[<h2 id="the-project-and-the-annotation-system">The Project and the Annotation System</h2>
<p>This paper will describe a project for the digital annotation of ritual and religious facts in ancient Greek tragedy. The project in question is the result of a collaboration between the Institute of Computational Linguistics A. Zampolli at CNR, Pisa and the Laboratory of Anthropology of the Ancient World at the University of Pisa. As part of this collaboration an annotation system, named  <em>Euporia</em>  has been developed in order to offer digital support to the historico-anthropological study<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>  carried out by the first author, as part of her doctoral research, on the dramatic functions of rituals in ancient Greek tragedy, (<a href="#mugelli2018b">Mugelli 2018b, defended on November 15 2018</a>). The study in question involves the comparison of ancient Greek rituals as represented or described corpus of surviving Greek dramatic texts, with those same rituals as they have been reconstructed by scholars from literary, archaeological and epigraphic sources. The primary sources of this study were the texts of the surviving ancient Greek tragedies, texts which often allude to various kinds of rituals such as sacrifices, supplications, prayers, libations, funerary rites.<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>  Greek tragedies themselves were originally performed as rituals and were performed during religious festivals in honour of the god Dionysus.<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>  Their audiences were mostly composed of Athenian citizens who were participants in these festivals. It is important to note that a fifth century Athenian citizen would have gained a substantial amount of ritual experience through participating in various public religious festivals, in festivals or rites performed as part of a smaller group (e.g. the  <em>demes</em>  or the  <em>phratries</em> ) and in familiar rites performed by the citizens themselves.<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup></p>
<p>The initial point of the departure of this historico-anthropological study was the idea that rituals as represented or described in the texts of ancient Greek tragedies do not constitute a faithful reproduction of the actual rites as they were performed in fifth century Athens. Indeed, when it comes to the representation of rituals in such works, compliance with the ritual norm was only one of the concerns of the tragic authors, along with respect of the performance rules and the function of the ritual in the dramatic plot, see <a href="#didonato2010">Didonato (2010)</a>; <a href="#taddei2015">Taddei (2015)</a>; <a href="#taddei2016">Taddei (2016)</a>. Even though the rites represented in Greek tragedy had to be plausible enough to be recognisable as such by their intended audiences, the authors of Greek tragedy were able to make use of a series of different strategies to adapt the representation of these rites to their tragic plots. These strategies included the use of allusion, the combination of different rites, and even modification of certain aspects of ritual norms <sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>. In any case, the public would have had enough ritual experience to be able to immediately understand every reference to the actual rites as they were practised and to recognize variations from the norm <sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup></p>
<p>The first step of the research was the retrieval and annotation of all the attestations of ritual facts in the corpus of tragic texts, with the purpose of establishing relationships between different passages in the corpus and then comparing this evidence with the rites as they have been reconstructed from other sources. The annotation system, Euporia, which was used to do this work was designed using a user-centred approach that was tailored to answer the specific research question <sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>  <sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>. The system adopts a domain specific language (DSL) <sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup> in order to avoid both a complex graphical user interface and verbose TEI-XML annotation. Translating Euporia DSL into a TEI-XML compliant document is easy as well as necessary in order to guarantee interoperability with other resources.<sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup>  The system is designed to be flexible enough to faithfully simulate the citation practices of classicists. Euporia’s lightweight web user interface enables an entire text which is to be annotated to be visualised in a single page and allows the annotator to easily scroll up and down the page to copy and paste passages in the original text in order to quote them. The unique identifiers (IDs) which are necessary to create machine readable citations are embedded in the hidden HTML tags that surround the segments of the original texts which have been copied and pasted. The DSL deals with portions of text of varying lengths and as well as with discontinuous segments of texts.<sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>  In addition, it also deals with textual operations (substitution, insertion, deletion and transposition), textual and interpretative variants and, finally, with constraints on the variants <sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>. The DSL is based on conventions that are similar to the conventions used in critical apparatuses and in philological, linguistic, historical or literary commentaries <sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup>; <sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup>. In addition it uses other conventions that will be familiar to classicists in the age of social media: for example, the annotations are expressed using Latin hashtags<sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup>  that, as in Twitter messages or del.icio.us taggings, are micro-annotations that can be retrieved in the context of other hashtags, in association with the document chunk that they annotate. The Latin language has been chosen for the sake of compatibility with  <em>Memorata Poetis</em> , a project for the annotation of themes and motives in epigraphic and literary epigrams in Greek, Latin, Arabic and Italian languages (<a href="http://www.memoratapoetis.it">http://www.memoratapoetis.it</a>). Memorata poetis combines a top-down approach (with a Latin taxonomy of an index of  <em>rerum notabilium</em> ), and a bottom-up approach, with unstructured tags that are organized in an ontology in a second phase of the work <sup id="fnref:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup>.</p>
<p>Given the complexity of the ritual facts which were annotated during the course of this research, facts which are difficult to organize in fixed schemes, we decided not to establish a taxonomy or fixed set of tags  <em>a priori</em> . Instead we adopted a completely bottom-up approach which allows the annotator the freedom to create new tags according to his or her needs, as well as to modify or delete of tags and even their hierarchical grouping within an ontology as part of an iterative process. In order to give an overview of the thousand tags which have been created so far, we can categorize the annotations in four different categories. Namely we can identify:<br>
Passages in which an entire ritual is performed (ex.  <code>#sacrificium/sacrifice</code> ,  <code>#supplicatio/supplication</code> ). These tend to be longer passages  Parts of the ritual such as gestures, words, actions, objects (ex.  <code>#victimam_iugulare/slauther_the_victim</code> ,  <code>#gemitus/cry</code> ,  <code>#vestis/dress</code> ,  <code>#culter/cutter</code> ,  <code>#terror/fear</code> ) and even the emotions or the attitudes of those performing a ritual and the various moments or the spaces of the ritual itself  The main implications of the ritual (ex.  <code>#ritus_propositum/purpose_of_rite</code> ,  <code>#ritus_effectus/effects_of_rite</code> ,  <code>#ritus_irritus/ineffective_ritual</code> )  Passages in which the characters or the chorus discuss the form or the implication of a ritual, or give instructions for how to perform it ( <code>#ritum_praescribere/order_the_rite</code> ;  <code>#ritus_parare/prepare_the_rite</code> )</p>
<p>We are also using two macro-categories of tags which are employed in combination with other tags. The tag  <code>#scaenica/on_scene</code>  is used to indicate everything that present on stage (objects, costumes, scenic design) as well as the movements and gestures performed on stage. This tag allows the annotator to indicate whether a rite is actually enacted on stage or whether it is simply described or alluded to. The tag  <code>#hiera/sacred_things</code>  indicates that an action may be considered as part of a ritual that is expected to be effective. This is useful for distinguishing those portions of the text which consist of references, mentions, or elements of a rite from those parts which are conceived of as real rites. In this paper we will illustrate the bottom-up approach adopted in this research by discussing some case studies related to three phases of the work: <a href="#section02">Section 2</a> gives an example of the annotation process by discussing the interpretation of a passage (Aesch.  <em>Ag.</em>  228 ff.) where the annotator has registered multiple interpretive variants; <a href="#section03">Section 3</a> describes the functioning of the Euporia search engine with some examples of multiple-searches that can be performed on the database of the tags; finally, <a href="#section04">Section 4</a> describes the on-going process of the construction of the ontology which organizes and categorizes the tags in the annotation tagset.</p>
<h2 id="an-annotation-case-study-the-sacrifice-of-iphigenia-in-aesch--_ag_--228-ff">An Annotation Case-Study: The Sacrifice of Iphigenia in Aesch.  <em>Ag.</em>  228 ff.</h2>
<p>The representation of sacrifice and other rituals is one of the main themes of Aeschylus’  <em>Agamemnon</em>  and of the entire  <em>Oresteia</em>  trilogy which was performed for the first time in Athens in 458 BCE. The  <em>Oresteia</em>  trilogy makes substantial use of so-called  <em>perverted sacrifice</em> , that is, of sacrificial images and metaphors in the description of violence and murder <sup id="fnref:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup>  Indeed, the main event of the plot of  <em>Agamemnon</em> , the murder of Agamemnon, is described using sacrificial metaphors. In line 1433 of  <em>Agamemnon</em> , for instance, Clytemnestra confirms that she has sacrificed (the verb  <em>sphazo</em>  meaning ritually slaughter) her husband to Ate and to the Erinyes.</p>
<blockquote>
<pre><code> **Κλυταιμήστρα**   1431 καὶ τήνδ’ ἀκούεις ὁρκίων ἐμῶν θέμιν·  1432 μὰ τὴν τέλειον τῆς ἐμῆς παιδὸς Δίκην,   **1433 Ἄτην Ἐρινύν θ’, αἷσι τόνδ’ ἔσφαξ’ ἐγώ,**   1434 οὔ μοι φόβου μέλαθρον ἐλπὶς ἐμπατεῖ,  1435 ἕως ἂν αἴθῃ πῦρ ἐφ’ ἑστίας ἐμῆς  1436 Αἴγισθος, ὡς τὸ πρόσθεν εὖ φρονῶν ἐμοί.     **Clytaemestra** : You will now also hear this righteous oath I swear: by the fulfilled Justice that was due for my child,  **[1433] by Ruin and by the Fury, through whose aid I slew this man,**  no fearful apprehension stalks my house, so long as the fire upon my hearth is kindled by Aegisthus and he remains loyal to me as hitherto; for he is an ample shield of confidence for me.[^10]   
</code></pre>
</blockquote>
<p>In order to annotate this passage we made use of the tags  <strong>#sphage/ritual_slaughter</strong> , that marks all the words in the semantic field of ritual slaughter, and  <code>#homicidium_sicut_sacrificium/homicide_as_sacrifice</code> , to indicate a metaphor that compares a homicide to a human sacrifice.<sup id="fnref:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup>  <br>
☛ [1433 αἷσι τόνδ’ ἔσφαξ’ ἐγώ]  <code>#sphage/ritual_slaughter</code>    <code>#homicidium_sicut_sacrificium/homicide_as_sacrifice</code>  ☚<br>
In lines 228 ff. of  <em>Agamemnon</em>  we come across an example of actual human sacrifice. The tragic chorus recalls and describes the famous sacrifice of Iphigenia, carried out by her father to appease Artemis’ anger and to allow the departure of the Greek army for Troy.</p>
<blockquote>
<pre><code>Χορός    228 λιτὰς δὲ καὶ κληδόνας πατρῴους  229 παρ’ οὐδὲν αἰῶ τε παρθένειον  230 ἔθεντο φιλόμαχοι βραβῆς.  231 φράσεν δ’ ἀόζοις πατὴρ μετ’ εὐχὰν  232  **δίκαν χιμαίρας ὕπερθε βωμοῦ**   233 πέπλοισι περιπετῆ παντὶ θυμῷ προνωπῆ  235  **λαβεῖν ἀέρδην,**  στόματός  236 τε καλλιπρῴρου φυλακᾷ κατασχεῖν  237 φθόγγον ἀραῖον οἴκοις,  238 βίᾳ χαλινῶν τ’ ἀναύδῳ μένει.    
</code></pre>
</blockquote>
<pre><code>  Chorus  
</code></pre>
<p>Her pleas, her cries of father!, and her maiden years, were set at naught by the war-loving chieftains. [231] After a prayer, her father told his attendants  <strong>to lift her right up over the altar with all their strength, like a yearling goat, face down, so that her robes fell around her</strong> , [235] and by putting a guard on her fair face and lips to restrain speech that might lay a curse on his house.</p>
<p>☛ [228 λιτὰς&hellip; 249 ἄκραντοι]  <code>#hiera/sacred_things</code>    <code>#hominem_sacrificare/human_sacrifice</code>  ☚ ☛ [229 παρθένειον]  <code>#virgo/virgin #victima/victim</code>  ☚</p>
<p>Sacrifices of virgins and human sacrifices in general are attested in Greek myth and in ancient Greek tragedy, but they were not a common fact of life in fifth century Athens <sup id="fnref:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup>; <sup id="fnref:20"><a href="#fn:20" class="footnote-ref" role="doc-noteref">20</a></sup>; <sup id="fnref:21"><a href="#fn:21" class="footnote-ref" role="doc-noteref">21</a></sup>; <sup id="fnref:22"><a href="#fn:22" class="footnote-ref" role="doc-noteref">22</a></sup>  <sup id="fnref:23"><a href="#fn:23" class="footnote-ref" role="doc-noteref">23</a></sup>. Since our research is focused on the reception of tragic rituals by the audience of Greek tragedy, our main intention in annotating the scenes of human sacrifices is not to establish relationships between those scenes and traces of human sacrifices in ancient Greece. The tragic scenes of human sacrifice are annotated so they can be used to compare aspects of the ritual (use of objects, attitude of the victims) with the actual ancient Greek animal sacrifice. Various sources represent the sacrifice of Iphigenia in a form that is very close to animal sacrifice; in Euripides’  <em>Iphigenia in Aulis</em>  the young girl is substituted with a deer just before the ritual slaughter. In Aeschylus’  <em>Agamemnon</em>  we can find a trace of the similarity between Iphigenia&rsquo;s sacrifice and an animal sacrifice at line 232, where the girl is lifted up on to the altar  “like a yearling goat”   <sup id="fnref:24"><a href="#fn:24" class="footnote-ref" role="doc-noteref">24</a></sup> . The passage in question is annotated with the tag  <strong>#capra/goat</strong>  specifying the type of animal victim mentioned. The tag  <strong>#virgo_sicut_victima/virgin_as_victim</strong>  can be used either in scenes of human sacrifice or in scenes of homicide, to indicate that a virgin is being compared to an animal victim.<br>
☛ [232 δίκαν χιμαίρας]  <code>#capra/goat</code>    <code>#virgo_sicut_victima/virgin_as_victim</code>  ☚</p>
<p>From line 228 to line 232 (included the words λαβεῖν ἀέρδην at line 235) the sense of the passage is quite clear: Agamemnon, indifferent to his daughter’s prayers, orders the soldiers to lift the girl up on to the altar. The action has to be carried out after the ritual prayer, at the moment of the slaughter of the victim.<br>
☛ [232 δίκαν&hellip; ὕπερθε βωμοῦ ~ 235 λαβεῖν ἀέρδην]  <code>#victimam_tollere/lift_the_victim</code>  ☚<br>
The translation of line 233 is less straightforward and scholars have imagined three different situations, that the annotator chose to register as interpretive variants, since they involve major changes in the interpretation of the ritual.<sup id="fnref:25"><a href="#fn:25" class="footnote-ref" role="doc-noteref">25</a></sup>  The three different solutions depend on the translation of the two words περιπετῆ and προνωπῆ that describe the attitude of Iphigenia at the moment of the sacrifice. According to <sup id="fnref:26"><a href="#fn:26" class="footnote-ref" role="doc-noteref">26</a></sup> πέπλοισι περιπετῆ means enwrapped in her robe and προνωπῆ means prone. <br>
☛ [233 πέπλοισι περιπετῆ]  <code>#victimam_vincire/tie_the_victim</code>    <code>#vestis/dress</code>  Maas1951 ☚<br>
Maas’ interpretation is based on a comparison between the sacrifice of Iphigenia and the sacrifice of Polyxena as it is represented on a well-known black figured amphora, where the young girl is lifted upon the altar by soldiers, with her dress tight to her body.<br>




























<figure ><img loading="lazy" alt="Timiades Painter British Museum" src="/dhqwords/vol/15/1/000538/resources/images/figure01.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000538/resources/images/figure01_huefcd8092b720f115f44fa30ca6725eb0_11898734_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000538/resources/images/figure01_huefcd8092b720f115f44fa30ca6725eb0_11898734_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000538/resources/images/figure01_huefcd8092b720f115f44fa30ca6725eb0_11898734_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000538/resources/images/figure01_huefcd8092b720f115f44fa30ca6725eb0_11898734_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/1/000538/resources/images/figure01_huefcd8092b720f115f44fa30ca6725eb0_11898734_1800x0_resize_box_3.png 1800w,/dhqwords/vol/15/1/000538/resources/images/figure01.png 3368w" 
     class="landscape"
     ><figcaption>
        <p>Attic red-figured amphora, Timiades Painter, ca. 570 BC - ca. 560 BC, London British Museum 1897.7-27.2, © Marie-Lan Nguyen / Wikimedia Commons.
        </p>
    </figcaption>
</figure></p>
<p>According to Maas&rsquo; interpretation the dress is used to tie Iphigenia. The image of the tied sacrificial victim recurs after two lines, when Agamemnon orders to gag Iphigenia to avoid her speaking words of bad omen and cursing her family.<br>
☛ [235 στόματός&hellip; 238 μένει]  <code>#victimae_dissensus/dissent_of_victim</code>    <code>#os_opprimere/hold_the_mouth</code>    <code>#victimam_vincire/tie_the_victim</code>  ☚<br>
The consent of the sacrificial victim has been identified as an essential trait in representations of ancient Greek sacrifice. <a href="#meuli1946">Meuli (1946)</a> invented the theory of the  “comedy of innocence,”  which was subsequently adopted (with different solutions) by <a href="#burkert1972">Burkert (1972)</a> and <a href="#detiennevernant1979">Detiennevernant (1979)</a> followed by <a href="#durand1986">Durand (1986)</a>. The theory, which asserts that participants in a sacrificial ritual felt themselves under the necessity of masking every sign of violence towards it victims,<sup id="fnref:27"><a href="#fn:27" class="footnote-ref" role="doc-noteref">27</a></sup>  was supported by evidence from the iconography of sacrifice which rarely represents tied animals. In addition late sources describing the origins of Greek sacrifice and of the  <em>Bouphonia</em>  festival interpreted by <a href="#durand1986">Durand (1986)</a>, represent the ox voluntary walking towards the altar and offering itself for sacrifice; finally, some sources show the victim nodding at the altar, ideally showing its consent to the sacrifice.<sup id="fnref:28"><a href="#fn:28" class="footnote-ref" role="doc-noteref">28</a></sup>  The idea of the willingness of the sacrificial victim has recently being questioned, however, thanks to a new attention to the sources that shifted the focus to attestations of tied, forced or recalcitrant animals <sup id="fnref:29"><a href="#fn:29" class="footnote-ref" role="doc-noteref">29</a></sup>; <sup id="fnref:30"><a href="#fn:30" class="footnote-ref" role="doc-noteref">30</a></sup>; <sup id="fnref:31"><a href="#fn:31" class="footnote-ref" role="doc-noteref">31</a></sup>; <sup id="fnref:32"><a href="#fn:32" class="footnote-ref" role="doc-noteref">32</a></sup>. The binding and gagging of Iphigenia in Aeschylus’  <em>Agamemnon</em>  is therefore worthy of attention, in contrast with the well-known conclusion of Euripides  <em>Iphigenia at Aulis</em> , where the girl is eventually convinced to offer herself in sacrifice. A comparison with the sacrifice of Polyxena in Euripides’  <em>Hekabe</em>  is useful for the parallels it offers to the Aeschylean passage: at the beginning of the rite a group of young men has been assigned the task to hold the victim. However Polyxena decides to offer herself voluntarily as she refuses to be touched by the soldiers, Eur.  <em>Hec.</em>  525-527, 545-549. E. Medda, in <sup id="fnref:33"><a href="#fn:33" class="footnote-ref" role="doc-noteref">33</a></sup> and in his recent commentary <sup id="fnref:34"><a href="#fn:34" class="footnote-ref" role="doc-noteref">34</a></sup>, has reassessed for the word πρoνωπή in Aesch.  <em>Ag.</em>  233, a translation proposed by LSJ9 and adopted by <a href="#fraenkel1950">Fraenkel (1950)</a>, according to which the expression means  “to take her as she fell, fainting forward.”  <br>
☛ [233 προνωπῆ]  <code>#animo_relictus/pass_out</code>  Fraenkel 1950 ☚<br>
This interpretation, which is corroborated by a comparison with the red-figured oinochoe of the Schuwalow painter, does not exclude the image of the young girl being enwrapped in her dress, but does exclude any explicit dissent on the part of the victim (at least at this moment of the ritual) which forces the soldiers to bind her.<br>




























<figure ><img loading="lazy" alt="oinochoe schuwalow painter" src="/dhqwords/vol/15/1/000538/resources/images/figure02.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000538/resources/images/figure02_hu6637bcc3fc0b358cb62fb5b639d7147f_8132634_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000538/resources/images/figure02_hu6637bcc3fc0b358cb62fb5b639d7147f_8132634_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000538/resources/images/figure02_hu6637bcc3fc0b358cb62fb5b639d7147f_8132634_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000538/resources/images/figure02_hu6637bcc3fc0b358cb62fb5b639d7147f_8132634_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/1/000538/resources/images/figure02_hu6637bcc3fc0b358cb62fb5b639d7147f_8132634_1800x0_resize_box_3.png 1800w,/dhqwords/vol/15/1/000538/resources/images/figure02.png 3264w" 
     class="landscape"
     ><figcaption>
        <p>Attic red-figured oinochoe, Schuwalow painter, 430-420 a.C. Kiel inv. B 538.
        </p>
    </figcaption>
</figure></p>
<p>A third interpretation of the passage, proposed by <a href="#lloydjones1990">Lloyd Jones (1990)</a> and adopted by <a href="#bonanno2006">Bonanno (2006)</a> in her article on the dramatic function of the messenger&rsquo;s speech describing the sacrifice in the  <em>Agamemnon</em> , shows Iphigenia supplicating to her father by touching his dress (literally falling with the arms around his dress). This interpretation entails some radical changes to our sacrificial image: we have to imagine Iphigenia falling forward, and grasping her father&rsquo;s knees. If we accept this interpretation then we have to add two tags that mark two common gestures in scenes of supplication: the act of falling at someone&rsquo;s knees and the gesture of touching someone&rsquo;s dress.<br>
☛ [233 πέπλοισι &hellip; προνωπῆ ]  <code>#supplicatio/supplication</code>  legit Lloyd Jones ☚ ☛ [233 πέπλοισι περιπετῆ]  <code>#vestem_tangere/touch_the_dress</code>  (cum 233  <code>#supplicatio/supplication</code>  Lloyd Jones) ☚ ☛ [233 προνωπῆ]  <code>#ad_genua_accidere/fall_at_knees</code>  (cum 233  <code>#supplicatio/supplication</code>  Lloyd Jones) ☚<br>
This last interpretation would certainly imply an act of dissent the part of Iphigenia, who in the middle of the sacrificial ritual and after the ritual prayer, continues begging her father and attempting to move him to pity. This hypothesis of the supplication also changes the reading of the two words παντὶ θυμῷ, with all the heart in line 233: in the two previous cases they refer to sacrificants who have to follow Agamemnon&rsquo;s order without hesitation. If we assume that Iphigenia is in the act of supplicating then the two words are referred to the suppliant&rsquo;s attitude.<br>
☛ [233 παντὶ θυμῷ]  <code>#animus_supplicis/attitude_of_suppliant</code>  (cum 233  <code>#supplicatio/supplication</code>  Lloyd Jones) ☚ ☛ [233 παντὶ θυμῷ}  <code>#animus_sacrificantis/attitude_of_sacrificant</code>  (recusando 233  <code>#supplicatio/supplication</code>  Lloyd Jones) ☚<br>
The supplication of Iphigenia to her father is known from Euripides&rsquo;  <em>IA</em>  1211 where the young girl is still trying to escape death (as we noticed earlier, in Eur.  <em>IA</em>  Iphigenia is represented as a willing victim at the very moment of the sacrifice). Scenes of supplication to Agamemnon are also shown on two terracotta reliefs which were directly inspired by the Euripidean play <sup id="fnref:35"><a href="#fn:35" class="footnote-ref" role="doc-noteref">35</a></sup> .</p>
<h2 id="searching-the-database">Searching the Database</h2>
<p>In the second phase of the project, once the annotation had been carried out using the criteria illustrated above, an SQL-based search engine was developed in order to query and to subsequently reorganize the database of the hashtags (The first version of EuporiaSearch is available at <a href="http://cophilab.ilc.cnr.it/euporiaSearchx/">http://cophilab.ilc.cnr.it/euporiaSearchx/</a>). The Euporia search engine allows users to search the database by matching up to three keywords which are represented as hashtags. The results show the list of the passages on which the three keywords co-occur along with the list of other hashtags co-occuring with the keywords on those passages. The user can specify:<br>
Up to three different keywords;  The range of words within which the keywords should co-occur (ex. 0, 0 for the intersection between the keywords, 10, 10 for a query on 2 hashtags in a range of +/- 10 words);  The range of words on which the hashtags visualized in the results have to co-occur (e.g. 10, 10 to show all the hashtags co-occurring within a range of +/- 10 words from the passages retrieved)</p>
<p>An example will help to clarify the operation of the search engine. For instance, in order to retrieve all of the passages in the corpus in which a virgin is the victim of a sacrifice the user can match the 2 keywords  <strong>#virgo/virgin</strong>  and  <strong>#victima/victim</strong>  (within a range of 0 words) and ask the system to show all the other hashtags coinciding within a range of +/- 10 words. EuporiaSearch will then list all of the passages retrieved; the entire list of other hashtags coinciding with the passage is also displayed by clicking on each result. Using the search engine, we can perform multiple searches that check the coherence and the consistency of the tags. We decided to start testing our first search engine from the tragedies related to the Atreides’ cycle in order to have a well defined sub-corpus for our initial trials of the annotation system. A few examples dealing with the theme of the perverted sacrifice and the interference between homicide and sacrifice will help to illustrate the first results which we obtained.</p>
<p>Since we made the decision to annotate only those actions which were actually conceived and performed as rituals with the tag  <code>#sacrificium/sacrifice</code>  or  <code>#hominem_sacrificare/human_sacrifice</code> , a search into the intersection between homicide and sacrifice will return those cases of homicides carried out within the context of a sacrifice, like the killing of Aegisthus during the sacrifice to the Nymphs in Euripides’  <em>Electra</em> . A search using the tag  <code>#homicidium_sicut_sacrificium/homicide_as_sacrifice</code>  returns the passage of the killing of Agamemnon discussed above (Aesch.  <em>Ag.</em>  1433) and the passage of the killing of Clytaemestra in Euripides’  <em>Electra</em>  (Eur.  <em>El.</em>  1142). A broader perspective on sacrificial metaphors can be gained by carrying out queries using both tags  <code>#homo_sicut_victima/human_as_victim</code>  and  <code>#virgo_sicut_victima/virgin_as_victim</code> . These will return a list of passages in which a human being is compared to a sacrificial victim amongst which there are various passages referring to Iphigenia’s sacrifice in Eur.  <em>IA</em> , where the virgin is sacrificed as if she were an animal victim. We will also find Aesch.  <em>Ag.</em>  1297, where Cassandra is invited to enter the house of Agamemnon to perform a sacrifice: how comes it that you are walking boldly towards it like an ox driven by god to the altar? The hashtag list shown in the search results will help users to understand the various contexts of the different occurrences of the keyword: for example, a query on the single hashtag  <code>#libatio/libation</code>  returns all the occurrences of libation events. Browsing the list of co-occurring hashtags returned by this query, the user can verify if the libation is performed on stage (as it is in Aesch.  <em>Cho.</em> , 15 where the tag #libatio/libation co-occurs with the tag  <code>#scaenica/on_stage</code> ) or whether it is simply mentioned or discussed by the characters (as it is for example in Soph.  <em>El.</em>  52, where Electra and her sister discuss a libation that has to be performed on Agamemnon’s tomb, and the tag  <code>#libatio/libation</code>  co-occurs with the tags  <code>#ad_ritum_ire/go_to_rite</code>  and  <code>#tumulus/tomb #extra_scaenam/off_stage</code> ).</p>
<h2 id="structuring-the-tags">Structuring the Tags</h2>
<p>Semantic technologies can be exploited to identify and structure the knowledge embedded in literary texts. They can support experts in defining hierarchical and associative relationships between semantically annotated chunks of text denoting relevant entities and thus allow for the visual structuring of knowledge. This knowledge, formally coded as part of an ontology, can then be used by scholars and students as an aid to further analysis of the text. In particular, the use of semantic technologies aims at facilitating intelligent searches on the text in a more sophisticated way in comparison to traditional keyword-based search as a means to the discovery of implicit information/knowledge. In the next two subsections, we will introduce the basics of formal ontologies along with the approach that we will use for structuring the tags, respectively.</p>
<h2 id="knowledge-representation-background-and-standards">Knowledge Representation Background and Standards</h2>
<p>Formal ontologies have nowadays become a standard means of representing knowledge about concepts and the relations among them in various different domains. The term ontology itself derives from philosophy and was first applied within the field of Information Systems in the late seventies to describe formal representations of knowledge about a given domain typically expressed in a manner that can be easily processed by machines (in a way that unstructured text generally cannot). More specifically, an ontology (in the Information Systems sense of the term) is a computational resource that explicitly represents the types of entities that can exist in a domain, the properties these entities can have, and the relationships they can have to one another. It also describes how these entities are decomposed into parts, and the events and the processes in which they can participate. A number of ontology definition languages has been developed over the past few years. One of the most popular such languages is the Ontology Web Language (OWL <a href="https://www.w3.org/OWL/">https://www.w3.org/OWL/</a>) <sup id="fnref:36"><a href="#fn:36" class="footnote-ref" role="doc-noteref">36</a></sup>, a recommendation of the W3C (<a href="https://www.w3.org">https://www.w3.org</a>). OWL was designed to meet the need for an ontology language for the Semantic Web (RDF: <a href="https://www.w3.org/standards/semanticweb">https://www.w3.org/standards/semanticweb</a>). It is based on the use of a common data framework, the Resource Description Framework (<a href="https://www.w3.org/RDF">https://www.w3.org/RDF</a>) in which the knowledge is represented as a series of statements each of which is in the form of a  <code>&lt;subject-predicate-object&gt;</code>  triple. These triples usually consist of three separate resource IDs (although they can also be so called blank nodes or literals in the case of objects), so called uniform resource identifiers (URIs), and this allows for the easy representation of such knowledge as graph structures. <a href="#figure03">Figure 3</a> (a) provides a simple example showing the involvement of participants in a sacrifice, by listing some RDF triples, such as  <code>&lt;_h2,rdf:type,Human&gt;</code>  meaning that _h2 is a Human,  <code>&lt;_h2,rdf:predicate,_s&gt;</code>  meaning that _h2 participates to the sacrifice _s, and so on.<br>




























<figure ><img loading="lazy" alt="knowledge structure participants in a sacrifice" src="/dhqwords/vol/15/1/000538/resources/images/figure03.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000538/resources/images/figure03_hud50419d69ba0e23a5afd178298ed3aff_58623_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000538/resources/images/figure03_hud50419d69ba0e23a5afd178298ed3aff_58623_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000538/resources/images/figure03.png 940w" 
     class="landscape"
     ><figcaption>
        <p>(a) RDF fragment of knowledge example. (b) Correspondent OWL ontology. (c) pseudocode of SPARQL query that retrieves all the sacrifices (i.e., the related text passages) which have _h1 or _h2 as participant.
        </p>
    </figcaption>
</figure></p>
<p>.</p>
<p>OWL is a decidable fragment of first order logic and a so called Description Logic (DL) <sup id="fnref:37"><a href="#fn:37" class="footnote-ref" role="doc-noteref">37</a></sup>, a class of languages specially intended for purposes of Knowledge Representation (KR). Ontologies created in OWL can be split up into two parts: an intentional and an extensional part. The former, the so called TBox, contains knowledge about concepts (i.e., classes) and complex relations between them (i.e., roles). The latter part, the so called ABox, contains knowledge about entities (i.e., individuals) and how they relate to the classes and roles from the TBox. <a href="#figure03">Figure 3</a> (b) shows the same example as before but encoded in OWL. OWL provides a sophisticated mathematical semantics for the interpretation of RDF triples: the resource Sacrifice is interpreted as a set, Sacrifice, which stands for the class of sacrifices, the resource Human is a set, Human, standing for the class of humans, and the predicate is defined as a semantic relation between the class Human and the class Sacrifice. Formally, the domain of participatesIn is the class Human and its range is defined by the class Sacrifice (note that we could further elaborate the semantic of participatesIn, by for example defining cardinality constraints, and so on). One of the most widely used tools for managing OWL ontologies is called Protégé (<a href="https://protege.stanford.edu/">https://protege.stanford.edu/</a>). It is free, open-source and is supported by a strong community of users working both in academia as well as in industry.</p>
<p>Another important Semantic Web related standard is SPARQL (<a href="https://www.w3.org/TR/rdf-sparql-query/">https://www.w3.org/TR/rdf-sparql-query/</a>), a powerful query language which allows users to make queries over RDF knowledge bases by carrying out graph pattern matching on RDF triples. <a href="#figure03">Figure 3</a> (c) shows a query related to the example. We assume that the individuals to be modelled take part in two different relations, for example hasStartingIndex and hasEndingIndex, which specify the sections in the text to which the related tags refer. The example shows how it is possible to retrieve all text passages referring to sacrifices in which at least one of the two specific humans (_h1 and _h2) is a participant.</p>
<p>One of the great benefits of developing Semantic Web based resources is the fact that it is straightforward to reuse and to link to other datasets. In this case, for example, we can make use of other already existing semantic web ontologies to provide a layer of more general, abstract concepts without having to create these from scratch ourselves.</p>
<h2 id="the-bottom-up-approach">The Bottom-up Approach</h2>
<p>The annotation strategies adopted on the passages of Aeschylus’  <em>Agamemnon</em>  and which were discussed in <a href="#section02">Section 2</a> should have made it clear that the variation of details, such as the consciousness or the willingness of the victim, its gestures or movements, the presence of blood and the use of particular garments, can strongly affect the tragic representation of the ritual. This is because every ritual has its own norms, varying through space and time, that govern the ritual: its actions, words and gestures, the use of objects in it, and even the right mood in which to perform the rite. At the same time, ritual norms are not always strictly followed. They can be modified depending on the circumstances or (in certain cases) they can even be subverted. Moreover, the same gestures, actions or words can have different meanings depending on the context.<sup id="fnref:38"><a href="#fn:38" class="footnote-ref" role="doc-noteref">38</a></sup></p>
<p>The bottom-up approach adopted in this project allows the annotator to create and to organize the tags in order to follow the complex intersections between textual, interpretative and ritual issues. The organization of the tags is an iterative process, performed as a process of ontology construction establishing relations between single tags and groups of tags. More precisely we organise the tags by defining the different relations between ritual actions, gestures, words, objects as part of a formal ontology. This allows us to connect a single action to the ritual contexts in which it can be performed, and to indicate the main implications of a ritual performed in a certain way.<sup id="fnref:39"><a href="#fn:39" class="footnote-ref" role="doc-noteref">39</a></sup></p>
<p>As was shown in the annotation example in <a href="#section02">Section 2</a>, the sacrifice of Iphigenia in Aeschylus’ Agamemnon is annotated in our corpus as a human sacrifice with a virgin in the role of victim and as discussed in <a href="#section03">Section 3</a>, passages like this can be retrieved using the search engine, by matching two keywords (virgin and victim, or human sacrifice and virgin). However, a query on these keywords would not be sufficient to find all the occurrences of sacrifices of virgins in the database. In fact it became evident during the annotation process that there exist a number of tragic passages that, although they refer to human sacrifice in general, are actually alluding to the sacrifice of a virgin, even if the identity of the victim is not directly stated in the text. This happens, for example, in Euripides  <em>IA</em> , where a large part of the drama relies on the ambiguity between the rites for the fake marriage of Iphigenia and Achilles and the sacrifice of Iphigenia herself <sup id="fnref:40"><a href="#fn:40" class="footnote-ref" role="doc-noteref">40</a></sup>, <sup id="fnref:41"><a href="#fn:41" class="footnote-ref" role="doc-noteref">41</a></sup>. In this tragedy it is therefore important to distinguish between passages alluding to sacrifices in general and passages alluding to the sacrifice of Iphigenia. The annotator found it useful, for the sake of clarity, to mark the sacrifices of virgins with the more specific tag  <code>#virginem_sacrificare/sacrifice_of_virgins</code> . The two annotation strategies (the more general tag  <code>#hominem_sacrificare/human_sacrifice</code>  with the specification of the type of victim involved, or the more specific tag  <code>#virginem_sacrificare/sacrifice_of_virgins</code> ) are not contradictory and can be managed in the construction of the ontology.</p>




























<figure ><img loading="lazy" alt="SPARQL query sacrifice virgins" src="/dhqwords/vol/15/1/000538/resources/images/figure04.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000538/resources/images/figure04_hu0155484800593f31a2fed10feb4e6b61_45978_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000538/resources/images/figure04_hu0155484800593f31a2fed10feb4e6b61_45978_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000538/resources/images/figure04.png 707w" 
     class="landscape"
     ><figcaption>
        <p>(a) Hypothesis of a fragment of knowledge concerning Sacrifices and victims. (b) pseudocode of SPARQL query that retrieves all the sacrifices of virgins.
        </p>
    </figcaption>
</figure>
<p>We can do this by arranging the tags in a hierarchy so that whenever we carry out a query using the more general tag we will also get all the results from the more specific tags. In our case it means that it will be possible using a single query to retrieve the whole set of texts marked as human sacrifices along with the two subsets of the sacrifices of virgins and the sacrifices of other human victims. Figure 2 shows a possible arrangement of some of the relevant classes in a taxonomy. There the class ‘Human Sacrifice’ is split into two different kinds, ‘Virgin Sacrifice’ and ‘Other Sacrifice’; similarly we have also arranged the potential victims of a sacrifice into a hierarchy. The definition of the different types of human sacrifice is the starting point for more complex queries such as that which we describe below.</p>
<p>As we demonstrated in the annotation example in <a href="#section02">Section 2</a>, the attitude of the victim is an important trait in the representation of sacrifices, and it is one of the characteristics of the rite that allows the comparison to be made between human and animal sacrifice. The attitude of the sacrificial victim is expressed by two opposing tags, namely,  <code>#victimae_consensus/consent_of_victim</code>  and  <code>#victimae_dissensus/dissent_of_victim</code> ). A simple query on the two tags returns all the occurrences of victims (either human or animal) who either consent to or dissent from the ritual taking place. The tag  <code>#victimae_consensus/consent_of_victim</code> , for example, marks the passages of Eur.  <em>IA</em>  where Iphigenia consents to being sacrificed and the passage of Eur.  <em>IT</em> , 469 where Orestes and Pilades, victims of a fake human sacrifice, are untied so they can go to the ritual of their own volition. Among the occurrences of willing victims there is also Aesch.  <em>Ag.</em>  1297, cited above, where Cassandra is compared to an ox willing to approach the sacrifice. Among the attestations of the  <code>#victimae_dissensus/dissent_of_victim</code>  tag there are the passages of Aeschylus’  <em>Agamemnon</em>  cited above, various passages of Eur.  <em>IA</em>  preceding the voluntary sacrifice (for example Eur.  <em>IA</em>  1243 where Iphigenia is supplicating to her father), and Eur.  <em>Hel.</em>  1559, where an ox refuses to get in the ship where it will be sacrificed. Both the consent and the dissent of the victim are attitudes that can be adopted by the sacrificial victims, either animal or human.<br>




























<figure ><img loading="lazy" alt="SPARQL query consensual victims" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>(a) Hypothesis of fragment of knowledge concerning attitude of victims. (b) pseudocode of SPARQL query that retrieves all the sacrifices of consensual virgins.
        </p>
    </figcaption>
</figure></p>
<p>The construction of the ontology will also help researchers to retrieve and study so-called perverted sacrifices. As we described previously, in <a href="#section02">Section 2</a> and in the example given in <a href="#section03">Section 3</a>, the tag  <code>#homicidium_sicut_sacrificium/homicide_as_sacrifice</code>  marks all those passages in which the comparison between homicide and sacrifice is explicitly stated in the text or where it is clear to the annotator. It is possible to say that a homicide is being described in sacrificial terms by a tragic author if the description includes details that are unequivocally associated with rituals. This dramatic mechanism is recognisable (and will have been recognised by an audience), for example, in cases when the ritual slaughter  <code>#sphage</code>  is mentioned in the description of a homicide, when a murder is carried out on an altar <sup id="fnref:42"><a href="#fn:42" class="footnote-ref" role="doc-noteref">42</a></sup> or when it is carried out with ritual objects. The perception of an interference between homicide and sacrifice on the part of the tragic audience should not, however, be taken for granted in all the passages that describe a homicide with ritual features. It would be useful, however, for further research on sacrifice and supplication in Greek tragedy, if it were possible to retrieve the largest possible number of passages where an interference between homicides and rituals may be present, in order to give scholars and experts easy access to important textual evidence. To this end, in our ontology we can create a subclass of Instruments called Ritual Instruments that covers instruments used in ritual (see <a href="#figure06">Figure 6</a>). We can then define the class of events  <code>Homicide_as_Sacrifice</code>  using a logical axiom (presented below in the diagram in a description logic formalism) as the intersection of all events that are classified as a Homicide and where the instrument used belongs to Ritual Instrument.<br>




























<figure ><img loading="lazy" alt="ontology ritual intruments" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Hypothesis of fragment of knowledge concerning the event Homicide as Sacrifice.
        </p>
    </figcaption>
</figure></p>
<p>A query on all the events in the class Homicide as Sacrifice will return the list of the tragic passages where a homicide is carried out by means of a ritual instrument and where the mechanism of the so-called perverted sacrifice may be involved. The resulting list of passages would be extremely interesting for the domain expert, who will be able to study the dramatic function of the tragic passages and to compare them with the ritual practices that can be reconstructed from other, different kinds of sources. In doing this, however, the domain expert will have to take into account the plasticity of the ancient Greek ritual norm: in the actual ritual practices as well as in their representations we can observe variations from the norm which do not always result in a perversion of the ritual process. The case of the ritual instruments used in violent context is exemplary: as a matter of fact, the set of ritual objects that can be used as weapons in murders is hard to define, and it is an interesting starting point for the further development of our ontology. The texts of Greek tragedy are rarely accurate as far as the technical lexicon of sacrificial instruments is concerned. The word μάχαιρα, denoting the sacrificial knife, is used only twice in the corpus of extant tragedies (Aesch.  <em>Pers.</em>  56 and Eur.  <em>Supp.</em>  1206) and twice in Euripides’  <em>Cyclops</em> . In Euripides  <em>Electra</em> , where the ritual competence of Orestes in the sacrificial butchery of the victim is at stake, the two technical terms σφαγίς and κοπίς are employed to indicate two different types of knives, the second of which is specifically chosen to kill Aegisthus. The terms generally used to indicate the sacrificial knife are the two unmarked terms indicating swords, ξίφος and φάσγανον <sup id="fnref:43"><a href="#fn:43" class="footnote-ref" role="doc-noteref">43</a></sup>. These types of knives can be used as ritual instruments, and they are actually used for slaughtering victims in various tragic human sacrifices. At the same time, however, they should not be considered as instruments exclusively for use in rituals since they are used as instruments in battle and in other contexts. Some similar observations can be made concerning the axe used by Clytaemestra in the killing of Agamemnon (see <a href="#section02">Section 2</a>). The double-axe (πέλεκυς) is not part of the hoplite armour, and it was not used as a weapon in Classical Greece. Being primarily an instrument for felling trees, it was not a ritual object per se. At the same time, its use in animal killings can be interpreted as a sacrificial use: various sources attest the presence of the double-axe in sacrificial contexts, and it is used in Homer to stun sacrificial victims of large size. A query on the intersection between the two keywords #ascia/axe and  <code>#homicidium/homicide</code>  returns various mentions of Agamemnon’s killing by his wife (e.g. Soph.  <em>El.</em>  486, Eur.  <em>El.</em>  170, 279, 1160).</p>
<p>A further development of our ontology may include the distinction between objects that are unequivocally ritual instruments, objects that are not ritual instruments but may be used in rites and finally objects that are to be considered ritual instruments when used in a certain way. The insertion of the killer-axe and of the technical terms denoting ritual knives in a set of ritual instruments to be retrieved in the context of murders and violent actions could help interpreters of Greek tragedy to cast new light on various aspects of the so-called perverted sacrifice. A search on murders that present ritual features (e. g. that are accomplished with ritual instruments) will return a series of passages that are not explicitly characterized as perverted rituals, but that may be insisting on the tragic ambiguity between homicide and sacrifice.</p>
<h2 id="conclusion">Conclusion</h2>
<p>In this paper we have discussed a system for the annotation of a corpus of Greek tragic texts that focuses on the ritualistic and religious aspects of those texts. The annotation system adopts a bottom-up approach, three different phases of which have been described in this paper: first there was the annotation stage proper, with the possibility of registering different textual and interpretive variants, then there was the design of a search engine that allows the database to be tested, and finally the on-going construction of an ontology of the tags. The system was designed to answer a specific research question, described in the introductory section of this paper, and therefore finds its first application in the retrieval of tragic passages in support of this research. As we have shown in <a href="#section03">Section 3</a>, the search engine allows the domain expert to perform multiple-searches that return all those portions of the text which attest to the annotated phenomena. The passages which are retrieved as a result have of course to be studied in their textual and dramatic context (the list of hashtags shown in the search results is extremely useful for a quick look at the context of the passage). At the same time, since the construction of the ontology is based on the ritual norms reconstructed both from tragic texts themselves and from historical sources (as was described in <a href="#section04">Section 4</a>), the system offers users the possibility of performing queries on various aspects of Greek ritual such as the consenting of the sacrificial victim to the ritual or the so-called perverted sacrifice. This research on sacrifice and supplication in Greek tragedy is based on the idea that, in the tragic representations of rituals, variations from the norm could be recognized and understood by the audience. From this perspective, the lists of passages resulting from the queries illustrated above will help the domain expert to broaden the corpus of his/her sources, and to question his/her sources in a different way. The annotation combines textual variants as well as modern interpretations on the tragic texts and theories on ancient ritual based on different sources, and it strongly depends on the needs and the choices of the annotator. Therefore, the search results should not be taken to be certain attestations of given phenomena, but they can be read as signs of possible ambiguities resulting from a comparison between the actual ritual norm and its tragic representation. For instance, the list of passages resulting from the query pertaining the murders accomplished with ritual instruments discussed at the end of <a href="#section06">Section 6</a>, is not to be taken as a list of all the attestations of the so-called perverted sacrifice in the corpus of Greek tragic texts, but as a list of all the tragic passages where the public could have perceived a ritualistic connotation in the description of a violent action. Analysing the passages in their context, the domain expert will be able to verify if the tragic text actually refers to the ritual and relies on the ritual skills of the audience.</p>
<p>As we have described above, the peculiarity of the annotation system lies in the fact that it was designed to study religious aspects of ancient Greek civilisation on the basis of a well-defined corpus of literary texts. In future work we plan to make the tagset available (both in TEI format and as Linked Open Data) along with the ontology in order to allow these resources to be reused in other projects. We can imagine at least two axes of possible interchanges: projects for the annotation of themes and motifs in ancient Greek texts, and projects aiming to study ritual and religion as they are attested in other corpuses of ancient Greek sources (e.g. Greek epic and historical texts).</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>On the history and epistemology of Historical Anthropology of the Ancient World see <a href="#didonato1990">Didonato (1990)</a> and <a href="#didonato2013">Didonato (2013)</a>.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>On the religious context of Ancient Drama see <a href="#pickardcambridge1968">Pickard (1968)</a>; <a href="#didonato2002">Didonato (2002)</a>; <a href="#sourvinouinwood2003">Sourvinouinwood (2003)</a>.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>The ritual dimension of Greek tragedy has been widely discussed by <a href="#winklerzeitlin1990">Winklerzeitlin (1990)</a>; <a href="#seaford1998">Seaford (1998)</a>; <a href="#scullion2002">Scullion (2002)</a>; <a href="#calame2013">Calame (2013)</a>; <a href="#calame2017">Calame (2017)</a>.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>The works by Robert Parker <sup id="fnref:44"><a href="#fn:44" class="footnote-ref" role="doc-noteref">44</a></sup>; <sup id="fnref:45"><a href="#fn:45" class="footnote-ref" role="doc-noteref">45</a></sup>; <sup id="fnref:46"><a href="#fn:46" class="footnote-ref" role="doc-noteref">46</a></sup>; offer a complete discussion of the religious life of fifth century Athens. On the public of Greek tragedy see <a href="#csaposlater1994">Csaposlater (1994)</a>; <a href="#sommerstein1997">Sommerstein (1997)</a>; <a href="#loscalzo2008">Loscalzo (2008)</a>.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>Mugelli, G.  “Eracle e il sacrificio interrotto: immagini tragiche di sacrificio nelle Trachinie di Sofocle e nell’Eracle di Euripide.”    <em>Scienze dell&rsquo;Antichità</em> , 23, 3 (2018): 123-139.&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>See <a href="#taddei2009">Taddei (2009)</a> and <a href="#taddei2014">Taddei (2014)</a>; in general, on the competence of the tragic public see <a href="#revermann2006">Revermann (2006)</a>.&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>Hemminger, B.  “NeoNote. Suggestions for a Global Shared Scholarly Annotation System” ,  <em>D-Lib Magazine</em>  15 (May/June 2009) (5/6).&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>Gibbs, F. and Owens, T.  “Building Better Digital Humanities Tools: Toward broader audiences and user-centered designs” ,  <em>Digital Humanities Quarterly</em> , 6, 2 (2012).&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>Parr, T.  <em>Language implementation patterns: create your own domain-specific and general programming languages</em> , Raleigh NC, (2010).&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>The annotation system and the DSL are described in <a href="#mugelli2016">Mugelli (2016)</a>. The current version of our DSL grammar is available at <a href="https://github.com/CoPhi/euporia"> (last access: 15/07/2019).</a>&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>In the annotation, the three dots (&hellip;) indicate continuity and the tilde (~) indicates discontinuity.&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p>Boschetti, F.  “Annotations in collaborative environments.”    <em>Studia Graeco Arabica</em>  3 (2013): 185-194.&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13">
<p>Boschetti, F.  “Alignment of variant readings for linkage of multiple annotations.”  In P. Zemanek (ed), Proceedings of the ECAL 2007: electronic corpora of ancient languages, Praha (2008), pp. 11-24.&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14">
<p>Lamé, M., Sarullo, G. et al.  “Technology &amp; Tradition: A Synergic Approach to Deciphering, Analyzing and Annotating Epigraphic Writings” ,  <em>LEXIS</em>  33 (2015): 9-30.&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:15">
<p>For the sake of readability in this paper the tags are cited in the form  <code>#hashtag/english_translation</code> .&#160;<a href="#fnref:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:16">
<p>Khan, F., Arrigoni, S., Boschetti, F., Frontini, F.  “Restructuring a Taxonomy of Literary Themes and Motifs for More Efficient Querying.”    <em>Estudios Literários Digitales</em> , 2, 4, (2016): 11–27.&#160;<a href="#fnref:16" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:17">
<p><a href="#zeitlin1965">Zeitlin (1965)</a> was the first to introduce, for the  <em>Oresteia</em> , the notion of  <em>perverted sacrifice</em> . On the history of the notion of perverted ritual and its applicability to Greek tragedy, see <a href="#henrichs2004">Henrichs (2004)</a>. See also <a href="#vidalnaquet1972">Viadlnaquet (1972)</a> on the metaphorical use of sacrifice and hunting in the  <em>Oresteia</em> .&#160;<a href="#fnref:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:18">
<p>All the tags in the form  <em>X_as_Y</em>  marks explicit metaphors or comparisons, e. g. in Eur.  <em>Hel.</em>  we used the tag  <code>#tumulus_sicut_altaria/tomb_as_altar</code>  to mark that a tomb is used as an altar in supplication.&#160;<a href="#fnref:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:19">
<p>Bonnechere, P.  <em>Le sacrifice humain en Grèce ancienne</em>  Kernos Suppléments 3, Presses universitaires de Liège, Athènes-Liège (1994).&#160;<a href="#fnref:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:20">
<p>Georgoudi, S.  “À propos du sacrifice humain en Grèce ancienne: remarques critiques”    <em>Archiv für Religionsgeschichte</em> , 1 (1999): 61–82.&#160;<a href="#fnref:20" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:21">
<p>Bonnechere, P. and Gagné, R.  <em>Sacrifices humains. Perspectives croisées et représentations.</em>  Presses universitaires de Liège, Liége (2013).&#160;<a href="#fnref:21" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:22">
<p>Nagy, A.A., Prescendi, F.  <em>Sacrifices humains : dossiers, discours, comparaisons actes du colloque tenu à l’Université de Genève, 19-20 mai 2011</em> , Bibliothèque de l’École des hautes études. Brepols, Turnhout (2013).&#160;<a href="#fnref:22" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:23">
<p>Georgoudi, S.  “Le Sacrifice humain dans tous ses états”    <em>Kernos</em> , 28 (2015): 255-273.&#160;<a href="#fnref:23" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:24">
<p>On the lifting of the sacrificial victim see <sup id="fnref:47"><a href="#fn:47" class="footnote-ref" role="doc-noteref">47</a></sup> and <sup id="fnref1:45"><a href="#fn:45" class="footnote-ref" role="doc-noteref">45</a></sup>. Cfr. the Attic black-figured amphora from Vulci, 550 ca BCE, Museo Nazionale Archeologico, Rocca Albornoz, Viterbo <a href="http://www.beazley.ox.ac.uk/record/6F03DCDE-FEBD-4317-9490-2C84BE43618B">http://www.beazley.ox.ac.uk/record/6F03DCDE-FEBD-4317-9490-2C84BE43618B</a>.&#160;<a href="#fnref:24" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:25">
<p>Our annotation registers only the variants or interpretations that involve changes in the description of the ritual, see <a href="#mugelli2016">Mugelli (2016)</a>. On a similar selection process for textual variants see the project  <em>Musisque Deoque</em> , <sup id="fnref:48"><a href="#fn:48" class="footnote-ref" role="doc-noteref">48</a></sup>  <a href="http://mizar.unive.it/mqdq/public/">http://mizar.unive.it/mqdq/public/</a>.&#160;<a href="#fnref:25" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:26">
<p>Maas, P.  “Aeschylus,  <em>Agamemnon</em>  231 ff. illustrated” ,  <em>Classical Quarterly</em>  (1951): 94.&#160;<a href="#fnref:26" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:27">
<p>The concept of victim as it is conceived in modern languages was absent from the Ancient Greek language <sup id="fnref:49"><a href="#fn:49" class="footnote-ref" role="doc-noteref">49</a></sup>.&#160;<a href="#fnref:27" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:28">
<p>The nodding of the victim was theorized by <a href="#burkert1972">Burkert (1972)</a>. For a list of all the sources  <em>pro et contra</em>  the theory of the willing victim see <a href="#naiden2007">Naiden (2007)</a>. On the history of the different modern interpretations of Ancient Greek animal sacrifice see <a href="#parker2011">Parker (2011)</a> and <a href="#naiden2013">Naiden (2013)</a>.&#160;<a href="#fnref:28" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:29">
<p>Bonnechere, P.  “La μάχαιρα était dissimulée dans le κανοῦν?” ,  <em>REA</em> , 101 (1999): 21–35.&#160;<a href="#fnref:29" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:30">
<p>Georgoudi, S.  “L’occultation de la violence dans le sacrifice grec: données anciennes, discours modernes” . In S. Georgoudi, R. Koch Piettre, F. Schmidt (eds),  <em>La cuisine et l’autel. Les sacrifices en questions dans les sociétés de la Méditerranée Ancienne</em> . Turnhout, Brepols (2005).&#160;<a href="#fnref:30" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:31">
<p>Georgoudi, S.  “Le consentement de la victime sacrificielle : une question ouverte” . In V. Mehl, P. Brulé, R. Parker (eds),  <em>Le sacrifice antique. Vestiges, procédures et stratégies</em> . Presses universitaires de Rennes, Rennes (2008).&#160;<a href="#fnref:31" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:32">
<p>Naiden, F. S.  “The Fallacy of the Willing Victim” ,  <em>Journal of Hellenic Studies</em> , 127 (2007): 61-73.&#160;<a href="#fnref:32" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:33">
<p>Medda, E.  “Ifigenia all’altare. Il sacrificio di Aulide tra testo e iconografia” .  <em>EIKASMOS</em> , XXIII (2012): 87–114.&#160;<a href="#fnref:33" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:34">
<p>Medda, E.  <em>Eschilo. Agamennone. Edizione critica, traduzione e commento</em> . Accademia nazionale dei Lincei, Roma (2017).&#160;<a href="#fnref:34" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:35">
<p>Terra-cotta bowl, II century bce, BerlinStaatl.Mus.3161=  <em>LIMC</em>  3.2 s.v.  <em>Iphigenia</em> , 9.&#160;<a href="#fnref:35" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:36">
<p>Heflin, J.  “An Introduction to the OWL Web Ontology Language.”  Lehigh University. National Science Foundation (NSF) (2007).&#160;<a href="#fnref:36" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:37">
<p>Baader, F., Horrocks, I., Sattler, U.  “Description logics as ontology languages for the semantic web.”    <em>Mechanizing Mathematical Reasoning</em> . Springer Berlin Heidelberg (2005): 228-248.&#160;<a href="#fnref:37" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:38">
<p>The concept of ritual norm has recently been investigated, see <sup id="fnref:50"><a href="#fn:50" class="footnote-ref" role="doc-noteref">50</a></sup>. In particular relating to the corpus of the so-called Greek Sacred Laws, see <a href="#parker2004">Parker (2004)</a>, <a href="#chaniotis2009">Chaniotis (2009)</a>, <a href="#carbonpirennedelforge2012">Carbon Pirennedel Forge (2012)</a>. A digital collection of Greek Ritual Norms is under construction at the University of Liège ( <a href="http://web.philo.ulg.ac.be/thiasos/cgrn-collection-of-greek-ritual-norms/">http://web.philo.ulg.ac.be/thiasos/cgrn-collection-of-greek-ritual-norms/</a>).&#160;<a href="#fnref:38" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:39">
<p>The structure of our ontology is described in <a href="#mugelli2017">Mugelli (2017)</a>, where we also discuss the use of the ontology itself within a system for querying the annotated corpus.&#160;<a href="#fnref:39" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:40">
<p>Foley, H.P.  “Marriage and Sacrifice in Euripides’ Iphigeneia in Aulis” .  <em>Arethusa</em>  15 (1982): 159–180.&#160;<a href="#fnref:40" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:41">
<p>Foley, H.P.  <em>Ritual irony. Poetry and Sacrifice in Euripides</em> . Cornell University Press, Ithaca London (1985).&#160;<a href="#fnref:41" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:42">
<p>Durand, J.-L. and Lissarrague, F.  “Mourir à l’autel. Remarques sur l’imagerie du «sacrifice humain» dans la céramique attique” .  <em>Archiv für Religionsgeschichte</em>  (1999): 83–106.&#160;<a href="#fnref:42" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:43">
<p>Bruit Zaidman, L.  “Objets rituels tragiques chez Euripide.”    <em>Revue de l’histoire des religions</em>  (2014): 581–598.&#160;<a href="#fnref:43" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:44">
<p>Parker, R.  <em>Athenian Religion. A History</em> . Clarendon Press, Oxford (1996).&#160;<a href="#fnref:44" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:45">
<p>Parker, R.  <em>Polytheism and Society at Athens</em> . Oxford University Press, Oxford (2005).&#160;<a href="#fnref:45" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:45" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:46">
<p>Parker, R.  <em>On greek religion</em> . Cornell University Press, London Ithaca (2011).&#160;<a href="#fnref:46" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:47">
<p>Van Straten, F.  <em>Hierà Kalà: Images of Animal Sacrifice in Archaic and Classical Greece.</em>  Brill, Leiden (1995).&#160;<a href="#fnref:47" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:48">
<p>Mastandrea, P.  “Gli archivi elettronici di Musisque deoque. Ricerca intertestuale e cernita fra varianti antiche (con qualche ripensamento sulla tradizione indiretta dei poeti latini)” . In P. Mastandrea, L. Zurli (eds), Poesia latina. Nuova e-filologia. Opportunità per l’editore e per l’interprete: atti del convegno internazionale, Perugia, 13-15 settembre 2007. Herder, Roma (2009) pp. 41–72.&#160;<a href="#fnref:48" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:49">
<p>Brulé, P. and Touzé, R.  “Le hiereion: phusis et psuchè d’un medium.”  In Mehl, V. (ed)  <em>Le sacrifice antique: vestiges, procédures et stratégies</em> , Presses universitaires de Rennes, Rennes (2008): 111–138.&#160;<a href="#fnref:49" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:50">
<p>Brulé, P.  <em>La norme en matière religieuse en Grèce ancienne: Actes du XIIe colloque international du CIERGA (Rennes, septembre 2007).</em>  Presses universitaires de Liège, Liège (2009).&#160;<a href="#fnref:50" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry><entry><title type="html">Can an author style be unveiled through word distribution?</title><link href="https://rlskoeser.github.io/dhqwords/vol/15/1/000539/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://rlskoeser.github.io/dhqwords/vol/15/1/000539/</id><author><name>Giulia Benotto</name></author><published>2021-05-21T00:00:00+00:00</published><updated>2021-05-21T00:00:00+00:00</updated><content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>Stylometry, that is the application of the study of linguistic style, offers a means of capturing the elusive character of an author’s style by quantifying some of its features. The basic stylometric assumption is that each writer has a  “human stylome”   <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>, that is a set of certain stylistic idiosyncrasies that define their style. Analysis based on stylometry is often used for Authorship Attribution (AA) tasks, since the main idea behind computationally supported AA is that by measuring some textual features, it is possible to distinguish between texts written by different authors <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. One of the less investigated stylistic features is the way in which authors use words from a semantic point of view, e.g. if they tend to use more, when dealing with polysemous words, a certain sense over the others, or senses that differ (even slightly) from the one that is more commonly used (as it happens, typically, in poetry). It would then be interesting to discover if the semantics an author bestows to words is actually part of its  “human stylome.”  Computing semantics, though, is far from easy.</p>
<p>A possible approach to the analysis of this characteristic is to consider the textual contexts in which certain words appear. According to Distributional Semantics (DS), certain aspects of the meaning of lexical expressions depend on the distributional properties of such expressions, or better, on the contexts in which they are observed <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>  <sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>. The semantic properties of a word can then be defined by inspecting a significant number of linguistic contexts, representative of the distributional behavior of such word.</p>
<p>In this work, we would like to investigate if the analysis of the distribution of words in a text can be exploited to provide a stylistic cue. In order to inspect that, we have experimented with the application of DS to the stylometric analysis of literary texts belonging to a corpus constituted by texts pertaining to the work of six Italian writers of the late nineteenth century. In the following, Section 2 both provides a brief survey on the works related to stylometry and introduces the fundamental Distributional Semantics concepts on which this works relies upon. Section 3 describes the approach together with the corpus used to conduct our investigation and Section 4 discusses results. Finally, Section 5 draws some conclusions and outlines some possible future works.</p>
<h2 id="background-knowledge">Background Knowledge</h2>
<h2 id="stylometry">Stylometry</h2>
<p>Even if the first attempt at computing the writing style of an author dates back to the first half of the 20th century (<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>  <sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>  <sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>), the work that is believed to be the starting point of the so-called non-traditional Authorship Attribution is a study by <a href="#MostellerWallace1964">Mosteller and Wallace (1964)</a>. They conducted an investigation on the authorship of the  “Federalist Paper,”  a series of political essays written by John Jay, Alexander Hamilton, and James Madison, 12 of which have ambiguous paternity, being claimed by both Hamilton and Madison. From then on (to, at least, the end of the 1990s), research in AA mainly coincided with stylometry, i.e. defining features to quantify the style of an author by using measures based on counting frequencies of words, characters, and sentences <sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>  <sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup>.</p>
<p>Despite their working well, these systems followed a methodology that underwent some limitations, such as the little statistical homogeneity of the analyzed texts or the fact that the evaluation of the developed methods was mainly intuitive, using corpora that were not controlled for topic; moreover the lack of benchmark data made it difficult to compare different methods. These problems were partially overcome at the end of the 1990s, when the internet made a vast amount of electronic texts available, also highlighting all different areas in which AA could be useful, beyond that of literary research (i.e. intelligence <sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup>, criminal and civil law <sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>  <sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>, computer forensic <sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup> and so on). Nowadays, the main emphasis on AA tasks regards the objective evaluation of the proposed methods using common benchmark corpora <sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup>.</p>
<p>As previously stated, the very first attempts to analyze the style of an author were mainly based on lexical features, such as sentence length count or words length count, which can be applied to any language and corpus without additional requirements <sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup>  <sup id="fnref:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup>  <sup id="fnref:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup>  <sup id="fnref:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup>. Character measures, too, have been proven to be useful in quantifying the writing style. A text can then be viewed as a sequence of characters on which various measures (such as alphabetic, digit, uppercase and lowercase characters count) can be defined <sup id="fnref:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup>  <sup id="fnref:20"><a href="#fn:20" class="footnote-ref" role="doc-noteref">20</a></sup>  <sup id="fnref:21"><a href="#fn:21" class="footnote-ref" role="doc-noteref">21</a></sup>. A more elaborate text representation method is based on the assumption that authors tend to use similar syntactic patterns, so syntactic information is employed, being considered a more reliable authorial fingerprint than lexical information <sup id="fnref:22"><a href="#fn:22" class="footnote-ref" role="doc-noteref">22</a></sup>  <sup id="fnref:23"><a href="#fn:23" class="footnote-ref" role="doc-noteref">23</a></sup>  <sup id="fnref:24"><a href="#fn:24" class="footnote-ref" role="doc-noteref">24</a></sup>  <sup id="fnref:25"><a href="#fn:25" class="footnote-ref" role="doc-noteref">25</a></sup>  <sup id="fnref:26"><a href="#fn:26" class="footnote-ref" role="doc-noteref">26</a></sup>.</p>
<p>Very few attempts to exploit high-level features for stylometric purposes have been made, due to the fact that tasks such as full syntactic parsing, semantic analysis, or pragmatic analysis cannot yet be handled adequately by current NLP technologies. The most important method of exploiting semantic information, so far, was based on the theory of Systemic Functional Grammar (SFG) <sup id="fnref:27"><a href="#fn:27" class="footnote-ref" role="doc-noteref">27</a></sup> and consisted on the definition of a set of functional features that associate certain words or phrases with semantic information, as described in <a href="#Argamon2007">Argamon (2007)</a>.</p>
<p>However, the goal of our work is to use only information about words distribution, in order to discover if a correlation between an author&rsquo;s style and words distribution exists. In order to analyze words distribution, we rely on the Distributional Hypothesis and, consequently, on Distributional Semantics. Their theoretical basis will be presented in the next subsection.</p>
<h2 id="distributional-semantics">Distributional Semantics</h2>
<p>The assumption behind all distributional semantics models (DSMs) is that the notion of semantic similarity can be defined in terms of linguistic distributions. This is known as the Distributional Hypothesis, which is stated as follows: The degree of semantic similarity between two linguistic expressions a and b depends on the similarity of the linguistic contexts in which a and b can appear. In accord with this definition, certain aspects of the meaning of lexical expressions depend on the contexts in which they are observed. The semantic properties of a word can then be defined by inspecting a significant number of linguistic contexts, representative of the distributional behavior of such word.</p>
<p>Despite the huge consensus reached lately by this theory in Computational Linguistics, its origins reside in the context of Zellig Harris’ proposal of Distributional Semantics as the bedrock of linguistics as a scientific discipline <sup id="fnref:28"><a href="#fn:28" class="footnote-ref" role="doc-noteref">28</a></sup>. Harris’ proposal was conceived for phonemic analysis and later turned into a general methodology to be applied at every linguistic level. The distribution procedure was regarded as a way for linguists to give a methodological base to their analysis. He then, not only extended his theory to meaning but assumed that meaning could actually be explained on distributional grounds. The Distributional Hypothesis can be considered a cognitive hypothesis about the form and origin of semantic representations <sup id="fnref1:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>. Some of the most influential models for distributional semantics, such as Latent Semantic Analysis (LSA; <sup id="fnref:29"><a href="#fn:29" class="footnote-ref" role="doc-noteref">29</a></sup>) and Hyperspace Analogue to Language (HAL; <sup id="fnref:30"><a href="#fn:30" class="footnote-ref" role="doc-noteref">30</a></sup>) have been developed for cognitive and psychological research, meant to represent how semantic representations are learned by extracting co-occurrence patterns <sup id="fnref:31"><a href="#fn:31" class="footnote-ref" role="doc-noteref">31</a></sup>.</p>
<p>Within the corpus linguistics tradition, there was no need to motivate the Distributional Hypothesis as a methodological principle for semantic analysis. This is better summarized in the well-mentioned slogan by Firth:  “You shall know a word by the company it keeps”   <sup id="fnref:32"><a href="#fn:32" class="footnote-ref" role="doc-noteref">32</a></sup>. In corpus linguistics, the Distributional Hypothesis is often claimed to be the only possible source of evidence for the exploration of meaning.</p>
<p>Distributional Semantics has gained popularity in computational linguistics starting from the late 1980s when there was the progressive predominance of corpus-based statistical methods for language processing. Within this new paradigm, statistical methods were naturally applied to the lexical-semantic analysis. Corpora are indeed connected to Distributional Semantics since they can be used as repositories of linguistic usages, then representing the primary source of information to identify the world distributional properties. Their role has been enhanced by the current availability of a huge collection of texts, contextually with increasingly sophisticated computational linguistics techniques to process them and extract the relevant context feature to build distributional semantic representations. Despite its currently being corpus-based, distributional semantics is not prevented to underline aspects of the format and origin of word meaning and the issue of how and to what extent features extracted from the linguistic input actually shape meaning.</p>
<p>The way in which it is possible to proceed in order to infer a geometric representation starting from distributional information can be originated from <a href="#Harris1970">Harris (1970)</a>, who writes that  “the distribution of an element will be understood as the sum of all its environments.”  In linguistics, an environment is called a context, and we here assume a context to be the setting of a word, phrase, etc., among the surrounding words, phrases, etc., often used for helping to explain the meaning of the word, phrase, etc.</p>
<p>One way to collect this information is to tabulate the contextual information, so that for each word we provide a list of the co-occurrents of the word and the number of times they have co-occurred. In a second step, we take away the actual words and only leave the co-occurrence counts. Also, we make each list equally long by adding zeroes in the places where we lack co-occurrence information. We also sort each list so that the co-occurrence counts for each context come in the same places in the lists. The mathematical backbone of Latent Semantic Analysis <sup id="fnref1:29"><a href="#fn:29" class="footnote-ref" role="doc-noteref">29</a></sup>, is Singular Value Decomposition, a well-known linear algebra technique aimed at extracting the most informative dimensions in a matrix of data and here used to reconstruct the latent structure behind the distributional hypothesis <sup id="fnref:33"><a href="#fn:33" class="footnote-ref" role="doc-noteref">33</a></sup>. The names vector semantics, word or semantic spaces merely highlight specific mathematical techniques used to formalize the notion of contextual representation. This information can then be modeled as vectors, as described in <a href="#Sch%C3%BCtze1992">Schütze (1992)</a>, <a href="#Sch%C3%BCtze1993">Schütze (1993)</a>, who builds context vectors (which he calls  “term vectors”  or  “word vectors” ) in the following way: co-occurrence counts are collected in a words-by-words matrix, in which the elements record the number of times two words co-occur within a set window of word tokens.</p>
<p>Context vectors are then defined as the rows or the columns of the matrix (the matrix is symmetric, so it does not matter if the rows or the columns are used). A cell fij of the co-occurrence matrix records the frequency of occurrence of the word i in the context of the word j or of the document j, as shown in <a href="#figure01">Figure 1</a>.<br>




























<figure ><img loading="lazy" alt="Words-by-words co-occurrences matrix" src="/dhqwords/vol/15/1/000539/resources/images/Figure1.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000539/resources/images/Figure1_hua10b3c84d9fdaad3740429b981db66f7_29664_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000539/resources/images/Figure1_hua10b3c84d9fdaad3740429b981db66f7_29664_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000539/resources/images/Figure1_hua10b3c84d9fdaad3740429b981db66f7_29664_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000539/resources/images/Figure1_hua10b3c84d9fdaad3740429b981db66f7_29664_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/1/000539/resources/images/Figure1.png 1532w" 
     class="landscape"
     ><figcaption>
        <p>Words-by-words co-occurrences matrix.
        </p>
    </figcaption>
</figure></p>
<p>Context vectors do not only allow us to go from distributional information to a geometric representation, but they also make it possible for us to compute proximity between words. Thus, the point of the context vectors is that they allow us to define (distributional, semantic) similarity between words in terms of vector similarity. There are many ways to compute the similarity between vectors, and the measures can be divided into similarity measures and distance measures. The difference is that similarity measures produce a high score for similar objects, whereas distance measures produce a low score for the same objects: large similarity equals small distance, and conversely. A convenient way to compute normalized vector similarity is to calculate the cosine of the angles between two vectors x and y, defined as: $$ sim_{cos(\vec x,\vec y)} = \frac{x \cdot y}{|x| \cdot |y|}=\frac{\sum_{i=1}^n x_{i} \cdot y_{i}}{\sqrt{\sum_{i=1}^n x_{i}^{2}} \cdot \sqrt{\sum_{i=1}^n y_{i}^{2}}}$$ The cosine measure corresponds to taking the scalar product of the vectors and then dividing by their norms. It is the most frequently utilized similarity metric in word-space research. It is attractive because it provides a fixed measure of similarity: it ranges from 1 for identical vectors, over 0 for orthogonal vectors. <a href="#figure02">Figure 2</a> shows an example of the graphical representation of words vectors related to the matrix depicted in Figure 1. The words cat and dog in the matrix above are depicted as never appearing together in the same context. This does not mean the ey are not semantically similar, because they can actually happen in really similar contexts, it just means they don&rsquo;t appear together in the context windows we defined. Anyway, here, we represent cat and dog as dissimilar, being the angle between their vector of almost 90 degrees. The vector representative of the word animal is instead as similar to that of dog than to that of cat, while the vector representative of the word canine is closer to the vector representative of dog than to cat and the vector representative of feline is closer to cat than to dog meaning that canine is more semantically similar to dog and feline is more semantically similar to cat. Also, the vectors of canine and feline are both close to animal, suggesting that the two words are often used in similar contexts in the texts analyzed to generate the co-occurrence vectors here represented. Of course, this example is not representative of the linguistic and semantic reality of things, but entirely indicative and apt to properly describe and illustrate the concept of vectorial representation of semantic similarity.<br>




























<figure ><img loading="lazy" alt="Vector Similarity" src="/dhqwords/vol/15/1/000539/resources/images/Figure2.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000539/resources/images/Figure2_hu227c363e8ef643c8f67f8d04599c1df8_66054_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000539/resources/images/Figure2_hu227c363e8ef643c8f67f8d04599c1df8_66054_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000539/resources/images/Figure2.png 842w" 
     class="landscape"
     ><figcaption>
        <p>Vector Similarity.
        </p>
    </figcaption>
</figure></p>
<h2 id="experimental-setup">Experimental Setup</h2>
<p>First, we want to specify that it is not our purpose to propose new ways to improve state-of-the-art AA algorithms. Indeed, our aim is just to verify the hypothesis that the distribution of words can provide an indication of a distributional stylistic fingerprint of an author. To do this, we have set up a simple classification task. Subsection 3.1 briefly depicts the data set we used, Section 3.2 describes why and how we chose the authors that would constitute our dataset and Section 3.3 depicts the steps implemented in our experiment.</p>
<h2 id="data-set-construction">Data Set Construction</h2>
<p>In order to build the reference and test corpora, we started from texts pertaining to the work of six Italian writers working at the turn of the 20th century, namely, Luigi Capuana, Federico De Roberto, Luigi Pirandello, Italo Svevo, Federigo Tozzi and Giovanni Verga. We chose contiguous authors in a chronological sense, whose texts are available in digital format (in fact we could not do a similar survey on the narrative of the 1990s because it is still under copyrights). Indeed, we used texts freely available for download from the digital library of the Manunzio project, via the LiberLiber website (<a href="http://www.liberliber.it">www.liberliber.it</a>). Since they were encoded in various formats, such as .epub, .odt, and .txt, our pre-processing consisted in converting them all in .txt format and getting rid of all XML tags, together with footnotes and editors’ notes and comments.</p>
<h2 id="authors-and-texts-choice">Authors and Texts Choice</h2>
<p>In between the 1875 and the early 1900s, a literary movement peaked in Italy: Verismo (meaning realism, from Italian vero, meaning true). The main exponents of this literary movement, as well as the authors of its manifesto, were Giovanni Verga and Luigi Capuana. Verismo did not constitute a formal school, but it was still based on specific principles, its birth being influenced by a positivist climate which put absolute faith in science, empiricism, and research and which developed from 1830 until the end of the 19th century.</p>
<p>All the authors selected to build the corpus used for this work pertained to the temporal span in which the Literary Verismo developed, but not all of them are proponents of such genre. Indeed, three of the selected authors are considered to be Verist Authors (Giovanni Verga, Federico De Roberto, and Luigi Capuana) while three (Luigi Pirandello, Federico Tozzi, and Italo Svevo) are representative of another Literary Movement: Modernism. We decided to choose texts pertaining to those authors and literary movements firstly because of their being all written in the same temporal span. This allowed us to get rid of any eventual lexical bias, due to the difference in languages of works published in different epochs.</p>
<p>Also, the selection was performed having in mind an eventual future evolution of this work. Using texts pertaining to the same period, but to different literary movement, would allow us to investigate the ability of our method in recognizing the literary movement the texts pertain to, instead of the author that wrote them. This style-based text categorization tasks, known as genre detection, is quite similar to authorship attribution, even if there are characteristics that distinguish the one from the other. An important question to investigate, then, is how it would be possible to discriminate between two basic factors: authorship and genre, and if semantics could be useful not only for recognizing the author of a literary work but also the literary genre it pertains to.</p>
<p>Another line of research that has not been adequately examined so far is the development of robust attribution techniques that can be trained on texts from one genre and applied to texts of another genre by the same authors. The way we selected and balanced the texts composing the corpus could be useful for this task, too.</p>
<h2 id="experiment-description">Experiment Description</h2>
<p>According to <a href="#Rudman1997">Rudman (1997)</a>, a striking problem in stylometry is due to the lack of homogeneity of the examined corpora, in particular to the improper selection or fragmentation of the texts that might cause alterations in the writers’ style. As already depicted in the previous section, the corpus has been built according to this assumption, trying to use texts pertaining to the same time span and balanced between the two selected literary movements. Also, in order to create balanced reference corpora, i.e. covering all the authors’ different stylistic and thematic phases, for each author, as shown in <a href="#figure03">Figure 3</a>, we built a reference corpus as the composition of the 70% of every single work (usually a novel). The same technique was used to create the test corpus by using the remaining 30% of each work. Typical AA approaches consist of analyzing known authors and assigning authorship to previously unseen text on the basis of various features. Train and test sets should then contain different texts. Contrary to the classical AA task, our train and test sets contain different parts of the same texts. Indeed, with this experiment, we wanted to understand if the semantics that an author bestows to a word, is peculiar to his writing. To prove this, we wanted to cover all the different stylistic and thematic phases an author can go through during his activity, hence the partition of all his texts in a reference and a test portion.</p>
<p>Our work relies on the assumption that the works of an author are representative of the author&rsquo;s thought, so it is assumed that the semantics that an author associates with certain words are representative of its thoughts. One possible flaw of this kind of approach is that if an author changes the semantics that he associates with concepts in different works overtime, the method might not work. It can happen that an author who has a long career changes its point of view on things and this should obviously reflect on its works. This is one of the main reasons why we decided to use all the works from each author as training text. We wanted to take into account each different phase an author may go through during its career, especially considering changes in the semantics associated with concepts.</p>
<p>Using all the works of each author allows us to have complete photography of the author itself, and allows to understand the semantics associated with concepts through all its work, even accounting for changes. In fact, the association between words extracted using our method would highlight changes in semantics by changing the score associated with a pair of words. Let’s hypothesize that a strong association for the young Verga (i.e. for Verga in his first works) is sun and joy, while later on the strongest association is, let’s say, moon and joy. Our hypothesis would be that, while in first works Verga associated the concept of sun with that of joy, later on, is the concept of moon that is associated with that of joy. Deciding to use some works of Verga as train as some other as tests might then be deceiving, because what is semantically true for his first work is not true later on. Using our method, if an association is true just for some works, and not for all its score is evened out and the pair of words is not that semantically relevant, and thus is not used for classification. Pair with high score are those for which the association between the concepts are true throughout all the work of an author, or reports a score that is so high in one or more work, that is could not be evened out from the score reported in all the other association from all the other works from that particular author.</p>
<p>We then analyzed each reference and test corpora with a Part-of-Speech (PoS) tagger and a lemmatizer for Italian <sup id="fnref:34"><a href="#fn:34" class="footnote-ref" role="doc-noteref">34</a></sup>. For every author, we built two lists of word pairs (with their lemma and PoS), one relative to the tagged reference corpus (reference pairs) and the other to the tagged test set (test pairs), where each word was paired with all the other words with the same PoS. We also filtered the pairs to leave only nouns, adjectives and verbs. Starting from the tagged corpora, we built two words-by-words matrixes of co-occurrence counts for each author. Being the corpus relatively small and not having particular computability issues, we chose not to apply decomposition techniques to reduce the size of the matrixes (and thus not losing any information). We performed different empiric setup of the window’s size and chose the one that showed more suitable results, according to what is stated by Kruszewski and Baroni: the context window was then set to 3 words prior and 3 words following the one under examination <sup id="fnref:35"><a href="#fn:35" class="footnote-ref" role="doc-noteref">35</a></sup>. The chosen DS model <sup id="fnref:36"><a href="#fn:36" class="footnote-ref" role="doc-noteref">36</a></sup> was applied to each matrix to calculate the cosine between the vectors representing the two words of each pair. This allowed us to evaluate the semantic relatedness between the words by assessing their proximity in the distributional space as represented by the cosine value: as explained in Section 2.2, the more this value tends to 1, the more the two words of the pair are considered to be related. We then obtained two related word pair (RWP) lists for each author A: RWPrefA and RWPtestA. Figure 3 depicts the process described above.<br>




























<figure ><img loading="lazy" alt="RWPref and RWPtest creation process for an author" src="/dhqwords/vol/15/1/000539/resources/images/Figure3.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000539/resources/images/Figure3_hu1911bfa3f558b938ff85f66de45023d6_369712_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000539/resources/images/Figure3_hu1911bfa3f558b938ff85f66de45023d6_369712_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000539/resources/images/Figure3.png 903w" 
     class="landscape"
     ><figcaption>
        <p>RWPref and RWPtest creation process for an author.
        </p>
    </figcaption>
</figure></p>
<h2 id="hypothesis-and-discussion">Hypothesis and Discussion</h2>
<p>Since we wanted to focus on the analysis of the semantic distribution of words, we decided to exclude any possible lexical bias. For this reason, we restricted the analysis on a common vocabulary, i.e. a vocabulary constituted by the intersection of the six authors’ vocabularies. In this way, we prevent our classifier to exploit, as a feature, the presence of words used by some (but not all) of the authors. Moreover, we removed from the RWPtest lists all those pairs of words occurring frequently together in the same context, since they might constitute a multiword expression that, once again, could be pertaining with the signature lexicon of each author. To remove them, we computed the number of times (#co-occ in <a href="#figure04">Figure 4</a>) they appeared together in the context window, as well as their total number of occurrences (#occa and #occb) and we excluded from the analysis those pairs for which the ratio between the number of co-occurrences and the total occurrences of the less frequent word was higher than the empirically set threshold of 0.5. The first two pairs of Figure 4 would be removed as probable multiword (PM column in Figure 4): scoppio (burst) and risa (laughter) could mostly co-occur in scoppio di risa (meaning burst of laughter) and the words man and mano (both meaning hand) could mostly co-occur in man mano (meaning little by little, or progressively).<br>




























<figure ><img loading="lazy" alt="An example of co-occurring RWPs from Pirandello’s test list: the first two pairs would be removed" src="/dhqwords/vol/15/1/000539/resources/images/Figure4.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000539/resources/images/Figure4_hu72cae980d4a56df6954e372cab715120_55953_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000539/resources/images/Figure4_hu72cae980d4a56df6954e372cab715120_55953_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000539/resources/images/Figure4.png 1147w" 
     class="landscape"
     ><figcaption>
        <p>An example of co-occurring RWPs from Pirandello’s test list: the first two pairs would be removed.
        </p>
    </figcaption>
</figure></p>
<p>Finally, we reduced the size of the six RWPref and RWPtest lists by sorting them in decreasing order of the cosine value and then by keeping the pairs with the highest cosine, selected using a percentage parameter θ as a threshold. We chose to introduce the parameter θ for two reasons: first of all we wanted to avoid the classification algorithm to be disturbed by noisy (i.e. not significative) pairs which would not hold any relevant stylistic cue, also, we would like to ease a literary scholar in the interpretation of the results by having to analyze just a limited selection of (potentially) semantically related word pairs. For the last phase of our experiment, we defined a classification algorithm to test the effective presence of stylistic cues inside the obtained RWPtest lists. We defined a classifier using a nearest-cosine method to attribute each test list to an author. The method consisted in searching for a pair of words contained in the test list inside each reference list and incrementing by 1 the score of the author whose reference list included the pair with the more similar cosine value (i.e. having the minimum difference): the chosen author was the one with the highest score. Figure 5 shows the classification results for θ = 5%.<br>




























<figure ><img loading="lazy" alt="Classification results, obtained via the nearest-cosine method for θ = 5%" src="/dhqwords/vol/15/1/000539/resources/images/Figure5.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000539/resources/images/Figure5_hu61ec159d7e8a3d1c123ce84ce3c0e7c8_83585_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000539/resources/images/Figure5_hu61ec159d7e8a3d1c123ce84ce3c0e7c8_83585_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000539/resources/images/Figure5.png 1129w" 
     class="landscape"
     ><figcaption>
        <p>Classification results, obtained via the nearest-cosine method for θ = 5%.
        </p>
    </figcaption>
</figure></p>
<p>As summarized in Figure 6, the correct classification of all RWPs in RWPtest lists has been obtained with a θ value of 5%.<br>




























<figure ><img loading="lazy" alt="Results of the classification. Classification errors are highlighted" src="/dhqwords/vol/15/1/000539/resources/images/Figure6.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000539/resources/images/Figure6_hufe86844be45605a5b070042140965a43_65917_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000539/resources/images/Figure6_hufe86844be45605a5b070042140965a43_65917_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000539/resources/images/Figure6.png 881w" 
     class="landscape"
     ><figcaption>
        <p>Results of the classification. Classification errors are highlighted.
        </p>
    </figcaption>
</figure></p>
<p>To help in interpreting the failure of the algorithm in classifying Tozzi’s test list for θ values lower than 5% (as shown in Figure 6) we calculated the cardinality of the RWPtest lists for each author with the change in θ value (Figure 7).<br>




























<figure ><img loading="lazy" alt="Cardinality of RWPtest for each author and for each θ value" src="/dhqwords/vol/15/1/000539/resources/images/Figure7.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000539/resources/images/Figure7_hu406ece9ed2429d8eb1ab7b4ae8af9ad8_82677_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000539/resources/images/Figure7_hu406ece9ed2429d8eb1ab7b4ae8af9ad8_82677_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000539/resources/images/Figure7.png 878w" 
     class="landscape"
     ><figcaption>
        <p>Cardinality of RWPtest for each author and for each θ value.
        </p>
    </figcaption>
</figure></p>
<p>It is possible to observe how the choice of θ influences the correct classification of Tozzi’s test list. Indeed, the use of a θ sense below 5% has the effect of remarkably reducing an already small test list (RWPtextTozzi) as shown in Figure 7. It is apparent that increasing the value of θ and consequently the number of significant RWPs that are analyzed, the system is able to correctly classify RWPtestTozzi (see the values in Tozzi’s row of Figure 6).</p>
<h2 id="conclusion-and-next-steps">Conclusion and Next Steps</h2>
<p>In this paper, we investigated the possibility that an analysis of the semantic distribution of words in a text can be potentially exploited to get cues about the style of an author. In order to validate our hypothesis, we conducted the first experiment on six different Italian authors. Of course, it is not our intent, with this paper, to define new methods for enhancing state-of-the-art authorship attribution algorithms. However, the obtained results seem to suggest that the way words are distributed across a text, can provide a valid stylistic cue to distinguish an author’s work. In light of what we have shown up to this point, the direction of our next steps can be twofold. On the one hand, our research will focus on detecting and providing useful indications about the style of an author. This can be done by highlighting, for example, atypical distributions of words (e.g. with contrastive methods) or by analyzing their distributional variability. Furthermore, it could be interesting to use a different distributional measure than the cosine, to test our hypothesis. On the other hand, it would be interesting to confront the computational task of authorship attribution, by measuring the effective contribution that a feature based on distributional semantics would provide to a canonical classification process. Also, as highlighted in <a href="#section3.2">Section 3.2.3</a>, another interesting development of this work would regard the investigation of the ability of our method in recognizing the literary movement the texts pertain to, instead of the author that wrote them.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Van Halteren H., Baayen H., Tweedie F., Haverkort M., and Neijt A. 2005.  “New machine learning methods demonstrate the existence of a human stylome.”    <em>Journal of Quantitative Linguistics</em> , 12(1):65–77.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Stamatatos E. 2009.  “A survey of modern authorship attribution methods.”    <em>J. Am. Soc. Inf. Sci. Technol.</em> , 60(3):538–556, March.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Lenci A. 2008.  “Distributional semantics in linguistic and cognitive research.”    <em>Italian journal of linguistics</em> , 20(1):1–31.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Miller G. A. and Charles W. G.. 1991.  “Contextual correlates of semantic similarity.”    <em>Language and cognitive processes</em> , 6(1):1–28.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>Yule G. U. 1938.  “On sentence-length as a statistical characteristic of style in prose, with application to two cases of disputed authorship.”    <em>Biometrika</em> , 30, 363-390.&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>Yule G. U. 1944.  _ The statistical study of literary vocabulary._  Cambridge University Press.&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>Zipf G. K. 1932.  <em>Selected studies of the principle of relative frequency in language.</em>  Harvard University Press, Cambridge, MA.&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>Holmes D. I. 1994.  “ Authorship attribution.”    <em>Computers and the Humanities</em> , 28, 87–106.&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>Holmes D. I. 1998.  “The evolution of stylometry in humanities scholarship.”  Literary and Linguistic Computing, 13(3), 111-117.&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>Abbasi A., Chen H. 2005.  “Applying authorship analysis to extremist-group web forum messages.”    <em>IEEE Intelligent Systems</em> , 20(5), 67-75.&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>Chaski C. E. 2005.  “Who’s at the keyboard? Authorship attribution in digital evidence investigations.”    <em>International Journal of Digital Evidence</em> , 4(1).&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p>Grant T. D. 2007.  “ Quantifying evidence for forensic authorship analysis.”    <em>International Journal of Speech-Language and the Law</em> , 14(1), 1 -25.&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13">
<p>Frantzeskou G., Stamatatos E., Gritzalis S. and Katsikas S. 2006.  “Effective identification of source code authors using byte-level information.”  In  <em>Proceedings of the 28th International Conference on Software Engineering</em>  (pp. 893-896).&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14">
<p>Juola P. 2004.  “ Ad-hoc authorship attribution competition.”  In  <em>Proceedings of the Joint Conference of the Association for Computers and the Humanities and the Association for Literary and Linguistic Computing</em>  (pp. 175-176).&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:15">
<p>Koppel M. and Schler J. 2004.  “ Authorship verification as a one-class classification problem.”  In  <em>Proceedings of the twenty-first international conference on Machine learning</em> , page 62. ACM.&#160;<a href="#fnref:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:16">
<p>Stamatatos E. 2006.  “ Authorship attribution based on feature set subspacing ensembles.”    <em>International Journal on Artificial Intelligence Tools</em> , 15(05):823–838.&#160;<a href="#fnref:16" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:17">
<p>Zhao Y. and Zobel J. 2005.  “Effective and scalable authorship attribution using function words.”  In  <em>Information Retrieval Technology</em> , pages 174–189. Springer.&#160;<a href="#fnref:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:18">
<p>Argamon S., Whitelaw C., Chase P., Hota S. R., Garg N., and Levitan S. 2007.  “Stylistic text classification using functional lexical features.”    <em>Journal of the American Society for Information Science and Technology</em> , 58(6):802– 822, April.&#160;<a href="#fnref:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:19">
<p>Zheng R., Li J., Chen H., and Huang Z. 2006.  “A framework for authorship identification of online messages: Writing-style features and classification techniques.”    <em>Journal of the American Society for Information Science and Technology</em> , 57(3):378–393, February.&#160;<a href="#fnref:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:20">
<p>Grieve J. 2007.  “Quantitative Authorship Attribution: An Evaluation of Techniques.”    <em>Literary and Linguistic Computing</em> , 22(3):251–270, May.&#160;<a href="#fnref:20" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:21">
<p>De Vel O., Anderson A., Corney M., and Mohay G. 2001.  “Mining e-mail content for author identification forensics.”    <em>ACM Sigmod Record</em> , 30(4):55–64.&#160;<a href="#fnref:21" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:22">
<p>Gamon M. 2004.  “Linguistic correlates of style: authorship classification with deep linguistic analysis features.”  In  <em>Proceedings of the 20th international conference on Computational Linguistics</em> , page 611. Association for Computational Linguistics.&#160;<a href="#fnref:22" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:23">
<p>Stamatatos E., Fakotakis N., and Kokkinakis G. 2000.  “Automatic text categorization in terms of genre and author.”    <em>Computational linguistics</em> , 26(4):471–495.&#160;<a href="#fnref:23" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:24">
<p>Stamatatos E., Fakotakis N., and Kokkinakis G. 2001.  “Computer-based authorship attribution without lexical measures.”    <em>Computers and the Humanities</em> , 35(2):193–214.&#160;<a href="#fnref:24" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:25">
<p>Hirst G. and Feiguina O. 2007.  “Bigrams of Syntactic Labels for Authorship Discrimination of Short Texts.”    <em>Literary and Linguistic Computing</em> , 22(4):405–417, September.&#160;<a href="#fnref:25" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:26">
<p>Uzuner O. and Katz B. 2005.  “A comparative study of language models for book and author recognition.”  In Natural Language Processing–IJCNLP 2005, pages 969–980. Springer.&#160;<a href="#fnref:26" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:27">
<p>Halliday M. A. K. 1994.  <em>Functional grammar.</em>  London: Edward Arnold.&#160;<a href="#fnref:27" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:28">
<p>Harris Z. S. 1970.  <em>Distributional structure.</em>  Springer.&#160;<a href="#fnref:28" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:29">
<p>Landauer, Thomas K., and Susan T. Dumais 1997.  “A solution to Plato&rsquo;s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge.”    <em>Psychological review</em>  104.2 (1997): 211.&#160;<a href="#fnref:29" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:29" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:30">
<p>Burgess, Curt, and Kevin Lund. 1997.  “Representing abstract words and emotional connotation in a high-dimensional memory space.”  Proceedings of the Cognitive Science Society. 1997.&#160;<a href="#fnref:30" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:31">
<p>Landauer, Thomas K. 2007.  “LSA as a theory of meaning.”    <em>Handbook of latent semantic analysis</em>  3 (2007): 32.&#160;<a href="#fnref:31" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:32">
<p>Firth J. R. 1957.  “Modes of meaning.”  Papers in Linguistics.&#160;<a href="#fnref:32" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:33">
<p>Deerwester, Scott; Dumais, Susan T.; Furnas, George W.; Landauer, Thomas K.; Harshman, Richard. 1990.  “Indexing by Latent Semantic Analysis.”    <em>Journal of the American Society for Information Science</em> . 41 (6): 391–407&#160;<a href="#fnref:33" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:34">
<p>Dell’Orletta F., Venturi G., Cimino A., and Montemagni S. 2014.  “T2k2: a system for automatically extracting and organizing knowledge from texts.”  In LREC, pages 2062–2070.&#160;<a href="#fnref:34" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:35">
<p>Kruszewski G. and Baroni M. 2014.  “Dead parrots make bad pets: Exploring modifier effects in noun phrases.”    <em>Lexical and Computational Semantics</em>  (* SEM 2014), page 171.&#160;<a href="#fnref:35" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:36">
<p>Baroni M. and Lenci A. 2010.  “Distributional memory: A general framework for corpus-based semantics.”    <em>Computational Linguistics</em> , 36(4):673–721.&#160;<a href="#fnref:36" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry><entry><title type="html">Computer Vision and the Creation of a Database of Printers’ Ornaments</title><link href="https://rlskoeser.github.io/dhqwords/vol/15/1/000537/?utm_source=atom_feed" rel="alternate" type="text/html"/><link href="https://rlskoeser.github.io/dhqwords/vol/14/4/000490/?utm_source=atom_feed" rel="related" type="text/html" title="Developing Geographically Oriented NLP Approaches to Sixteenth–Century Historical Documents: Digging into Early Colonial Mexico"/><id>https://rlskoeser.github.io/dhqwords/vol/15/1/000537/</id><author><name>Hazel Wilkinson</name></author><author><name>James Briggs</name></author><author><name>Dirk Gorissen</name></author><published>2021-05-21T00:00:00+00:00</published><updated>2021-05-21T00:00:00+00:00</updated><content type="html"><![CDATA[<h2 id="heading"></h2>
<p>Throughout the hand press period (roughly from 1440–1830), many printed books were decorated with printers’ ornaments and ornamental type.<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>  These have all but vanished from modern books, with the exception of the small number still printed on private hand presses. The term printers’ ornaments encompasses wood- and metalcut blocks, where designs were cut by hand (see <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>, <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>, and <sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>) as well as cast metal blocks, and copies of hand-cut woodblocks made from molten type-metal (a process known as dabbing, recently investigated by <sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup> and <sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>). Books were also decorated with fleurons, or ornamental pieces of type (also known as printers’ flowers), which could be used individually for a tiny flourish, or arranged into larger, complex patterns. The history and use of fleurons has been documented by <sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>, <sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>, and <sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup>. Ornaments and fleurons were used to decorate title pages, the beginnings of chapters or sections (headpieces), blank spaces between sections (tailpieces), and initial letters (using blocks, arrangements of fleurons, or factotums).<sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup></p>
<p>Printers’ ornaments of all kinds are useful as evidence in bibliographical studies. Those cut by hand are unique, and although casts were reproducible, they could develop unique patters of wear or damage over time. This means that in the right circumstances ornaments can be used to identify when, where, and by whom a book, pamphlet, or other printed item was produced. Books and printed ephemera did not always include the name of their printer, or the date or location of printing. Sometimes these details were omitted to conceal the printer’s identity, in the case of a controversial work or pirated printing. Most books include a date and location, but often the publisher’s name(s), and the names of the booksellers who stocked it, are given instead of the printer’s (the publisher was the book’s financial backer and marketer). Pamphlets and leaflets are less likely to have complete production information than books. It was very common for a printer to put his or her name to some but not all of their output. A representative example is the eighteenth-century printer John Darby II, who put his name to approximately 230 books between 1707 and 1732, but probably printed over 1000 items during this period.<sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>  If the items signed by a specific printer contain ornaments, however, we begin to attribute unsigned items. If an ornament is uniquely identifiable (not a cast, or a cast with unique wear), and it can be found in multiple books with a known printer, we can infer that it belonged to that printer. When it appears in an item with no known printer, then, we can attribute the item to the printer who owned the ornament. When making printer identifications in this way, it is good practice to find as many examples as possible, since occasionally printers lent each other ornaments, or shared printing jobs with one another. When several ornaments known to have been owned by a single printer appear in an un-signed item, however, we can identify the printer with some certainty. If an item is undated, or the location of its production is unknown, the identification of the printer can usually offer some useful information. Fleurons can also help in printer identification, using a slightly different method. It is normally impossible to distinguish between multiple casts of the same fleuron, but the arrangements into which they were combined can be identifiable, because more complex arrangements were sometimes kept as standing type and used in multiple books. When this occurred, they are good indicators of when and where an item was printed, and can potentially date items very exactly, as shown by <sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>. It is possible that the identification of a book’s printer, or the date or location of printing, can also help us to identify unknown authors, or confirm or refute suspected authorship.</p>
<p>Printers’ ornaments and fleurons are potentially very useful as bibliographical evidence. They are also neglected resources for the study of graphic design and book production. Their role in the reading experience during the hand press period has received very little attention. Samuel Richardson’s novel  <em>Clarissa</em>  (1748) is the only book to have attracted serious attention for its fleurons, in particular from <sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup>, and <sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup>, largely because Richardson himself was a printer of particular creativity. Despite their potential as evidence and their interest for research, printers’ ornaments are seldom used because they have been difficult to locate. Unlike illustrations, the presence of ornaments is not routinely noted in library catalogues, and they have not been tagged in digital repositories. To identify printers using evidence from ornaments and fleurons, it has been necessary to search books page by page. The mass digitisation of books has made searching page by page more convenient, but it is still a time-consuming task. Occasionally scholars like <sup id="fnref1:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>, <sup id="fnref1:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>, and <sup id="fnref1:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup> have produced catalogues of the ornaments belonging to individual printers, but these have been the work of years, or decades.</p>
<p><em>Fleuron: A Database of Eighteenth-Century Printers’ Ornaments</em>  was created to speed up the process of locating and studying printers’ ornaments. In 2014, Gale Cengage announced their intention to make their content available for data mining projects. Gale Cengage are the publishers of Gale Digital Collections, which includes the repository of digitised eighteenth-century books, Eighteenth-Century Collections Online (ECCO) (<a href="http://www.gale.com/primary-sources/eighteenth-century-collections-online/">http://www.gale.com/primary-sources/eighteenth-century-collections-online/</a>). Together, Parts I and II of ECCO contain over 33 million page images from over 150,000 titles (in 200,000 volumes). ECCO has an emphasis on books in English, but also includes items published throughout Europe and America, and in many languages. All 33 million page images (2TB of data) were supplied to Cambridge University on a hard drive. It was previously possible to download only 250 page images at a time from ECCO, so the provision of 33 million page images in a single location presented new possibilities for the location and extraction of printers’ ornaments. This paper describes how a team of researchers used computer vision and machine learning to create a database of more than 1 million printers’ ornaments from the 33 million page images supplied by Gale Cengage.</p>
<p>Computer vision has been applied to printers’ ornaments before. In 1997, the library of the University of Lausanne and the Université Paul Valéry, Montpellier, created Passe Partout, a searchable database of printers ornaments, described by <sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup> (<a href="http://bbf.enssib.fr/consulter/bbf-2001-05-0073-010">http://bbf.enssib.fr/consulter/bbf-2001-05-0073-010</a>) It is based on software produced by the Federal Polytechnic School of Lausanne. Passe Partout is built around archives of locally-provided data, provided by research groups working on specific printing offices, predominantly in Switzerland. The images can be searched using a program called T.O.D.A.I, or Typographic Ornament Database and Identification, developed in 1996 and 2000 at the University of Lausanne, by Stephane Michel and Heike Walter, under the supervision of Josef Bigün. T.O.D.A.I uses orientation radiograms to match ornaments within the database. It is documented by <sup id="fnref:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup>, and <sup id="fnref:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup>. Passe Partout uses relatively small datasets of 200 DPI jpeg images. It is enormously useful for studies of specific printers operating in Enlightenment Switzerland (particularly in Geneva and Lausanne).</p>
<p>Recently a team at Oxford University, headed by Giles Bergel and Andrew Zisserman, developed a program called ImageMatch for Bodleian Broadside Ballads Online, a catalogue of single-sheet printed ballads of the hand press period. 900 seventeenth-century ballads were scanned by the Bodleian Library in Oxford for the project. Using these high quality scans, Bergel, Zisserman, and their team employed a bag-of-words approach to develop ImageMatch, which allows image searching of the woodcut illustrations in the Ballads, as outlined by <sup id="fnref:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup>.</p>
<p>The data from ECCO presents a very different problem from the data used by these two previous projects. There is much more data in ECCO, and it is of lower quality. The images used in Passe Partout and Bodleian Broadside Ballads Online are direct scans of books (in greyscale), whereas the images in ECCO are secondary scans of the Eighteenth-Century Microfilm Set (in black and white). The original microfilms were made decades ago in libraries around the world. To identify and extract printers’ ornaments from the images contained in ECCO required a different approach, on a bigger scale.</p>
<p>The problem of extracting printers&rsquo; ornaments programmatically from scanned pages can be approached in different ways, each with their relative trade-offs. The approach which was taken for Fleuron, which is outlined in this paper, is a morphological one. A series of morphological operations (filtering, dilution, erosion, etc.) was applied to each image, followed by a series of heuristics to filter out those connected components that are deemed to be printers’ ornaments. This was possible because ornaments do not occur randomly on the page, and they have particular size and shape constraints. For example, ornaments depicting capital letters have a fairly constrained aspect ratio (roughly square) and can be found at the left-most margin of the text. Other ornaments are typically horizontally centred with the text and surrounded by white space. This helps us to distinguish them from illustrations and blobs of text that end up glued together as a result of the morphological operations. Examples of these normal ornaments are shown in Figure 1.</p>




























<figure ><img loading="lazy" alt="Three examples of typical printers’ ornaments. 1a: A headpiece and factotum; 1b: Two rows of fleurons; 1c: A Tailpiece" src="/dhqwords/vol/15/1/000537/resources/images/figure01.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000537/resources/images/figure01_hua0d4d4b24f4da3a102e4452ac738b95f_208399_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/15/1/000537/resources/images/figure01_hua0d4d4b24f4da3a102e4452ac738b95f_208399_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/15/1/000537/resources/images/figure01_hua0d4d4b24f4da3a102e4452ac738b95f_208399_1200x0_resize_q75_box.jpg 1200w,/dhqwords/vol/15/1/000537/resources/images/figure01_hua0d4d4b24f4da3a102e4452ac738b95f_208399_1500x0_resize_q75_box.jpg 1500w,/dhqwords/vol/15/1/000537/resources/images/figure01.jpg 1568w" 
     class="landscape"
     ><figcaption>
        <p>Three examples of typical printers’ ornaments. 1a: A headpiece and factotum; 1b: Two rows of fleurons; 1c: A Tailpiece
        </p>
    </figcaption>
</figure>
<p>Each page can be treated independently, so the processing can happen in parallel and the implementing code can make use of all the cores on a machine. The processing was implemented with OpenCV, using the following steps. Illustrations of the process are shown in two different examples, in Figures 2 and 3.</p>
<p>Preprocessing: Conservatively cleaning up the scanned image, removing small artefacts introduced by the scanning process.  Threshold the image to black/white such that all white pixels are a 1 and all black pixels are 0.  Apply a series of open and closing morphological operators in order to remove small (white) speckles and close small (black) holes.  The contours of the image are found, and they are all closed; isolated contours with a bounding box area of less than 50 pixels are removed.  Of the remaining blobs, a rough estimate of what are just lines of text is performed, and these are removed.  Heavily dilate the image. This will cause ornaments that are made out of many different small separate elements to be joined together as a whole. Note this has as side effect that the letters in the text will be glued together as well. Something has to be filtered out again later.  Remove small, negligible contours.  Loop over all remaining contours and decide for each one whether it is an actual ornament, a full page illustration, a blob of glued together text, or something else. This decision is made based on a set of heuristics which were chosen to be optimistic. It was deemed preferable to minimise the chance of type two errors (false negatives) at the cost of more type one errors (false positives), in order to ensure we captured as many ornaments as possible.   If we are not sure whether the blob under consideration is an ornament or not, we try to break it up into smaller pieces using the original (undilated image) as a guide and recurse.  If, after recursing, it is still not clear what the object is, we treat it as an ornament.  Finally, for each ornament we extract the bounding box, merge any overlapping bounding boxes and write the corresponding coordinates to a json file.</p>




























<figure ><img loading="lazy" alt="Illustrations of the steps described: a) thresholding to black and white; b) removing speckles and holes; c) removing text; d) heavily dilating the image; e) small elements of ornaments are joined together; f) loop over remaining contours and decide whether each is an ornament; recurse if necessary." src="/dhqwords/vol/15/1/000537/resources/images/figure02.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000537/resources/images/figure02_hubc0d76948e65384fb9d8322f8f0c140b_426975_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/15/1/000537/resources/images/figure02_hubc0d76948e65384fb9d8322f8f0c140b_426975_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/15/1/000537/resources/images/figure02_hubc0d76948e65384fb9d8322f8f0c140b_426975_1200x0_resize_q75_box.jpg 1200w,/dhqwords/vol/15/1/000537/resources/images/figure02_hubc0d76948e65384fb9d8322f8f0c140b_426975_1500x0_resize_q75_box.jpg 1500w,/dhqwords/vol/15/1/000537/resources/images/figure02.jpg 1699w" 
     class="portrait"
     ><figcaption>
        <p>Illustrations of the steps described: a) thresholding to black and white; b) removing speckles and holes; c) removing text; d) heavily dilating the image; e) small elements of ornaments are joined together; f) loop over remaining contours and decide whether each is an ornament; recurse if necessary.
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="Another illustration of the steps described in Figure 2." src="/dhqwords/vol/15/1/000537/resources/images/figure03.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000537/resources/images/figure03_hubc0d76948e65384fb9d8322f8f0c140b_334218_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/15/1/000537/resources/images/figure03_hubc0d76948e65384fb9d8322f8f0c140b_334218_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/15/1/000537/resources/images/figure03_hubc0d76948e65384fb9d8322f8f0c140b_334218_1200x0_resize_q75_box.jpg 1200w,/dhqwords/vol/15/1/000537/resources/images/figure03_hubc0d76948e65384fb9d8322f8f0c140b_334218_1500x0_resize_q75_box.jpg 1500w,/dhqwords/vol/15/1/000537/resources/images/figure03.jpg 1775w" 
     class="portrait"
     ><figcaption>
        <p>Another illustration of the steps described in Figure 2.
        </p>
    </figcaption>
</figure>
<p>Each image contained in ECCO also has an associated metadata file, and the metadata for each extracted ornament was also written to the json file. The metadata is originally derived from the English Short Title Catalogue, so it tells us the book’s title, author, date, publisher, location, format, and ESTC number (the unique identification code assigned to all books by ESTC).</p>
<p>The permissive approach that we adopted was necessary because of the presence in the dataset of images like those shown in Figure 4. In Figure 4a the decorated initial letter has no white space to the left side, because a tight binding has prevented the scanner from reaching the full margin of the page. It is also placed particularly close to the text on the right. Figure 4b features two arrangements of fleurons (which we need to capture), but also manuscript notes and significant shadowing. Figure 4c does not contain any ornaments, but it does contain several elements that might mistakenly be characterised as ornaments. It is from an almanac, where the frequent appearance of tables and charts present problems, as they disrupt the normal division between white space and content. This page also features the problem of show-through. The cheap, thin paper means that the text from overleaf is showing through to this side, creating a blurred area that may be mistaken for a non-textual element, hence the code’s specification that an ornament has clear bounds.</p>




























<figure ><img loading="lazy" alt="Problem images. 4a: A factotum close to the text and page boundary; 4b: Manuscript notes interfere with the image; 4c: A table, and show-through." src="/dhqwords/vol/15/1/000537/resources/images/figure04.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000537/resources/images/figure04_hud47fc491ca72c80b0b2dc8639307b55d_298467_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/15/1/000537/resources/images/figure04_hud47fc491ca72c80b0b2dc8639307b55d_298467_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/15/1/000537/resources/images/figure04_hud47fc491ca72c80b0b2dc8639307b55d_298467_1200x0_resize_q75_box.jpg 1200w,/dhqwords/vol/15/1/000537/resources/images/figure04_hud47fc491ca72c80b0b2dc8639307b55d_298467_1500x0_resize_q75_box.jpg 1500w,/dhqwords/vol/15/1/000537/resources/images/figure04.jpg 1596w" 
     class="landscape"
     ><figcaption>
        <p>Problem images. 4a: A factotum close to the text and page boundary; 4b: Manuscript notes interfere with the image; 4c: A table, and show-through.
        </p>
    </figcaption>
</figure>
<p>The next step was to run the program, which we named Fleuron, on all 36 million page images. This was undertaken in batches using the High Performance Computing cluster  “Darwin”  at Cambridge University Information Services. Each batch consisted of 50 books, and it required on average 6 hours for Fleuron to process a single batch on a single compute node of Darwin. It took five weeks to process all the batches using between 8 and 30 nodes in parallel at any given time. In total, 3,257,956 images were extracted from the pages.</p>
<p>Due to Fleuron’s permissive approach to image extraction, described above, there were a lot of type one errors in the extracted data set. Rather than try to improve Fleuron’s heuristics, a more reliable approach is to use the data produced by Fleuron to train a machine learning model that can review the extracted images and automatically detect which images are type one errors. A random set of 5,000 images was hand labelled as either valid or invalid. To represent the images, a bag-of-visual-words representation was used. RootSIFT vectors (see Arandjelović and Zisserman (2012)) were extracted from all of the images in the labelled data set to train a visual word dictionary with 20,000 visual words. The RootSIFT vectors of each image were then matched to the nearest equivalent word in the dictionary to create a tf-idf vector that represents that image. With the images expressed as feature vectors a machine learning model can be employed.</p>
<p>The data set was split into 12000 images for training and 3000 images for testing, and linear SVM was trained on the data. It is more preferable in this case to classify an invalid image as valid than it is to classify a valid image as invalid. Therefore, the class weights were tuned in the SVM in order to trade off some recall, and gain precision. The resulting algorithm has 95.4% accuracy, 93.8% recall, and 99.5% precision when detecting invalid images. After applying the algorithm to the rest of the data set, 1,988,841 images were classified as invalid, leaving 1,269,115 images.</p>
<p>The resultant database was launched online in October 2016 at <a href="http://fleuron.lib.cam.ac.uk">http://fleuron.lib.cam.ac.uk</a>. It is hosted by Cambridge University Library. Although the full-page images collected in ECCO are kept behind a paywall, all of the content is in the public domain, so the images collected in Fleuron are freely disseminated for public use. The database still contains a number of type one errors that were not identified in the first round of removals. The images in Fleuron are currently searchable by keyword in two ways, by book or by ornament. The Book Search function allows searches by author, publication place, publisher, and ESTC ID. The results can be sorted according to the following genres: history and geography; social sciences; general reference; law; fine arts; religion and philosophy; literature and language; medicine, science and technology. The Ornament Search function allows individual ornaments to be located by their size, and the results to be limited according to the criteria already outlined.</p>
<p>The ornament search can be improved considerably by using content-based image retrieval. This will involve using images themselves as search queries and retrieving images from the data set that look identical or very similar to the query image. Like the machine learning filtering employed to remove bad images from the data set, image searching can also be done using the bag-of-visual-words methodology. By representing each image as a tf-idf vector, similar images should have a smaller distance compared to dissimilar images. Performing image retrieval like this is effectively a nearest neighbour search. This application will allow researchers to quickly find identical ornaments from different publications in the database, which can provide valuable evidence of the authorship and publisher of documents where these are unknown.</p>
<p>Fleuron has significantly improved and sped up the process of finding and browsing printers’ ornaments of the eighteenth century. Fleuron offers an opportunity for book historians, art historians, and historians of graphic design to examine a wealth of ornaments with ease in one place. These miniature works of art have much to offer, and the Fleuron blog<sup id="fnref:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup>  documents some of the directions that may be taken by future researchers into the interpretation of ornaments. For scholars working on the history of printing, and particularly on printer identification, Fleuron will facilitate new bibliographical research. It is currently possible to browse all ornaments with a known association (from the imprint) with a particular printer, publisher, place, author, or year. This will facilitate the compilation lists of ornaments known to belong to a particular printer, or known to have been in use in a particular city. These can in turn help to identify the printer etc. of books where the imprint is lacking such information. As for solving the problem of an unknown printer where we have no such leads, this process will be greatly facilitated by the image-match function described in the previous paragraph, which could potentially enable the identification of unknown printers on an unprecedented scale. We conclude, however, with some necessary cautions on the use of Fleuron for printer identification. The identification of unknown printers using ornaments should be undertaken while bearing two facts in mind: first, that printers occasionally lent one another their blocks, and second that printers often shared jobs (for example, one printer printed sheets A–C of a book, and another printed D–E, though actual arrangements could be more complicated than this). Both of these facts mean that the presence of a single ornament belonging to a known printer in an unsigned book does not offer concrete evidence that the unsigned item (or all of the unsigned item) was printed by the individual who owned the ornament. The ornament could have been lent to another printer, or the printer to whom it belonged could have printed only one sheet of the publication, with a different unknown printer (or printers) responsible for the rest. If only one ornament appears in a given item, we can certainly speculate as to the identity of its printer, but for a definitive identification multiple examples from throughout the book are preferable. Likewise, when assigning an ornament to a printer, it is desirable to find more than one example of the ornament appearing under that printer’s name, again to eliminate the possibility of shared printing and borrowed ornaments. Finally, because of the low quality of the images in Fleuron, it is not always possible to see all of the delicate details of the ornaments. Consultation of the original book will occasionally reveal very minor differences between ornaments that do not show up in Fleuron, and indicate the presence of a copy or a cast. Likewise, it is not always possible to determine, from examinations of the grain and texture of the impression, whether a given ornament on Fleuron was made from a woodcut, a metal cut, or a cast. The original document should always be consulted where possible, before printer identifications are published. Fleuron, then, is designed as a finding aid: its aim is to make printers’ ornaments easily searchable, locatable, and browsable, and to direct researchers back to the original documents for further interpretation. Those interpretations, when grounded in bibliographical study, have the potential to open up thousands of new avenues for research into the history of printing, authorship, reading, and design.</p>
<h2 id="about-fleuron">About Fleuron</h2>
<p>Fleuron was launched in 2016; since then, a new collaboration has been established with the Oxford Visual Geometry Group (VGG). The VGG created Seebibyte, a suite of publicly available programmes for image analysis, including the VGG Image Search Engine (VISE) (<a href="http://www.robots.ox.ac.uk/~vgg/software/vise/">http://www.robots.ox.ac.uk/~vgg/software/vise/</a>) and the VGG Image Classification (VIC) Engine (<a href="http://www.robots.ox.ac.uk/~vgg/software/vic/">http://www.robots.ox.ac.uk/~vgg/software/vic/</a>).<sup id="fnref:20"><a href="#fn:20" class="footnote-ref" role="doc-noteref">20</a></sup>  In 2018-19 the VGG indexed the Fleuron database, and have used VISE to create clusters of similar images from the database. Wilkinson has annotated the clusters to make them searchable, and to provide a dataset for future work on the database using VIC. The indexing of the database has also made it possible to remove the many type one errors that remain in the database (library stamps, diagrams, clippings of text, etc.) In 2020 the website <a href="http://fleuron.lib.cam.ac.uk">http://fleuron.lib.cam.ac.uk</a> was replaced with a new site at <a href="https://compositor.bham.ac.uk">https://compositor.bham.ac.uk</a>, and rebranded as Compositor. With this update, it is possible to perform image searches, supported by the VISE tool, and to search the database by keywords pertaining to the subjects depicted in the ornament. This advance will make the database easier to navigate, and will greatly improve the efficiency with which we can perform the research described at the beginning of this article.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>A Version of this paper was first given at the Göttingen Dialog in Digital Humanities, Georg-August-Universität Göttingen, 2016.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Maslen, Keith.  <em>The Bowyer Ornament Stock</em> . Oxford: The Bibliographical Society, 1974.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Goulden, Richard.  <em>The Ornament Stock of Henry Woodfall</em> . Oxford: The Bibliographical Society, 1988.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Ross, John C.  <em>Charles Ackers&rsquo; Ornament Usage</em> . Oxford: The Bibliographical Society, 1990.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>Mosley, James.  “Dabbing, abklatschen, clichage&hellip;&rsquo;”    <em>Journal of the Printing Historical Society</em>  23 (2015).&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>Bergel, Giles.  “Printing Cliches”  (2016). <a href="www.printing-machine.org/notes/2016/6/4/printing-cliches">www.printing-machine.org/notes/2016/6/4/printing-cliches</a>&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>Meynell, Francis and Stanley Morison.  “Printers&rsquo; Flowers and Arabesques.”    <em>Fleuron</em> , 1 (1923):1–45.&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>Baines Reed, Talbot.  <em>A History of the Old English Letter Foundries</em>  (1887), rev. by A. F. Johnson. London: Faber and Faber, 1952.&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>Ryder, John.  <em>Flowers and Flourishes</em> . London: The Bodley Head, 1972.&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>A block with a central hole in which a type letter could be inserted.&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>Darby&rsquo;s signed output is estimated from the  <em>The English Short Title Catalogue</em>  ( <em>ESTC</em> ), and <sup id="fnref:21"><a href="#fn:21" class="footnote-ref" role="doc-noteref">21</a></sup>. When printers have been subjected to comprehensive study, their output has been found to be in the thousands rather than the hundreds. See <sup id="fnref:22"><a href="#fn:22" class="footnote-ref" role="doc-noteref">22</a></sup>.&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p>Wilkinson, Hazel.  “Printers’ Flowers as Evidence in the Identification of Unknown Printers: Two Examples from 1715.”    <em>The Library</em> , 7th ser., 14 (2013): 70–9.&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13">
<p>Barchas, Jeanine.  <em>Graphic Design, Print Culture, and the Eighteenth-Century Novel</em> . Cambridge: Cambridge University Press, 2003.&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14">
<p>Toner, Anne.  <em>Ellipsis in English Literature: Signs of Omission</em> . Cambridge: Cambridge University Press, 2015.&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:15">
<p>Corsini, Silvio.  “Passe-Partout.”    <em>Bulletin des bibliothèques de France</em>  5 (2001): 73–9.&#160;<a href="#fnref:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:16">
<p>Bigün, J., S. K. Bhattacharjee, and S. Michel.  “Orientation Radiograms for Image Retrieval: an Alternative to Segmentation.”    <em>ICPR ’96: Proceedings of the International Conference on Pattern Recognition</em>  (Vienna, 1996): 346–50.&#160;<a href="#fnref:16" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:17">
<p>Michel, S., B. Karoubi, J. Bigün, and S. Corsini.  “Orientation radiograms for indexing and identification in image databases.”    <em>European Conference on Signal Processing (Eupsico)</em> , (Trieste, 10-13 Sep. 1996), 1693–96.&#160;<a href="#fnref:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:18">
<p>Bergel, G., A. Franklin. M. Heaney, R. Arandjelović, A. Zisserman, and D. Funke.  “Content-Based Image-Recognition on Printed Broadside Ballads: The Bodleian Libraries’ ImageMatch Tool.”    <em>IFLA WLIC</em>  (Singapore 2013). <a href="http://library.ifla.org/209/1/202-bergel-en.pdf">http://library.ifla.org/209/1/202-bergel-en.pdf</a>&#160;<a href="#fnref:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:19">
<p><a href="https://fleuronweb.wordpress.com">https://fleuronweb.wordpress.com</a>&#160;<a href="#fnref:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:20">
<p>Development and maintenance of VISE and VIC is supported by EPSRC programme grant Seebibyte: Visual Search for the Era of Big Data (EP/MO13774/1).&#160;<a href="#fnref:20" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:21">
<p>Foxon, David.  <em>English Verse 1701–50: A Catalogue of Separately Printed Poems</em> . Cambridge: Cambridge University Press, 1975.&#160;<a href="#fnref:21" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:22">
<p>Maslen, Keith.  <em>Samuel Richardson of London, Printer: A Study of his Printing Based on Ornament Use and Business Accounts</em> . Dunedin NZ: University of Otago, 2001.&#160;<a href="#fnref:22" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry><entry><title type="html">German Narratives in International Television Format Adaptations: Comparing Du und Ich (ZDF 2002) with Un Gars, Une Fille (Quebec 1997-2002)</title><link href="https://rlskoeser.github.io/dhqwords/vol/15/1/000545/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://rlskoeser.github.io/dhqwords/vol/15/1/000545/</id><author><name>Edward Larkey</name></author><published>2021-05-21T00:00:00+00:00</published><updated>2021-05-21T00:00:00+00:00</updated><content type="html"><![CDATA[<p>The proliferation of global television formats since the 1950s, particularly those of scripted fictional narratives and their local adaptations, raise important questions about the nature and content of global culture and discourses, and the local contributions to those discourses. Each iteration is the result of cultural, historical, political, technical, and economic conditions specific to each country, and help determine how culturally, aesthetically, and discursively proxemic local narratives are created. German producers have long participated in the global format trade, both officially and inofficially. The German adaptation of the US crime series  <em>Dragnet</em>  in the 1950s, known as  <em>Stahlnetz</em> , became paradigmatic for a long tradition of German crime shows currently still popular, such as the West German  <em>Tatort</em>  (ARD from 1971), and the East German  <em>Polizeiruf 110</em> , and the plethora of similar shows currently on both public service broadcasters ARD and ZDF, as well as their private broadcasting counterparts. Some of the more successful and popular ones, such as  <em>Ein Herz und eine Seele</em>  (1973-1976), a loose adaptation of the British series  <em>Till Death Us Do Part</em>  (BBC1 1965-1975), may not be immediately recognizable as an adaptation, and some of them, such as  <em>Lindenstrasse</em>  (ARD 1985-2020) or  <em>Gute Zeiten Schlechte Zeiten</em>  RTL 1992-) may have long surpassed the broadcast period of the original and therefore have become shows in their own right.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000545/resources/images/table01.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000545/resources/images/table01_hu659fc29ff8a3aa0d4729f487a0088a61_327537_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/15/1/000545/resources/images/table01_hu659fc29ff8a3aa0d4729f487a0088a61_327537_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/15/1/000545/resources/images/table01.jpg 635w" 
     class="portrait"
     ><figcaption>
        <p>List of different versions of <em>Un Gars, Une Fille</em> and the dates of their first broadcast. Table courtesy of Avanti Cine Group.
        </p>
    </figcaption>
</figure>
<p>Some of the more recent prominent adaptations in German television like  <em>Verliebt in Berlin</em>  (ZDF 2006-2008), adapted from the Colombian telenovela  <em>Yo soy Betty, la fea</em>  (RCN 1999-2001), or  <em>Stromberg</em>  (ProSieben 2004-2012), adapted from the Ricky Gervais/Steve Merchant UK mockumentary comedy  <em>The Office</em>  (BBC2 2001-2003), are more or less recognizable adaptations of the originals. With the popularity and expansion of particular genres such as doctor and hospital dramas and comedies, it is sometimes difficult to determine if a series such as  <em>In aller Freundschaft-Die jungen Ärzte</em>  (ARD 2015-) is an adaptation of the US  <em>Grey’s Anatomy</em>  (ABC 2005) as is the clearly identifiable Turkish series  <em>Doktorlar</em>  (Show TV 2006-2011) of the Colombian  <em>El Corazon Abierto</em>  (RCN 2010-2011).</p>
<p>Less successful adaptations have also been attempted on German television. The UK doctor series  <em>Doc Martin</em>  (ITV 2004 -), adapted for German television as  <em>Doktor Martin</em>  (ZDF 2006-2009) starring Axel Milberg, was cancelled after two seasons, while the US sitcom  <em>Married with Children</em>  (Fox 1987-1997), known in its German adaptation  <em>Hilfe, meine Familie spinnt</em>  (1993), was less successful and unable to achieve the success of adaptations or the original in other markets such as the Russian, Argentinian, Chilean, or Colombian. Among the less successfully adapted series on German television was the adaptation of the Quebec series  <em>Un Gars, Une Fille</em>  (Radio Canada 1997-2002). Since the series went on to be successfully adapted in a wide variety of other markets up to the present, I believe it warrants a comparative investigation into factors which may or may not have contributed to its lack of success. The series started out in Quebec as a 6-10 minute stopgap sketch comedy show between primary half-hour or hour-long shows. Later, it was transformed into a half-hour sitcom-like comedy incorporating the separate sketch segments. While the French, Italian, and Spanish adaptations retain the same or similar sketch comedy length, most of the other adaptations, including the German, incorporate three such sketches to create a half-hour sitcom-like structure. In the twenty-year history of adapting the format (see Table 1 for the official versions), it has undergone some changes, but still retains its basic structure and sequencing.</p>
<p>The current study will investigate adaptations from the standpoint of creating the content and structure of the adaptation as a hybrid narrative subject to analytical deconstruction and reconstruction by means of computer annotation software. For the purposes of the analysis in this study, Chalaby’s definition of a television format will be used, which, even though he applies it to reality and other non-scripted series, can also be applied to scripted dramas and comedies such as  <em>Un Gars, Une Fille</em> . Chalaby (2011) defines a format as  “a show that can generate a distinctive narrative and is licensed outside its country of origin in order to be adapted to local audiences”   <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. Chalaby’s definition includes also a narrative component and he refers to the concept used by the Format Recognition and Protection Association (FRAPA) that includes  “a distinctive narrative progression”   <sup id="fnref1:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>,  “dramatic arcs,”    “storylines,”  and  “trigger moments”   <sup id="fnref2:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. Chalaby cites Michel Rodrigue, one of the driving forces behind the international distribution of  <em>Un Gars, Une Fille</em> , who Chalaby also calls  “one of the industry’s founding fathers”  to highlight the inherently transnational nature of formats. According to Rodrigue, a series becomes a format  “only once it is adopted outside its country of origin”   <sup id="fnref3:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. A format entails  “a transfer of expertise”  (Page 3 Top) contained in a set of rules of international and local knowledge passed down to the producers in a bible. Therefore, formats are hybrid and function as bridges to  “take viewers through a succession of emotional states.”</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000545/resources/images/figure01.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000545/resources/images/figure01_huf926bad709eee2006a0d16ff4c8714d2_147808_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/15/1/000545/resources/images/figure01_huf926bad709eee2006a0d16ff4c8714d2_147808_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/15/1/000545/resources/images/figure01.jpg 813w" 
     class="landscape"
     ><figcaption>
        <p>Bar graph of Part 1 of the Mother-in-Law scene illustrating shot length (in percentage) of the scene, with the smaller blue-colored fields indicating the transitions between the shots. The longest segments in most versions is the last light-brown colored segment which is the confrontation between the boyfriend and the mother-in-law. That has been excluded from the German version.
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000545/resources/images/figure02.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000545/resources/images/figure02_hud62669899a20f0606f6de642bca49c4e_192968_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/15/1/000545/resources/images/figure02_hud62669899a20f0606f6de642bca49c4e_192968_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/15/1/000545/resources/images/figure02.jpg 1166w" 
     class="landscape"
     ><figcaption>
        <p>Bar graph of Part 2 of the Mother-in-Law scene showing the longest shot to be the mother-daughter talk about trauma, a segment which has been excluded from the German version.
        </p>
    </figcaption>
</figure>
<p>There are digital instruments available for measuring shot length (<a href="http://www.cinemetrics.lv">www.cinemetrics.lv</a>) recognizing facial characteristics such that male and female characters may be distinguished and their screen times measured (<a href="http://neurotechnology.com">http://neurotechnology.com</a>). There are also digital instruments, both commercial (Atlas.ti and MMA Video) and open source (ELAN, Anvil), which assist in the measurement of different kinds of segments to reveal specific localization strategies underlying success or lack thereof. The current project uses Adobe Premiere and Final Cut to multimodally compare the German  <em>Du und ich</em>  with the Quebecois original and counterpart  <em>Un Gars, Une Fille</em> . The bar graph for Part 1 and Part 2 of the mother-in-law scene were developed using the annotation function of Adobe Premiere Pro, the results of which were tables of segment lengths after exporting to Microsoft Excel. In Excel, the tables were transformed into bar graphs that are displayed here.</p>
<p>A multimodal investigation is appropriate since the entire range of modes of communication may be deployed in culturally salient ways in order to achieve what I am calling, with <a href="#uribe-johngbloed2014">Uribe-Jungbloed and Medina (2014)</a>, discursive proximity. Multimodal scholar Kay O’Halloran was a leading figure in the development of the video annotation software MMA-Image and MMA-Video. The software was developed at the Multimodal Analysis Lab in the Interactive and Digital Media Institute at the National University of Singapore and was designed to use a  “multimodal digital semiotics approach involving the development of interactive software with functionalities for systematic multimodal analysis of text, images, and videos”   <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. In asserting that transcription is transduction, <a href="#flewitt2013">Flewitt et al (2013: 52)</a>, point to the challenge of balancing  “the accurate notation of events,”  the  “clear description for the research  reader,  and the transcription format adequate for the research purpose while doing justice to the type of data collected”   <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>. According to them, the use of annotation software results in  “reduced versions of observed reality, where some details are prioritized and others are left out”   <sup id="fnref1:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>. Television scholar Jeremy Butler, whose book employs a narratological approach to the study of televisual texts, investigated the use of digitally generated statistical data to study the editing style of the sitcom  <em>Happy Days</em>  (1974-1984) with the online Shot Logger software <a href="http://www.shotlogger.org">www.shotlogger.org</a> to measure and compare cutting rates of season 1 and season 2 to determine a statistically significant difference between them due to the switch from a single to a multiple camera production <sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000545/resources/images/table02.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000545/resources/images/table02_hu239a65e27b32fac08e5543de060a67d8_216629_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/15/1/000545/resources/images/table02_hu239a65e27b32fac08e5543de060a67d8_216629_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/15/1/000545/resources/images/table02.jpg 1044w" 
     class="landscape"
     ><figcaption>
        <p>Illustration of the anomalous characteristics of the German version: Notice only one part of the Mother-in-Law visit scene instead of the two parts of the other versions. It is also little more than half the length of the longer pre-visit segments.
        </p>
    </figcaption>
</figure>
<p>Relevant approaches to cross-cultural and critical study of multimodality in my own work use concepts of <a href="#jewitt2016">Jewitt (2016)</a> for understanding general principles of multimodality in their interaction with each other in multimodal ensembles which are intentionally selected and configured for particular — and in our case cross-culturally salient — types of meaning making <sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>, especially the interaction of image and writing <sup id="fnref1:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>. <a href="#bateman2011">Bateman and Schmidt (2011)</a> are pertinent for cross- cultural film and television studies, since — derived from Metz, Bordwell, and others — they explicate the importance of and expound upon the notions of shots, scenes, spatiality, temporality and sequentiality underlying cross-cultural film and television analyses which include qualitative and quantitative data. Finally, <a href="#machin2013">Machin (2013)</a> assists in developing a methodology for cross-cultural multimodal critical discourse analysis. For him, critical discourse studies represent not only  “a kind of knowledge about what goes on in a particular social practice, ideas about why it is the way it is and what is to be done,”  but also reveal  “models of the world and why these are legitimate and reasonable ways of acting in the world”   <sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>. I am therefore multimodally extending the notion of cultural proximity proposed by <a href="#straubhaar2005">Straubhaar and La Pastina (2005)</a> which is  “at work at multiple levels due to peoples’ complex identities beyond linguistic, religious, geographic or other group cultures based on dress, ethnicity, gestures, humor etc.”   <sup id="fnref1:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>, whereby audiences will prefer television programs that are  “most proximate or most directly relevant to them in cultural and linguistic terms”   <sup id="fnref2:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>. <a href="#vankeulen2016">Van Keulen (2016)</a> introduces the notion of aesthetic proximity to address the fact that certain technologically- based aesthetic components of audiovisual texts go beyond the linguistic and cultural systematic patterns to encompass the entire range of modes of communication in the adaptation process.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000545/resources/images/figure03.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000545/resources/images/figure03_hu5d4d4af13a53f9dee6df2227e73445ae_332558_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/15/1/000545/resources/images/figure03_hu5d4d4af13a53f9dee6df2227e73445ae_332558_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/15/1/000545/resources/images/figure03.jpg 620w" 
     class="landscape"
     ><figcaption>
        <p>Screen shot of video clips of nine different versions of <em>Un Gars Une Fille</em> (l to r, upper to lower: Quebec, Serb, Israeli, Polish, Ukrainian, Turkish, Latvian, Russian, Bulgarian) showing the use of the same music segment at different points in the narrative of the crucial confrontation scene between the boyfriend and the mother-in-law. The music indicates sympathy for the different characters in or outside the frame in the confrontation. The music cue therefore complete changes the focus of identification purely by its insertion at different times during the shot.
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000545/resources/images/figure04.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000545/resources/images/figure04_hua0d4d4b24f4da3a102e4452ac738b95f_2097363_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/15/1/000545/resources/images/figure04_hua0d4d4b24f4da3a102e4452ac738b95f_2097363_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/15/1/000545/resources/images/figure04_hua0d4d4b24f4da3a102e4452ac738b95f_2097363_1200x0_resize_q75_box.jpg 1200w,/dhqwords/vol/15/1/000545/resources/images/figure04_hua0d4d4b24f4da3a102e4452ac738b95f_2097363_1500x0_resize_q75_box.jpg 1500w,/dhqwords/vol/15/1/000545/resources/images/figure04_hua0d4d4b24f4da3a102e4452ac738b95f_2097363_1800x0_resize_q75_box.jpg 1800w,/dhqwords/vol/15/1/000545/resources/images/figure04.jpg 3242w" 
     class="landscape"
     ><figcaption>
        <p>Screen shot of Final Cut Pro timeline with the Mother-in-Law scene(s) of the one-part German series <em>Du und ich</em> on top, and the two-part Quebec series scenes in <em>Un Gars, Une Fille</em> on the bottom.
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000545/resources/images/figure05.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000545/resources/images/figure05_hua0d4d4b24f4da3a102e4452ac738b95f_1997117_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/15/1/000545/resources/images/figure05_hua0d4d4b24f4da3a102e4452ac738b95f_1997117_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/15/1/000545/resources/images/figure05_hua0d4d4b24f4da3a102e4452ac738b95f_1997117_1200x0_resize_q75_box.jpg 1200w,/dhqwords/vol/15/1/000545/resources/images/figure05_hua0d4d4b24f4da3a102e4452ac738b95f_1997117_1500x0_resize_q75_box.jpg 1500w,/dhqwords/vol/15/1/000545/resources/images/figure05_hua0d4d4b24f4da3a102e4452ac738b95f_1997117_1800x0_resize_q75_box.jpg 1800w,/dhqwords/vol/15/1/000545/resources/images/figure05.jpg 3250w" 
     class="landscape"
     ><figcaption>
        <p>Final Cut Pro timeline with Mother-in-Law scene(s) showing exploded view of the German series <em>Du und ich</em> lined up with the corresponding segments in <em>Un Gars, Une Fille</em> .
        </p>
    </figcaption>
</figure>
<p>The confluence of cross-culturality, multimodality and narratology is evident in Jeremy Butler’s characterization of the televisual text as polysemic, but he states the televisual text  “does not present all meanings equally positively”  and explains that meanings are adjusted differently  “through dialogue, acting styles, music, and other attributes of the text”   <sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>. Butler also points out that the televisual text is implicitly structured in a pattern to be polysemic but reflects and supports those in a position of political power while allowing space for alternative or contested meanings <sup id="fnref1:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>. Multimodal research aims for systematic interpretative analysis of televisual and filmic texts by  “finding systems of contrasts that organize and pre-structure filmic devices”   <sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>. The assumption underlying the research comparing the German  <em>Du und ich</em>  with  <em>Un Gars, Une Fille</em>  is that different modes and complexes of modes of communication are interculturally salient and are the component of a multimodal cross-cultural critical analysis.</p>
<p>My own previous cross-cultural comparative research has identified different types of adaptation based on similarities and differences in narrative structure, content, and sequencing <sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup> of various adaptations, including  <em>Verliebt in Berlin</em>  (2009a) and  <em>Stromberg</em>  (2009b), and other versions of  <em>Un Gars, Une Fille</em> . This research created a typology of adaptations which measured proximity to, or distancing from, the narrative structure, content, and sequencing of the original or many other adaptations.</p>
<p>In a study entitled  “Narrative as a Mode of Communication: Comparing TV Format Adaptations with Multimodal and Narratological Approaches,”  (2019) in a volume edited by Wildfeuer, Pflaeging, Bateman, Tseng and Seizov (Eds.) entitled  <em>Multimodality. Towards a New Discipline</em> , I argue for incorporating quantitative and qualitative narratological data on the content, structure and sequencing into multimodal analyses of television format adaptations, using the example of the mother-in-law scenes in  <em>Un Gars, Une Fille</em> , in order to supplement multimodal hermeneutic interpretative analyses as an extra dimension. For this current study, I completed bar graphs (see Figures 2 and 3) of 10 different versions which illustrated the relative lengths and durations of shots in percentages of total length of episodes to register culturally specific additions, deletions, and modifications made to the different versions to accommodate cultural proximity.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000545/resources/images/figure06.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000545/resources/images/figure06_hua0d4d4b24f4da3a102e4452ac738b95f_1941250_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/15/1/000545/resources/images/figure06_hua0d4d4b24f4da3a102e4452ac738b95f_1941250_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/15/1/000545/resources/images/figure06_hua0d4d4b24f4da3a102e4452ac738b95f_1941250_1200x0_resize_q75_box.jpg 1200w,/dhqwords/vol/15/1/000545/resources/images/figure06_hua0d4d4b24f4da3a102e4452ac738b95f_1941250_1500x0_resize_q75_box.jpg 1500w,/dhqwords/vol/15/1/000545/resources/images/figure06_hua0d4d4b24f4da3a102e4452ac738b95f_1941250_1800x0_resize_q75_box.jpg 1800w,/dhqwords/vol/15/1/000545/resources/images/figure06.jpg 3236w" 
     class="landscape"
     ><figcaption>
        <p>Final Cut Pro timeline illustrating the gaps in the German series <em>Du und ich</em> indicating the exclusion of segments included in the Quebec original version, <em>Un Gars, Une Fille</em> .
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000545/resources/images/figure07.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000545/resources/images/figure07_hua0d4d4b24f4da3a102e4452ac738b95f_1913420_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/15/1/000545/resources/images/figure07_hua0d4d4b24f4da3a102e4452ac738b95f_1913420_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/15/1/000545/resources/images/figure07_hua0d4d4b24f4da3a102e4452ac738b95f_1913420_1200x0_resize_q75_box.jpg 1200w,/dhqwords/vol/15/1/000545/resources/images/figure07_hua0d4d4b24f4da3a102e4452ac738b95f_1913420_1500x0_resize_q75_box.jpg 1500w,/dhqwords/vol/15/1/000545/resources/images/figure07_hua0d4d4b24f4da3a102e4452ac738b95f_1913420_1800x0_resize_q75_box.jpg 1800w,/dhqwords/vol/15/1/000545/resources/images/figure07.jpg 3236w" 
     class="landscape"
     ><figcaption>
        <p>Final Cut Pro timeline showing how the additional segments thee German series <em>Du und ich</em> line up with the Quebec original version of that series <em>Un Gars, Une Fille</em> .
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000545/resources/images/figure08.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000545/resources/images/figure08_hu40999cd691ba84ba8fae7a87b9d2bb07_1046103_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/15/1/000545/resources/images/figure08_hu40999cd691ba84ba8fae7a87b9d2bb07_1046103_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/15/1/000545/resources/images/figure08_hu40999cd691ba84ba8fae7a87b9d2bb07_1046103_1200x0_resize_q75_box.jpg 1200w,/dhqwords/vol/15/1/000545/resources/images/figure08_hu40999cd691ba84ba8fae7a87b9d2bb07_1046103_1500x0_resize_q75_box.jpg 1500w,/dhqwords/vol/15/1/000545/resources/images/figure08.jpg 1753w" 
     class="landscape"
     ><figcaption>
        <p>Screen shot of the pre-visit decision in <em>Une Gars, Une Fille</em> to visit the Mother-in-Law just before the seduction of the boyfriend. The intimate and endearing seduction takes place in the couple’s bathroom.
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000545/resources/images/figure09.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000545/resources/images/figure09_hu49c03114a9ce8ad2bc0bd62c0ddb3f2a_558169_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/15/1/000545/resources/images/figure09_hu49c03114a9ce8ad2bc0bd62c0ddb3f2a_558169_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/15/1/000545/resources/images/figure09.jpg 1168w" 
     class="landscape"
     ><figcaption>
        <p>Screen shot of the pre-visit decision to visit the mother-in-law ( <em>Du und ich</em> ) to get financial support for payment of a tax debt. The decision takes place in the kitchen and the couple displays less affection than the encounter in the Quebec version.
        </p>
    </figcaption>
</figure>
<p>One of the unsolved challenges I encountered when presenting knowledge gained from this methodology was the inability of current computer visualizations based on graphics, tables, and charts derived from Microsoft Excel to adequately display the complex relationships of multimodal knowledge at the heart of audiovisual texts in general, and different television format adaptations in particular.</p>
<p>In recent multimodal research on the same mother-in-law scenes <sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup>, I compared different versions of  <em>Un Gars, Une Fille</em>  and the role of music to guide the empathy of the viewer toward or away from different characters in culturally specific ways by slightly adjusting length and position of a particular segment of sad music in the audiovisual text to that particular character. These findings were placed within the context of gender role depictions and applied the dual-concern model of family conflict management strategies in the mother-in-law scenes of the different versions. It was thus possible to determine whether, in the different versions, different family members deployed yielding, problem-solving, contending, or inaction strategies toward the other members of the family in culturally specific ways, whether it be the mother-in-law, the daughter, or the male protagonist. This is illustrated in the video clip in which the same music segment is deployed in different positions of the shot to guide sympathy or empathy with a different character.</p>
<p>The story of  <em>Un Gars, Une Fille</em>  revolves around the relationship and domestic lives of a heterosexual, non-married and childless 30-something couple, who has been together for at least 15 years. Because of its sketch comedy heritage, most adaptation contain three scenes of approximately 7-8 minutes during which the couple is engaged in activities and conversations about themselves, friends, and relatives in a variety of spaces, including domestic spaces at home (kitchen, bedroom, bathroom, living room) and public spaces (shopping centers, restaurants, bars, art exhibits, in a car, at a police station, etc.). The scene at the heart of the current investigation occurs at the apartment of the female’s mother, the mother-in-law of the male partner. The scene is divided into two parts in most of the versions except the German, and each shot or topic of conversation is divided by a distinctive transition which is comprised of music and graphics. The scene is usually monospatial, i.e., it takes place in the dining room of the mother-in-law only, with a stationary and largely point-of-view camera in which the mother is, to varying extents, outside of the frame. The visit to the mother-in-law is instigated by a sex scene during which the daughter has to convince the partner to go with her to her mother’s, and the scene ends when the couple leaves the apartment of the mother-in-law with leftovers.</p>
<h2 id="the-german-versions-anomalous-narrative-structure-and-content-transactional-relationships">The German Version’s Anomalous Narrative Structure and Content: Transactional Relationships</h2>
<p>All of my above-mentioned investigations excluded deeper comparisons with the German version, for the most part, due to the anomalous nature of the German version. The chart in Figure 4 illustrates that the German version of the pre-visit scene, in which the visit to the mother-in-law is anticipated, is more the twice as long as all other versions at 8:14, while the visit itself, divided into two separate episodes in most other versions, is put into one episode in the German version, and is thus half the length of the others. The anomalous character of the narrative sequencing and structure of the German version can be seen not only in the Figure, but also when the German version is lined up with the Quebec version on a Final Cut Pro timeline as in the four illustrations (Figures 5-8).</p>
<p>This is one of several structural, content, and sequence modifications in the German adaptation which give an indication of the radically different historical and political context of family and gender discourses in that country compared to others. In most of the other versions, the motivation to visit the mother-in-law can be traced back to the daughter’s affection and emotional bond with her mother. In order to convince her boyfriend/husband to visit her mother, who hates him and likewise hates the mother-in-law, the female partner seduces him with oral sex, filmed tightly-framed in a bathroom while he is shaving, brushing his teeth, or cutting the hair in his nose. The shot takes approximately two minutes and is usually done in a comical, yet intimately positive manner highlighting the affection between them. This can be seen in the screen shot of the Quebec version in Figure 10.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000545/resources/images/figure10.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000545/resources/images/figure10_hu964d10a175e07bf2e44221a083f99141_70411_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/15/1/000545/resources/images/figure10_hu964d10a175e07bf2e44221a083f99141_70411_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/15/1/000545/resources/images/figure10.jpg 994w" 
     class="landscape"
     ><figcaption>
        <p>Loyalty quality of German version with a primarily obligation-oriented relationship motivation vs. other versions with a more affection- oriented motivation to visit the mother of the daughter.
        </p>
    </figcaption>
</figure>
<p>In the German version, on the other hand, the visit to the mother-in-law is motivated by an effort to get money from the mother to pay back a tax debt discovered after the female protagonist discarded her partner’s receipts needed to take deductions. She suggests a visit to her mother to see if the mother can help them out. This transactional instrumentalization of personal and family relationships stands in sharp contrast to the justification for visiting the mother-in-law in all other versions. This story line also forms the narrative arc for the visit in the German version since at the end of the visit the mother-in-law merely hands the couple an envelope upon which the address of her ex-husband’s tax attorney is attached so that they might consult with him later. Even though the transactional nature of the relationship seems to be discredited and reproached, the disappointed expectation of financial support casts the mother as calculatingly miserly and selfish, which resignifies both the mother-daughter relationship and the couple’s relationship to the mother as instrumentalized and transactional. This both legitimizes the skepticism of the male character while simultaneously revealing traces of affection between the mother and the daughter during the exchange. The screen shot of the German version (Figure 11) illustrates the difference in atmosphere by the change in location (the kitchen), the topic of conversation (tax debt vs mother), and the physical distance between the two during the conversation (close vs. distanced).</p>
<h2 id="obligatory-vs-affectionate-loyalty">Obligatory vs. Affectionate Loyalty</h2>
<p><em>Family loyalty</em>  is at the root of the visit in all versions, but the German version portrays this loyalty largely as an obligation, whereas the other versions foreground affection and the empathetic emotional bond between mother and daughter, even though the male character is sidelined in many of the shots. Psychologist Müller-Hohagen (1994) defines loyalty as a  “system of structured ethical demands and expectations”  internalized unconsciously as a family obligation by the child which guides it throughout its life <sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>. The chart (Figure 12) illustrates the dual nature of loyalty - incorporating aspects of obligation as well as affection and emotion - as it is realized in the different versions of the series. In the face of the lighthearted and ironically comical, and, for the male partner, self-deprecating reason to visit the mother (he states that  “men have no principles”  when the opportunity for oral sex arises), the German version closes with a confrontation and argument between the couple about how the male partner refuses to take money from the mother. The sequence after that shows the couple in front of the mother’s apartment door, with the (out of frame) mother looking through the peep hole in a point-of-view shot, listening to the couple continue the argument about not taking money from the mother. There is a noticeable lack of warmth and intimacy between the couple in the German version compared to others, and this behavior is continued throughout the visit, during which irreconcilable differences between the male partner and the mother generate confrontation and derisive comments from both of them. It is interesting to note that important sequences in the other versions are excluded in the German version. In these sequences, the male partner exhibits compassion and understanding for the mother or the daughter in spite of their differences.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000545/resources/images/figure11.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000545/resources/images/figure11_hub91e396b916747dd46c078efe91d5eaa_407760_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/15/1/000545/resources/images/figure11_hub91e396b916747dd46c078efe91d5eaa_407760_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/15/1/000545/resources/images/figure11_hub91e396b916747dd46c078efe91d5eaa_407760_1200x0_resize_q75_box.jpg 1200w,/dhqwords/vol/15/1/000545/resources/images/figure11.jpg 1275w" 
     class="portrait"
     ><figcaption>
        <p>The cover of the original US-version of the book by Nancy Friday, to which different local versions of <em>Un Gars, Une Fille</em> made explicit reference in many local versions, excluding the German <em>Du und ich</em> .
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000545/resources/images/figure12.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000545/resources/images/figure12_hu9718e0bf3dc2cf9e96d8a1504143a59e_1838773_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/15/1/000545/resources/images/figure12_hu9718e0bf3dc2cf9e96d8a1504143a59e_1838773_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/15/1/000545/resources/images/figure12_hu9718e0bf3dc2cf9e96d8a1504143a59e_1838773_1200x0_resize_q75_box.jpg 1200w,/dhqwords/vol/15/1/000545/resources/images/figure12.jpg 1275w" 
     class="portrait"
     ><figcaption>
        <p>The German edition of the Nancy Friday book, references to which were excluded in the German <em>Du und ich</em> .
        </p>
    </figcaption>
</figure>
<p>Finally, instances of physical contact between the male and female partners - all of which are included in the other versions - are likewise not as prevalent in the German version. One of the excluded segments in the German version is a discussion introduced by the daughter about the Nancy Friday book  <em>My Mother, My Self</em> , also published in Germany as  <em>Wie meine Mutter</em>  ( <em>Like My Mother</em> )  <em>My Mother My Self</em> . The daughter in the other versions uses this book to introduce the notion of trauma perpetrated upon the daughter by the mother, and engage the mother in a discussion about how the mother traumatized her in childhood. While there are variations on the activities created the trauma in the different versions - washing a beloved teddy bear in bleach by accident, refusing to let the daughter take a bath with the father, not giving porridge to the daughter before the father receives his own portion during a meal - it is significant that the notion of trauma in these versions is an individual psychological condition inflicted on the daughter by the mother with no further social consequences whatsoever and no trace of political or historical contextualization. In all other versions, the family relationship is depicted as a purely mother-daughter problem.</p>
<p>This stands in sharp contrast to notions of trauma at the heart of psychoanalytical practice in Germany, as well as to notions of trauma at the heart of social and political discourse in Germany as a result of World War II and the Nazi period. The lack of mention of trauma in the German version is glaring due to its prominent role in German historical and psychoanalytic discourse. This was intitiated by Alexander and Margarete Mitscherlich with their 1967 book  <em>Die Unfähigkeit zu trauern</em>  ( <em>The Inability to Mourn</em> ) and led to a broad array of subsequent investigations and publications of psychoanalytic practice into the role and effects of trauma in the transgenerational transmission of historical experiences in postwar German society since the 1940s. In comparing the German version of  <em>Un Gars, Une Fille</em>  with any one of the other versions, particularly those from countries whose populations were drastically affected by World War 2 such as France, Poland, Russia, Ukraine, Greece, Israel, and Italy, the political consequences of the psychological issues should be incorporated into the analysis. This would necessitate a much longer analysis than can be accomplished here.</p>
<h2 id="the-political-roots-of-german-psychological-trauma">The Political Roots of German Psychological Trauma</h2>
<p>Generic concepts of trauma and traumatization focus on individual experiences considered potentially life-threatening. These are combined with overwhelming feelings of fear and helplessness, and processing is hindered by a lack of physical and mental or emotional resources <sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>. Traumatization emerges after a particular event or series of events which are prevented from further processing and social or physical-psychological integration by the individual in its aftermath <sup id="fnref1:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>. In the German context, trauma discourse has been applied to the postwar experiences of both victims and perpetrators of the Nazi period and entails a social and historical, transgenerational component that is intertwined with the individual experiences of the traumatized and their second and third generation offspring. Psychoanalysts in Germany, Israel and other countries have grappled with trauma among the victims and perpetrators.</p>
<p><a href="#hondrich2011">Hondrich (2011)</a> asserts that 55 to 60% of the German civilian population during the war had traumatic experiences, while approximately 30% were traumatized. In addition, Hondrich advocates including all European societies in the analysis of postwar trauma because according to him these societies also experienced trauma but there was no effort made to speak about the trauma nor engage in therapies to treat it, even though it deeply penetrated the personal biographies of many members of most of these other European societies. For this reason, the methodology employed in this comparative study, which investigates the intimate trauma-affected interconnection and layering of the personal and the political at the root of the encounter of the mother-in-law with the couple, could be paradigmatic for comparing other versions of this television format in other countries, particularly in light of the fact that other versions make explicit reference to trauma within the framework of Nancy Friday’s book  <em>My Mother, My Self</em> . The book, while it does not focus on trauma, still discusses the non-verbal transgenerational transmission of the contradictory sexual identities of being a sexual woman and partner to a male spouse on the one hand, and a non-sexual nurturer and provider of emotional security and love to all family members on the other. The nonverbal or paralinguistic transmission of these ambivalent identities participates in the same socialization and internalization processes as the traumatic experiences of the wartime and postwar generations.</p>
<p>In spite of its lack of its explicit mention in the German version, there are several signals of transgenerational trauma transmission in the mother-in-law scene. The first evidence is the shock of the mother upon hearing that the daughter has visited a psychologist, an event that would explicitly and implicitly violate the efforts to keep silent about family trauma and its transmission to further generations observed in patients of German psychoanalytic practitioners (A and M Mitcherlich, Haarmann, Müller-Hohagen, Bode, and a plethora of others). In addition to showing and admitting fear, such visits would reveal the de-coupling of fright from the traumatic experiences among the mother’s post-war generation so repressed and denied by the subsequent generation <sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup>.</p>
<p>The second piece of evidence is the response of the daughter to her mother that the two of them  “never spoke about anything”  in the family. This is a reference to what psychologists Frick-Baer and Baer (2015) and others point to as  “keeping silent”  about crimes of the Nazis and other forms of war-induced trauma. They concluded that this  “speechlessness”  facilitated the transmission of feelings of guilt to the second generation, who may not have directly experienced the cause of trauma, but were affected by its aftermath through the silence of the perpetrator generation <sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup>.</p>
<p>The third indicator is the aforementioned transactional nature of the family relationship foregrounded for motivating the visit in which there is a noticeable deficit of affection between the couple. According to Gesine Schwan (72) and others, Beschweigen or keeping silent about the extent of culpability in the participation of Nazi atrocities, as well as about the trauma as a result of the wartime experiences such as expulsion, bombings, hunger, fear, rape, death, and atrocities against others affects the ability to feel and express empathy in political and personal life, develop trust in oneself and strangers, and accept responsibility for one’s own life. Schwan speaks of Gefühlskälte (coldness of feeling), i.e., a lack of emotional warmth and a sign of a damaged personality which seemed to prevail in many German families of the postwar period. Traditional pre-Nazi and Nazi varieties of masculine virtues as hardness (Härte), fulfillment of duty, self- sacrifice, and refusal of empathy were passed on to the postwar generation, which learned to hide their pain, but perpetuated the emotional emptiness induced by that behavior in interpersonal and social relationships practiced by their parents, which in turn was compensated for by the following generation in obsessions about work and pursuing a consumer lifestyle as a new utopia  <sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup> inherent in the Stunde Null or Zero Hour myth of the beginning of the German economic miracle.</p>
<p>One shot in the German version is shared among all other versions in almost unmodified form, and epitomizes the transactional instrumentalization of relationships characterizing that version to a greater extent than all others. In all versions, the mother is playing the board game  <em>Monopoly</em>  with the daughter and the daughter’s boyfriend, and the boyfriend is the first loser in the game and is thus the first to exit. This prompts the mother to mercilessly gloat about beating the male protagonist, while the daughter offers to bail out her partner, an impossibility since she has insufficient funds to do so. Still, the mother strenuously objects to the daughter’s bail-out attempt by claiming  “winner’s rights”  and displays a coarse ruthlessness towards the other two. The daughter vehemently protests to no avail, while her boyfriend is more than happy to put an end to his participation in the game. While the aggressive ruthlessness of the mother’s gaming strategy in the German version is framed as just one more aspect of the transactional and emotionally distanced relationship between the mother and the couple, in the other versions this kind of behavior is mitigated by other shots in which the mother and daughter are shown emotionally bonding over photographs, a shot that is missing in the German version. In addition, a further segment in the other versions features the boyfriend agreeing with the mother that she should rent out the daughter’s former room in the house/apartment, while the German version modifies the debate such that the mother appears to prefer the money to maintaining the daughter’s legacy in the house.</p>
<p>All of the above-mentioned indicators point to the disturbed dialog between the generations, a dialog which <a href="#schwan1997">Schwan (1997: 126)</a> considers crucial for the basic consensus of society about values and transgenerational relationships. <a href="#muller-hohagen1994">Müller-Hohagen (1994: 45)</a> emphasizes that the dialog between the generations was impaired by the war experiences of the perpetrator generation, their denial, guilt and silence such that the trust and loyalty in families, which are otherwise imbued in the child before its conscious awareness, are undermined by the contradiction that the parents are both the child’s enablers and protectors, but were also culpable in the crimes of the Nazis. <a href="#moser1993">Tilman Moser (1993)</a> underscores the collusionary culpability of second generation children in creating what he calls the  “contract”  between the generations based on the de-realized ideal of child innocence at the core of parental silence. Through identification with the parental generation, the next generation learned to hide pain and fear. The children spared their parents from embarrassing and revelatory questions about their participation in the Nazi period in order to uphold the myth of the new beginning. Moser asserts that many of the repressed, denied, and manipulated experiences and values upheld by the Nazi regime were incorporated into the minds and behaviors of the war and postwar generations, but they were also transgenerationally transmitted to the third generations, something which he states is becoming harder and harder to recognize, especially since much of the transference of this influence is unconscious due to the fact that the children of the perpetrators, the passive supporters and victims kept silent and remained emotionally damaged instead of working through their trauma.</p>
<h2 id="the-mother-boyfriend-relationship-adversarial-animosity-vs-begrudging-competition">The Mother-Boyfriend Relationship: Adversarial Animosity vs Begrudging Competition</h2>
<p>One of the most important segments in all other versions of  <em>Un Gars, Une Fille</em>  is the confrontation between the mother-in-law and the boyfriend. This segment is excluded from the German version with serious consequences for the narrative. In all other versions with the exception of the German, the adversarial relationship, mentioned in the pre-visit segment as the crucial reason the boyfriend needs extra persuasion to go along, is the topic of conversation between the mother-in-law and the boyfriend in the last shot before the end of the scene in the first part of the meeting, and helps reveal the reason for the animosity on the part of the mother. In this segment, the daughter, who is about to go to the toilet, admonishes both her mother and her boyfriend to behave themselves and not fight with each other during her absence. In most versions, the boyfriend then challenges the mother to state the reasons why she doesn’t like him. The mother replies that the boyfriend looks dishonest and untrustworthy, and looks  “like a polygamist.”  This statement undermines the credibility and justification for her animosity toward the boyfriend since she uses an obsolete term for her prejudice that the boyfriend is unfaithful to the daughter and has affairs on the side. In addition, the term polygamy is a legal term that is invoked when a couple is married, which is not the case with the relationship between the boyfriend and the daughter. Finally, the couple has been together for more than five, and in most cases for seven years in most of the versions, including the German, so the couple is clearly in a long-term, committed, and stable family-like relationship that the mother is obviously unable to accept because they are not officially married.</p>
<p>In the German version of this series, the reason for the animosity between the mother and the boyfriend is never directly revealed nor resolved in an open conflict between the two. Compared to the other versions, in which there are several segments in which the two are able to treat each other with more magnanimity, the relationship between them in the German version is more adversarial and uncompromising, with merely a statement by the boyfriend stating to the daughter that her mother doesn’t like him because they are not married, but the mother herself never reveals the real reason for her hatred of the boyfriend and the needlessly destructive adversarial relationship between them is left unexplained. This could be interpreted as a further sign of keeping silent that characterizes the war-time and the postwar generation to which the mother belongs.</p>
<p>If one were to classify the mother as a member of the postwar generation, i.e., the second generation of wartime parents, she displays a need for maintaining the value of a traditional, conventional marriage in which a child is produced and she becomes a grandmother. Even though she herself is a divorcee whose doctor husband has left her, she still has no tolerance for ambiguities in the relationship between her daughter and her boyfriend. The fact that she is divorced, a status unique to the German version since in all other versions the father has died, signals that she is at least somewhat culpable in her — seemingly — defective (in her eyes) status, while the death of the father/husband would absolve her of any culpability. Perhaps she is trying to overcompensate her guilt and culpability in the divorce from her husband by displaying her belligerence to the boyfriend, to whom she is projecting her distrust of men in general.</p>
<p>This interpretation could be buttressed by the fact that another conversation in all other versions is omitted in the German, which is a discussion between the mother and her daughter about a former boyfriend. While the topic is unpleasant for the daughter since the relationship between her and the former boyfriend was years in the past, the mother still insists on pursuing it in the presence of her current partner, who has obviously been witness to this topic on previous occasions. He comments (ironically) to both that the old boyfriend must be gay because he maintains a Christmas-card correspondence with the mother throughout the years. Even though the mother’s remarks are directed against the current boyfriend, it provides a hint that perhaps not all men in general are questionable partners.</p>
<p>It is unclear if the mother adheres to a relationship with her daughter that could be what <a href="#bohleber2011">Bohleber (2011)</a> considers to be  “narcissistic,”  one which egotistically instrumentalizes the children to satisfy the needs of the parent and not guide the children to be loving, self-sufficient, and independent personalities with their own identities. Psychologist Claudia Haarman, citing Diane Poole, explains several different types of narcissistic relationships between mothers and daughters, but it is ultimately unclear in the series if they have a disorienting, an avoiding, or an ambivalent relationship based on the depictions in the various shots of the mother-in-law scene in the German version. On the other hand, the mother seems less emotionally available to the daughter than in other versions: The German version eliminates the segment in which both mother and daughter enjoy looking at old family photographs together, which marginalizes and ignores the boyfriend. It also excluded a segment in which the mother mentions that either an aunt or a family friend has been injured in a car accident, a segment that is in all other versions. In some situations, the mother seems to be emotionally “available” to the daughter and exhibits a semblance of the nurturing role that typifies the role of the mother throughout childhood, while in other situations such as the monopoly game, she appears distant.</p>
<p>One particular instance in which the mother appears most distant is when the mother announces that she will be renting out the daughter’s former bedroom and requests that the daughter retrieve her belongings from the mother’s apartment. In all other versions of the series, the boyfriend agrees that the mother has the prerogative to undertake this step toward loosening relationship of both of them and letting go. In the German version of this series, however, the boyfriend supports his girlfriend’s protest against the mother’s efforts at removing the daughter’s cherished memorabilia from where she had grown up. The German version combines into one segment two different segments of the other versions with the result that the mother is depicted as emotionally unavailable to the daughter.</p>
<p>The aforementioned segment in the German version is a combination of a discussion in all other versions about the mother-daughter relationship initiated with the mention of the Nancy Friday book, a discussion which results in the daughter in tears because of the mother’s insensitivity (and seeming emotional unavailability) to the mother-induced trauma. In several of the versions, the boyfriend consoles the daughter and is (falsely) accused by the mother of causing the daughter’s distress, something which is plainly not the case. Because of the elimination of the confrontation between the boyfriend and the mother in the German version, this is the only instance of him consoling the daughter.</p>
<p>Still, if one were to take a comprehensive look at the relationships in the mother-in-law segment of the German version and compare these with other versions, it seems that in all versions the mother is irritated by the ambivalent nature of the relationship of her daughter with the boyfriend. If this were a traditional and conventional relationship of a married couple with a child, the mother would relinquish her role as nurturer to the boyfriend and evolve into a more equal partner with her daughter, in accordance with Nancy Friday’s characterization of these types of mother-daughter relationships. Because of the non-traditional nature of her daughter’s relationship with her boyfriend, however, the mother seems at times to be in competition with the boyfriend for the nurturing of the daughter. Friday asserts that the mother will lose this struggle due to the dual role of the daughter’s male partner as both a nurturer (in place of the mother) and as sexual partner (which the mother is unable to fulfill). Friday emphasizes that both partners see in each other  “an escape from the mother’s rules, dependence, and control”   <sup id="fnref:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup>. While this is more prominent in the other versions — the consolation segment with regard to the non-pregnancy in the conflict scene between the mother and the boyfriend prompts a snide remark by the mother that it is hard to watch them embrace and be intimate with each other. In these other versions, the daughter becomes most upset with the mother in the face of trying to determine whether the mother wants to be a friend or a — nurturing mother to the daughter.</p>
<h2 id="contribution-to-digital-humanities">Contribution to Digital Humanities</h2>
<p>Measuring the length and duration of the narrative structure, content, and sequencing of all different adaptations enables a means of precisely determining the additions, modifications, and deletions when making cross- cultural comparisons of various versions of television formats. As shown above, it is possible to identify those segments and frame these within global socio-cultural and political discourses — in this case about gender roles and identities and family conflict management strategies — operating in each local society and cross-culturally compare these with each other. This kind of methodology represents a multimodal critical discourse analysis of different versions which may be applied to a cross-cultural analysis. This kind of quantitative data represents an additional dimension of analysis which may be used to identify complex multimodal relationships that may be culturally determined, and contributes to revealing culturally, aesthetically, or discursively proximate multimodal patterns and interrelationships that would otherwise not be visible.</p>
<p>This study comprises a contribution to digital humanities that applies  “computation to the disciplines of the humanities”   <sup id="fnref:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup>. It opens new questions and comparative forms of analysis of societies and cultures by using computation to gather and compare data otherwise unavailable to hermeneutic processing and interpretation. The computer-derived quantitative multimodal data on the conflict between the mother-in-law and the boyfriend allows us to register culturally significant, yet quantitatively slight shifts in the placement of musical cues in the shots of both characters, which otherwise would not be immediately revealed in a comparative analysis.</p>
<p>From a narratological standpoint, drawing on computer-derived quantitative and qualitative data on the durations of multimodal narrative segments allows a more precise determination of narrative components explained by Jeremy Butler, such as the protagonist and antagonist and the amount of screen time devoted to them. For <a href="#butler2012">Butler (2012: 25)</a>, the protagonist of a film is a central character around which the story revolves and with whom an audience can identify. Especially in a case where the roles of protagonists may change and shift within a series or scene, placing a quantitative value on screen time may assist in defining the prominence of certain types of gender-specific roles, relationships, and power distribution between the characters. The narrative enigma or dilemma <sup id="fnref:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup>, which represents the explicit or implicit question in a drama in the series  <em>Un Gars, Une Fille</em> , revolves, in this scene, around the problematic mother-couple relationship. We used digital quantitative means for analyses to help decide if and how the conflictual relationship is resolved in the scene. Even though the German version varies substantially from the other versions in this respect, in none of the versions is the conflict definitively resolved. However, on the basis of different complexes of multimodal discursive components the empathy of the audience is directed to different characters on the basis of their identities and behaviors within the particular culturally specific narrative of the version.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Jean K. Chalaby, 2011.  “The making of an entertainment revolution: How the TV format trade became a global industry,”  in  <em>European journal of Communication</em> , 26:(4) 293-309. DOI: 10.1177/0267323111423414&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref3:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Kay O’Halloran, Marissa K.L.E, Sabine Tan, 2016.  “Multimodal Analytics. Software and visualization techniques for analyzing and interpreting multimodal data,”  in  <em>Routledge Handbook of Multimodal Analysis</em> , Jewitt (Ed.) 2nd Edition, New York: Routledge, pp. 386-395.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Rosie Flewitt, Regine Hampel, Mirjam Hauck, Lesley Lancaster, 2013.  “What are multimodal data and transcription?”  in  <em>Routledge Handbook of Multimodal Analysis</em> , 2nd Edition, New York: Routledge, pp. 44-59.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Jeremy G. Butler, 2014.  “Statistical Analysis of Television Style: What Can Numbers Tell Us about TV Editing,”  in  <em>Cinema Journal</em>  Vol. 54, No. 1, Fall, pp. 25-44.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>Carey Jewitt (Ed.), 2016.  <em>Routledge Handbook of Multimodal Analysis</em> , New York: Routledge.&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>David Machin, 2013.  “What is multimodal critical discourse studies?”  in  <em>Critical Discourse Studies</em> , 10:4, 347-355, DOI: 10.1080/17405904.2013.813770&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>Jeremy G. Butler, 2007.  <em>Television. Critical Methods and Applications</em> , 3rd Edition, New York: Routledge.&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>John A. Bateman, Karl-Heinrich Schmidt, 2011.  <em>Multimodal Film Analysis. How Films Mean,</em>  New York: Routledge. Routledge Studies in Multimodality.&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>Edward Larkey, Landry Digeon, Ibrahim Er, 2016.  “Measuring Transnationalism: Comparing TV Formats Using Digital Tools,”  in  <em>VIEW. Journal of European Television History and Culture</em> , Vol. 5, No. 14, pp. 72-92.&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>Edward Larkey, 2019a.  “Narratological Approaches to Multimodal Cross-Cultural Comparisons of Global TV Formats,”  in  <em>VIEW, Journal of European Television History and Culture,</em>  Vol 7, No. 14, pp. 38-58.&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>Jürgen Müller-Hohagen, 1994.  <em>Geschichte in uns. Psychogramme aus dem Alltag</em> , München: Knesebeck.&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p>Lydia Hantke, Hans-J. Gorges (Eds.) 2012.  <em>Handbuch Traumakompetenz. Basiswissen für Therapie, Beratung und Pädagogik</em> , Paderborn: Junfermann Verlag.&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13">
<p>Werner Bohleber, 2011.  “Trauma — Transgenerationelle Weitergabe und Geschichtsbewusstsein,”  in  <em>Vererbte Wunden. Traumata des Zweiten Weltkrieges, die Folge für Familie, Gesellschaft und Kultur</em> , Curt Hondrich (Hsg.), Lengerich: Pabst Science Publishers, pp. 9-24.&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14">
<p>Udo Baer, Gabriele Frick-Baer, 2015.  <em>Kriegserbe in der Seele. Was Kindern und Enkeln der Kriegsgeneration wirklich hilft</em> , Weinheim: Beltz.&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:15">
<p>Tilmann Moser, 1993.  <em>Der Erlöser der Mutter auf dem Weg zu sich selbst: eine Körperpsychotherapie</em> . Frankfurt/Main: Suhrkamp.&#160;<a href="#fnref:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:16">
<p>Nancy Friday, 1977.  <em>My Mother, My Self. The Daughter’s Search for Identity</em> , New York: Delacorte Press.&#160;<a href="#fnref:16" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:17">
<p>David M. Berry, Anders Fagerjord, 2017.  <em>Digital Humanities. Knowledge and Critique in a Digital Age</em> , Cambridge UK: Polity Press.&#160;<a href="#fnref:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:18">
<p>Jeremy G. Butler, 2012.  <em>Television. Critical Methods and Applications</em> , 4th Edition. New York: Routledge.&#160;<a href="#fnref:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry><entry><title type="html">Hierarchical or Non-hierarchical? A Philosophical Approach to a Debate in Text Encoding</title><link href="https://rlskoeser.github.io/dhqwords/vol/15/1/000525/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://rlskoeser.github.io/dhqwords/vol/15/1/000525/</id><author><name>Alois Pichler</name></author><published>2021-05-21T00:00:00+00:00</published><updated>2021-05-21T00:00:00+00:00</updated><content type="html"><![CDATA[<h2 id="1-introduction">1. Introduction</h2>
<p>Amongst the many theoretical questions about text, there is a philosophical, or more specifically, an ontological question. The most general form of this question is perhaps ‘What  <em>is</em>  text?’. In Digital Humanities, the issue has partly been focused around the question whether texts are hierarchical or rather non-hierarchical structures. Examples of this discussion include the statement that  “text is best represented as an ordered hierarchy of content object (OHCO), because that is what text really is”   <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>, or, in opposition to it, the statement that  “humanists are trying to represent what they all agree are non-hierarchical structures”   <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. Conflicting lessons for text encoding have been drawn from these two opposed approaches to the question. Some have concluded that the hierarchical markup grammar XML can be regarded as adequate for text encoding, because that is in their view what texts basically are, viz., hierarchical entities. Others reject XML and embedded markup more generally as inadequate, precisely on the basis of the view that texts have, or at least can have, non-hierarchical structures. In this paper, I want to argue that both conclusions have been drawn prematurely due to an erroneous approach to the ontological question about text.</p>
<p>I shall start with presenting a brief example of philosophical authorship from the last century. A reflection on the editorial history of this example and other writings from the same authorship will lead us to a view into editorial philology and, in particular,  <em>digital</em>  editorial philology. It is in this digital context that the question about the ontological nature of text and its consequences for text encoding have most forcefully been asked. I shall attempt to demonstrate that a philosophical reflection on the hermeneutical nature of our text practices not only helps to understand better the question about the ontology of texts, but also to dispel the idea that the nature of text would as such, i.e. as independent from our text practices, dictate either hierarchical or non-hierarchical markup.</p>
<h2 id="2-writing">2. Writing</h2>
<p>In July 1931, a philosopher in Cambridge reads Augustine’s  <em>Confessiones</em> . Augustine’s account of how he learned to speak as a child makes a strong impression on him. Our philosopher reads the account at a time when he is struggling with theoretical questions about language and meaning. He therefore is very sensitive to anything that even remotely deals with these things. Augustine’s description seems generally fair and representative of how we think language acquisition works. But our philosopher gets puzzled about a few sentences. Perhaps he draws a line in the margin of the book, to highlight the passages that he finds perplexing. Later, then, he notes down his thoughts in a notebook, recording what he believes was right and what he believes was wrong with the account. Later still he returns to these notes, and develops his ideas into a longer discussion. He develops an entire argument around Augustine’s account. He regards his discussion of Augustine as a way of becoming clearer about his own thought concerning linguistic meaning, and about the role that  <em>humans</em>  play in establishing the relation between words and objects. The exact intentions behind Augustine’s original account are of less importance to him now.</p>
<p>He has the discussion of Augustine, together with many other notes and remarks, typed. The resulting typescript he then cuts into paper slips. The slips are of varying sizes: some contain one remark or even only part of a remark, others contain series of remarks or also an entire page. He collects the slips together with cuttings from other typescripts. Next, he reorganizes the contents of his collection. He inserts additional sheets, with handwritten titles for chapters and subchapters. Soon he has this new arrangement of his remarks typed again, hereby producing a large  <em>new</em>  typescript. It contains more than 4,000 remarks — he calls them Bemerkungen. The Bemerkungen are typically separated from each other by one or more blank lines. The typescript looks much like an advanced book manuscript. But soon our philosopher starts to make changes, namely adding, deleting, rearranging and revising remarks and sentences in it. In many places, he adds alternative phrasings. Some parts of the typescript he goes through more than once, making changes in pencil, black ink and red ink. The amount of changes and revisions grows larger and larger. The changes now begin to also extend into parallel notebooks and other writing books. Entire new sections are added, some in the margins of the typed pages, others on the typescript’s verso pages, and yet others in separate notebooks.</p>
<p>About a year later he begins considering the idea of making his discussion of Augustine the beginning of a new book in philosophy, to appear in a parallel German-English edition. About fifteen years earlier he had published the  <em>Logisch-philosophische Abhandlung</em> ; this book had given him some status. He now produces a concise summary of his argument about Augustine, making it the beginning of a discussion about small and well-defined samples of language use. These he calls  “language games”  (Sprachspiele), and he intends to make the idea of language games the backbone of his entire new book project.</p>
<p>Eventually, after ten more years’ hard work of producing many new Bemerkungen and revising, rearranging, adding and deleting, he has yet another typescript produced that looks ready for the press. The typescript even includes a title ( <em>Philosophische Untersuchungen</em> ), a motto and a preface. However, in the remaining five years of his life, our philosopher can never bring himself to finish the work for publication. Two years after his death, in 1953, his friends finally edit and publish it with a parallel translation in English.<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup></p>
<h2 id="3-scholarship">3. Scholarship</h2>
<p>So far I have done nothing but portray a real example of philosophical authorship from the twentieth century. Note that I did not use the word text even once. I could have used the word, but I need not have used it. In some places, I could have said text instead of remark or, in others, instead of discussion or argument or book or work. But in all these cases it would have been replaceable. Our author  <em>may</em>  sometimes have asked himself: Which text should I choose here? Will I ever finish my text? Is my text good enough? Etc. However, again, the occurrences of the expression text in such questions are replaceable with words such as manuscript, book, phrasing, sequence or version. Regarding the notion of text, our real case example did not seem to pose any special theoretical difficulties. Most importantly, the  <em>ontological</em>  question of what text  <em>is</em> , clearly need not have bothered our  <em>author</em> . For the author the notion of text need not be problematic at all. An author may just write, delete, rearrange, rewrite, compose, and so on. Neither did  <em>I</em>  have to be bothered by the notion when telling the story of the example. So, if there is a specific philosophical, ontological issue about text, where does it come in?</p>
<p>Considering the further development of our philosopher’s story may help to find the answer to our question. Let us first try to locate, with the help of our narrative, the points at which text can become a theoretical issue of  <em>any</em>  sort. As this particular tale goes, before he dies, our philosopher appoints three friends who are to manage the publication of his writings. The three find themselves confronted with a huge mass of pages (which they first have to collect from different places), some handwritten, some typed, some bound in notebooks, some on loose sheets, some in orderly dossiers. This is now standardly referred to as our philosopher’s Nachlass. For some of the books and pieces that they decide to edit from this  <em>Nachlass</em> , they are able to use neat enough typescripts — for most of the publications, however, they have to make selections and combinations on both large and small scales, and need to do some substantial editing. They have to decide what to choose for publication; which version to use; how to arrange it; how many and which of our author’s variant phrasings to include; whether to use also variants added in other manuscripts; whether to obey all his instructions or only those that they find conducive; whether always to omit what our philosopher himself had deleted; whether to stick to at least some of his idiosyncratic style and punctuation; whether, and how much, to expand on his elliptic references to either his own ideas or also the ideas and works of others; how much to bother the reader with information about the character of the original  <em>Nachlass</em>  source; etc. etc.</p>
<p>If not for our author, text now seems to have become an issue at least for his editors, or for any editors of a  <em>Nachlass</em>  such as Wittgenstein’s. In the processes of editorial decision-making, such editors will often refer to precisely this thing, the text, and find themselves confronted with issues of so-called  <em>textual</em>  criticism. We can imagine them discussing and debating these issues both amongst themselves and with the users of their editions. Both the editors and their critics will argue for their respective positions by reference to what they call the text; and this invocation of the text, while the word itself often seems to refer to different things for the different sides, always seems to lend their respective standpoints and arguments strength and significance. Surely, for many of the arising disputes, the expression text will again be replaceable by some other words, e.g. source. In several cases, however, the expression clearly carries something which is not contained in those other expressions, something like the marker of a norm or standard, or of the right interpretation, and the text is precisely the expression to be used.</p>
<p>Let us complete the story with some perceptions and questions from the readers’ side. The Wittgenstein readers asked: Have we received all the text, or are parts missing? Is the text displayed in the correct sequence? Does it contain transcription errors? Does the edition maybe mislead me to adopt a wrong interpretation? Have I been given the right text? Have I been given the text as it was intended by the author? To what extent is the text authorized by its author Ludwig Wittgenstein? Does the edited text correspond to the original? The textual situation in the Wittgenstein  <em>Nachlass</em>  itself is often far from clear. The edited text could be something that physically never existed before, or no longer existed — thus, was something that had to be (re)constructed. Or, the editors came at different times to different conclusions about what the text was referring to, and for a few items different editions, as also different translations, were produced. Readers would again ask: Which is the text / translation I should use for my interpretation?</p>
<p>Now, it is true that these questions and issues bring us closer to theoretical discourse about text. But none of them necessarily brings us to the  <em>ontological</em>  question about  <em>what text is</em> . Moreover, there are disciplines that not only treat these questions and issues, but also provide answers and solutions to them. I think here in particular of editorial philology, and, of course, especially digital editorial philology. In the following, I will first stress that digital editorial philology provides solutions to the above-mentioned issues and questions. But in this context we will notice that the very same disciplines that provide the solutions, also in fact seem to give rise to our ontological question about text.<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup></p>
<h2 id="4-digital-scholarship">4. Digital scholarship</h2>
<p>Methods of textual criticism have been developed for many purposes, including for finding solutions to exactly the kind of issues and questions brought up in the previous section. Twentieth-century textual criticism has improved these methods further through the application of digital techniques. For instance, while the practice of producing editions comprising both facsimiles and transcriptions, ranging from ultra-diplomatic to so-called students versions, has already existed in the pre-digital age, the introduction of digital techniques has made producing such editions easier, cheaper and more efficient. But the digital medium has not only provided improved ways of implementing solutions that had already existed before — it has furthermore brought  <em>new</em>  solutions and possibilities. XML-based user-steered or  “interactive dynamic presentation”   <sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup> of online text archives is  <em>entirely</em>  new, that is, a genuine achievement of digital editorial philology, and it offers something that had not been possible before.</p>
<p>Many of the innovations are due to the discipline of text encoding (<sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>; <sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>). Text encoding enables us to deal with editorial challenges such as the issues from the previous section by, first, separating representation or transcription matters from presentation matters, and, second, serving the different interests we might have in editing a source by explicitly addressing them through different groups of codes (<sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>; <sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup>). For example, while one group of codes may record a manuscript’s chronological sequence, another one can take care of the physical sequence, and a third one of a specific sequence in content. Subsequently, the three encodings can be invoked independently of each other, or also in various combinations, just as required by individual users’ research needs. While pre-digital book editing, if it had not been for certain material restrictions, could have delivered some of the same possibilities, it could never have delivered the same degree of interaction and transparency which characterizes digital editorial philology that is based on text encoding. In traditional editing, the editor and publisher decide how the source is presented, while the user mostly tends to remain in a purely passive role. The typical user of traditional editions merely receives what experts have prepared for her and is rarely in a position to adequately verify the edition received. In contrast to this, with digital editing and publishing driven by text encoding, users are no longer dependent, so-to-say purely on the basis of good faith, on the decisions made by editors. Instead they are now able to check editorial decisions and, moreover, with interactive dynamic presentation tools to also complement the experts’ editing by producing alternative filterings and presentations of the source materials. Editorial philology today can satisfactorily address most of the issues about text brought up in the previous section. We can now make available all versions of a work, all variants of alternative phrasings, all editorial interpretations of a passage — in principle all options between which editors before had to choose due to material restrictions. The user of the editions will still have questions: Which of the many versions made available to me is the one I shall use? But this was a question also for our Cambridge author  <em>himself</em> .</p>
<p>These achievements of digital editorial philology have become possible through text encoding. At the same time, it is also  <em>precisely</em>  scholars of text encoding who have forcefully embarked on the  <em>ontological</em>  question What is text?.</p>
<h2 id="5-philosophy">5. Philosophy</h2>
<h2 id="hierarchical-vs-non-hierarchical-representation">Hierarchical vs. non-hierarchical representation</h2>
<p>It appears that it is exactly digital editorial philology with text encoding at its heart which has motivated the emerging, or at least the notable reinforcement, of what I have called the ontological question about text. It is particularly the question whether hierarchical text encoding grammars such as XML are adequate for the transcription of manuscript source materials that has caused considerable controversy.<sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup>  Opponents of the view often justify their position by invoking a non-hierarchical conception of the nature of text: it is the belief that texts are non-hierarchical which leads them to conclude that hierarchical encoding or markup cannot be the correct method. Paradigmatic cases they appeal to include complex manuscript materials which, so their view, are fundamentally characterized by non-hierarchy or at least multiple structures which overlap with each other. Our philosopher’s  <em>Nachlass</em>  could be regarded as such a case in question. Against this kind of argument, in turn, proponents of hierarchical markup grammars — though they grant that overlap and multiple hierarchies exist — have argued in favour of adopting the precisely opposite conception of the nature of text, namely a chiefly hierarchical one. Thus, the fundamental issue is no longer one about Which is the right text?, but concerns the ontological nature of text.</p>
<p>But is the view that text is a hierarchical object <sup id="fnref1:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> or, in opposition to it, the view that it is a non-hierarchical object <sup id="fnref1:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>, justified? And if either of the two is justified, does this lend argumentative support to a hierarchical or a non-hierarchical approach in text encoding? In answering this question more fully I would have to address at least the following two sets of questions. First, can the general assumption according to which texts are either hierarchical or non-hierarchical, put any demands on the structure of any particular markup system? Does the fact that a particular object of encoding is hierarchical, entail the demand that the encoding itself be hierarchical or, if it is non-hierarchical, that the encoding be non-hierarchical? Against the view that it does, one could argue that we ordinarily accept that three-dimensional entities are represented in two-dimensional structures. Similarly, we make use of hierarchical taxonomies for domains that in fact can be regarded as non-hierarchical; and, whilst being fully aware of the general vagueness, context-sensitivity, ambiguity etc. of ordinary language, we nevertheless take advantage of exact grammars, logics, strictly organized thesauri or computational ontologies for their analysis and processing. What, then, is it that makes it unacceptable to use hierarchical markup-languages for non-hierarchical sources, or non-hierarchical markup-languages for hierarchical sources? Secondly, are the assumptions that texts are either hierarchical or non-hierarchical objects themselves justified? On what grounds, and in what sense, can it be said that the nature of text is either of a hierarchical or a non-hierarchical structure?</p>
<h2 id="document-carriers--documents--texts">Document carriers — Documents — Texts</h2>
<p>In this paper, I have a direct focus on the second set of questions, but will provide at least a partial answer also to the first set of questions. Now, to answer the question whether texts are hierarchical entities, we should first try to find out what sort of entities texts could be on a  <em>general</em>  level. This is after all also what Renear and others wanted: To answer the question what text (really)  _ is_ . But this ontological question, in turn, should first bring us back to the issue of  <em>writing</em> . What is writing? It seems a safe thing to say that writing is an action, and as such it should be possible to describe it in terms of action theory. This implies the application of concepts such as agent, basic action, action result, and others. I would like to suggest the following characteristics of writing:</p>
<ul>
<li>First: Writing is, at least in terms of its physical movements, a basic action <sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup> and thus not caused by other actions.</li>
<li>Second: Writing produces a finite action result, the written. The written is writing’s intended result; we call it document.</li>
<li>Third: Writing does not need more than  <em>one</em>  agent.</li>
</ul>
<p>It seems important to appreciate the fact that producing documents, writing, is not the same as producing texts, and thus, to distinguish the action of producing documents from the action of producing texts. One important difference is that producing texts is producing documents with meaning, as we normally do when we write, or also furnishing documents with meaning, as we do when we read with understanding. Writing on the other hand does not  <em>need</em>  to produce meaningful documents and can also be performed by machines. Reading as such can equally be performed by machines (namely reading machines), but not reading  <em>with understanding</em> .<sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>  I would now like to introduce for the rest of this paper the technical term texting for the action of producing texts. Let us look at some more differences between writing and texting in terms of action theory:</p>
<ul>
<li>First, texting is not a basic action but is co-caused by two other actions, writing and reading. (Or: If you look at the matter as one of spoken communication, the two actions that co-cause texting are speaking and hearing.)</li>
<li>Secondly (and consequently), while the action of writing can be performed by only  <em>one</em>  agent, it seems then clear that texting  <em>is</em>  performed by more than one agent. One agent is the author, another is the understanding reader (naturally, the author and the reader can coincide in one and the same person). Consequently, texting is, unlike writing can be, not under the sole control of the author alone. Rather, texting evolves through actions that are shared among a multitude of agents. Therefore, when attempting to adequately describe texting, it is vital to include not only the author agent, but also the reader agent.</li>
<li>Third, while writing produces a finite and rather stable result (namely documents), texting does not; rather it produces an instable and potentially continuously ongoing, endless and open-ended result. Writing has a clearly determinable beginning and end in time. Texting can have a clearly determinable beginning in time, coinciding with the beginning of the action of writing with understanding, but it does not have a clearly determinable end. Now, ontologically speaking: What sort of entities exactly are then the results of texting, namely  <em>texts</em> ?</li>
</ul>
<p>If we start from a widely accepted tripartite division of what exists into objects, properties and events, it seems to make perfect sense to think of written documents, the products from writing, as  <em>objects</em> . Equally it seems to make perfect sense to conceive of the carriers of written documents — paper, trees, stone, pergament etc. — as objects. More specifically, documents and document carriers are  <em>concrete, material</em>  objects. But does the same hold true of texts? Very often the expression text is used to mean the same as document. However, it is important to note that text often also denotes something very different from a document, and that the conditions of identity in the case of text in this sense are not the same as the conditions of identity for documents. This applies for example when we say The work exists in many drafts and different versions (one text, many documents), or to any ambiguous sentence, e.g. John went to the bank, as well as cases of homonymy and polysemy (one document, many texts). Texts in this sense clearly cannot be concrete objects.<sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup>  Some have suggested that texts are abstract objects (e.g. Renear in <sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup>; <sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup>). But there are also some factors which speak against this view, be text now conceived as an abstract object in the sense of a type or as an abstract object in the sense of being an immaterial object.<sup id="fnref:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup>  Consequently, though both document and text are nouns, and many nouns denote objects, it may be that text does not denote an object — or that, to speak with Wittgenstein, the surface grammar of text misleads us into believing that it denotes an object <sup id="fnref:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup>).</p>
<p>Some of the arguments which speak against the view that texts are some kind of abstract object, are the same arguments which actually support the view that texts may be events. To classify texts as events rather than as abstract objects or a property will at first seem a strange thing to say, but it is merely so because we are used to think of texts in analogy to documents, or even document carriers: manuscripts, books, sheets of paper, computer screens etc. which all belong to the domain of objects rather than events. One of the aspects which speak in favour of the event view is that a text at no single (non-durative) point in time seems to be present in its  <em>entirety</em>  — which is a characteristic of events <sup id="fnref:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup>. A consequence from the event view of text is that the locus of a text is temporally and spatially distributed: As any event’s locus is the locus of its bearers, so must then also a text’s locus be the locus of its bearers. The text bearers cannot however only be books or computer screens; these, considered by themselves, are document rather than text bearers. If the event view of text is correct, then not only the document itself must be regarded a text bearer, but also the author and the understanding reader. Thus, the text event will need to be seen as taking place exactly in the geographically and chronologically dispersed interplay between authors, documents and readers. This fits very well with our observation above, namely that texts are shared among and coproduced by authors and readers. One advantage from the event view of texts seems to be that it does, ontologically speaking, not demand more than the following ontologically rather uncontroversial entities: as bearers of the event the concrete object document, the concrete object author, and the concrete object reader, and as event proper the action of (understanding) reading.</p>
<p>This implies that it not only makes sense to conceive of texts as events, but indeed events of a special kind, namely actions. Thus, texts not only seem to be produced by actions — they seem themselves to be actions. Within the group of actions, texts can then further be characterized by being actions which are co-produced by authors and readers, thus  <em>shared</em>  actions.<sup id="fnref:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup></p>
<p>In the last couple of paragraphs I have proposed a way of looking at the ontological nature of text which recognizes text as event rather than object, and within the category of event as action, and within the category of action, as shared action. But independent of whether the reader wants to follow me in my proposal to conceive of texts as actions that are co-produced by authors and readers, or rather wants to perceive of texts as abstract objects, or as properties of some kind — the reader will still be able to go along with me in the view that text is something which cannot exist without being sustained by an act of reading with understanding. A text that loses the understanding reader will fall back on pure document level and cease to exist  <em>as a text</em> . This aspect of the relation between document and text can be compared to the relation between music score and music: There is no music unless the music score is played (played at least in one’s mind). Naturally, the document can continue to exist even when the text ceases or pauses its existence. But the  <em>text</em>  is for its existence mind-dependent on the reader agent. A paused text can resume its existence as soon as the document is processed again in its significatory potential — in short: read with understanding by a reader.<sup id="fnref:20"><a href="#fn:20" class="footnote-ref" role="doc-noteref">20</a></sup>  This position at least, I hope, should not be controversial, at least if one agrees with the principle that signs have meaning because they are furnished with meaning by humans, and that reading with understanding is thus meaning and structure  <em>constituting</em>  rather than merely meaning and structure depicting — a principle that is treasured by hermeneutics <sup id="fnref:21"><a href="#fn:21" class="footnote-ref" role="doc-noteref">21</a></sup>.<sup id="fnref:22"><a href="#fn:22" class="footnote-ref" role="doc-noteref">22</a></sup>  However, the view that reading a document with understanding is constitutive for the meaning of this document, has then also consequences for our conception of what is going on in text encoding.</p>
<h2 id="text-encoding">Text encoding</h2>
<p>Text encoding can record data about the document carrier, the document as well as the text. Saying that the source is a notebook or a typescript or that it is written in ink or pencil, pertains to the first; recording which words it contains or which letters are deleted and which are added, pertains to the second; talking about the document’s meaning and stating that there are implicit references and allusions to a work by another author in the document, pertains to the third.<sup id="fnref:23"><a href="#fn:23" class="footnote-ref" role="doc-noteref">23</a></sup>  On whatever level text encoding moves, it will always also record data about the encoder’s engagement with the source. This becomes particularly clear where it aims at recording the  <em>text</em>  and thus moves on the third level. However, already on the level of recording data about the document carrier, the encoding attributes structure to the source rather than simply depicting a pre-existing structure (D.R. Raymond in <sup id="fnref:24"><a href="#fn:24" class="footnote-ref" role="doc-noteref">24</a></sup>). In the language of the above suggested event conception of text one could say that the encoder becomes herself inevitably one of the bearers of the text.</p>
<p>What are then the implications of our philosophical investigation for our question whether hierarchical or rather  <em>non</em> -hierarchical markup is appropriate for the encoding of texts? I think the main implication is, to make a long story short, that both are equally appropriate. For, following the present argument, what we encode are as much our own signifying text actions as the source (the source as such, as one is tempted to say). Transcription is, with Sahle’s words,  “a protocol of perception, mapping and interpretation”   <sup id="fnref:25"><a href="#fn:25" class="footnote-ref" role="doc-noteref">25</a></sup>. Whether the text itself will be hierarchical or non-hierarchical will therefore depend on  <em>us</em>  as encoders. Therefore, both the position holding that markup is to be hierarchical because text itself is hierarchical and the opposed view, can be seen to be in one sense correct, but wrong in another. Both seem to draw their consequences for text encoding on an — at least ontologically — unfounded basis. They are making it sound as though the question would be essentially a matter of finding out which is the right representation of a pre-given structure of text. But hierarchical or non-hierarchical describe aspects of our active engagement with the source and therefore concern the nature of our own  <em>actions</em>  rather than the nature of independent entities.  “ An ‘OHCO structure’ is , as Dino Buzzetti says,  not a model of the text, but a possible model of its expression ”   <sup id="fnref:26"><a href="#fn:26" class="footnote-ref" role="doc-noteref">26</a></sup>. The OHCO view of text could thus be rephrased to: Text is a hierarchical  <em>ordering</em>  of content objects. According to Desmond Schmidt, complex manuscript variant structures pose overwhelming challenges for hierarchical markup, and consequently form a primary case for the  <em>non-</em> hierarchical approach (as also for non-embedded markup). However, text variants are, on the background of the argument proposed here, not independent entities that put insurmountable constraints on our mapping acts either. What makes up a text variant is namely already co-constituted by our reading and mapping of the source. With Wittgenstein we could thus say that  <em>both</em>  sides of the debate mix sign talk and symbol talk, and that the primary field of text encoding belongs to the realm of symbols rather than that of signs. A symbol is the sign  <em>with meaning</em> : the sign as  <em>symbolized</em>   <sup id="fnref:27"><a href="#fn:27" class="footnote-ref" role="doc-noteref">27</a></sup>. Whether to encode a source in hierarchical or non-hierarchical ways is a question of how to map — symbolize — the signs of the source.</p>
<p>What could, or rather: what  <em>should</em>  then bring us to encode hierarchically rather than non-hierarchically, or the other way around? In the end, it can only be our scholarly interests and needs. If we are interested in encoding  <em>document structures</em> , then it may be important to record what we regard as overlapping structures, e.g. overlapping structures at the cross points between sentence or paragraph units on the one hand and page units on the other, through non-hierarchical encoding, or even standoff markup. If we are interested in encoding the sequence of (as such: genetically linear)  <em>writing acts</em> , a markup system permitting for recording the points where these writing acts’ manifestations cross, equally may be the thing to choose. But even in these cases, practicing one of the TEI’s recommendations for handling overlap through hierarchical XML may be equally in place.<sup id="fnref:28"><a href="#fn:28" class="footnote-ref" role="doc-noteref">28</a></sup>  In any way, it seems problematic to hold that it is the text’s nature, as something independent of us, which requires overlap markup. It is rather the nature of our  <em>representation</em>  of the source which requires hierarchical or non-hierarchical markup, thus something which is under our, not the source’s control.</p>
<p>If this wasn’t true, and consequently: if it wasn’t true that we can adequately transcribe complex primary sources in hierarchical XML, it would be quite mysterious why so many projects manage to encode and edit intricate and multifaceted, so-called overlapping and non-hierarchical handwritten materials with hierarchical XML. They do so in an effective manner, living up to the (still evolving) standards for digital scholarly editions. One example is editorial work on the Wittgenstein  <em>Nachlass</em>  by the Wittgenstein Archives at the University of Bergen (WAB). It is the ambition of WAB’s XML transcriptions to contain an accurate graphemic record of each single letter that Wittgenstein wrote in the  <em>Nachlass</em> , and of the writing acts it was produced by, or subjected to. This information is converted to diplomatic version outputs in HTML which, in short, represent the source on the level of its letters and the author’s writing acts. At the same time, our XML transcriptions also permit to produce linearized and normalized versions, and make yet other, strongly user-steered outputs produced via  “interactive dynamic presentation”   <sup id="fnref1:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup> in the spirit of Web 2.0 possible. A characteristic of the twenty thousand pages Wittgenstein  <em>Nachlass</em>  is the abundance of, partly rather complicated, text variance. Each of the around 65,000 occurrences is at WAB XML encoded not only on letter, but also on word level, which again makes outputs in diplomatic, linearized, normalized and other formats possible. It is  <em>XML</em>  that permits all this. However, at least in my view there is nothing in the  <em>source</em>  which requires us to choose the hierarchical XML over a non-hierarchical approach for achieving all this, or a non-hierarchical approach over hierarchical XML.<sup id="fnref:29"><a href="#fn:29" class="footnote-ref" role="doc-noteref">29</a></sup></p>
<h2 id="6-conclusion">6. Conclusion</h2>
<p>The version of the comment on Augustine’s account of language acquisition that our Cambridge philosopher, Ludwig Wittgenstein, eventually ended up with, includes the following passage:</p>
<blockquote>
<p>In this picture of language we find the roots of the following idea: Every word has a meaning. This meaning is correlated with the word. It is the object for which the word stands.<br>
<sup id="fnref1:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup> An analogous observation can be made about the debate on hierarchical vs. non-hierarchical markup. The way in which this debate is largely conducted suggests that the central issue concerns the accurate representation of some mind- and action-independent reality. It is assumed that, if texts are hierarchical, the correct depiction must be hierarchical; if they are non-hierarchical, the correct depiction must be non-hierarchical. According to this picture, text encoding is an act of correlating codes with objects and structures  <em>in and of themselves</em> . But any text action including text encoding is a creative symbolizing action and, thus, already in the realm of symbols. This is nothing out of the ordinary; it is simply what meaningfully engaging with the world looks like on an everyday basis; it is what each of us does all the time, without running into any theoretical difficulties. Moreover, though one sometimes can hear that the need to escape relativism and to produce encodings and editions that will benefit others (including future generations) requires strict avoidance of interpretation in the domain of encoding, it needs being said that the way of looking at things proposed here does not entail any support to relativism. Rather than worrying about relativism, we simply have to ensure — and all the time work to ensure! — that there is sufficient agreement in our interpretations. Successful communication is not dependent on there being non-interpreted facts, but on there being shared interpretations (or rather, more generally, shared understandings). The TEI substantially helps with that.</p>
</blockquote>
<p>The issues from Section 3, as we are now in a position to appreciate, are not to be regarded as pre-given. Rather, as much as they concern the sources to be edited, studied, translated etc., they equally concern ourselves: as authors, editors, readers and scholars, with our preferences, intentions, and the purposes of our actions. A simple question such as Should the edition follow the physical, chronological or content order of the written? is as much about what  <em>we</em>  want to do with the source as about the source as such. Questions of this kind ask for an engaged  _ action_ . Through websites such as WAB’s Interactive Dynamic Presentation platform this aspect is put to the fore, and the fact that actions are required, is, at least exemplarily, made explicit. Users of the WAB site can utilize XML transcriptions and XSLT tools as basis for creating text following their own editorial choices. The resulting texts will be shared actions, co-produced by at least the following agents: Ludwig Wittgenstein, WAB’s transcribers and editors, the software authors, the interacting users. The ways in which we talk and argue about text manifests that texts originate in and are carried by understanding and acting human subjects. Texts are mappings of signs onto symbols. Thus, when discussing which of the texts emerging from a rich and complex  <em>Nachlass</em>  to choose, or what to identify as a work in it, etc., we are discussing, first, how to best map this  <em>Nachlass’</em>  significatory potential onto symbols and, secondly, which of the symbolizations to give preference to. If it is true that texts are actions, then it therefore lies in the nature of text-talk that it can be evaluative and normative. For it lies in the nature of talk about actions that it can be evaluative and normative. With the later Wittgenstein, we might say that scholarly talk about text typically exhibits a normative grammar. This explains why the editorial issues described in Section 3 indeed are  <em>issues</em> .</p>
<p>In this paper, I have tried to show that the debate about hierarchical vs. non-hierarchical markup can be resolved by a reflection on the  “depth grammar”   <sup id="fnref2:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup> or  “logical grammar”   <sup id="fnref1:27"><a href="#fn:27" class="footnote-ref" role="doc-noteref">27</a></sup> of text. This grammar is, due to texts’ specific ontological nature, categorially different from the grammar of document. Texts in the sense in which they are different from documents are ontologically difficult entities and may, as I tried to argue for here, not be objects. Writing alone does not produce texts, but documents. It seems however a fact that texts are in their existence dependent on human understanding, and that it is the meaning and structure constituting aspects of document understanding which at the same time make texts something under  <em>our</em>  command and responsibility. Therefore, text encoding is no passive depiction but co-constitutes its subject: It never records the mind-independent state of the source alone; rather, it always also records its own actions of recording, its specific representation of the source. Naturally, this goes also for WAB’s own XML transcriptions of the Wittgenstein  <em>Nachlass</em> : They are no understanding-free depictions of the source, but already the results from precisely acts of understanding. The point that texts, and also transcriptions, result from acts of understanding, does however, as I have tried to explain, not need to involve any sort of unwanted relativism. The fact that it is us as understanding subjects that decide on the structure of texts explains in turn why XML can be such a successful markup system also for the encoding of complex manuscript materials as indeed it is — which it should not be if it were an independent hierarchical or non-hierarchical structure of the source that decides on the success or failure of our encoding. It is only when these central points are neglected that the debate about hierarchical vs. non-hierarchical markup can arise in the first place.<sup id="fnref:30"><a href="#fn:30" class="footnote-ref" role="doc-noteref">30</a></sup></p>
<p>The string The trees  <em>are green</em>    <em>with</em>    <em>white</em>  flowers can be seen to contain overlap between the italics of are green with white and the underlining of trees are green. A transcription such as this one:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-xml" data-lang="xml"><span style="display:flex;"><span>The <span style="color:#f92672">&lt;underline&gt;</span>trees <span style="color:#f92672">&lt;italic&gt;</span>are green<span style="color:#f92672">&lt;/underline&gt;</span> with white<span style="color:#f92672">&lt;/italic&gt;</span> flowers.
</span></span></code></pre></div><p>would not be well-formed in terms of XML, since it is non-hierarchical, that is: the content of the italic-element is not fully embedded in the content of the underline-element, but overlapping with it. However, the passage can also be transcribed as well-formed XML text in the following way, applying what is called fragmentation:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-xml" data-lang="xml"><span style="display:flex;"><span>The <span style="color:#f92672">&lt;underline&gt;</span>trees <span style="color:#f92672">&lt;italic</span> <span style="color:#a6e22e">part=</span><span style="color:#e6db74">&#34;I&#34;</span><span style="color:#f92672">&gt;</span>are green<span style="color:#f92672">&lt;/italic&gt;&lt;/underline&gt;&lt;italic</span> <span style="color:#a6e22e">part=</span><span style="color:#e6db74">&#34;F&#34;</span><span style="color:#f92672">&gt;</span> with white<span style="color:#f92672">&lt;/italic&gt;</span> flowers.
</span></span></code></pre></div><p>On overlap see more in <a href="http://www.tei-c.org/release/doc/tei-p5-doc/en/html/NH.html">http://www.tei-c.org/release/doc/tei-p5-doc/en/html/NH.html</a>. It should be emphasized that TEI XML equally allows for stand-off markup; see more in <a href="http://www.tei-c.org/Activities/Workgroups/SO/sow06.xml">http://www.tei-c.org/Activities/Workgroups/SO/sow06.xml</a>.</p>
<p>A simple example from Wittgenstein  <em>Nachlass</em>  Ms-106 can be used to exemplify the basic idea of distinguishing  <em>document</em>  and  <em>text</em>  levels through diplomatic and normalized versions, respectively:<br>




























<figure ><img loading="lazy" alt="Figure 1: Wittgenstein Nachlass Ms-106,90" src="/dhqwords/vol/15/1/000525/resources/images/figure01.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000525/resources/images/figure01_hu4e61f8a1660bdbf6f330b9ea7268a153_1216407_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/15/1/000525/resources/images/figure01_hu4e61f8a1660bdbf6f330b9ea7268a153_1216407_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/15/1/000525/resources/images/figure01_hu4e61f8a1660bdbf6f330b9ea7268a153_1216407_1200x0_resize_q75_box.jpg 1200w,/dhqwords/vol/15/1/000525/resources/images/figure01_hu4e61f8a1660bdbf6f330b9ea7268a153_1216407_1500x0_resize_q75_box.jpg 1500w,/dhqwords/vol/15/1/000525/resources/images/figure01_hu4e61f8a1660bdbf6f330b9ea7268a153_1216407_1800x0_resize_q75_box.jpg 1800w,/dhqwords/vol/15/1/000525/resources/images/figure01.jpg 4984w" 
     class="landscape"
     ><figcaption>
        <p>Wittgenstein Nachlass Ms-106,90 (<a href="http://wittgensteinsource.org/BFE/Ms-106,90_f">http://wittgensteinsource.org/BFE/Ms-106,90_f</a>). With the kind permission of © 2015 The Master and Fellows of Trinity College, Cambridge; The Österreichische Nationalbibliothek, Vienna; The University of Bergen, Bergen
        </p>
    </figcaption>
</figure></p>
<p>In this passage, Wittgenstein did  <em>not</em>  actually write out the two variants Allgemeinheitsbezeichnung and Allgemeinheit, as he did with the two variants brauchen and verwenden. He wrote only Allgemeinheitsbezeichnung and subsequently deleted part of it, yielding our reading of the passage as containing the two variants Allgemeinheitsbezeichnung (being eventually discarded) and Allgemeinheit. The diplomatic version can look something like this:</p>
<blockquote>
<p>Dann aber scheint es mir als könne<br>
man die Allgemeinheitsbezeichnung  — alle etc —<br>
in der Mathematik überhaupt nicht  brau-<br>
chen  verwenden.<br>
<sup id="fnref:41"><a href="#fn:41" class="footnote-ref" role="doc-noteref">41</a></sup> In the diplomatic version Allgemeinheitsbezeichnung is not spelled out to contain two words (Allgemeinheitsbezeichnung and Allgemeinheit), while in a normalized version it will of course be:<br>
Dann aber scheint es mir als könne<br>
man die Allgemeinheitsbezeichnung Allgemeinheit — alle etc. —<br>
in der Mathematik überhaupt nicht<br>
brauchen  verwenden.<br>
<sup id="fnref1:41"><a href="#fn:41" class="footnote-ref" role="doc-noteref">41</a></sup> Both the diplomatic and the normalized presentation are produced from one and the same transcription in XML:</p>
</blockquote>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-xml" data-lang="xml"><span style="display:flex;"><span><span style="color:#f92672">&lt;s</span> <span style="color:#a6e22e">type=</span><span style="color:#e6db74">&#34;es&#34;</span><span style="color:#f92672">&gt;</span>Dann aber scheint es mir als k&amp;ouml;nne man die <span style="color:#f92672">&lt;choice</span> <span style="color:#a6e22e">type=</span><span style="color:#e6db74">&#34;em&#34;</span><span style="color:#f92672">&gt;&lt;orig</span> <span style="color:#a6e22e">type=</span><span style="color:#e6db74">&#34;em1&#34;</span><span style="color:#f92672">&gt;</span>Allgemeinheit <span style="color:#f92672">&lt;del</span> <span style="color:#a6e22e">type=</span><span style="color:#e6db74">&#34;d&#34;</span><span style="color:#f92672">&gt;</span>sbezeichnung<span style="color:#f92672">&lt;/del&gt;&lt;/orig&gt;&lt;orig</span> <span style="color:#a6e22e">type=</span><span style="color:#e6db74">&#34;em2&#34;</span><span style="color:#f92672">&gt;</span> <span style="color:#f92672">&lt;choice</span> <span style="color:#a6e22e">type=</span><span style="color:#e6db74">&#34;dsl&#34;</span><span style="color:#f92672">&gt;&lt;orig</span> <span style="color:#a6e22e">type=</span><span style="color:#e6db74">&#34;alt1&#34;</span><span style="color:#f92672">&gt;</span>Allgemeinheitsbezeichnung<span style="color:#f92672">&lt;/orig&gt;</span> <span style="color:#f92672">&lt;orig</span> <span style="color:#a6e22e">type=</span><span style="color:#e6db74">&#34;alt2&#34;</span><span style="color:#f92672">&gt;</span>Allgemeinheit<span style="color:#f92672">&lt;/orig&gt;&lt;/choice&gt;&lt;/orig&gt;&lt;/choice&gt;</span> &amp;dash; alle <span style="color:#f92672">&lt;abbr</span> <span style="color:#a6e22e">type=</span><span style="color:#e6db74">&#34;abb&#34;</span><span style="color:#f92672">&gt;</span>etc<span style="color:#f92672">&lt;corr</span> <span style="color:#a6e22e">type=</span><span style="color:#e6db74">&#34;tra&#34;</span><span style="color:#f92672">&gt;</span>&amp;p.abb;<span style="color:#f92672">&lt;/corr&gt;</span> <span style="color:#f92672">&lt;/abbr&gt;</span> &amp;dash; in der Mathematik &amp;uuml;berhaupt nicht <span style="color:#f92672">&lt;choice</span> <span style="color:#a6e22e">type=</span><span style="color:#e6db74">&#34;dsl&#34;</span><span style="color:#f92672">&gt;</span> <span style="color:#f92672">&lt;orig</span> <span style="color:#a6e22e">type=</span><span style="color:#e6db74">&#34;alt1&#34;</span><span style="color:#f92672">&gt;&lt;del</span> <span style="color:#a6e22e">type=</span><span style="color:#e6db74">&#34;d&#34;</span><span style="color:#f92672">&gt;</span>brau<span style="color:#f92672">&lt;lb</span> <span style="color:#a6e22e">rend=</span><span style="color:#e6db74">&#34;shyphen&#34;</span><span style="color:#f92672">/&gt;</span>chen<span style="color:#f92672">&lt;/del&gt;&lt;/orig&gt;</span> <span style="color:#f92672">&lt;orig</span> <span style="color:#a6e22e">type=</span><span style="color:#e6db74">&#34;alt2&#34;</span><span style="color:#f92672">&gt;</span>verwenden<span style="color:#f92672">&lt;/orig&gt;&lt;/choice&gt;</span>&amp;p.es;<span style="color:#f92672">&lt;/s&gt;</span> 
</span></span></code></pre></div><p><sup id="fnref2:41"><a href="#fn:41" class="footnote-ref" role="doc-noteref">41</a></sup>   The same transcription can be converted also to other outputs, so — through interactive dynamic presentation — also by the external user (see <a href="http://wittgensteinonline.no/">http://wittgensteinonline.no/</a>). More WAB Wittgenstein  <em>Nachlass</em>  transcription samples in XML are available from <a href="http://wab.uib.no/cost-a32_xml">http://wab.uib.no/cost-a32_xml/</a>.</p>
<p>I am indebted to many colleagues for exchanges on the ideas appearing in this paper, including D. Apollon, S. Bangu, R. Falch, N. Gangopadhyay, S. Gradmann, A. Greve, S. Greve, C. Huitfeldt, C. Kanzian, J. Macha, S. Markewitz, G. Meggle and A. Renear. I would also like to thank the organizers and participants of GDDH 2016 (6.6.2016; see <a href="http://www.etrap.eu/activities/gddh-2016/">http://www.etrap.eu/activities/gddh-2016/</a>) for giving me the opportunity to present and discuss an earlier version of this paper, and seminars in Bergen (8.9.2016) and Innsbruck (28.4.2017) for the opportunity to discuss the philosophical ontology behind. Further I would like to thank two anonymous reviewers from DHQ for helpful and constructive comments.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>DeRose, St.J., Durand, D.G., Mylonas, E. and Renear A.  “What Is Text, Really?” ,  <em>Journal of Computing in Higher Education</em> , 1/2 (1990): 3-26.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Schmidt, D.  “The inadequacy of embedded markup for cultural heritage texts” ,  <em>Literary &amp; Linguistic Computing</em> , 25/3 (2010): 337-356.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>This is a very brief account of the story of the Austrian–British philosopher Ludwig Wittgenstein’s book  <em>Philosophical Investigations</em>   <sup id="fnref3:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup>, with a focus on its first four paragraphs where Augustine’s description (in  <em>Confessiones</em>  I, 6 and 8 <sup id="fnref:31"><a href="#fn:31" class="footnote-ref" role="doc-noteref">31</a></sup>) of how he learned to speak and understand is discussed. The book was published posthumously in 1953 by Wittgenstein’s heirs from his  <em>Nachlass</em> . The Wittgenstein  <em>Nachlass</em>  is described and catalogued by G.H. von Wright in  “The Wittgenstein Papers”  (<sup id="fnref:32"><a href="#fn:32" class="footnote-ref" role="doc-noteref">32</a></sup>, first published 1969). The earliest preserved version of  <em>Philosophical Investigations</em>  §§1–4 <sup id="fnref4:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup> is from July 1931 and can be found in Ms-111 (<a href="http://wittgensteinsource.org/Ms-111,15_f">http://wittgensteinsource.org/Ms-111,15_f</a>  <sup id="fnref:33"><a href="#fn:33" class="footnote-ref" role="doc-noteref">33</a></sup>). For a concise and easily accessible account of Wittgenstein’s way of working in the early 1930s see Joachim Schulte on <a href="http://wittgensteinsource.org/Ms-111_m">http://wittgensteinsource.org/Ms-111_m</a>  <sup id="fnref1:33"><a href="#fn:33" class="footnote-ref" role="doc-noteref">33</a></sup>.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>For an illustrative assessment of challenges and achievements in the pre-digital era of editing Wittgenstein as well as the scholarly reactions to the editions see <sup id="fnref:34"><a href="#fn:34" class="footnote-ref" role="doc-noteref">34</a></sup>.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>Pichler, A. and Bruvik, T.M.  “Digital Critical Editing: Separating Encoding from Presentation” . In D. Apollon, C. Bélisle and Ph. Régnier (eds.),  <em>Digital Critical Editions</em> , Urbana Champaign (2014), 179-202.&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>Text Encoding Initiative Consortium.  <em>TEI: P5 Guidelines for Electronic Text Encoding and Interchange. 3.0.0, March 29, 2016</em> : <a href="http://www.tei-c.org/Guidelines/P5/%5Bhttp://www.tei-c.org/Guidelines/P5/%5D(http://www.tei-c.org/Guidelines/P5/)">http://www.tei-c.org/Guidelines/P5/[http://www.tei-c.org/Guidelines/P5/](http://www.tei-c.org/Guidelines/P5/)</a> (2007).&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>Hockey, S.  <em>Electronic Texts in the Humanities: Principles and Practice</em> . Oxford (2000).&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>Huitfeldt, C.  “Multi-Dimensional Texts in a One-Dimensional Medium” ,  <em>Computers and the Humanities</em> , 28 (1994): 235-241.&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>Pichler, A.  “Transcriptions, Texts and Interpretation” . In K.S. Johannessen and T. Nordenstam (eds.),  <em>Culture and Value. Beiträge des 18. Internationalen Wittgenstein Symposiums. 13.-20. August 1995</em> , Kirchberg am Wechsel (1995), 690-695.&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>Examples include <sup id="fnref2:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>, <sup id="fnref1:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>, <sup id="fnref1:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup>, <sup id="fnref1:24"><a href="#fn:24" class="footnote-ref" role="doc-noteref">24</a></sup>, <sup id="fnref1:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup>, <sup id="fnref1:26"><a href="#fn:26" class="footnote-ref" role="doc-noteref">26</a></sup>, <sup id="fnref2:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>, <sup id="fnref:35"><a href="#fn:35" class="footnote-ref" role="doc-noteref">35</a></sup>, <sup id="fnref:36"><a href="#fn:36" class="footnote-ref" role="doc-noteref">36</a></sup>.&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>Danto, Arthur C.  “What We Can Do” ,  <em>Journal of Philosophy</em> , 60 (1963): 435-445.&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p>The distinction between reading with and reading without understanding is also explicitly commented upon by our philosopher <sup id="fnref5:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup>.&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13">
<p>Some may object that we frequently use text and document interchangeably and that the distinction between text in the sense of document and text in a sense in which it is very different from documents and document carriers, is counterintuitive and goes against standard usage of the expression. However, not only is it generally accepted that words can have a variety of different uses, and from a philosophical perspective defendable that one distinguishes between the surface and the depth grammar of our concepts and expressions, but also that there is a categorial distinction between document and text is moreover a view held by many digital humanists and scholars of textual criticism (see for example Renear in <sup id="fnref2:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup> and <sup id="fnref:37"><a href="#fn:37" class="footnote-ref" role="doc-noteref">37</a></sup>).&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14">
<p>Hockey, S., Renear, A. and McGann J.J.  “Panel: What is text? A debate on the philosophical and epistemological nature of text in the light of humanities computing research” . In  <em>Annual joint meeting of the Association for Computers and the Humanities (ACH) and the Association for Literary and Linguistic Computing (ALLC)</em> , University of Virginia, Charlottesville (1999): <a href="http://www2.iath.virginia.edu/ach-allc.99/proceedings/hockey-renear2.html%5Bhttp://www2.iath.virginia.edu/ach-allc.99/proceedings/hockey-renear2.html%5D(http://www2.iath.virginia.edu/ach-allc.99/proceedings/hockey-renear2.html)">http://www2.iath.virginia.edu/ach-allc.99/proceedings/hockey-renear2.html[http://www2.iath.virginia.edu/ach-allc.99/proceedings/hockey-renear2.html](http://www2.iath.virginia.edu/ach-allc.99/proceedings/hockey-renear2.html)</a>&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:15">
<p>Huitfeldt, C., Vitali, F. and Peroni, S.  “Documents as Timed Abstract Objects” ,  <em>Proceedings of Balisage: The Markup Conference 2012. Balisage Series on Markup Technologies</em> , 8 (2012): <a href="http://www.balisage.net/Proceedings/vol8/html/Huitfeldt01/BalisageVol8-Huitfeldt01.html%5Bhttp://www.balisage.net/Proceedings/vol8/html/Huitfeldt01/BalisageVol8-Huitfeldt01.html%5D(http://www.balisage.net/Proceedings/vol8/html/Huitfeldt01/BalisageVol8-Huitfeldt01.html)">http://www.balisage.net/Proceedings/vol8/html/Huitfeldt01/BalisageVol8-Huitfeldt01.html[http://www.balisage.net/Proceedings/vol8/html/Huitfeldt01/BalisageVol8-Huitfeldt01.html](http://www.balisage.net/Proceedings/vol8/html/Huitfeldt01/BalisageVol8-Huitfeldt01.html)</a>.&#160;<a href="#fnref:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:16">
<p>Some of the challenges that conceptions of text as abstract object have to face have been addressed by philosophical critiques of the FRBR (Functional Requirements for Bibliographic Records, <sup id="fnref:38"><a href="#fn:38" class="footnote-ref" role="doc-noteref">38</a></sup>) ontology which conceives text works, expressions and manifestations as  <em>types</em>  (see e.g. <sup id="fnref:39"><a href="#fn:39" class="footnote-ref" role="doc-noteref">39</a></sup>). Other challenges to that conception have to do with the fact that texts have a beginning in time and also change with time and place, while abstract objects in a standard sense do not.&#160;<a href="#fnref:16" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:17">
<p>Wittgenstein, Ludwig.  _Philosophical Investigations / Philosophische Untersuchungen. _ Ed. by P. M. S. Hacker &amp; Joachim Schulte, transl. by G. E. M. Anscombe, P. M. S. Hacker &amp; Joachim Schulte. Wiley-Blackwell, Oxford (2009).&#160;<a href="#fnref:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref3:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref4:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref5:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref6:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:18">
<p>Kanzian, Chr.  “Kunstwerke als Artefakte” ,  <em>Metafísica: Problemas Contemporâneos</em> , 71 (2015): 895-912.&#160;<a href="#fnref:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:19">
<p>There is a long tradition in twentieth century literary theory, poststructuralism, phenomenology, hermeneutics, reader-response criticism, linguistic pragmatics and speech act theory as well as semiotics to see an intimate connection between text and event / action, or even to view texts as some kind of event or action. However, one has to pay attention to the fact that each of these schools come with their own specific terminologies and conceptualizations which may not agree with each other, or with the approach taken here. Here, the view that texts are (shared) actions is, undertaken from the perspective of analytic philosophy, more specifically analytic ontology and action theory, although discussing these views in great detail is beyond the scope and space of the current paper.&#160;<a href="#fnref:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:20">
<p>The point of mind-dependency is discussed by <sup id="fnref1:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup> for artefacts in general, with a particular focus on works of art. My view that texts do not exist if they are not produced and maintained as such through  <em>understanding reading</em> , has stronger implications for their mind-dependency than Kanzian’s position. Kanzian holds that artefacts are for their subsistence mind-dependent only in the sense that they need a mind that  <em>can</em> , but does not actually need to recognize them as such:  “Artefakte hängen hinsichtlich ihres Bestehens zu jedem Zeitpunkt davon ab, dass mindestens ein Bewusstsein in der Lage ist, sie als solche anzuerkennen.”   <sup id="fnref2:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup>&#160;<a href="#fnref:20" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:21">
<p>Gadamer, H.-G.  <em>Wahrheit und Methode</em> . J.C.B. Mohr, Tübingen (1960).&#160;<a href="#fnref:21" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:22">
<p>It is interesting to note that Augustine himself (the very opponent criticized in Wittgenstein’s  <em>Philosophical Investigations</em>  §1 <sup id="fnref6:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup>) in fact promoted the view that  <em>includes</em>  the human as a sine qua non (e.g.  <em>De Dialectica</em>  V <sup id="fnref:40"><a href="#fn:40" class="footnote-ref" role="doc-noteref">40</a></sup>) and that will inform Wittgenstein’s entire mature philosophy: Nothing is a sign unless it is understood (and practiced) as a sign by a human.&#160;<a href="#fnref:22" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:23">
<p>Also the term text technology (cf.  <em><a href="https://texttechnology.humanities.mcmaster.ca/home.html">TEXT Technology: The Journal of Computer Text Processing</a></em> ) can be applied on all three levels, thus using text in a wide sense. Preservation methods, for example, deal with the document carrier; OCR addresses the level of the document itself; and semantic technologies again refer to the level of text in the narrow sense.&#160;<a href="#fnref:23" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:24">
<p>Biggs, M. and Huitfeldt, C.  “Philosophy and Electronic Publishing” ,  <em>The Monist. Interactive Issue</em> , 80/3 (1997): 348-367.&#160;<a href="#fnref:24" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:24" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:25">
<p>Sahle, P.  “Traditions of Scholarly Editing and the Media Shift” . Presentation at International Seminar on Digital Humanities: Scholarly Editing and the Media Shift – Procedures and Theory. Verona (2015), 8.9.2015&#160;<a href="#fnref:25" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:26">
<p>Buzzetti, D.  “Digital Representation and the Text Model” .  <em>New Literary History</em> , 33/1 (2002): 61-88.&#160;<a href="#fnref:26" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:26" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:27">
<p>Wittgenstein, Ludwig.  <em>Tractatus Logico-Philosophicus</em> . Translated by D. F. Pears and B. F. McGuinness. Routledge and Kegan Paul, London (1963).&#160;<a href="#fnref:27" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:27" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:28">
&#160;<a href="#fnref:28" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></li>
<li id="fn:29">
&#160;<a href="#fnref:29" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></li>
<li id="fn:30">
&#160;<a href="#fnref:30" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></li>
<li id="fn:31">
<p>Augustinus, A.  <em>Confessions</em> . Trans. Fr Benignus O&rsquo;Rourke O.S.A, foreword by M. Laird. DLT Books, London (2013).&#160;<a href="#fnref:31" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:32">
<p>Wright, G. H. von.  <em>The Wittgenstein Papers</em> . In G. H. von Wright,  <em>Wittgenstein</em> . Oxford (1982), 35-62.&#160;<a href="#fnref:32" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:33">
<p>Wittgenstein, Ludwig.  <em>Wittgenstein Source Bergen Nachlass Edition (BNE).</em>  Edited by the Wittgenstein Archives at the University of Bergen under the direction of Alois Pichler. In: Wittgenstein Source (2009-) [wittgensteinsource.org]. WAB, Bergen (2015-).&#160;<a href="#fnref:33" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:33" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:34">
<p>Hintikka, J.  “An impatient man and his papers” ,  <em>Synthese</em> , 87/2 (1991): 183-201&#160;<a href="#fnref:34" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:35">
<p>Schmidt, D.  “The Role of Markup in the Digital Humanities” ,  <em>Historical Social Research / Historische Sozialforschung</em> , 37/3 (2012): 125-146.&#160;<a href="#fnref:35" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:36">
<p>Sahle, P.  <em>Digitale Editionsformen. Zum Umgang mit der Überlieferung unter den Bedingungen des Medienwandels</em> . BoD, Norderstedt (2013): <a href="http://kups.ub.uni-koeln.de/5353/%5Bhttp://kups.ub.uni-koeln.de/5353/%5D(http://kups.ub.uni-koeln.de/5353/)">http://kups.ub.uni-koeln.de/5353/[http://kups.ub.uni-koeln.de/5353/](http://kups.ub.uni-koeln.de/5353/)</a>.&#160;<a href="#fnref:36" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:37">
<p>Gabler, H.W.  “Wider die Autorzentriertheit in der Edition” ,  <em>Jahrbuch des Freien Deutschen Hochstifts</em>  (2012): 316-342.&#160;<a href="#fnref:37" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:38">
<p>International Federation of Library Associations (IFLA).  <em>Functional Requirements for Bibliographic Records: Final Report</em> . UBCIM Publications-New Series 19, München (1998).&#160;<a href="#fnref:38" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:39">
<p>Renear, A., Dubin, D.  “Three of the Four FRBR Group 1 Entity Types are Roles, not Types” . In A. Grove (eds.),  <em>Proceedings of the 70th Annual Meeting of the American Society for Information Science and Technology (ASIST)</em> , Milwaukee (2007).&#160;<a href="#fnref:39" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:40">
<p>Augustinus, A.  <em>De Dialectica</em> . Trans. B. D. Jackson, from the text newly edited by J. Pinborg. Synthese Historical Library, no. 16. Reidel, Dordrecht (1975).&#160;<a href="#fnref:40" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:41">
<p>Wittgenstein, Ludwig.  _Interactive Dynamic Presentation (IDP) of Ludwig Wittgenstein&rsquo;s philosophical Nachlass [<a href="http://wittgensteinonline.no/">http://wittgensteinonline.no/</a>]. _  Ed. by the Wittgenstein Archives at the University of Bergen under the direction of Alois Pichler. WAB, Bergen (2016).&#160;<a href="#fnref:41" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:41" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:41" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry><entry><title type="html">Inferring standard name form, gender and nobility from historical texts using stable model semantics</title><link href="https://rlskoeser.github.io/dhqwords/vol/15/1/000544/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://rlskoeser.github.io/dhqwords/vol/15/1/000544/</id><author><name>Davor Lauc</name></author><author><name>Darko Vitek</name></author><published>2021-05-21T00:00:00+00:00</published><updated>2021-05-21T00:00:00+00:00</updated><content type="html"><![CDATA[<h1 id="heading"></h1>
<h2 id="introduction">Introduction</h2>
<p>The necessary condition of many endeavours in the digital humanities domain is the preparation of data in a suitable form. In the case of historical demography, this means extracting structured data about people from sources like national censuses, cadastral data, tax lists, etc. These sources are unstructured information, concealed formats that are hard to decode and laden with ambiguity. This task is often performed manually by a trained historiographical researcher, who scrutinizes archived documents. It is an extremely tedious exercise that is prone to error.</p>
<p>Due to the availability of many historiographical sources in various digital formats – from scans to transcripts – it is possible that a computation model able to achieve this task could be developed. Ideally, this system would be able to transform unstructured data, like scans of historical documents, into structured formats. In this sense, it would be great to have historical data in highly structured form, with ambiguity reduced on every level – the semantic web is a good representational format to do this.<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></p>
<p>However, the implementation of such models comes with many challenges, even when transcripts of sources are available in digital or printed formats, and the problematic recognition of indecipherable handwritten text <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> can be omitted.</p>
<p>One of the first, seemingly trivial, challenges relates to parsing name expressions that include individuals’ names, titles and occupations, and inferring basic facts about persons, such as gender and some aspects of their social statuses. This is often a prerequisite for more advanced processing, for example named entity resolution (record linkage), ontology construction, etc. Even when more contemporary data sources are involved, the ambiguity, multitude and various combinations of first name/last name/titles that are in use can make this task quite difficult to model. Sometimes, there is just not enough information available in text to reach reliable conclusions, and only an educated guess is possible. In the case of historical source transcripts, the task is even more challenging because many of the names and personal titles involved are now extinct and cannot be found in modern dictionaries. Furthermore, there are no standard transcriptions of names, and those transcripts that exist are often mottled and dirty.</p>
<p>Not much research on this particular topic has been done within the digital humanities community, but the authors expect that this will become an active field of research. As Anna Foka claims:</p>
<blockquote>
<p>The imminent assessment and representation of historical data has admittedly challenged the boundaries of historical knowledge and generated new research questions.<br>
<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup> The related problem of entity resolution, and its importance to digital humanities for have been has been researched more extensively <sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>  <sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>  <sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>  <sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>. Proper solution of this particular problem of structuring proper names can have important application to many digital humanities endeavours, from improvements of handwriting recognition systems where correct parse of a proper name can be used to improve the loss function, to usage in multilingual family narrative generation from genealogical data <sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>.</p>
</blockquote>
<h2 id="materials">Materials</h2>
<p>The primary type of the historical sources in the scope of this research are the so-called serial sources. These are sources like parish books (one of the biggest serial sources in European history), tax lists, and censuses. One of their important characteristics is systematic repetition of structure, which makes them a kind of predecessors of modern database systems, but unfortunately not in a modern structured format. Typical historiographical method of processing such sources includes a taxing process of translating them into structured forms, and it often happens that even when this is done, data are underused due to number of reasons, such as low-tech solution, large number of errors, etc. It often takes a lifetime of one lonely researcher to finish analysing just one such source, for example one tax census <sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup>.</p>
<h2 id="characteristics-of-serial-sources-in-croatian-demography">Characteristics of Serial Sources in Croatian Demography</h2>
<p>During the Middle Ages, Croatian state had undeveloped state institutions, which was mostly caused by weak central government. This was the main reason why Ottoman Empire successfully conquered it in the 15th and 16th century. Therefore, the number of sources from that period is quite limited and they are almost exclusively in Latin.<sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup></p>
<h2 id="test-case">Test Case</h2>
<p>The test case used for the research was from the middle 19th century census for the old town centre of Zagreb. This census, which was performed in 1857, was the first modern census in Croatia. It is a valuable source of information about the social structure of the 19th century Zagreb. A systematic analysis of the data has yet to be performed due to a number of difficulties. At the time of the census, Croatia was a part of the Habsburg monarchy and the language of the census is German. As the Latin language was dominant in the public service, many Latin name forms were Germanized, and many Czech, Slovak, Italian and German names were Croatized or Latinized. Additionally, there was no standard order of first and last names, many of the first and last names were not separated, and various additional notes were not clearly separated from names, especially when it comes to members of the nobility. Because of all these difficulties, the manual transformation of this source into structured data represents a very tedious and error-prone task.</p>
<p>Our goal was to develop a highly accurate model that can parse name expressions and infer standard names from Zagreb’s middle 19th century census and similar historical texts. There was also a requirement to predict gender and nobility of the person from the name expression. The transcription was available as a text file, and after some standard pre-processing and chunking, 1755 records were extracted, including expressions of person names, dates of births, occupation and similar. We decided to apply standard natural language processing tools to this text.</p>
<h2 id="methods-and-methodology">Methods and Methodology</h2>
<p>Our hypothesis was that the best model for this task could be achieved by combining a probabilistic approach with a rule-based approach in the framework of Answer Set Programming.</p>
<h2 id="applying-standard-nlp-pipeline-in-a-historical-text">Applying Standard NLP Pipeline in a Historical Text</h2>
<p>A common approach to tackle the problem of transforming unstructured images to structured information is designing a natural language programming (NLP) pipeline, wherein the first step includes optical character recognition (OCR)/handwriting recognition (HWR) or transcription. Unfortunately, both of these processes are laden with challenges, and more than often they include a lot of decrypting. The result of converting a historical source to text is usually very messy, and it is commonly referred to as a dirty text. As further processing requires relatively clean text, the dirty text usually needs to be corrected – this includes spell-checking, joining separated word parts, etc.</p>
<p>The next phase is tokenisation, i.e. splitting text into tokens, usually words. However, the situation is often quite complicated. Defining what exactly counts as a token (e.g. does it include full stops) is an important decision that can have big impact on further steps. Ready-available tokenizers (usually rule-based) did not perform well on our test case. Therefore, we have applied an aggressive tokenisation in our research, splitting everything into sequences of letters vs. non-letters. Another connected step in the NLP pipeline is the segmentation of text, usually into sentences. Although this step is not exactly applicable to our test set, it is a well-known fact that sentence splitters do not perform well on historical texts, as they are trained on the modern ones <sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>.</p>
<p>The following step would be name-entity recognition (NER), i.e. marking the beginning and end of named entities and classifying each such expression as person, organisation, place, temporal expression, etc. Since serial sources consist mostly of such named entities, successful NER would chunk almost the entire text. Unfortunately, modern NER systems do not perform well on historical text. We have tested some of the best NER systems like Stanford NER, which has an f-score of over 90% on modern texts, but it has performed badly on our test case. We have not the calculated exact measure, but the results were so flawed that it seemed needless to do a formal evaluation.</p>
<p>Another important standard task in this context would be Entity resolution, also called record linkage or record deduplication, where different occurrences of the name of same entities are connected. For example, in parish church books, one person might first be baptised, then married, then mentioned as godfather and finally be deceased. It is worth identifying all these names as referencing the same object, in this case the same person.</p>
<p>Finally, the last phase would be relation or relationship extraction, i.e. extracting relations among entities in the text, for example if a person is born at some place, married to someone, etc. In the broader sense, relation extraction can also include inferring the relationships. For example, if we have a relation being  <em>father of</em>  between a father and two daughters, we can infer relation  <em>being sister</em>  between them. Similarly, as the relation  <em>being mother of</em>  is functional, when it occurs between one entity and two different entities, the system can identify those entities as the same. This is something that is standardly done by semantic web technologies.</p>
<p>Naturally, since the NER systems did not perform well on the test case, it was impossible to apply the existing relationship extraction systems.</p>
<p>It is worth mentioning than an alternative approach to a segmented NLP pipeline could be a technique called joint inference <sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>. In joint inference all levels of processing are performed in the same time, with constraints and information on all levels used for inferring the most probable inference. This approach is more similar to the process of analysis performed by a human researcher.</p>
<h2 id="proper-names-parsing">Proper Names Parsing</h2>
<p>The segmented parts of the census were available, so our research started with analysing what would a historian working with such data do with them. We first wanted to understand how this process is performed by a human, in order to build an effective computational model. The first obstacle we encountered was understanding proper names. When historians analyse such sources, they often unconsciously extract a lot of information from the name, and these information constrain their search by reducing ambiguity. For example, if two persons in the same part of the text have the same surname, they are probably related; if someone has a female name, it is very improbable that that person is a godfather, etc. Therefore, it is worth extracting these information from the name, especially in the context of joint inference, where all available information should be used to resolve ambiguity.</p>
<p>Parsing of proper names, such as analysing inner structure of names, is not a standard part of NLP pipeline. Parts of text containing proper names are usually recognised by a NER subsystem, and other techniques are used for entity resolution and relation extraction subsystems. The task of parsing proper names does seem to be easy; in the case of personal names, one just has to use regular expressions – first token is first name and second token is last name. After that, the only thing left to do is to check the ending of the first name to classify the person as a female or a male. In reality, situation is much more complex. Even when only the modern names are analysed, there is no easy solution to the name parsing, at least no ready-available system.</p>
<p>To illustrate the problem, even in modern languages there is a great variety of names, name forms and cultural conventions. In the case of famous Icelandic singer Björk Guðmundsdóttir, her second name is not really a surname and one should address her with full name. This means that you would find her listed in address book under letter B not G. Similar case is with Arabic, Chinese, Russian, Polish, Serbian and other names that usually start with the last name. Spanish people traditionally have four names, and are addressed by the third one, and the situation in even more complicated in Brazilian Spanish.</p>
<p>The situation with names is even more complex in historical contexts. Until the second half of the 18th century, which is relatively recent, there were no standard name forms in many European countries. In the example of the first Croatian king, Kralj Tomislav, Kralj means king – it is a title, not a first name, and Tomislav is the first name. In the modern language, however, Kralj is a quite common last name. The situation gets even more complicated with noble titles, maiden names, etc.</p>
<p>It would be very helpful to have an accurate proper name parser, the results of which could be used in a more advanced analysis. To the best of the authors’ knowledge, this problem did not receive much attention in NLP related communities.</p>
<h2 id="proper-names-parsing-state-of-the-art">Proper Names Parsing State of the Art</h2>
<p>Parsing name records into constituent parts can be modelled as a sequence labelling problem, viewed as a special case of part-of-speech tagging and shallow parsing. Although this particular problem has not received much attention in recent literature, extensive work has been done on the related and more general problems of Part-Of-Speech (POS) tagging, shallow parsing and named entity recognition <sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup>  <sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup>  <sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup>.</p>
<p>Early sequence labelling systems were rule-based; for example, those developed by Brill <sup id="fnref:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup>, which are still used in some application domains <sup id="fnref:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup>. Today, the best performing models are probabilistic, and are generally based on probabilistic graphical models. In particular, models that use conditional random fields represented the state-of-the-art models <sup id="fnref:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup>  <sup id="fnref:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup>, until the recent usage of deep learning models <sup id="fnref:20"><a href="#fn:20" class="footnote-ref" role="doc-noteref">20</a></sup>.</p>
<p>However, studies on the application of probabilistic models on historical texts have yet to yield satisfactory results. It is very tedious to annotate historical texts, especially when many different sources have to be analysed and the reuse of existing training datasets is not a feasible option. Another reason why there is a need to consider alternatives to the probabilistic approach is that, due to the noisiness of historical sources, the integration of sequence labelling with a joint inference model <sup id="fnref1:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup> is promising as an alternative to the use of a traditional language processing pipeline. Joint inference can reduce OCR ambiguities, and an approach that combines text correction and sequence labelling with the higher-level syntax, semantics and historiographical constraints is more representative of the way in which a human historiographer would perform the task. For example, the probability of social status depends on location, members of the household and place of origin, and ambiguous last names can be resolved by family member records. Although joint inference is compatible with the probabilistic approach <sup id="fnref:21"><a href="#fn:21" class="footnote-ref" role="doc-noteref">21</a></sup>, the rule-based approach seems more promising in this domain. As the Markov Logic Networks, a framework that Sha and Pereira <sup id="fnref2:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup> used, does not readily scale to this kind of problem, we selected a rule-based framework that was based on stable model semantics.</p>
<h2 id="answer-set-programming">Answer Set Programming</h2>
<p>Stable model semantics can be viewed as the semantics of logic programming. So the rules can use (almost) the full expressivity power of the first-order logic. A further benefit relates to the non-monotonicity of negation as failure, which enables easy modelling of interaction among general and specific rules. Answer set programming (ASP) implements stable model semantics. Due to modern, highly optimized grounders and SAT solvers, ASP implementations are fast enough for many applications and are mostly used for high-level reasoning tasks such as planning, diagnostics, learning and scheduling <sup id="fnref:22"><a href="#fn:22" class="footnote-ref" role="doc-noteref">22</a></sup>. This framework also looks very promising for NLP applications <sup id="fnref:23"><a href="#fn:23" class="footnote-ref" role="doc-noteref">23</a></sup>, and especially for our problem. The leading ASP modelling language, Potassco, which was developed at the University of Potsdam <sup id="fnref:24"><a href="#fn:24" class="footnote-ref" role="doc-noteref">24</a></sup>, includes support for weak constraint and optimization. This enables the formalization of the sequence labelling task as an optimization problem and, therefore, seems particularly promising in the context of the joint inference model.</p>
<h2 id="dataset">Dataset</h2>
<p>The test set for the models consisted of 1774 transcribed records from the census, including name expressions, gender and nobility labels. The available dataset of 4018 labelled modern international names (12,075 tokens) was used for the initial training and test dataset.</p>
<h2 id="name-parsing-models">Name parsing models</h2>
<p>In order to evaluate and compare the appropriateness of the probabilistic and rule-based models for the task, both conditional random field (CRF) and rule-based models based on stable model semantics (SM Rules) were developed. They shared tag set and features. The selection of a tag set is an important task, since it influences the accuracy of the learned models and the usability of the model. If there are fewer numbers of categories, the accuracy will generally improve, but less structure will be introduced and less ambiguity will be reduced.</p>
<p>For a tag set, the following classes were selected:</p>
<ul>
<li>N.FN.(M/F): male/female first name; e.g., Gustav/ Josephine</li>
<li>N.LNlast name, e.g., Philippovich</li>
<li>N.LN.PREFlast name prefix, e.g. de,  von</li>
<li>N.TITLE: person title, e.g., pl. (noble), dr.</li>
<li>N.QUAL: surname qualification, e.g., ml (junior)</li>
<li>N.SALUTperson salutation, e.g., herr (mister)</li>
<li>GEO:geographic/location term, e.g., Zagreb,  Ilica</li>
<li>OTHER, terms not in the above list, like notes, comments, etc.</li>
</ul>
<p>All tags, except OTHER, had standard –B and –I suffixes, for denoting multi-token tags.</p>
<p>The features for both models included token, n-grams, packed word forms (lower/upper-case combination sequence, packed to three characters), and a dictionary entry from an available international name dictionary, including estimation of monogram frequency.</p>
<p>The CRF model was trained in the standard way, using the CRFsuite <sup id="fnref:25"><a href="#fn:25" class="footnote-ref" role="doc-noteref">25</a></sup>.</p>
<h2 id="rule-based-system">Rule based-system</h2>
<p>The rule-based model was implemented in the Potassco ASP system <sup id="fnref:26"><a href="#fn:26" class="footnote-ref" role="doc-noteref">26</a></sup>.</p>
<p>Head of the rules were in the following form:  <code>tag(I,P,</code>  _ <code>[tag],[weight], [level]</code> _ ) where  <code>I</code>  is id of record,  <code>P</code>  is position of token in name expression,  <code>[tag]</code>  is one of the tags from the tag set,  <code>[weight]</code>  is an estimation of certainty of the rule, and  <code>[level]</code>  represents generality of the rule, as explained below. Rule body is a set of (possible negated) features. An example of this rule is:</p>
<pre tabindex="0"><code> tag(I,P,n_title_b,70,1) :- lexc(I,P,n_title_b,_,_), wordform(I,P,&#34;LlLlLl&#34;), wordform(I,P-1,&#34;LuLlLl&#34;), lexc2(I,P-1,n_fn,_,1), lexc2(I,P+1,n_ln,_,1), not specExists(I,P,1). 
</code></pre><p>It defines that token at position  <code>P</code>  is a title if it has a dictionary entry for title at any frequency, it is lowercase (LlLlLl wordform), token before it is capitalized and the most frequently used first name, and the token after is the most frequently used last name according to the dictionary.</p>
<p>The last atom in the rule ( <code>specExists</code> ) used the non-monotonic nature of stable model semantics, stating that the rule is satisfied only if there is no other rule for the same token that is more specific. The most general (default) rules were on level 1, more specific on level 2, etc.</p>
<p>Some of the initial rules were hand-coded, but the majority of them were learned from the initial training dataset. Although much research has been performed on rule induction <sup id="fnref:27"><a href="#fn:27" class="footnote-ref" role="doc-noteref">27</a></sup>, there is no suitable rule learning system available for ASP; as such, a rudimentary one was developed, inspired by Inductive logic programming algorithms.</p>
<h2 id="learning-rules">Learning rules</h2>
<p>The pseudo code for the preliminary rule induction system was as follows.</p>
<pre tabindex="0"><code> Generate all features of examples in the training set Generalize features [replace constants with variables, relativize positions] Select top-n features (eliminate all with low chi-square in the training set) for lev in 1 to maxLevel predicted = tag training set with rules up to level lev-1 for tag in tag-set for x in power set of features up to length maxCardinality gain = count false negative matching x in predicted loss = count true negative matching x in predicted if gain&gt;loss add x to rules candidates for y in rules candidates sorted by gain-loss if rules does not overlap with rules add y to rules 
</code></pre><p>The hyper-parameters  <code>maxLevel</code>  and  <code>maxCardinality</code>  control the number of level of specific rules and the number of atoms in rules. For performance reasons, the learning system was implemented in Python and Potassco ASP. Trained on the modern language training set, the system generated 218 rules on four levels of generality. Token level f1-score, measured on 20% of the modern data-set was 0.95.</p>
<h2 id="results">Results</h2>
<p>We were interested to see whether the model could correctly classify all the parts of a record; i.e., name expressions. Therefore, instead of the more common precision/recall/f-score measure of token level classification accuracy, only the items where all tokens were correctly classified were counted as correct. Therefore, the parsing accuracy was defined as the percentage of test records for which the test results were identical to the manually parsed records.</p>
<p>Statistical and rule-based models were evaluated as trained on initial modern language training set and after improving models. The SM Rules model was improved by hand-writing four additional rules that were obvious from the errors in the first model. The CRF model was improved by labelling 100 additional records from the source and adding these to the dataset. This task took approximately twice the time it took to write the rules. This can be considered to represent a similar investment of resources, although it is not a precise measure of the effort invested in improving the models because both procedures depended on the characteristics of the datasets and the experience of the researcher.<br>
Parsing evaluation       Initial model    Improved model          SM rules    CRF    SM rules    CRF        Accuracy rate   79.82%  67.93%  97.01%  76.21%       Support   1416  1205  1721  1352   <br>
As the data in the table above clearly indicate, the models were significantly different, with the p-value of McNemar test for both being <code>lt 2.2e-16</code>.</p>
<h2 id="gender-and-nobility-model-results">Gender and nobility model results</h2>
<p>The gender and nobility prediction model was based on character n-Gram (length 1-9) of name expressions in census data. Name expressions were pre-processed by marking the beginning and end, lowercasing and stripping accents. A support vector classifier with linear kernel was used, and the parameters were obtained by grid search. As the results of k-folding cross validation were satisfactory, the initial plan for building rule-based classifiers was abandoned.<br>
Gender and nobility evaluation table     Nobility status prediction    Gender prediction       Class  precision  recall  f1-score  support  Class  precision  recall  f1-score  support      Noble  0.98  1.0  0.99  321  Male  0.98  0.99  0.98  192      Common  1.0  0.85  0.92  34  Female  0.99  0.98  0.98  163</p>
<h2 id="discussion">Discussion</h2>
<p>Summary of the researchers’ experience in applying statistical and rule-based approach to historical text is given in the following table:<br>
Comparison between the statistical and the rule-based approach    CRF  Rule-based      Drawbacks  Advantages  Drawbacks  Advantages      CRFs must be trained on a new training set whenever a historical source is systematically different from a previously built model.  CRFs are widely used, so it is easy to use implementations of CRF models.  If rules are hand-coded, it has to be done by researchers, trained and experienced in both domain specific knowledge and a rule-based system.   Possibility of coding general and domain specific constraints and rules.      Models are next to impossible to be modified ad hoc, in order to explore observed regularities in a new domain or historical source.  Outperforms other models (including HMM) in many application domains.   Learning algorithms are inferior to the ones used to train statistical models.   Learning can be performed on top of the hand-coded rules.      In semantically opaque models, there is no easy understandable answer to a why question.  Models can be developed from dataset labelled by persons lacking linguistic and/or computer science skills.  Complex interaction of rules can make it difficult to understand and modify the rules ad hoc.   Resulting rules are relatively semantically transparent and can be modified and improved ad hoc.</p>
<h2 id="conclusion-and-outlook">Conclusion and Outlook</h2>
<p>The preliminary results indicated that the rule-based approach, which was based on stable model semantics, is more suitable for inferring standard name forms from historical texts than the more widespread statistical approach. To confirm this result, the experiment should be repeated using additional historical sources and statistical models. To predict gender and nobility, it seems more convenient to use standard statistical classifiers when labelled data is available. The generalization accuracy of the models should be tested on additional historical sources. A model ensemble that includes both a rule-based method and the CRF model is another interesting development that is worth a future research.</p>
<p>In order to make the model more suitable for real-world applications in historiographical research, it would be worthwhile to develop an interactive interface that would enable incremental rule learning. It should use a simple web interface and the rule induction system should recommend source-specific rules to the researcher, hiding the underlying complexity of the rule system.</p>
<p>The development of a more complex system that includes joint inference from the scan of a source to a historical demography web ontology is a worthwhile longer-term goal. This research represents a small step toward the development of such a system.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Although the exact boundary between structured and unstructured data is imprecise, for the scope of this research we can define structured data as those data in which ambiguity is reduced to the level where no additional human intervention is needed to perform desired processing. Ambiguity exists on different levels. In the analysis of text, one commonly distinguishes between lexical and structural ambiguity. However, in the context of analysing historical documents, semantic ambiguity is important. Even if we have, for example, a name of the person or a place in proper digital format, ambiguity of whether this person is the same one as in previous document makes this information unstructured for some purposes. Similarly, if the word father occurs in the text, it is important to distinguish whether this is a binary predicate (a relation between two individuals) or a singular predicate (property of one individual being a priest). In the context of our use case, if we have a list of only full names of persons in database and we want to make mailing labels for them, one can say that they are structured; but if we want to list them by surnames, they are unstructured because that cannot be performed easily.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Vamvakas, G. e. a.,  “A complete optical character recognition methodology for historical documents.”  (2008) IEEE.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Foka, A.,  “Digital Technology in the Study of the Past.”    <em>Digital Humanities Quarterly</em>  Vol 12/2 (2018).&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Johannessen, J. B., e. a.,  “Nemed Entity Recognition for the Mainland Scandinavian Languages.”    <em>Digital Scholarshipin the Humanities</em> , pp. (2005) Vol 20/1. 91-102.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>Boren, L., e. a.,  “Naming the Past: Named Entity and Animacy Recognition in 19th Century Swedish Literature.”  In:  <em>ACL 2007. Proceedings of the Workshop on Language Technology for Cultural Heritage Data.</em>  (2007) pp. 1-9.&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>Heckmann, D., e. a.,  “Citation segmentation from sparse &amp; noisy data: A joint inference approach with Markov logic networks.”    <em>Digital Scholarship in the Humanities</em>  (2014) pp. Vol 31/2. 333-356.&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>De Wilde, M.,  “Semantic Enrichment of a Multilingual Archive with Linked Open Data.”    _ Digital Humanities Quarterly_  Vol 11/4 (2017).&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>Lauc, D., Vitek, D.  “From the History to the Story: Harvesting Non-Monotonic Logic and Deep Learning to Generate Multilingual Family Narratives from Genealogical Data.”  DH Budapest 2018 Conference. (2018)&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>Vrbanus, M.  “Skrivena povijest – tajnoviti svijet brojki.”  Povijesni prilozi. Vol. 39/39., (2010) pp. 39-71.&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>The notable exception is the Baška tablet, written in Glagolitic script. It is one of the first monuments containing an inscription in the Croatian recension of the Church Slavonic language, dating from c. 1100. However, most of the historic documents were written in the Latin language, mostly within diplomatic collections.&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>Petran, F.,  “Studies for Segmentation of Historical Texts: Sentences or Chunks?. On Annotation of Corpora for Research in the Humanities”  ACRH-2, (2012) p.75.&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p>Poon, H. &amp; Domingos, P.,  “Joint inference in information extraction.”    <em>AAAI</em> m (2007) pp. 913-918.&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13">
<p>Graves, A.,  “Supervised Sequence Labelling.”  In:  <em>Supervised Sequence Labelling with Recurrent Neural Networks</em> . s.l.:Springer Berlin Heidelberg, (2012) pp. 5-13.&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14">
<p>Osborne, M. “Shallow parsing as part-of-speech tagging.”  s.l., ACM, (2000) pp. 145-147.&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:15">
<p>Nadeau, D. &amp; Sekine, S.,  “A survey of named entity recognition and classification.”    “Lingvisticae Investigationes.”  (2007)&#160;<a href="#fnref:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:16">
<p>Brill, E.,  “A simple rule-based part of speech tagger.”  Stroudsburg, PA, USA, Association for Computational Linguistics, (1992) pp. 152-155.&#160;<a href="#fnref:16" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:17">
<p>Chiticariu, L. a. a., 2010.  “Domain adaptation of rule-based annotators for named-entity recognition tasks.”  Massachusetts, USA, Association for Computational Linguistics , (2010) pp. 1002-1002.&#160;<a href="#fnref:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:18">
<p>Sha, F. &amp; Pereira, F.,  “Shallow parsing with conditional random fields.”  Edmonton, Canada, Association for Computational Linguistics, (2003) pp. 134-141.&#160;<a href="#fnref:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:19">
<p>Viet Cuong, N. a. a.,  “Conditional random field with high-order dependencies for sequence labeling and segmentation.”    <em>The Journal of Machine Learning Research</em>  Vol 15/1, (2014) pp. 981-1009.&#160;<a href="#fnref:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:20">
<p>Devlin, J., Chang, M.W., Lee, K. and Toutanova, K.,  “Bert: Pre-training of deep bidirectional transformers for language understanding” . (2018) arXiv preprint arXiv:1810.04805.&#160;<a href="#fnref:20" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:21">
<p>McCallum, A.,  “Joint inference for natural language processing.”  Boulder, Colorado, Association for Computational Linguistics. (2009)&#160;<a href="#fnref:21" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:22">
<p>Gelfond, M., 2008.  “Answer sets.”  In:  <em>Handbook of Knowledge Representation</em> . s.l.:Elsevier, p. 285–316.&#160;<a href="#fnref:22" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:23">
<p>Balduccini, M. “Some Recent Advances in Answer Set Programming (from the Perspective of NLP),”    <em>2013 CEUR Workshop Proceedings</em> . 1044. 1-6.&#160;<a href="#fnref:23" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:24">
<p>Gebser, M. &amp; all, a.,  “Potassco: The Potsdam Answer Set Solving Collection.”    <em>AI Communications - Answer Set Programming</em> , (2011) pp. 107-124.&#160;<a href="#fnref:24" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:25">
<p>Okazaki, N.,  “CRFsuite: a fast implementation of Conditional Random Fields (CRFs),”  s.l.: s.n. (2007)&#160;<a href="#fnref:25" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:26">
<p>Leuschel, M. &amp; Schrijvers, T.,  “Technical Communications of the Thirtieth International Conference on Logic Programming (ICLP'14).”  s.l., s.n. (2104)&#160;<a href="#fnref:26" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:27">
<p>Muggleton, S. H. W. a. H. W.,  “Latest Advances in Inductive Logic Programming.”  s.l.:Imperial College Press. (2015)&#160;<a href="#fnref:27" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry><entry><title type="html">Introduction to Göttingen Dialogues 2016</title><link href="https://rlskoeser.github.io/dhqwords/vol/15/1/000551/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://rlskoeser.github.io/dhqwords/vol/15/1/000551/</id><author><name>Marco Büchler</name></author><published>2021-05-21T00:00:00+00:00</published><updated>2021-05-21T00:00:00+00:00</updated><content type="html"><![CDATA[<h1 id="heading"></h1>
<h2 id="about-the-göttingen-dialog-in-digital-humanities">About the Göttingen Dialog in Digital Humanities</h2>
<p>The Göttingen Dialog in Digital Humanities is an initiative that started in 2015. This event creates a meeting place for researchers, students, and practitioners of Digital Humanities in Göttingen and neighbouring universities and research institutions. External speakers receive an invitation to provide a keynote lecture that is the starting point for thematic discussions and ensured several fruitful project collaborations. The best paper receives a 500 Euro award. Criteria are equally for the submitted paper and the presentation of the research during the Göttingen Dialog.</p>
<h2 id="about-the-special-issue">About the Special Issue</h2>
<p>This special issue contains selected papers from a past event. It covers topics of historical and modern natural language processing, video analysis, and research on manuscripts and ornaments ranging from antiquity to today. A complete list of all presented papers is available under <a href="https://www.etrap.eu/activities/gddh-2016/">https://www.etrap.eu/activities/gddh-2016/</a> including both the video recordings of the presentation and the slides. Hazel Wilkinson received the best paper award for the contribution  “A database of printers’ ornaments.”</p>
<p>[^]:</p>
]]></content></entry><entry><title type="html">Using an Advanced Text Index Structure for Corpus Exploration in Digital Humanities</title><link href="https://rlskoeser.github.io/dhqwords/vol/15/1/000526/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://rlskoeser.github.io/dhqwords/vol/15/1/000526/</id><author><name>Tobias Englmeier</name></author><author><name>Marco Büchler</name></author><author><name>Stefan Gerdjikov</name></author><author><name>Klaus U. Schulz</name></author><published>2021-05-21T00:00:00+00:00</published><updated>2021-05-21T00:00:00+00:00</updated><content type="html"></content></entry><entry><title type="html">A Review of Intergenerational Connections in Digital Families</title><link href="https://rlskoeser.github.io/dhqwords/vol/15/1/000530/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://rlskoeser.github.io/dhqwords/vol/15/1/000530/</id><author><name>Sucharita Sarkar</name></author><published>2021-03-05T00:00:00+00:00</published><updated>2021-03-05T00:00:00+00:00</updated><content type="html"><![CDATA[<p>As a matricentric feminist researcher, I am deeply interested in how mothers negotiate and maintain relationships within and outside the family (both immediate and extended) in ways that both stereotype and empower them, especially in an increasingly mediatized environment <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. As a mother, I belong to a dispersed family where the immediate as well as extended members are scattered across urban locations in India, and where we stay closely connected through different WhatsApp groups. I, for instance, have separate WhatsApp groups for my immediate family (my partner and daughters), and for my extended family (a cousins’ group), besides also staying in touch with my mother, brother and his family through WhatsApp chats, in addition to phone calls. Hence, it is from a scholarly as well as a personal perspective that I started to read Sakari Taipale’s book,  <em>Intergenerational Connections in Digital Families</em>  (Springer International Publishing, 2019), which examines the use and impact of digital social media communication among family generations. Although Taipale addresses the mother’s role in digital families, he moves beyond to explore the web of complex, multi-generational relationships that form modern families and traces the role of digital media in shaping and sustaining these relationships.</p>
<h2 id="defining-digital-families">Defining Digital Families</h2>
<p>Taipale’s ethnographic research is based on extended group interviews conducted with sixty-six key respondents in Italy, Finland, and Slovenia in 2014 and 2015. He contextualizes his research against the rapid digitization of family life since the late 1980s and 1990s, limiting his observations to North America and Western Europe. Specifically, he notes a  “general trend of families  <em>consisting up to three generations</em>  now [becoming] digitally increasingly connected”   <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> (emphases added). The way he chooses to define family affords his research both scope and uniqueness. He rejects the stereotypical notion of the urban nuclear family confined to one household, preferring to engage with the emerging phenomena of  “numerous mixed and extended families made up of members regularly switching between households and belonging to many families at once”   <sup id="fnref1:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. That shift in definition offers him a research gap: most existing research on digital communication practices in families focuses on dyadic connections in one-household families, especially communication between young members or between children and parents. Intra-family, digital, group communication practices of multi-generational, multi-household, geographically distributed families are an under-researched area, and Taipale’s research addresses this gap.</p>
<p>Taipale examines the existing scholarship on extended families, referring to approximate terms like Rainie and Wellman’s concept of  “networked families”  as a  “good starting point for understanding the digitalization of family relationships”   <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>  <sup id="fnref2:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. However, such earlier definitions are mostly rooted in one-to-one communication technologies, and are thus too limited to accommodate the increasing popularity of group communication through social media networks within families. Building upon, and expanding, existing definitions, Taipale writes,  “Digital family, as defined for the purposes of this book, is one form of distributed extended family, consisting of related individuals living in one or more households who utilize at least basic information and communication technologies and social media applications to stay connected and maintain a sense of unity despite no more than occasional in-person encounters between them. Families of this type are, in fact, only now developing and becoming visible, after older family members, grandparents in particular, have begun to adopt and make use of a larger variety of digital technologies for family communication”   <sup id="fnref3:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. His definition is flexible, accommodating a diverse range of family compositions beyond the immediate, from multiple generations living in a single household (like many of his respondents in Slovenia); to extended families that include cousins, aunts and uncles; to blended families that include step-relations; and dispersed families where children have moved out and may or may not be living with their own partners and/or children. Most significantly, in this fluid web of genetic, marital and even affective relationships, Taipale includes and explores the intra-family,  “skipped-generation”  communication between grandparents and grandchildren, a relation which is often excluded from  “official European data”  that focuses only on  “first degree family relationships”   <sup id="fnref4:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>.</p>
<p>Examining the factors that led to the rise of the digital family, Taipale expectedly skims over the sociocultural transformations, like developments in Assisted Reproductive Technology (ART) and the reconfigurations of patriarchal systems. Instead, he focuses on the advancements in communication technologies that have changed the ways families relate and connect to each other. He considers the swiftly changing mobile-phone-that-has-morphed-into-the-smartphone, where the latest model  “grows old in just a couple of years,”  as a metaphor and metonym of the  “fast pace of technological advancement”   <sup id="fnref5:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. Taipale uses Madianou and Miller’s term  “polymedia”  to explain the  “ever expanding catalogue of personal media technologies”  as well as the  “personalizable”  content of smartphones that is accessible to members of digital families <sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>  <sup id="fnref6:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. For families, personalization means that communication is often compartmentalized — children may use separate apps to connect with their parents and with their peers — and also that families may together choose specific apps like WhatsApp to form common interactive platforms. Digital families, thus, need increasingly larger numbers of digital appliances and technologies, as well as new skills and changing roles to be digitally ready and updated.</p>
<p>Although classic sociological theories focus on parents’ roles in socialization, Taipale cites modern family studies research which explores the  “two-way influences between older and younger family members”   <sup id="fnref7:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. Taipale believes two-way influences have not been adequately addressed because most existing research slants towards either the parents’ influences or the children’s agency. Taipale attempts to balance both aspects: how children teach some digital media use to their elders (smartphone and app use, but not e-banking or emails), whereas parents are increasingly more pro-technology at home. He also nuances this two-way support by exposing the escalation of conflict that sometimes accompanies it; by underscoring the context of wider social changes that has transformed conventional parent-child relations; and, also, by emphasizing the contextual contingency of any generalized conclusions. For instance, the eroding of parent-child hierarchies is not uniform across locations: Finland has more separation between adult children and parents than Slovenia or Italy. There are certain critical strategies that Taipale consistently deploys in the book: he takes a conclusion from established sociological research and either broadens its application or uses his triangular ethnographic research in Finland, Italy and Slovenia to indicate that most generalizations can be uneven or problematic.</p>
<p>In a book on intergenerational digital communications, it is expected that there will be a central chapter examining the concept of generation and the processes of generationing. Taipale recognizes the obsoletion of the conventional notion of kinship-based, lineage-oriented generation in families because of the rapid and diverse pluralization of family forms. He rejects the concept of  “strict generational division”  and considers a more flexible and processual  “post-Mannheimian approach to generational identity”   <sup id="fnref8:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. Mannheim distinguished between generation as location (based on birth year) and as actuality (where generational potential is actualized when people belonging to a particular generation unit live through and experience certain historical events in similar ways) <sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>. Taipale attempts to  “update Mannheim’s original conception”  of generational identity by inserting an  “ _active process of  doing _  behind the formation of every generation”   <sup id="fnref9:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> (emphasis in original). This can occur through technology adoption and technology use in families. Taipale considers generationing as a life-course-long process which occurs as families are reconfigured over and over; for instance, when a person retires, he may need to stay in touch with his family through Skype even though he might never have needed it before. Instead of staying locked in generational binaries like digital natives and digital immigrants, Taipale urges for a more integrated approach that understands and examines how  “each cohort generation has no choice but to over and over again reassess its technological self-understanding and reconsider its relative position vis-à-vis other generations, as new digital tools, applications and services are constantly being introduced that soon become perquisites for a well-functioning independent life”   <sup id="fnref10:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. Taipale mostly uses ethnographic methods like interviews to study how family generationing processes are impacted through the everyday learning and use of digital media. However, these changes in digital families may also be studied through a digital humanities approach: for instance, by using methods of  “digital pedagogy”  and  “digital literacy”  to understand the uneven digital learning processes and outcomes in family generations <sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>. Analysing everyday digital learning within families through digital humanities methods would also perhaps open up possibilities for applying such pedagogic tools to improve digital learning in family generational cohorts.</p>
<h2 id="the-gendered-role-of-warm-experts">The Gendered Role of Warm Experts</h2>
<p>In the second part of his book, Taipale continues his critical strategy of re-contextualizing and nuancing current theoretical concepts, as he attempts to articulate the new roles and everyday practices that are shaping digital families. One of the pivotal concepts that Taipale revisits is the role of the  “warm expert,”  Maria Bakardjieva’s term for  “an Internet/computer technology expert in the professional sense or simply in a relative sense”  who is in a  “close personal relationship”  with, and is  “immediately accessible”  to, the  “less knowledgeable other”  (in contrast with the cold expert — the external professional helper) <sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>. Taipale deploys the concept of the warm expert to analyse the digital family relations that he researches, and he concludes that there are one or two younger family members in most families who are assigned the role of warm experts. These experts help to improve or sustain the digital skills of older members.</p>
<p>Warm experts may be physically present and may co-use digital technologies and applications with older members (for instance, grandchildren using Skype when visiting grandparents), or, they may be proxy users for older members who are unable to learn new digital skills (like paying bills online). Taipale’s research indicates that most warm experts are youth between 20-35 years helping their parents or older siblings; some are  “skipped-generation warm experts,”  who provide help to grandparents without parents getting involved; and sometimes, even, older family members are acting as warm experts  “for their age-mates”  especially if they lived in the same household <sup id="fnref11:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. In countries like Slovenia, where multi-generation families stay in the same household, help-giving and help-taking between warm experts and others are organic; whereas in dispersed families in Finland, help is often given via the telephone. Taipale also explores the complex affects generated through the warm expert-novice relationships. While most warm experts feel a sense of reward, some do occasionally feel frustrated, especially at the need to repeat instructions multiple times or at the excessive time taken by novices. Conversely, the older generation learners are sometimes dissatisfied with the limited expertise of the warm experts. Taipale’s recuperation of the role of the warm expert resonates with me as a digital family member, as I recall learning how to navigate smartphone apps from my often-impatient daughters, and as I recall both me and them being more patient in teaching my mother how to use Facebook and WhatsApp.</p>
<p>Along with rejuvenating the notion of the warm expert, Taipale introduces the concept of digital housekeeping, referring to all the responsibilities and tasks required for the functioning of the digital family. As the digital housekeeper, the warm expert is consulted — in varying degrees — about most digital  “hardware purchases”  by the family, although the  “cultural norm”  is that parents make decisions regarding purchase of appliances, since they pay for them <sup id="fnref12:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. Moreover, in all cases the warm experts are given responsibility for the proper functioning of the appliances, as well as for installing and teaching others about latest software and applications. However, Taipale’s research suggests that knowledge transfer within the family is often two-way, with the older generation, especially parents, teaching the younger members, including warm experts, about the risks of certain digital practices like data oversharing. Taipale’s research uses the concepts of the warm expert and digital housekeeping to flip normative, hierarchical generational relations, and to propose a more fluid, intergenerational cooperation that  “empowers younger family members, consolidates family connections and enhances solidarity across generations”   <sup id="fnref13:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. Taipale here restructures intergenerational relations through flexible digital concepts that allow him to reassign varied family dynamics into categorizable and analysable data. His concepts of warm experts and digital housekeeping recall Carlson’s talk on data cleaning that demonstrates how computing processes aid in the study of human culture (and society), or Schöch&rsquo;s view of  “smart data”  which is  “clean,”    “structured,”    “ <em>selectively constructed</em> ”  and represents  “ <em>some aspects of a given object of humanistic inquiry</em> :”  the object of inquiry, in this case, being the digital family as interpreted through the data and lens selected by Taipale <sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>  <sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup> (emphases in original).</p>
<p>However, Taipale is insistent about not replacing one homogenized norm with another generalized conclusion. He points out how gendered anomalies often exist even in the generationally radical concept of digital housekeeping. Some of his key respondents articulated  “normative expectations”  of motherhood:  “Digitally skilled mothers sometimes considered themselves responsible for ensuring the proper functioning of software and applications in the family”   <sup id="fnref14:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. Taipale acknowledges the sense of empowerment such mothers feel, but does not limit himself to this one-dimensional perspective. He problematizes the extension of the  “traditional role of mothers as the maintainers of the home and domestic social relationships”  into the domain of  “software care,”  because, with the rise of women’s digital skills, even the task of digital housekeeping would perhaps  “quietly end up being included”  in the already time-consuming burden of domestic care work that women are expected to manage <sup id="fnref15:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. As a matricentric feminist researcher, I wish that Taipale had further unpacked this gendered anomaly:  <em>why</em>  should the empowering function of the warm expert become burdensome for mothers? The answer, of course, is not that the work of digital housekeeping is specifically challenging for mothers; it is that the non-digital housekeeping and caregiving have always been considered the primary responsibility of mothers, and so any addition to that pre-existing workload is, often, an overload. My lived experience also pluralizes Taipale’s argument: I am mostly so overloaded with domestic and professional work that the digital housekeeping is done by my partner, while our teenaged daughters are the warm experts. Conversely, it is only when the non-digital domestic housekeeping is shared by others (when the partner cooks, for instance), that I can devote some time to digital housekeeping or communicating. Readers will do well to reflect on how more  “visible feminism”   <sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup> around the use of personal technologies would shift the assignments and everyday assumptions of roles.</p>
<p>Taipale does, however, reveal other findings that are significant from a motherhood studies perspective. In his analysis of the increasing uses of WhatsApp in intra-family communication, he notes how this multimodal, scalable, private, instant messenger service enables both dyadic and also larger group communication between family members. For Taipale, the  “larger meaning of sharing and exchanging small messages, photos and video clips”  on WhatsApp resides in the insertion and expansion of the ethics of sharing as caring into the everyday family digital space <sup id="fnref16:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. Taipale’s research indicates that this sharing-as-caring aspect of WhatsApp communication is usually gendered as it is most visible in mother-daughter interactions. Many of his respondents emphasized the centrality of mothers in creating and maintaining WhatsApp groups as opposed to fathers’ more limited involvement. For Taipale, this finding consolidates his earlier argument about the gendered inequities of digital housekeeping functions. Therefore, even specific technologies like WhatsApp emerge as  “new forms of immaterial labour”  or care work that is gendered <sup id="fnref17:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>.</p>
<h2 id="implications-of-re-familization">Implications of Re-familization</h2>
<p>Moving around feminist analyses of intra-family digital communication, Taipale chooses to probe deeper by using Bengtson and Roberts’s model of intergenerational solidarity <sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>. Bengtson and Roberts contend that there are six types of solidarity within families: associational solidarity (spontaneous and ritual forms of communication); affectual solidarity (exchange of emotions and sentiments like trust); functional solidarity (exchange of help); normative solidarity (endorsement of family obligations); consensual solidarity (shared beliefs, etc), and structural solidarity (availability of family members, which depends on physical proximity and health). Taipale argues that digital families demonstrate these solidarities to varying degrees, as new media communication technologies are mostly associated with affectual, associational and functional forms of intergenerational solidarity. Forming of family WhatsApp groups is an action based on associational solidarity; the inclusion or exclusion of family members from family WhatsApp groups depend on the affectual solidarity within groups; whereas functional solidarity is evident in, for instance, grandchildren providing intergenerational digital knowledge and help to grandparents. However, Taipale is careful never to flatten his research findings. He stresses the differences in WhatsApp use in the three countries where his research is conducted. Finland has small-sized, scattered families that use group messaging services to reinforce family ties. In Italy, family WhatsApp groups are larger, including cousins, aunts, uncles, even those who live abroad. In Slovenia, family members often live in close proximity, and consequently do not feel the need for digital connections — this is distinct from other digital skills and practices — as much as the others.</p>
<p>Grounded in the theory of intergenerational solidarities, and acknowledging the differences in the various respondents, Taipale introduces the  “notion of re-familization”  to understand the  “cohesive impact of digital technologies in the context of extended and geographically distributed families;”  he politicizes this notion by contextualizing the current social thrust on re-familization against the earlier policies of de-familization pursued by welfare states <sup id="fnref18:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. De-familization refers to the combination of social policies between 1950s to the late 1980s that promoted increased participation by women in workplaces and independence of the citizenry through governmental spending on welfare measures like childcare, elderly care, and paid maternity leave. In contrast, the European Union’s post-1980s thrust on re-familization, which claims to promote  “citizen empowerment”  through policies accelerating digitalization, is rooted in  “the need to restrain public expenditure”   <sup id="fnref19:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. Taipale notes how re-familization manifests itself both positively and problematically through his research findings. Generational hierarchies in digital families are often democratized through the rise of warm experts; members spend more time and effort in doing families; the internal, intergenerational solidarities within digital families increase. However, Taipale also notes the unevenness in re-familization in the three countries studied. He notes the continuation of inequities marking re-familization, as  “fathers and grandparents are left out while mother-children communicate”   <sup id="fnref20:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>.</p>
<p>Taipale mentions the digital skills gap that exists between older and younger generations, and also between the less educated and more educated members in families. However (and perhaps expectedly), there is no mention of the digital divide that is so marked in countries like India, where I am located. Even his choice of respondents excludes ethnic minorities or immigrants. This may be considered a limitation, but it also helps in sharpening the focus of his investigations on his selected group of ethnic-majority digital families in Finland, Italy and Slovenia that may be multi-generational, blended, extended, and distributed. In Taipale’s concise book, each chapter has an abstract, keywords and a separate references section. It is, according to the publisher’s strategy, explicitly directed towards a mixed readership of students, lay readers, and researchers. The heterogeneous target readership perhaps accounts for the repetitiveness of certain concepts, arguments and conclusions throughout the book. The book also reads somewhat drily, without the conversational flows and narrative interest that often mark interview-based ethnographic research. Taipale’s interview extracts are inserted within his analyses in specifically marked sections, and they are not integrated into the flow of the writing. We do not get to know the fleshed-out stories of the respondents, even those whose quotes appear multiple times in the book. In contrast, Julie Wilson and Emily Yochim’s recent book,  <em>Women’s Work and Digital Media</em> , develops the life stories of the mothers interviewed by the researchers, and almost each chapter begins with a narrative probe into the life of a mother <sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>. As a digital humanist, I wish that the rigid structure of Taipale’s book had flexed enough to document and narrate the nascent stories of the respondents.</p>
<p>Despite the structural and stylistic stiffness, the book adds significant insights to the emerging scholarship about digital communication and family, which is where Taipale locates his research. He moves beyond earlier research that focuses on technology use by individuals or by diasporic/transnational families, and focuses on studying the everyday use of digital media in families. Unlike existing research which often concludes that digital media has negatively impacted the affective, intimate relations between family members, Taipale takes a moderate, balanced view in his study of linkages between digital connections and caring relationships within families. In his concluding chapter, Taipale suggests that further research may be done to  “investigate how caring relationships are played out in practice in the digital family”  or on  “any possible positive long-term effects of the help and care provided by warm experts”   <sup id="fnref21:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. Early on in the book, Taipale emphasized the concept of  “doing family”  rather than  “being family,”  configuring family as asynchronous, always-in-process, and mediated through communication technologies: his aim, he states, is  “to promote thinking that deviates from that represented by the individual networking and one-household approaches”   <sup id="fnref22:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. His research is exclusively focused on intra-family use of ICTs and social media connections, and his definition of the digital family includes multiple generations, relations and households. This flexible definition of the digital family opens up other domains of research into the processes and connections that do families. For instance, Taipale acknowledges that although use of WhatsApp between peer-to-peer groups among children and youth have been documented, the growing use of WhatsApp in  “the everyday life of extended families is still an unexplored territory”   <sup id="fnref23:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. This indicates potentialities of research at various intersections of relationships (families/peers) and technologies (newer forms of social media) that allow participants to fulfil the needs of social bonding rather than merely exchanging information. I know my own story — the frequent updates, messages and video-chats that my partner, daughters and I have several times every day are essential in sustaining and strengthening the bonds among us. I know of so many similar emerging stories that are undocumented and under-researched. As a matricentric feminist scholar and a mother belonging to a dispersed, digital family located in South Asia, Taipale’s work makes me hope for further research that narrates, compares and theorizes our shared yet unique experiences.</p>
<p>There are several possibilities for further research that extends digital family studies to digital humanities, and this can include scholarship with a matricentric feminist focus. Scholars have made persuasive arguments for including  “genealogy and family history”  in the cohort of humanities computing and digital humanities <sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup>. There can be similar overlaps between digital humanities and the domains of digital family communication/learning or digital family/generational relations. To envision one such project (located in Taipale’s research problem but moving away from his chosen ethnographic methods) we can archive, compare and analyse the data from anonymised WhatsApp conversations within or between family cohorts through digital humanities tools for data mining and text analysis, which may lead to new understandings of how the concepts of warm experts and digital housekeeping operate within digital families. Focusing on the quality and quantity of maternal involvement in these WhatsApp conversation-texts would constitute a much-needed matricentric feminist intervention. Similarly, digital family relations, networks, learning (and other ways of doing families) can be mapped and studied through other ‘texts’ such as mom-blogs or Facebook posts. Expanding the possibilities beyond social media texts, we may apply the findings regarding relational dynamics within digital families to analyses of family relations in literary texts studied in digital humanities. Many such interventions from multiple theoretical and multidisciplinary standpoints are possible in the imbrications of digital family studies and digital humanities. The Digital Humanities 2.0 Manifesto states that  “Digital Humanities studies the cultural and social impact of new technologies as well as takes an active role in the design, implementation, interrogation, and subversion of these technologies”   <sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup>. Taipale’s ethnographic research studies the impact and implementation of new technologies in digital families, and future researchers of family studies and digital humanities can use it as a pivot or a springboard for many exciting and forward-looking explorations into family-digital interactions.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>O’Reilly, A.  <em>Matricentric Feminism: Theory, Activism, Practice</em> . Demeter Press, Bradford, ON (2016).&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Taipale, S.  <em>Intergenerational Connections in Digital Families</em> . Springer International Publishing (2019).&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref3:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref4:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref5:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref6:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref7:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref8:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref9:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref10:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref11:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref12:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref13:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref14:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref15:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref16:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref17:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref18:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref19:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref20:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref21:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref22:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref23:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Rainie, L. and Wellman, B.  <em>Networked: The New Social Operating System</em> . MIT Press, Cambridge, MA (2012).&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Madianou, M. and Miller, D.  <em>Migration and New Media: Transnational Families and Polymedia</em> . Routledge, London and New York (2012).&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>Mannheim, K.  “Essay on the Problem of Generations.”  In P. Kecskemeti (ed.),  <em>Essays on the Sociology of Knowledge by Karl Mannheim</em> , Routledge and Kegan Paul, New York (1952), pp. 276-320.&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>Kennedy, K.  “A Long-Belated Welcome: Accepting Digital Humanities Methods into Non-DH Classrooms,”    <em>Digital Humanities Quarterly</em>  11.3 (2017). <a href="/dhqwords/vol/11/3/000315/">https://www.digitalhumanities.org/dhq/vol/11/3/000315/000315.html</a>&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>Bakardjieva, M.  <em>Internet Society: The Internet in Everyday Life</em> . Sage, London (2005).&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>Carlson, S.  “Grateful Data: Digital Humanities, Data Cleaning, and the Grateful Dead,”     <em>Digital Frontiers</em> . UNT Digital Library, Texas (September 22, 2016). <a href="https://digital.library.unt.edu/ark:/67531/metadc948134/m1/">https://digital.library.unt.edu/ark:/67531/metadc948134/m1/</a>&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>Schöch, C.  “Big? Smart? Clean? Messy? Data in the Humanities,”    _Journal of Digital Humanities _ 2.3 (2013). <a href="http://journalofdigitalhumanities.org/2-3/big-smart-clean-messy-data-in-the-humanities/">http://journalofdigitalhumanities.org/2-3/big-smart-clean-messy-data-in-the-humanities/</a>&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>Wernimont, J.  “Whence Feminism? Assessing Feminist Interventions in Digital Literary Archives,”    <em>Digital Humanities Quarterly</em>  7.1 (2013). <a href="/dhqwords/vol/7/1/000156/">http://www.digitalhumanities.org/dhq/vol/7/1/000156/000156.html</a>&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>Bengtson, V. L. and Roberts, R. E.  “Intergenerational Solidarity in Aging Families: An Example of Formal Theory Construction,”    <em>Journal of Marriage and Family</em>  53.4 (1991): 856-870.&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p>Wilson, J. A. and Yochim, E. C.  <em>Mothering through Precarity: Women’s Work and Digital Media</em> . Duke University Press, Durham (2017).&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13">
<p>Hoeve C.D.  “Finding a place for genealogy and family history in the digital humanities,”    <em>Digital Library Perspectives</em>  34.3 (2018): 215-226. <a href="https://doi.org/10.1108/DLP-11-2017-0044">https://doi.org/10.1108/DLP-11-2017-0044</a>&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14">
<p>Svensson, P.  “Envisioning the Digital Humanities,”    <em>Digital Humanities Quarterly</em>  6.1 (2012). <a href="/dhqwords/vol/6/1/000112/">https://www.digitalhumanities.org/dhq/vol/6/1/000112/000112.html</a>&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry><entry><title type="html">A Review of Twitter and Tear Gas</title><link href="https://rlskoeser.github.io/dhqwords/vol/15/1/000535/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://rlskoeser.github.io/dhqwords/vol/15/1/000535/</id><author><name>Nanditha Narayanamoorthy</name></author><published>2021-03-05T00:00:00+00:00</published><updated>2021-03-05T00:00:00+00:00</updated><content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>Zeynep Tufekci’s book  <em>Twitter and Tear Gas</em> , even within its title, attempts a relativization and comparison of high profile, horizontalist, and anti-authoritarian street protests with its integration of a  “reconfigured public sphere that now incorporates digital technologies as well”   <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. Her study eloquently engages with street protest movements like the Encuentro Zapatista, Podemos in Spain, Occupy Wall Street in Washington, Gezi Park in Turkey, and the Arab Spring in Egypt, and functions to systematically enable newer perspectives and dialogues on the need for a reconfiguration of digitally networked online spaces and the trajectories of social movements online.</p>
<h2 id="positionality">Positionality</h2>
<p>As a social scientist, activist, programmer and faculty member in the Sociology Department at the University of North Carolina, Tufekci is uniquely positioned to underscore the metamorphosis that digital affordances have enabled in social mechanisms of protest. As a scholar of social movements and surveillance, Tufekci’s work explores digital protest at the interstices of technology, society, effects of big data on politics and the public sphere. With her background in Computer Science, she is able to offer a counter-discourse to Silicon Valley’s techno-utopianism, and her work disrupts the view that social media and tech companies have built over the years – that they are neutral, passive or even positive platforms affecting change in society. At this juncture, she locates the rise of Facebook, Twitter, and Google in relation to contemporary social movements. Her research could potentially be considered a repository of protest movements across the globe, and a comparativist analysis could engender the strengths they could draw from each other. Owing to versatility of her own disciplines, her research can be made available to a wide range of audiences and can function as a textbook or an academic resource within the university and beyond.</p>
<h2 id="attention-over-information">Attention over Information</h2>
<p>In Twitter and Tear Gas, Tufekci argues for both strengths and weaknesses; affordances and constraints of social media in organized digital protests, as she quotes historian Melvin Kranzberg from 1985,  “Technology is neither good nor bad; nor is it neutral”   <sup id="fnref1:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. Social media movements, in short, can both empower and keep protests from attaining their capacities. Her work speaks volumes on the technological affordances that accompany modern protest cultures to accomplish a coming together of a large networked public in the overthrow of anti-authoritarian institutions and power structures. In tandem, Tufekci contends a fundamental fragility of leaderless networked movements that function ad hoc, providing flexibility to anyone who wishes to join and their debilitating lack of organization, authority and negotiation, when required. The ability to grow rapidly encompasses a lack of organization, a phenomenon she terms tactical freeze or an inability to enable tactical maneuvering. Moreover, as a techno-sociologist, Tufekci aptly identifies new media challenges of attention over information and new non-traditional forms of gatekeeping. According to the author, the new networked public sphere enables the amplification and abundance of false and unverified information <sup id="fnref2:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> to  “distract the audience, dilute attention, and sow fear and doubt in their minds”   <sup id="fnref3:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. Tufekci highlights  “the importance of attention as a key resource for social media movements,”  and underscores how  “Facebook, Google and Twitter monetize attention in ways that may or may not be conducive to the success of protest movements online”   <sup id="fnref4:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. In short, our move to digital technologies has enabled a trend where attention is manufactured as part of an emerging economy, and social media choreographs it as a tool to control and manipulate the masses. The book deftly maneuvers between the success of attention building in online activism and its challenges in the form of algorithmic control through bully bots and censorship by denial of attention, implemented by governments or private companies that seek to distract and exhaust both user and participant from partaking in the movement.</p>
<h2 id="social-media-platforms">Social Media Platforms</h2>
<p>Pragmatically, Tufekci explores the role of policies of larger social media platforms like Facebook, Google and Twitter that contribute aptly to the discussion on the softer biopolitics of algorithmic control. For example, Facebook’s real name policy that effectively disrupts protester experience and Twitter’s mention policy that generates attention across the platform are two mechanisms of algorithmic ebb and flow that create and disrupt success of protest movements online. Furthermore, protests, here, are an example of a signal that corresponds to one of several underlying capacities: narrative, disruptive and electoral capacities that are, in essence, muscles that need to be always prepared, and will enable a movement to scale enough to fight anti-authoritarian institutions.</p>
<p>The non-chronological order of the discussions of power and fragility of protest movements online unnerves and oftentimes exhausts the reader. Nevertheless, there remains an effective freshness to the argument of technological non-neutrality that Tufekci constructs. She plays to her strengths as both a social scientist and ethnographer engaged in protest fieldwork across the world, and a programmer who studies algorithmic affordances, in the construction of a dialogue surrounding both online and offline protests.</p>
<h2 id="at-the-intersection-of-social-justice-and-digital-humanities">At the Intersection of Social Justice and Digital Humanities</h2>
<p>Tufekci’s project undertakes a broad investigation of digital activism, uncovers key players that influence the relationship between technology and contemporary social protest, and reflects on the power of attention over information in the age of New Media. Her work reconceptualizes social media movements in a new light, and applies rigorous empirical social science research to demonstrate the future of protest in digital media. She connects her discourse at the intersection of social justice, digital activism, and Digital Humanities. Although social science research in activism and Digital Humanities carries an important distinction, Digital Humanities scholar Roopika Risam argues that  <em>Digital Humanities methods can be effective tools for calling attention to, and enabling social activism, particularly for marginalized communities</em>   <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. According to Risam,  “Digital Humanities makes activism possible, offering hope for re-appropriating knowledge production”   <sup id="fnref1:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. Digital humanists are poised to contribute to studies of social media activism through research, teaching, and community outreach and engagement. Digital Humanities envisions social media as a positive tool for transformative social change in digital activism, and for highlighting the scope and content of a humanistic inquiry. Although Tufekci’s  <em>Twitter and Tear Gas</em>  lacks the humanistic perspective in its study of protest movements online that other scholars in Digital Humanities like Elizabeth Losh <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>, Moya Bailey, Sarah Jackson, Brooke Foucault Welles <sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>  <sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>, and Nishant Shah <sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup> tackle, it successfully builds a counter-narrative to techno-utopianism and left-leaning anti-authoritarianism in an effort to reframe knowledge production in the Humanities. Tufekci, in essence, grapples with critical questions in Digital Humanities about who creates, engages with, and controls digital spaces. This book is in a constant flux between the strengths and weaknesses of functioning and navigating within media spaces. It is an invaluable resource for anyone interested in how social activism constructs itself online and a must-read for all that have scholarly aims in the field of digital activism.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Tufekci, Z.  <em>Twitter and Tear Gas: The Power and Fragility of Networked Protest</em> . Yale University Press, New Haven, CT (2017).&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref3:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref4:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Bagger, J.  “Dr. Roopika Risam: Calling Attention to Activism Through Digital Humanities.”  Digital Humanities at Washington and Lee University (October 10, 2018). <a href="https://digitalhumanities.wlu.edu/blog/2018/10/10/dr-roopika-risam-calling-attention-to-activism-through-digital-humanities/">https://digitalhumanities.wlu.edu/blog/2018/10/10/dr-roopika-risam-calling-attention-to-activism-through-digital-humanities/</a>&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Losh, E.  “Hashtag Feminism and Twitter Activism in India,”    <em>Social Epistemology Review and Reply Collective</em>  3.12 (2014): 10-22. <a href="http://wp.me/p1Bfg0-1Kx">http://wp.me/p1Bfg0-1Kx</a>&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Bailey, M., Jackson, S., and Foucault Welles, B.  “Women Tweet on Violence: From #YesAllWomen to #MeToo,”    <em>Ada: A Journal of Gender, New Media, and Technology</em>  15 (2019). <a href="http://doi.org/10.5399/uo/ada.2019.15.6">http://doi.org/10.5399/uo/ada.2019.15.6</a>&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>Bailey, M., Jackson, S., and Foucault Welles, B.  <em>#Hashtag Activism: Networks of Race and Gender Justice</em> . MIT Press, Cambridge, MA (2020). <a href="https://doi.org/10.7551/mitpress/10858.001.0001">https://doi.org/10.7551/mitpress/10858.001.0001</a>&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>Shah, N., Sneha, P. P., and Chattopadhyay S.  <em>Digital Activism in Asia Reader</em> . Meson Press, Lüneburg, Germany (2015). <a href="http://doi.org/10.14619/013">http://doi.org/10.14619/013</a>&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry><entry><title type="html">Advances in Digital Music Iconography: Benchmarking the detection of musical instruments in unrestricted, non-photorealistic images from the artistic domain</title><link href="https://rlskoeser.github.io/dhqwords/vol/15/1/000517/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://rlskoeser.github.io/dhqwords/vol/15/1/000517/</id><author><name>Matthia Sabatelli</name></author><author><name>Nikolay Banar</name></author><author><name>Marie Cocriamont</name></author><author><name>Eva Coudyzer</name></author><author><name>Karine Lasaracina</name></author><author><name>Walter Daelemans</name></author><author><name>Pierre Geurts</name></author><author><name>Mike Kestemont</name></author><published>2021-03-05T00:00:00+00:00</published><updated>2021-03-05T00:00:00+00:00</updated><content type="html"><![CDATA[<h2 id="introduction-the-era-of-the-pixel">Introduction: the era of the pixel</h2>
<p>The Digital Humanities constitute an intersectional community of praxis, in which the application of computing technologies in various subdisciplines in the  <em>Geisteswissenschaften</em>  plays a significant role. Surveys of the history of the field <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> have stressed that most of the seminal applications of computing technology were heavily, if not exclusively, text-oriented: due to the hardware and software limitations of the time, analyses of image data (but also audio or video data) remained elusive and out of practical reach until relatively late, certainly at a larger scale. In the past decade, the application of deep neural networks has significantly pushed the state of the art in computer vision, leading to impressive advances in tasks such as image classification or object detection <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>  <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>. Even more recently, improvements in the field of computer vision have started to find practical applications in study domains outside of strict machine learning, such as physics, medicine or even astrology. Supported by this technology&rsquo;s (at times rather naive) coverage in the popular media, the communis opinio has been eager to herald the advent of the &ldquo;Era of the Pixel&rdquo;.</p>
<p>In the Digital Humanities too, the potential of computer vision is nowadays increasingly recognized. A programmatic duet of two recent articles on &ldquo;distant viewing&rdquo; in the field&rsquo;s flagship journal <sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>  <sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup> leads the way in this respect, emphasizing the privileged role these new methodologies can play in the exploration of large data collections in the Humanities. The present paper too is situated in a multidisciplinary project in which we investigate how modern artificial intelligence can support GLAM institutions (galleries, libraries, archives, and museums) in cataloguing and curating their rapidly expanding digital assets. As a case study, we shall work with non-photorealistic depictions of musical instruments in the artistic domain.</p>
<p>The structure of this paper is as follows. First, we motivate and contextualize our case study of musical instruments from within the scholarly framework of music iconography and computer vision, but also from the more pragmatic context of the research project from which this focus has emerged. We go on to describe the construction and characteristics of an annotated benchmark dataset, the MINERVA dataset, that will be released together with this paper, through which we hope to stimulate further research in this area. Using this benchmark data, we stress-test the available technology for the identification and detection of objects in images and discuss the current limitations of systems. To illustrate the broader relevance of our approach, we apply the trained benchmark system &lsquo;in the wild&rsquo;, on unseen and out-of-sample heritage data, followed by a quantitative and qualitative evaluation of the results. Finally, we identify what seem to be the most relevant directions for future research.</p>
<h2 id="motivation">Motivation</h2>
<h2 id="music-iconography">Music iconography</h2>
<p>The present paper must be understood against the wider scholarly background of music iconography, a Humanities field of inquiry with a rich, interdisciplinary history in its own right. <sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup> concisely defined music iconography as a field being &ldquo;concerned with the study of the visual representation of musical topics. Its primary materials include portraits of performers and composers, illustrations of instruments, occasions of music-making, and the use of musical imagery for purposes of metaphorical or allegorical allusion&rdquo;. Because of this wide range of topics, at the intersection of art history and musicology <sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>  <sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>, the field takes pride of its interdisciplinarity.</p>
<p>Music iconography deliberately adopts a &ldquo;methodological plurality&rdquo; <sup id="fnref1:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup> which is increasingly complemented with digital approaches. A major achievement in this respect has been the establishment (in 1971) and continued expansion and curation of an international digital inventory for musical iconography, the  <em>Répertoire International d&rsquo;Iconographie Musicale</em>  (RIDIM). Now publicly available as an online web resource (<a href="https://ridim.org/">https://ridim.org/</a>), RIDIM functions as a reference image database, designed to facilitate the efficient yet powerful description and discovery of music-related art works <sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup>. The need for such an international inventory has been acknowledged as early as 1929 and its significant scope facilitates the international study of music-related phenomena and their depiction across the visual arts.</p>
<p>Music iconography has an important tradition of focused studies targeting the deep, interpretive analysis of individual artworks or small collections of them. Such hermeneutic case studies have the advantage of depth, but understandably lack a more panoramic perspective on the phenomena of interest and, for instance, diachronic or synchronic trends and shifts therein. The large-scale, &ldquo;serial&rdquo; study of musical instruments as depicted across the visual arts remains a desideratum in the field and has the potential of bringing a macroscopic perspective to historical developments. In the present paper, we explore the feasibility of applying methods from present-day computer vision, in an attempt to scale up current approaches. The primary motivation of this endeavour is that digital music iconography – or &ldquo;Distant&rdquo; music iconography, in an analogy to similar developments in literary studies <sup id="fnref1:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>  <sup id="fnref1:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup> – in principle has much to gain from such methods, at least if they are carefully applied and in continuous interaction with experts in the domain. Our focal point is the automated identification and detection of individual musical instruments in unrestricted, digitized materials from the realm of the visual arts.</p>
<p>This scholarly initiative is embedded in the collaborative research project INSIGHT (Intelligent Neural Systems as InteGrated Heritage Tools), which aims to stimulate the application of Artificial Intelligence to the rapidly expanding digital collections of a selection of federal museum clusters in Belgium.<sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup>  One important, transcommunal aspect to Belgium&rsquo;s cultural history relates to music and musical history, with the invention of the saxophone by Adolphe Sax as an iconic example. An additional factor is the presence of the Musical Instruments Museum in the capital (Brussels) that contributed significantly to international research projects in this area (and which is a partner in the INSIGHT project). This contextualization, finally, is also important to understand our specific choice for the topic of musical instruments, as a representative and worthwhile case study on the application of modern machine learning technology in digital heritage studies.</p>
<h2 id="computer-vision">Computer vision</h2>
<p>The methodology for the present paper largely derives from machine learning and more specifically computer vision, a field concerned with computational algorithms that can mimic the perceptual abilities of humans and their capacity to construct high-level interpretations from raw visual stimuli <sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>. In the past decade, this field has gone through a remarkable renaissance, following the emergence of powerful learning techniques based on so-called neural networks. In particular the advent of &ldquo;convolutional&rdquo; networks <sup id="fnref1:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> has led to dramatic advances in the state of the art for a number of standard applications, including image classification (&ldquo;Is this an image of a cat or a dog?&rdquo;) and object detection (&ldquo;Draw a bounding box around any cats in this image&rdquo;). For some of these tasks, modern computer systems have even been shown to rival the performance of humans <sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>. In spite of the impressive advances in recent computer vision research, it is generally acknowledged that the state of the art is still confronted with a number of major, as yet unsolved, challenges. In this section we highlight four concrete issues that are especially pressing, given the focus of this paper on image collections in the artistic domain. These challenges motivate our work from the point of view of computer vision, rather than art history.</p>
<h2 id="photo-realism">Photo-realism</h2>
<p>One major hurdle is that computer vision nowadays strongly gravitates towards so-called photo-realistic material, i.e. digitized or born-digital versions of photographs that do not actively attempt to distort the reality they depict. The best example in this respect is the influential ImageNet dataset <sup id="fnref1:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>, that offers highly realistic photographic renderings of everyday concepts drawn from WordNet&rsquo;s lexical database. While some more recent heritage collections of course abound in such photo-realistic material (e.g. advertisements in historic newspapers), traditional photography does not take us further back in time than the nineteenth century <sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup>. Additionally, the Humanities study many other visual arts that prioritize much less photorealistic representation and focus even on completely &lsquo;fictional&rsquo; renderings of (potentially imagined or historical) realities. While there has been some encouraging and worthwhile prior work into the application of computer vision to non-photorealistic depictions, this work is generally more scattered and the results (understandably) less advanced than those reported for the photorealistic domain. Inspiring recent studies in this area include <sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup>  <sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup>  <sup id="fnref:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup>  <sup id="fnref:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup>.</p>
<h2 id="data-scarcity">Data scarcity</h2>
<p>It is a well-known limitation that convolutional neural networks require large amounts of manually annotated example data (or training data) in order to perform well. To address this issue, the community has released several public datasets over the years <sup id="fnref:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup>  <sup id="fnref2:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>  <sup id="fnref:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup>  <sup id="fnref:20"><a href="#fn:20" class="footnote-ref" role="doc-noteref">20</a></sup>  <sup id="fnref:21"><a href="#fn:21" class="footnote-ref" role="doc-noteref">21</a></sup> which has allowed the successful training of a large set of neural architectures <sup id="fnref:22"><a href="#fn:22" class="footnote-ref" role="doc-noteref">22</a></sup>  <sup id="fnref:23"><a href="#fn:23" class="footnote-ref" role="doc-noteref">23</a></sup>  <sup id="fnref:24"><a href="#fn:24" class="footnote-ref" role="doc-noteref">24</a></sup>. However, the nature of the images included in these datasets is mostly photo-realistic, also because such images are relatively straightforward to obtain and annotate. These image collections are very different in terms of texture, content and availability from the sort of data that can nowadays be found in the digital heritage domain.</p>
<p>Computer vision researchers interested in the artistic domain have attempted to alleviate the relative dearth of training data by either releasing domain-specific datasets <sup id="fnref1:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup>  <sup id="fnref1:20"><a href="#fn:20" class="footnote-ref" role="doc-noteref">20</a></sup> or through the application of transfer learning <sup id="fnref:25"><a href="#fn:25" class="footnote-ref" role="doc-noteref">25</a></sup>, a machine learning paradigm which allows the application of neural networks to domains where training data is scarce. For image classification, for instance, these efforts have indeed greatly contributed to overall feasibility of applying computer vision outside the photo-realistic domain <sup id="fnref1:25"><a href="#fn:25" class="footnote-ref" role="doc-noteref">25</a></sup>. Both approaches, however, have limitations when it comes to the complementary task of object detection. Popular datasets such as the Rijksmuseum collection <sup id="fnref2:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup> or the more recent OmniArt dataset <sup id="fnref2:20"><a href="#fn:20" class="footnote-ref" role="doc-noteref">20</a></sup> do not come with the metadata required for object-detection problems.</p>
<p>With this work, we take one step forward in addressing these limitations. Firstly, the MINERVA dataset that we present below, specifically tackles the problem of object detection within the broader heritage domain of the visual arts, introducing a novel benchmark for researchers working at the intersection of computer vision and art history. Secondly, we present a number of baseline results on the newly introduced dataset. The results are reported for a representative set of common architectures, which were pre-trained on photo-realistic images. This allows us to investigate to what extent these methods can be reused when tested on artistic images.</p>
<h2 id="irrelevant-training-categories">Irrelevant training categories</h2>
<p>Previous studies have demonstrated the feasibility of &ldquo;pretraining&rdquo;: with this approach, networks are first trained on (large) photorealistic collections (i.e. the source domain) and then applied downstream (or further fine-tuned) on an out-of-sample target domain, that has much less annotated data available. While generally useful, this approach is still confronted with the problem that the annotation labels or categories attested in the source domain are often of little interest within the target domain (i.e. art history, in the present case). The popular Pascal-VOC dataset <sup id="fnref:26"><a href="#fn:26" class="footnote-ref" role="doc-noteref">26</a></sup>, for instance, tackles the detection of 20 classes, out of which more than a third constitute different kinds of transportation systems, such as trains, boats, motorcycles and cars. Naturally, these means of transportation are very unlikely to be represented in artworks that date back to the premodern period. The more complex MS-COCO dataset <sup id="fnref1:21"><a href="#fn:21" class="footnote-ref" role="doc-noteref">21</a></sup> presents similar problems: even though the amount of classes increases to 80, most of the objects which should be detected are again unlikely to be represented within historical works of art, since they correspond to objects which have only been relatively recently invented such as &ldquo;microwave&rdquo;, &ldquo;cell-phone&rdquo;, &ldquo;tv-monitor&rdquo;, &ldquo;laptop&rdquo;, or &ldquo;remote&rdquo;, and the like. This poses a serious constraint when it comes to the use of pre-trained object-detectors for artistic collections. As with most supervised learning algorithms, models trained on these collections will only perform well on the sort of data on which they have been explicitly trained. To illustrate this model bias, we report some (nonsensical) detections in the first row of images presented in <a href="#figure01">Figure 1a</a>.</p>
<h2 id="robustness-of-the-models">Robustness of the models</h2>
<p>Popular object detectors such as YOLO <sup id="fnref:27"><a href="#fn:27" class="footnote-ref" role="doc-noteref">27</a></sup> and Fast R-CNN <sup id="fnref:28"><a href="#fn:28" class="footnote-ref" role="doc-noteref">28</a></sup> have been designed to perform well on the above-mentioned photo-realistic datasets. However, the variance of the samples denoting a specific class within these datasets is usually much smaller when compared to that in artistic collections. As an example, we refer to a number of images representing the person class within the Pascal-VOC dataset: we can observe from the two leftmost images of the bottom row of <a href="#figure01">Figure 1</a> that the representation of a &lsquo;person&rsquo; is overall relatively unambiguous and hardly distorted. As a result, the person class is usually easily detected by e.g. the YOLO architecture. However, we can see that this task already becomes harder when a person has to be detected within a painting (potentially with a highly distorted representation of the humans in the scene). As shown by the two rightmost images of the bottom row (<a href="#figure01">Figure 1b</a>), a YOLO-V3 model does not see most of the persons represented in the paintings and misclassifies them as non-human beings (e.g. &ldquo;bear&rdquo;).</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000517/resources/images/figure01.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000517/resources/images/figure01_hu4070c7aea4b355fb07ee49db47090fab_2024888_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000517/resources/images/figure01_hu4070c7aea4b355fb07ee49db47090fab_2024888_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000517/resources/images/figure01_hu4070c7aea4b355fb07ee49db47090fab_2024888_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000517/resources/images/figure01.png 1370w" 
     class="landscape"
     ><figcaption>
        <p>Examples showing the limitations that occur when a standard object detector trained on photo-realistic images is tested in the domain of the visual arts. Figure 1a: Four anecdotal examples showing that the &ldquo;person” class is usually reasonably detected by the YOLO architecture, although other, non-sensical detections frequently occur. Figure 1b: the two images on the left show that the variation in depiction of people is limited in photorealistic material, in comparison to the artistic representations of people (two examples to the right).
        </p>
    </figcaption>
</figure>
<p>All examples in <a href="#figure01">Figure 1</a> come from a pretrained YOLO-V3 model <sup id="fnref1:27"><a href="#fn:27" class="footnote-ref" role="doc-noteref">27</a></sup> which has been originally trained on the COCO dataset <sup id="fnref2:21"><a href="#fn:21" class="footnote-ref" role="doc-noteref">21</a></sup> and then tested on artworks coming from <sup id="fnref3:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup> and <sup id="fnref3:20"><a href="#fn:20" class="footnote-ref" role="doc-noteref">20</a></sup>. The images presented in the first row illustrate that the network is biased towards making detections which are very unlikely to appear in premodern depictions. These detections correspond to the identification of objects such as &ldquo;suitcase&rdquo;, &ldquo;umbrella&rdquo; or &ldquo;frisbee&rdquo; and &ldquo;banana&rdquo;. The two last images presented in the second row show that standard models fail to properly recognize a simple class such as person. In fact, they fully fail in detecting most of the persons that are present in the artworks due to these representations being highly different from the persons that are present in the Pascal-VOC dataset (first two images of the second row).</p>
<h2 id="minerva-dataset-description">MINERVA: dataset description</h2>
<p>In this section, we describe MINERVA, the annotated dataset in the field of object detection that is presented in this work. This novel benchmark dataset will be released jointly with this paper.<sup id="fnref:29"><a href="#fn:29" class="footnote-ref" role="doc-noteref">29</a></sup>  The main task under scrutiny here is the detection of musical instruments in non-photorealistic, unrestricted image collections from the artistic domain. We have named the dataset with the acronym MINERVA, which stands for &lsquo;Musical INstrumEnts Represented in the Visual Arts&rsquo;, after the Roman goddess of the arts (amongst many other things).</p>
<h2 id="data-sources">Data Sources</h2>
<p>The base data for our annotation effort was assembled in a series of &lsquo;concentric&rsquo; collection campaigns, where we started from smaller, but high-quality datasets and gradually expanded into larger, albeit less well curated data sources.</p>
<p>RIDIM: We harvested a collection of high-quality images from the RIDIM database, in those cases where the database entries provided an unambiguous hyperlink to a publicly accessible image. These records were already assigned MIMO codes by a community of domain experts, which provided important support to our in-house annotators (especially during the first experimental rounds of annotations).  RMFAB/RMAH: We expanded on the core RIDIM data by including (midrange resolution) images from the digital collections of two federal museums in Brussels: the RMFAB (Royal Museums of Fine Arts of Belgium, Brussels) and the RMAH (Royal Museums of Art and History, Brussels). These images were selected on the basis of previous annotations that suggested they included depictions of musical instruments, although no more specific labels (e.g. MIMO codes) were available for these records at this stage. Copyrighted artworks could not be included for obvious reasons (copyright lasts for 70 years from the death of the creator under Belgian intellectual law).  Flickr: To scale up our annotation efforts, finally, we collected a larger dataset of images from the well-known image hosting service &lsquo;Flickr&rsquo; (<a href="https://www.flickr.com">www.flickr.com</a>). We harvested all images from a community-curated collection of depictions of musical instruments in the visual arts pre-dating 1800.<sup id="fnref:30"><a href="#fn:30" class="footnote-ref" role="doc-noteref">30</a></sup>  This third campaign yielded much more data than the former two, but these were more noisy and contained a variety of false positives that had to be manually deleted during the annotation phase.</p>
<p>Our collection efforts were inclusive, and the resulting dataset should be considered as &ldquo;unrestricted&rdquo;, covering a variety of periods, genres and materials (although it was not feasible to include more precise metadata about these aspects in the dataset). Note that, exactly because of this highly mixed data origin, the distribution in MINERVA does not give a faithful representation of any kind of historic reality: music iconography gives a highly colored perspective on &ldquo;popular&rdquo; instruments in art history and some instruments may not often have been depicted, even though they were popular at the time. Likewise, other instruments are likely to be over-represented in iconography.</p>
<h2 id="vocabulary">Vocabulary</h2>
<p>To increase the interoperability of the dataset, individual instruments have been unambiguously identified using their MIMO codes. The MIMO (Musical Instrument Museums Online) initiative is an international consortium, well known for its online database of musical instruments, aggregating data and metadata from multiple heritage institutions <sup id="fnref:31"><a href="#fn:31" class="footnote-ref" role="doc-noteref">31</a></sup>.<sup id="fnref:32"><a href="#fn:32" class="footnote-ref" role="doc-noteref">32</a></sup>  An important contribution is their development of a uniform metadata documentation standard for the field, including a (multilingual) vocabulary to identify musical instruments in an interoperable manner. The MIMO ontology is hierarchical, meaning that each individual leaf node in their concept tree (e.g. &lsquo;viola&rsquo;) is a hyponym of a wider instrument category (e.g. &lsquo;viola&rsquo; ∈ &lsquo;string instruments&rsquo;). <a href="#table01">Table 1</a> shows examples of annotation labels from this ontology. Our dataset provides a spreadsheet that allows for the easy mapping of individual instruments to their instrument category. Below, we shall report experiments for the more fine-grained and less granular, hypernym versions of the categorization task.<br>
Examples of annotation labels from the MIMO ontology (not all were encountered in MINERVA).     <strong>Instrument hypernym</strong>    <strong>Stringed instruments</strong>    <strong>Wind instruments</strong>    <strong>Percussion instruments</strong>    <strong>Keyboard instruments</strong>    <strong>Electronic instruments</strong>        <em>Example instruments</em>   Lute, psaltery, fiddle, viola da gamba, cittern  Transverse flute, end-blown trumpet, horn, shawm, bagpipe  Tambourine, cylindrical drum, frame drum, friction drum, bell  Pianoforte, virginal, portative organ, harpsichord, clavichord  Electric guitar, synthesizer, theremin, vocoder, mellotron</p>
<h2 id="annotation-process">Annotation process</h2>
<p>Using the conventional method of rectangular bounding boxes, we have manually annotated 16,142 musical instruments (of which 172 unique) in a collection of 11,765 images, within the open-source <a href="https://cytomine.be">Cytomine</a> software environment <sup id="fnref:33"><a href="#fn:33" class="footnote-ref" role="doc-noteref">33</a></sup>. Often multiple instruments appeared within the same images and bounding boxes were therefore allowed to overlap. Example annotations and a screenshot of the annotation environment are presented in <a href="#figure02">Figure 2</a>.</p>
<p>The dataset contains artistic objects from diverse periods and of various types, ranging from paintings, sculptures, drawings, to decorative arts, manuscript illuminations and stained-glass windows. Thus, they involve a daunting diversity of media, techniques and modes. Whereas in some cases the images were straightforward to annotate (e.g. an image representing a bell in full frame), several obstacles occurred on a recurrent basis. These obstacles can be linked to three parameters:</p>
<p>Representation: A challenging aspect was the variety of artistic depiction modes represented in the dataset, ranging from photo-realistic renderings to heavily stylized depictions from specific art-historical movements (e.g. impressionism, pointillism, fauvism, cubism, &hellip;) (<a href="#figure03">Figure 3a</a>). Additionally, visibility could be low due to a proportionally small instrument depiction or the profusion of details (<a href="#figure03">Figure 3b</a>). In some instances, the state of the depicted object and its medium made the detection of the instrument difficult, e.g. a damaged medieval tympanon (<a href="#figure03">Figure 3b</a>).  Quality: Other, more pragmatic issues arose from the images themselves. Occasionally, the quality of the images was too low to be able to detect the instruments (e.g. low resolution or compression defects) (<a href="#figure03">Figure 3c</a>). A great deal of the images did not meet international quality standards for heritage reproduction photography (uniform and neutral environment and lighting, frontal point of view), which implies that the instruments were even more difficult to detect.  Boxes: The use of a rectangular shape for the bounding boxes sometimes has limitations and implied a certain lack of precision, e.g. in the case of a diagonally positioned flute, or in the case of overlapping instruments (<a href="#figure03">Figure 3d</a>). For some instruments which consist of several parts, e.g. a violin and its bow, only the main part (the violin) was annotated.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000517/resources/images/figure02.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000517/resources/images/figure02_hu4ce60db94aec5388692da02c552ec389_1042212_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000517/resources/images/figure02_hu4ce60db94aec5388692da02c552ec389_1042212_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000517/resources/images/figure02_hu4ce60db94aec5388692da02c552ec389_1042212_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000517/resources/images/figure02.png 1370w" 
     class="landscape"
     ><figcaption>
        <p>Illustration of the annotation interface in Cytomine [^marée2016].
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000517/resources/images/figure03.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000517/resources/images/figure03_hu907a842ecac3afe416c0471084b56cf4_3144647_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000517/resources/images/figure03_hu907a842ecac3afe416c0471084b56cf4_3144647_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000517/resources/images/figure03_hu907a842ecac3afe416c0471084b56cf4_3144647_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000517/resources/images/figure03.png 1336w" 
     class="portrait"
     ><figcaption>
        <p>Examples of difficulties encountered when annotating images.
        </p>
    </figcaption>
</figure>
<h2 id="characteristics">Characteristics</h2>
<p>An important share of the annotations which we collected were singletons, i.e. instruments that were only encountered once or twice. Although we release the full dataset, we shall from now on only consider instruments that occurred at least three times that allow for a conventional machine learning setup (with non-overlapping train, validation and test sets, that include at least one instance of each label). Whereas the full MIMO vocabulary covers over 2,000 vocabulary terms for individual instruments, only a fraction of these were attested in the 4,183 images which we use below (overview in <a href="#table01">Table 1</a>). Note that this table shows a considerable drop in the original number of images that we annotated, because we only included images that (a) actually contained an instrument and (b) images depicting instruments that occurred at least thrice.</p>
<p>93 different instrument categories appear at least thrice in the dataset. A visualization of the heavily skewed distribution of the different instruments can be seen in <a href="#figure04">Figure 4</a>, where each instrument is represented together with its corresponding MIMO code (between parentheses). This distribution exposes two core aspects of this dataset (but also of music iconography in general): (i) its strong Western-European bias, which has been historically acknowledged, and which scholars are actively trying to correct nowadays, but which is a slow process; (ii) the &lsquo;heavy-tail&rsquo; distribution associated with cultural data in general; i.e. only a fraction of instruments, such as the lute, harp and violin, are depicted with a high frequency, the rest occurs much more sparsely.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000517/resources/images/figure04.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000517/resources/images/figure04_hu43301d202653713744abc1fcfcfde9a9_641695_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000517/resources/images/figure04_hu43301d202653713744abc1fcfcfde9a9_641695_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000517/resources/images/figure04_hu43301d202653713744abc1fcfcfde9a9_641695_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000517/resources/images/figure04.png 1336w" 
     class="landscape"
     ><figcaption>
        <p>Distribution of the instrument types in the full MINERVA dataset.
        </p>
    </figcaption>
</figure>
<h2 id="versions-and-splits">Versions and splits</h2>
<p>The label imbalance described in the previous paragraph is a significant issue for machine learning methods. We therefore experiment with the data in five versions (that are available from the repository) that correspond to object detection tasks of varying complexity. We start by exploring whether it is possible to just detect the presence of an instrument in the different artworks, without the additional need of also predicting the class of the detected instrument. We refer to this benchmark as single-instrument object detection. We then move to three more challenging tasks in which we also aim at correctly classifying the content of the detected bounding boxes. We include data for this detection task for the top-5, the top-10 and top-20 most frequently occurring instruments, a customary practice in the field. Finally, we also repeat this task for all images, but with the &ldquo;hypernym&rdquo; labels of the instrument categories (see <a href="#figure05">Figure 5</a>).</p>
<p>Each version of the dataset comes with its own training, development and testing splits, where we offer the guarantee that at least one of the instrument classes in the task is represented in each of the splits. Additionally, the splits are stratified so that the class distribution is approximately the same in each split. The number of images per split in each version is summarized in <a href="#table02">Table 2</a>. The hypernym version of the dataset is not reported in this table as it shares the same images and splits as the single-instrument version (they both contain all instruments). We used a standard implementation <sup id="fnref:34"><a href="#fn:34" class="footnote-ref" role="doc-noteref">34</a></sup> for a randomized and shuffled split at the level of images and the following, approximate proportions: 1/2 train, 1/4 dev, and 1/4 test. Images may contain multiple instruments, so that the actual number of instruments (as opposed to images) may vary relatively strongly across splits.<br>
Image and instruments distributions of the training, development and test sets for the four different benchmarks presented in this paper (single instrument, top-5 instruments, top-10 instruments and top-20 instruments).       <strong>Training-set</strong>      <strong>Dev-set</strong>      <strong>Test-set</strong>      <strong>Total</strong>           Imag  Inst  Imag  Inst  Imag  Inst  Imag  Inst      Single inst  1857  4243  1137  2288  1189  2102  4183  8633      Top-5 inst  952  1589  540  852  724  1173  2216  3614      Top-10 inst  1227  2147  680  1127  898  1506  2805  4780      Top-20 inst  1471  2915  860  1543   1047  1838  3378  6296   <br>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000517/resources/images/figure05.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000517/resources/images/figure05_hub582b45aa435eab0b01b739f89c2549d_371177_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000517/resources/images/figure05_hub582b45aa435eab0b01b739f89c2549d_371177_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000517/resources/images/figure05_hub582b45aa435eab0b01b739f89c2549d_371177_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000517/resources/images/figure05.png 1336w" 
     class="landscape"
     ><figcaption>
        <p>Distribution of the 5 hypernym categories over the three splits in the MINERVA dataset.
        </p>
    </figcaption>
</figure></p>
<h2 id="benchmark-experiments">Benchmark experiments</h2>
<h2 id="classification">Classification</h2>
<p>In the first benchmark experiment, we start by investigating whether convolutional neural networks are able to correctly classify the different instruments that are present in the dataset. That means that we focus on the image classification task and postpone the task of object detection to the next section. To this end, we have extracted the various patches delineated by the bounding boxes in the detection dataset as stand-alone instances. Note, however, that patches from the same images always ended in the same split, to avoid information leakage across the splits. Example patches are shown in <a href="#figure06">Figure 6</a>.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000517/resources/images/figure06.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000517/resources/images/figure06_hufc8115f4441db5b0c6fc85e3b2dcac5d_724679_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000517/resources/images/figure06_hufc8115f4441db5b0c6fc85e3b2dcac5d_724679_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000517/resources/images/figure06_hufc8115f4441db5b0c6fc85e3b2dcac5d_724679_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000517/resources/images/figure06.png 1336w" 
     class="landscape"
     ><figcaption>
        <p>Examples of the patches delineated by the bounding boxes, extracted from MINERVA images for the classification experiment.
        </p>
    </figcaption>
</figure>
<p>Next, we tackled this task as a standard machine-learning classification problem for which we applied a representative selection of established neural network architectures. All of these networks were pretrained on the Rijksmuseum dataset <sup id="fnref4:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup>, for which the weights are publicly available <sup id="fnref2:25"><a href="#fn:25" class="footnote-ref" role="doc-noteref">25</a></sup>. The tested architectures are: VGG19 <sup id="fnref1:24"><a href="#fn:24" class="footnote-ref" role="doc-noteref">24</a></sup>, Inception-V3 <sup id="fnref1:23"><a href="#fn:23" class="footnote-ref" role="doc-noteref">23</a></sup> and ResNet <sup id="fnref1:22"><a href="#fn:22" class="footnote-ref" role="doc-noteref">22</a></sup>. This approach is motivated by previous work <sup id="fnref3:25"><a href="#fn:25" class="footnote-ref" role="doc-noteref">25</a></sup> which shows that when it comes to the classification images from the domain of cultural heritage, popular neural architectures which have been trained on the large Rijksmuseum collection, can outperform the same kind of architectures that are pre-trained on ImageNet only. In order to maximize the final classification performance, all network parameters get fine-tuned, using the Adam optimizer <sup id="fnref:35"><a href="#fn:35" class="footnote-ref" role="doc-noteref">35</a></sup> and minimizing the conventional categorical cross-entropy loss function over mini-batches of 32 samples. Additionally, we applied 3 different learning rates: 0.001, 0.0001, 0.00001. In order to handle the skewed distribution of the classes, we experimented with models including and excluding oversampling. The training regime is interrupted as soon the validation loss does not decrease for five epochs in a row.</p>
<p>In <a href="#table01">Table 2</a> and <a href="#table03">Table 3</a> we report the results in terms of Accuracy and F1-score for the MINERVA test sets. For the individual instruments, we do so for four versions of the dataset of increasing complexity: the top-5 instruments, top-10 instruments, top-20 instruments and the entire dataset. Analogously we report the scores for a classification experiment where the object detector is trained on the instrument hypernyms as class labels.<br>
Classification results on the MINERVA test set for the three architectures (best results in bold).      Top-5 inst    Top-10 inst    Top-20 inst    All inst    Hypernyms        CNN  Acc.  F1  Acc.  F1  Acc.  F1  Acc.  F1  Acc.  F1      R-Net  68.71  64.10  52.85  41.55  30.73  8.45  26.36  2.08  72.26  52.66       V3   <strong>73.66</strong>    <strong>70.29</strong>    <strong>55.51</strong>    <strong>44.77</strong>    <strong>36.51</strong>    <strong>19.06</strong>    <strong>27.02</strong>    <strong>6.67</strong>    <strong>75.80</strong>    <strong>57.03</strong>       V19  48.33  35.92  37.52  15.22  33.41  9.87  20.17  1.72  66.41  40.35        Confusion matrix for the classification experiment with ResNet on the MINERVA test set (the top-10 most frequently occurring instruments).     <strong>Predicted label / Gold label</strong>    <strong>Bagpipe</strong>    <strong>E-b trumpet</strong>    <strong>Harp</strong>    <strong>Horn</strong>    <strong>Lute</strong>    <strong>Lyre</strong>    <strong>Por. organ</strong>    <strong>Rebec</strong>    <strong>Shawm</strong>    <strong>Violin</strong>        <strong>Bagpipe</strong>    <strong>31</strong>   0  10  6  8  1  2  0  7  17       <strong>E-b trumpet</strong>   4   <strong>72</strong>   19  2  14  1  3  1  38  21       <strong>Harp</strong>   8  2   <strong>227</strong>   1  10  3  11  0  10  19       <strong>Horn</strong>   7  5  14   <strong>9</strong>   16  9  1  2  5  14       <strong>Lute</strong>   6  10  17  6   <strong>199</strong>   6  5  1  5  42       <strong>Lyre</strong>   3  0  19  1  13   <strong>5</strong>   2  0  3  11       <strong>Por. organ</strong>   3  0  10  1  0  0   <strong>57</strong>   0  1  4       <strong>Rebec</strong>   5  2  14  0  9  0  4   <strong>7</strong>   1  23       <strong>Shawm</strong>   4  11  25  2  11  2  4  6   <strong>40</strong>   13       <strong>Violin</strong>   6  12  29  4  35  4  7  11  6   <strong>202</strong></p>
<h2 id="detection">Detection</h2>
<p>For the second benchmark experiment we report the results that we have obtained on the four of the five detection benchmarks introduced in the previous section. The way the different instruments are distributed in their respective test sets is visually represented in the first image of each row of <a href="#figure07">Figure 7</a>. For our experiments, we use the popular YOLO-V3 <sup id="fnref2:27"><a href="#fn:27" class="footnote-ref" role="doc-noteref">27</a></sup> architecture which we fully fine-tune during training. To explore the benefits that transfer learning could bring to the artistic domain, we initialize the network with the weights that are obtained after training the model on the MS-COCO dataset <sup id="fnref3:21"><a href="#fn:21" class="footnote-ref" role="doc-noteref">21</a></sup>. The network gets then trained either with the Adam optimizer <sup id="fnref1:35"><a href="#fn:35" class="footnote-ref" role="doc-noteref">35</a></sup> or RMSprop<sup id="fnref:36"><a href="#fn:36" class="footnote-ref" role="doc-noteref">36</a></sup>  over mini-batches of 8 images.</p>
<p>To assess the performance of the neural network, we follow the same evaluation protocol that characterizes object detection problems in CV <sup id="fnref4:21"><a href="#fn:21" class="footnote-ref" role="doc-noteref">21</a></sup>. Each detected bounding box is compared to the bounding box which has been annotated on the Cytomine platform. We only consider bounding boxes for which the confidence level is ≥ 0.05, following the protocol established in <sup id="fnref1:26"><a href="#fn:26" class="footnote-ref" role="doc-noteref">26</a></sup>. We then compute the &ldquo;Intersection over Union&rdquo; (IoU) for measuring how much the detected bounding-boxes differ from the ground-truth ones. To assess whether a prediction can be considered as a &ldquo;true positive&rdquo; or a &ldquo;false positive&rdquo;, we define two, increasingly restrictive metrics: first, IoU ≥ 10 and, secondly, IoU ≥ 50. This approach is again inspired by <sup id="fnref:37"><a href="#fn:37" class="footnote-ref" role="doc-noteref">37</a></sup>, where the authors report additional results with an IoU ≥ 10 on their IconArt dataset. <a href="#table05">Table 5</a> lists precision, recall and average precision (AP) scores for each detected class of each data version and <a href="#figure07">Figure 7</a> visually shows the number of true and false positive predictions in all cases. Examples of correct detections are shown in <a href="#figure08">Figure 8</a>.<br>
A quantitative analysis of the results obtained on the four localization benchmarks introduced in this work. To distinguish different benchmarks in the table we separate them by a double line. We report the precision, recall and average-precision scores for each detected class.     <strong>Instrument ≥ IoU</strong>    <strong>Precision</strong>    <strong>Recall</strong>    <strong>AP</strong>       Single-instrument ≥ 10 Single-instrument ≥ 50  0.63 0.47  0.42 0.31  0.35 0.22      Stringed-Instruments ≥ 10 Stringed-Instruments ≥ 50  0.65 0.53  0.36 0.29  0.28 0.20      Wind-Instruments ≥ 10 Wind-Instruments ≥ 50  0.43 0.32  0.07 0.05  0.04 0.02      Percussion-Instruments ≥ 10 Percussion-Instruments ≥ 50  0.32 0.21  0.04 0.03  0.02 0.01      Keyboard-Instruments ≥ 10 Keyboard-Instruments ≥ 50   0.61 0.45   0.11 0.08   0.07 0.04      Electronic-Instruments ≥ 10 Electronic-Instruments ≥ 50  - -  - -  - -      Harp ≥ 10 Harp ≥ 50  0.68 0.60  0.62 0.54  0.55 0.46      Lute ≥ 10 Lute ≥ 50  0.57 0.47  0.43 0.35  0.36 0.26      Violin ≥ 10 Violin ≥ 50  0.37 0.26  0.22 0.16  0.12 0.07      Shawm ≥ 10 Shawm ≥ 50  0.13 0.08  0.04 0.02  0.01 0.00      End-blown trumpet ≥ 10 End-blown trumpet ≥ 50  0.28 0.24  0.04 0.03  0.01 0.01      Harp ≥ 10 Harp ≥ 50  0.62 0.56  0.56 0.51  0.46 0.39      Lute ≥ 10 Lute ≥ 50  0.55 0.47  0.42 0.36  0.33 0.25      Violin ≥ 10 Violin ≥ 50  0.26 0.20  0.19 0.14  0.06 0.04      Shawm ≥ 10 Shawm ≥ 50  0.17 0.17  0.03 0.01  0.00 0.00      End-blown trumpet ≥ 10 End-blown trumpet ≥ 50  0.67 0.17  0.02 0.03  0.01 0.00      Bagpipe ≥ 10 Bagpipe ≥ 50  0 0  0 0  0 0      Portative-Organ ≥ 10 Portative-Organ ≥ 50  0.24 0.24  0.13 0.13  0.06 0.06      Horn ≥ 10 Horn ≥ 50  0 0  0 0  0 0      Rebec ≥ 10 Rebec ≥ 50  - -  - -  - -      Lyre ≥ 10 Lyre ≥ 50  - -  - -  &ndash; -   <br>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000517/resources/images/figure07.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000517/resources/images/figure07_huaf6ea5d8454f7b43e9acb28ca2b1acad_1500227_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000517/resources/images/figure07_huaf6ea5d8454f7b43e9acb28ca2b1acad_1500227_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000517/resources/images/figure07_huaf6ea5d8454f7b43e9acb28ca2b1acad_1500227_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000517/resources/images/figure07.png 1336w" 
     class="landscape"
     ><figcaption>
        <p>A visual representation of how many instruments should be detected in the testing sets of the four MINERVA benchmarks that are introduced in this paper (first plot of each row). The second and third plots represent the true and false detections that we have obtained with a fully fine-tuned YOLO network. Results are computed with respect to an IoU ≥ 10 and an IoU ≥ 50.
        </p>
    </figcaption>
</figure></p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000517/resources/images/figure08.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000517/resources/images/figure08_huf8341f699a54468c13e9ce51b76209af_3124631_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000517/resources/images/figure08_huf8341f699a54468c13e9ce51b76209af_3124631_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000517/resources/images/figure08_huf8341f699a54468c13e9ce51b76209af_3124631_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000517/resources/images/figure08.png 1336w" 
     class="landscape"
     ><figcaption>
        <p>Sample visualizations of the detections obtained on the MINERVA test set for a fully fine-tuned YOLO architecture. The first three rows report the detection of any kind of instrument within the images (single-instrument task), while the last three rows also report the correct classification of the detected bounding boxes.
        </p>
    </figcaption>
</figure>
<h2 id="additional-experiments">Additional experiments</h2>
<p>As an additional stress-test, we have applied a trained object detector to two external data sets, in order to assess how valid and performant our approach is when applied &ldquo;in the wild&rdquo;. We have considered two out-of-sample datasets:</p>
<ul>
<li>RMFAB/RMAH: 428 out-of-sample images from the digital assets of both museum collections that are not included in the annotated material (and which are thus not included the train and validation material of the applied detector), because the available metadata did not explicitly specify that they contained depictions of musical instruments. (This collection cannot be shared due to copyright restrictions.)</li>
<li>IconArt: a generic collection of 6,528 artistic images, collected from the community-curated platform  <em>WikiArt: Visual Art Encyclopedia</em>  (<a href="https://www.wikiart.org/">https://www.wikiart.org/</a>). The IconArt subcollection was previously redistributed by <sup id="fnref1:37"><a href="#fn:37" class="footnote-ref" role="doc-noteref">37</a></sup>: <a href="https://wsoda.telecom-paristech.fr/downloads/dataset/">https://wsoda.telecom-paristech.fr/downloads/dataset/</a>.</li>
</ul>
<p>Note that both external datasets differ in crucial aspects: RMFAB/RMAH can be considered &ldquo;out-of-sample&rdquo;, but &ldquo;in-collection&rdquo;, in the sense that these images derive from the same digital collections as many of the images represented in MINERVA. Additionally, we can expect extremely low detection rates for this dataset, because the presence of musical instruments will already have been flagged in a large majority of cases by the museum&rsquo;s staff. Thus, the application of RMFAB/RMAH should be viewed as a rather conservative stress test or sanity check, mainly checking for images that might have been missed by annotators in the past. The IconArt dataset is &ldquo;out-of-sample&rdquo; and &ldquo;out-of-collection&rdquo;, in the sense that these images derive from a variety of other sources. It is therefore fully unrestricted, and this test can be considered a curiosity-driven validation of the method &ldquo;in the wild&rdquo;. Importantly, IconArt was not collected with specific attention for musical instruments, so here too, we can anticipate a rather low detection rate (since many works of art simply do not feature any instruments). For all these reasons, we only evaluate the results on these external datasets in terms of precision (as recall is much less meaningful in this context).</p>
<p>Following these differences, we have applied the single-instrument detector to the RMFAB/RMAH data and the hypernym detector to IconArt. Keeping an eye on the feasibility of the manual inspection, we have limited the number of instances returned by only allowing detections with a confidence score ≥ 0.20 (which is a rather generous threshold). Next, the results have then been evaluated in terms of precision, i.e. the number of returned image regions that actually represent musical instruments. The results are presented in <a href="#table06">Table 6</a>. <a href="#figure10">Figure 10</a> showcases a number of cherry-picked successful examples of detections from the out-of-collection IconArt images.<br>
Quantitative evaluation of the method on two out-of-sample datasets in terms of precision, restricted to detections with a confidence score ≥ 0.20.     <strong>Collection</strong>    <strong>Total images</strong>    <strong>Detections</strong>    <strong>True positives</strong>        <strong>RMFAB/RMAH</strong>   428  162  6       <strong>IconArt</strong>   6528  118  42</p>
<h2 id="discussion">Discussion</h2>
<h2 id="skewed-results">Skewed results</h2>
<p>First and foremost, we can observe that the scores obtained across all benchmarks are generally much lower than those reported for other datasets in computer vision (outside of the strict artistic domain). This drop in performance was to be expected and can be attributed to both the smaller size of the training data and the higher variance in the representation spectrum of musical instruments (across periods, materials, modes and, artists). Secondly, one can observe large fluctuations in the identifiability and detectability of individual instrument categories across both tasks. Not all of the fluctuations are easy to account for.</p>
<p>We first consider the classification results. The confusion matrix reported in <a href="#table04">Table 4</a> clearly shows that the classes representing the top-4 of instruments (harp, lute, violin, and portative organ) can be learned rather successfully, but that the performance rapidly breaks down for instrument categories at lower frequency ranks. Thus, while the accuracies for the top-5 experiments are relatively satisfying, especially in terms of accuracy (V3:  <em>acc=73.66; F1=70.29</em> ), the performance rapidly degrades for the more difficult setups. The results for the &ldquo;all&rdquo; classification experiment, where every instrument category is included no matter its frequency, are nothing less than dramatic (V3:  <em>acc=27.02; F1=6.67</em> ) and call for in-depth further research. The significant divergence between accuracy scores and F1 scores demonstrate that class imbalance is thus another aspect in which MINERVA presents a more challenging benchmark than its photorealistic counterparts.</p>
<p>The skewness of the class distribution in MINERVA is representative of the long-tail distribution that we commonly encounter in cultural data. This imbalance is somewhat alleviated in the hypernym setup, where the labels are of course much better distributed over a much smaller number of classes ( <em>n=5</em> ). The general feasibility of this specific task is demonstrated by the encouraging scores that can be reported for the Inception-V3 architecture on this task ( <em>acc=75.80; F1=57.03</em> ). Note, additionally, that the &ldquo;Electronic instruments&rdquo; hypernym is included for completeness in this task, although the label is very infrequent and inevitably pulls down the (macro-averaged) F1-score in this respect. Overall, we notice than the Inception-V3 architecture yields the highest performance on average for the classification task.</p>
<p>Similar trends can be observed for the musical instrument detection task. First of all, we should emphasize the encouraging scores for the &ldquo;single-instrument&rdquo; detection task that simply aims to detect musical instruments (no matter their type). Here, a relatively high precision score is obtained ( <em>prec=0.63</em>  for  <em>IoU ≥ 10</em> ), which seems on par with comparable object categories for modern photo-realistic collections <sup id="fnref1:28"><a href="#fn:28" class="footnote-ref" role="doc-noteref">28</a></sup>. Thus, this algorithm might not be fully apt at retrieving every single instrument from an unseen collection, but when it detects an instrument, we can be relatively sure that the detection deserves further inspection by a domain expert. Equally heartening scores are evident for most of the instrument hypernyms (with the notable exception of the under-represented &ldquo;Electronic instruments&rdquo; hypernym). While these detection tasks are of course relatively coarse, this observation nevertheless entails that this sort of detection technology can already find useful applications in the field (see below).</p>
<p>When making our way down the frequency list in <a href="#table05">Table 5</a>, we again observe how the results break down dramatically for less common instrument categories. The fact that an over-represented category like harps can be reasonably well detected ( <em>AP(IoU ≥ 10)=0.55; AP(IoU ≥ 50)=0.46</em> ), should not lead the attention away from the fact that a state of the art object detector, such as YOLO, fails miserably at detecting a number of iconographically highly salient instruments, such as lyres and end-blown trumpets. At this stage, it is unclear whether this is caused by mere class imbalance or by the higher variance in the iconographic depiction of specific instruments. Bagpipes, for instance, occur frequently across images in MINERVA but might display much more depiction variance than, for instance, a harp.</p>
<h2 id="saliency-maps">Saliency maps</h2>
<p>The results from the previous question call into question which visual properties the neural networks find useful to exploit in the identification of instruments. Importantly, the characteristic features exploited by a machine learning algorithm need not coincide with the properties that are judged most relevant by human experts and the comparison of both types of relevance judgements is worthwhile. In this section, we therefore perform model criticism or &ldquo;network introspection&rdquo; on the basis of the so-called &ldquo;saliency maps&rdquo; that can be extracted from a trained model <sup id="fnref:38"><a href="#fn:38" class="footnote-ref" role="doc-noteref">38</a></sup>. These saliency maps make visible to which regions in the original image the network paid most attention to, before arriving at its final classification decision. All examples discussed below come from the experiments on the hypernym dataset for the VGG19 network. <a href="#figure09">Figure 9</a> shows a series of manually selected, insightful examples, including the original image (as inputted into the network after preprocessing), as well as the saliency map obtained for it. We limit these examples to the representative hypernyms &lsquo;Stringed instruments&rsquo; and &lsquo;Wind instruments.&rsquo;</p>
<p>The maps in <a href="#figure09">Figure 9</a> vividly illustrate that the network focuses on two broad types of regions: properties of the instruments itself (which was expected) but also the immediate context of the instruments, and more specifically the way they are operated, handled or presented by people, c.q. musicians. The characteristics of the salient regions in the examples in <a href="#figure09">Figure 9</a> could be described as:</p>
<p>Stringed instruments:</p>
<p>(a) Focus on the neck of the stringed instrument, as well as the characteristic presence of tuning pins at the end of the neck;  (b) Sensitive to the presence of stretched fingers in an unnatural position;  (c) Typical conic shape of a lyre, with outward pointing ends connected by a bridge;</p>
<p>Wind instruments:</p>
<p>(d) Symmetric presence of tone holes in the areophone;  (e) Elongated, cylindric shape of the main body of the areophone with wider end;  (f) Mirrored placement of fingers and hands (close to one another).</p>
<p>These characteristics strongly suggest that the way an instrument is handled (i.e. its immediate iconographic neighborhood) is potentially of equal importance as the shape of the actual instrument, an insight that we will further expand on below.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000517/resources/images/figure09.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000517/resources/images/figure09_hu67ca31ff1266f24a91c776a66854e5b4_1161962_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000517/resources/images/figure09_hu67ca31ff1266f24a91c776a66854e5b4_1161962_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000517/resources/images/figure09.png 920w" 
     class="portrait"
     ><figcaption>
        <p>Saliency maps for several stringed (subfigures (a) to (c)) and wind (subfigures (d) to (f)) instruments.
        </p>
    </figcaption>
</figure>
<h2 id="error-analysis-false-positives">Error analysis: false positives</h2>
<p>In this section, we offer a qualitative discussion of the false positives from the out-of-sample tests reported in the previous section, i.e. instances where the detectors erroneously thought to have detected an instrument. This eagerness is a known problem of object detectors: a system that is trained to recognize &ldquo;sheep&rdquo; will be inclined to see &ldquo;sheep&rdquo; everywhere. Anecdotally, people have noted how misleading contextual cues can indeed be a confounding factor in image analysis. One blog post for instances noted how a major image labeling service tagged photographs of green fields with the &ldquo;sheep&rdquo; label, although no sheep whatsoever were present in the images.<sup id="fnref:39"><a href="#fn:39" class="footnote-ref" role="doc-noteref">39</a></sup>  Eagerness-to-detect or over-association is therefore a clear first shortcoming of this method when applied in the wild, mainly because it was only trained in images that actually contain musical instruments. Interestingly, the false positives come in clusters that shed an interesting light on this issue. Below we list a representative number of error clusters:</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000517/resources/images/figure10.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000517/resources/images/figure10_hue4e861ec997f4f02beb2aaa8f0976c30_1905009_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000517/resources/images/figure10_hue4e861ec997f4f02beb2aaa8f0976c30_1905009_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000517/resources/images/figure10.png 920w" 
     class="portrait"
     ><figcaption>
        <p>Examples of successful detections in IconArt for &ldquo;stringed instruments”.
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000517/resources/images/figure11.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000517/resources/images/figure11_hu46f01e4b2279201636429bd1aa00a4e9_3820959_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000517/resources/images/figure11_hu46f01e4b2279201636429bd1aa00a4e9_3820959_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000517/resources/images/figure11.png 1072w" 
     class="portrait"
     ><figcaption>
        <p>Anecdotal examples of false positive detection, divided in 7 interpretive clusters (numbered a-g).
        </p>
    </figcaption>
</figure>
<p>The above categorization illustrates that the false positives are rather insightful, mainly because the absence of an instrument highlights the contextual clues that are at work. Of particular relevance is the observation that the iconography surrounding children closely resembles that of instruments. This seems related to the intimate and caring body language of both the caretakers and musicians in such compositions. The immediate iconographic neighborhood of children clearly reminds the detector of the delicacy and reverence with which instruments are portrayed and presented in historical artworks. This delicacy and intimacy in body language can be specifically related to the foregrounding of fingers, the prominent portrayal of which invariably triggers the detector, also in the absence of children. Some of these phenomena invite closer inspection by domain experts in music iconography and suggest that serial or panoramic analyses are a worthwhile endeavour in this field, also from the point of view of more hermeneutically oriented scholars.</p>
<h2 id="conclusions-and-future-research">Conclusions and future research</h2>
<p>In this paper, we have introduced MINERVA, to our knowledge the first sizable benchmark dataset for the identification and detection of individual musical instruments in unrestricted, digitized images from the realm of the visual arts. Our benchmark experiments have highlighted the feasibility of a number of tasks but also, and perhaps primarily, the significant challenges that state-of-the-art machine learning systems are still confronted with on this data, such as the &ldquo;long-tail&rdquo; of the instruments&rsquo; distribution and the staggering variance in depiction across the images in the dataset. We therefore hope that this work will inspire new (and much-needed) research in this area. At the end of this paper, we wish to formulate some advice and concerns in this respect.</p>
<p>One evident direction from future research is more advanced transfer learning, where algorithms make more efficient use of the wealth of photorealistic data that is provided, for instance, by MIMO <sup id="fnref1:31"><a href="#fn:31" class="footnote-ref" role="doc-noteref">31</a></sup>. The main issue with the MIMO data in this respect is that the bulk of these photographs are context-free (i.e. the instruments are photographed in isolation, against a white or neutral background), which is almost never the case in the artistic domain. Preliminary research demonstrated that this a major hurdle to established pretraining scenarios. Cascaded approaches, where instruments are detected first and only classified in a second stage might be a promising avenue here.</p>
<p>One crucial final remark is that AI has an amply attested tendency not only to be sensitive to biases in the input data but also to amplify them <sup id="fnref:40"><a href="#fn:40" class="footnote-ref" role="doc-noteref">40</a></sup>. Whereas the computational methods presented here have the potential to scale up dramatically the scope of current research in music iconography, it also comes with ideological dangers. The technology could further strengthen the bias on specific canonical regions and periods in art history and lead the attention even further away from artistic and iconographic cultures that are already in specific need of reappraisal. The community will therefore have to think carefully about bias correction and mitigation. Collecting training data in a diverse and inclusive manner, with ample attention for resource-lower cultures should be a key strategy in future data collection campaigns.</p>
<h2 id="acknowledgements">Acknowledgements</h2>
<p>We wish to thank Remy Vandaele for the help with Cytomine and for the fruitful discussions related to computer vision and object detection. Special thanks go out to our annotators and other (former) colleagues in the museums involved: Cedric Feys, Odile Keromnes, Lies Van De Cappelle and Els Angenon. Our gratitude also goes out to Rodolphe Bailly for his support and advice regarding MIMO. Finally, we wish to credit our former project member dr. Ellen van Keer with the original idea of applying object detection to musical instruments. This project is generously funded by the Belgian Federal Research Agency BELSPO under the BRAIN-be program.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Hockey, S.  “A History of Humanities Computing.” In S. Schreibman, R. Siemens, and J. Unsworth (eds.),  <em>A Companion to Digital Humanities</em> , Oxford (2004), pp. 3–19.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>LeCun, J., Bengio, Y., and Hinton, G.,  “Deep Learning”    <em>Nature</em> , 521 (2015): 436–444.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>, Schmidhuber, J.  “Deep Learning in Neural Networks: An Overview”    <em>Neural Networks</em> , 61 (2015), 85-117.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Wevers M., and Smits, T.  “The visual digital turn: Using neural networks to study historical images”    <em>Digital Scholarship in the Humanities</em> , 35 (2020), 194–207.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>Arnold, T., and Tilton, L.,  “Distant viewing: analyzing large visual corpora.”  <em>Digital Scholarship in the Humanities</em> , 34 (2019), i3-i16.&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>Buckley, A.  “Music Iconography and the Semiotics of Visual Representation”    <em>Music in Art</em> , 23 (1998), 5-10.&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>Baldassarre, A.  “Quo vadis music iconography? The Repertoire International d&rsquo;Iconographie Musicale as a case study”    <em>Fontes Artis Musicae</em> , 54 (2007), 440-452.&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>Baldassarre, A.  “Music Iconography: What is it all about? Some remarks and considerations with a selected bibliography”    <em>Ictus: Periódico do Programa de Pós-Graduação em Música da UFBA</em> , 9 (2008), 55-95.&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>Green, A., and Ferguson, S.  “RIDIM: Cataloguing music iconography since 1971”    <em>Fontes Artis Musicae</em> , 60 (2013), 1-8.&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>See <a href="https://hosting.uantwerpen.be/insight/">https://hosting.uantwerpen.be/insight/</a>. This project is generously funded by the Belgian Federal Research Agency BELSPO under the BRAIN-be program.&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>Ballard, D. H., and Christopher M. Brown, C. M.  <em>Computer Vision</em> , Upper Saddle River (1982).&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p>Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, K., Khosla, A., Bernstein, M. et al.  “Imagenet large scale visual recognition challenge”    <em>International journal of computer vision</em> , 115(3) (2015), 211–252.&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13">
<p>Hertzmann, A.  “Can Computers Create Art?”  Arts, 7 (2018) doi:10.3390/arts7020018.&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14">
<p>Crowley, E., and Zisserman, A.  “The State of the Art: Object Retrieval in Paintings using Discriminative Regions”  In Valstar, M., French, A., and Pridmore, T. (eds),  <em>Proceedings of the British Machine Vision Conference</em> , Nottingham (2014), s.p.&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:15">
<p>Van Noord, N., Hendriks, E., and Postma, E.,  “Toward Discovery of the Artist&rsquo;s Style: Learning to recognize artists by their artworks”    <em>IEEE Signal Processing Magazine</em> , 32 (2015), 46-54.&#160;<a href="#fnref:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:16">
<p>Seguin, B.  “The Replica Project: Building a visual search engine for art historians”  XRDS: Crossroads,  <em>The ACM Magazine for Students - Computers and Art</em> , 24 (2018), 24-29.&#160;<a href="#fnref:16" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:17">
<p>Bell, P., and Impett, L.  “Ikonographie und Interaktion. Computergestützte Analyse von Posen in Bildern der Heilsgeschichte”    <em>Das Mittelalter</em> , 24 (2019): 31–53.&#160;<a href="#fnref:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:18">
<p>Xiang, Y., Mottaghi, R., and Savarese, S.  “Beyond pascal: A benchmark for 3d object detection in the wild”  In  <em>IEEE Winter Conference on Applications of Computer Vision</em> , pages 75–82. IEEE, 2014.&#160;<a href="#fnref:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:19">
<p>Mensink, T. and Van Gemert, J.  “The Rijksmuseum challenge: Museum-centered visual recognition”  In  <em>Proceedings of International Conference on Multimedia Retrieval</em> , page 451. ACM, 2014.&#160;<a href="#fnref:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref3:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref4:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:20">
<p>Strezoski, G. and Worring, M.  “Omniart: multi-task deep learning for artistic data analysis”    <em>arXiv preprint arXiv:1708.00684</em> , 2017.&#160;<a href="#fnref:20" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:20" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:20" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref3:20" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:21">
<p>Lin T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollar, P and Zitnick, C. L.  “Microsoft COCO: Common objects in context”  In  <em>European conference on computer vision</em> , pages 740–755. Springer, 2014.&#160;<a href="#fnref:21" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:21" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:21" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref3:21" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref4:21" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:22">
<p>He, K., Zhang, X., Ren, S., and Sun, J.  “Deep residual learning for image recognition.” In  <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em> , pages 770–778, 2010.&#160;<a href="#fnref:22" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:22" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:23">
<p>Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, S., Erhan, D., Vanhoucke, V., and Rabinovich, A.  “Going deeper with convolutions”  In  <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em>  (2015), pp. 1–9.&#160;<a href="#fnref:23" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:23" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:24">
<p>Simonyan, K. and Zisserman, A. Very deep convolutional networks for large-scale image recognition.  <em>arXiv preprint arXiv:1409.1556</em> , 2014.&#160;<a href="#fnref:24" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:24" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:25">
<p>Sabatelli, M., Kestemont, M., Daelemans, W. and Geurts, P.  “Deep transfer learning for art classification problems”  In  <em>Proceedings of the European Conference on Computer Vision (ECCV)</em> , pages 631–646, 2018.&#160;<a href="#fnref:25" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:25" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:25" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref3:25" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:26">
<p>Everingham, M., Van Gool, L., Williams, C. K., Winn, J., and Zisserman, A.  “The Pascal visual object classes (VOC) challenge”  In  <em>International journal of computer vision</em> , 88(2) (2010): 303–338.&#160;<a href="#fnref:26" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:26" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:27">
<p>Redmon, J. and Farhadi, A.  “Yolov3: An incremental improvement”    <em>arXiv preprint arXiv:1804.02767</em> , 2018.&#160;<a href="#fnref:27" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:27" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:27" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:28">
<p>S. Ren, K. He, R. Girshick, and J. Sun,  “Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks”    <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em> , 39 (2017), 1137-1149.&#160;<a href="#fnref:28" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:28" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:29">
<p>All code used in this paper is publicly available from this repository: <a href="https://github.com/paintception/MINeRVA">https://github.com/paintception/MINeRVA</a>. Likewise, the MINERVA dataset can be obtained from this DOI on Zenodo: 10.5281/zenodo.3732580.&#160;<a href="#fnref:29" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:30">
<p><a href="https://web.archive.org/save/https://www.flickr.com/groups/1991907@N24/?rb=1">https://web.archive.org/save/https://www.flickr.com/groups/1991907@N24/?rb=1</a>&#160;<a href="#fnref:30" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:31">
<p>Dolan, E. I.  “Review: MIMO: Musical Instrument Museums Online”    <em>Journal of the American Musicological Society</em> , 70 (2017): 555-565.&#160;<a href="#fnref:31" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:31" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:32">
<p><a href="https://web.archive.org/save/https://www.mimo-international.com/MIMO/">https://web.archive.org/save/https://www.mimo-international.com/MIMO/</a>&#160;<a href="#fnref:32" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:33">
<p>Marée, R., Rollus, L. Stévens, B., Hoyoux, R., Louppe, G., Vandaele, R., Begon, J., Kainz, P., Geurts, P., and Wehenkel  “Collaborative analysis of multi-gigapixel imaging data using Cytomine”    <em>Bioinformatics</em> , 32 (2016): 1395–1401.&#160;<a href="#fnref:33" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:34">
<p>Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R. and Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., and Duchesnay, E.  “Scikit-learn: Machine Learning in Python”    <em>Journal of Machine Learning Research</em> , 12 (2011): 2825-2830.&#160;<a href="#fnref:34" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:35">
<p>Kingma, D. P., and Ba, J.  “A method for stochastic optimization”    <em>arXiv preprint arXiv:1412.6980</em> , 2014.&#160;<a href="#fnref:35" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:35" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:36">
<p>There is no officially published reference for RMSprop, but scholars commonly refer to this lecture from Geoffrey Hinton and colleagues: <a href="https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf">https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf</a>&#160;<a href="#fnref:36" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:37">
<p>Gonthier, N., Gousseau, Y., Ladjal, S. and Bonfait, O.  “Weakly supervised object detection in artworks”  In  <em>Proceedings of the European Conference on Computer Vision (ECCV)</em>  (2018): 692–709.&#160;<a href="#fnref:37" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:37" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:38">
<p>Mariusz Bojarski, Anna Choromanska, Krzysztof Choromanski, Bernhard Firner, Larry J. Ackel, Urs Muller, Philip Yeres, Karol Zieba,  “VisualBackProp: Efficient Visualization of CNNs for Autonomous Driving”    <em>Proceedings of the 2018 IEEE International Conference on Robotics and Automation (ICRA)</em> , 2018, 4701-4708. DOI: 10.1109/ICRA.2018.8461053.&#160;<a href="#fnref:38" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:39">
<p><a href="https://web.archive.org/save/https://aiweirdness.com/post/171451900302/do-neural-nets-dream-of-electric-sheep">https://web.archive.org/save/https://aiweirdness.com/post/171451900302/do-neural-nets-dream-of-electric-sheep</a>&#160;<a href="#fnref:39" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:40">
<p>Zou, J., and Schiebinger, L.  “AI can be sexist and racist — it&rsquo;s time to make it fair”    <em>Nature</em> , 559 (2018): 324-326.&#160;<a href="#fnref:40" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry><entry><title type="html">Afrofuturist Intellectual Mixtapes: A Classroom Case Study</title><link href="https://rlskoeser.github.io/dhqwords/vol/15/1/000516/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://rlskoeser.github.io/dhqwords/vol/15/1/000516/</id><author><name>Tyechia L. Thompson</name></author><author><name>Dashiel Carrera</name></author><published>2021-03-05T00:00:00+00:00</published><updated>2021-03-05T00:00:00+00:00</updated><content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>In &ldquo;Afrofuturism to Vibranium and Beyond,&rdquo; a cross-listed graduate and undergraduate English special topics course, taught by Tyechia Thompson and assisted by Dashiel Carrera (GTA) at Virginia Tech, we engaged several theoretical and artistic frameworks for defining Afrofuturism. One such definition was from Reynaldo Anderson and Charles E. Jones&rsquo;s edited collection  <em>Afrofuturism 2.0: The Rise of Astro Blackness</em> , in which they defined Afrofuturism 2.0 as:</p>
<blockquote>
<p>The early twenty-first century technogenesis of Black identity reflecting counter histories, hacking and or appropriating the influence of network software, database logic, cultural analytics, deep remixability, neurosciences, enhancement and augmentation, gender fluidity, posthuman possibility, the speculative sphere with transdisciplinary applications and has grown into an important Diasporic techno-cultural Pan African movement. <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></p>
</blockquote>
<p>This definition was current and broad enough to cover all the three modules of the course, including the first module, which was the Afrofuturist intellectual mixtape assignment. For this assignment, students took several samples from selected audio recordings on the syllabus and added their own voice and another audio recording of their choosing. Students then wrote &ldquo;liner notes&rdquo; in which they discussed their choice of samples, the use of their own voice, and how these choices connect to larger discourses within Afrofuturism. As such, the Afrofuturist intellectual mixtape assignment made use of hacking and/or appropriating as well as deep remixability while engaging a multitude of discourses that make up Afrofuturistic expression.</p>
<p>In the course, we also engaged other definitions of Afrofuturism such as Alondra Nelson&rsquo;s 2002 articulation of the term as &ldquo;&lsquo;African American voices&rsquo; with &lsquo;other stories to tell about culture, technology and things to come.&rsquo; The term [Afrofuturism] was chosen as the best umbrella for the concerns of &rsquo;the list&rsquo; [the AfroFuturism (AF) listserv] — as it has come to be known by its members — sci-fi imagery, futurist themes, and technological innovation in the African diaspora&rsquo;&rdquo; (9). Nelson&rsquo;s definition was fitting for our literature course that brought together sci-fi, futurist, and technological stories by Octavia Butler, Nnedi Okorafor, Wanuri Kahiu, among others. Moreover, Ytasha Womack&rsquo;s succinct definition of Afrofuturism as &ldquo;an intersection of imagination, technology, the future and liberation&rdquo; from her book  <em>Afrofuturism: The World of Black Sci-Fi and Fantasy Culture</em> , which introduces readers to the paradigm of Afrofuturism, was useful for engaging the works of several artists and the major discourses of Afrofuturism in preparation for the intellectual mixtape assignment.</p>
<p>Teaching Afrofuturism works within the intellectual mixtape assignment was an innovative method of teaching and analyzing literature and composition techniques that shifted the practice of literary and cultural analysis from top-down approaches that privileged the authority of academic or assigned &ldquo;texts&rdquo; to student-centered mixing that encouraged the use of audio, images, and text to create new knowledge. This assignment is within particular traditions of Africana studies and rhetoric and composition, in which students are encouraged and taught to improvise, remix, and sample in order to create distinctive work <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. In particular, the intellectual mixtape assignment also imparts skills such as audio editing, process writing, thematic-website-template building as well as collaboration and performance.</p>
<p>Furthermore, the Afrofuturist intellectual mixtape assignment is what Bryan Carter would consider digital Africana studies and what Kim Gallon refers to as Black digital humanities. Carter&rsquo;s digital Africana studies is an approach to &ldquo;using a number of digital tools to help us experience Africana studies very differently and to help students express their understanding of whatever that course content happens to be very very differently&rdquo; <sup id="fnref1:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. Carter&rsquo;s approach is innovative, technological, and pedagogical yet grounded in the field of Africana studies. Similarly, the Afrofuturist intellectual mixtape assignment is grounded in digital Africana studies (since Afrofuturism is a part of Africana studies), and the technologies taught in the assignment are used to strengthen students&rsquo; understanding of Afrofuturism in unconventional ways. Additionally, the Afrofuturist intellectual mixtape is a black digital humanities assignment, for it &ldquo;reveals how methodological approaches for studying and thinking about the category of blackness may come to bear on and transform the digital processes and tools used to study humanity&rdquo; <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>. In  <em>Afrofuturism to Vibranium and Beyond</em> , the category of blackness becomes the categories of blackness as the students study, critique, and create counter histories, alternate destinies, and posthuman identities through the aesthetics of their mixtape tracks and their interactive Afrofuturist performance, which are featured later in this article.</p>
<p>As the title suggests, this article is a case study of the Afrofuturist intellectual mixtape assignment. It proceeds as follows: we first describe the seven-part layout of the Afrofuturist intellectual mixtape assignment. Next, we provide three key aspects to teaching the intellectual mixtape assignment. Then we discuss additional approaches that buttress our analysis of the students&rsquo; mixtapes in connection to digital humanities and Afrofuturism. Subsequently, we discuss sampling, conversing, and &ldquo;flowing&rdquo; in relation to these approaches and examine three examples of students&rsquo; work. Afterward, we describe the process of preparing for the students&rsquo; midterm performance in which their mixtapes were curated and shared with a public audience. In the final part of the article, we question whether US Copyright Law is antithetical to multimodal digital literacy studies, flow, and empathic engagement with Afrofuturist source texts, and speculate about future uses of the intellectual mixtape.</p>
<h2 id="intellectual-mixtape-assignment-project-description">Intellectual Mixtape Assignment: Project Description</h2>
<p>The intellectual mixtape is an audio-visual-textual assignment with seven-parts. In part one, students listened to recordings (such as lectures, poems, songs, interviews, etc.) that were assigned (on the syllabus) for homework and class discussion. For the second part, students learned the basics of audio editing in order to create their first tracks of the intellectual mixtape. The first audio editing assignment asked students to improvise in order to learn how to edit audio; therefore, from the beginning, Afrofuturist concepts were pedagogically centered. Their track selections and the mixing decisions were, for the most part, impromptu. For the third part, students created the first of three audio tracks, which was 1 min long. All audio tracks included at least three regions of audio: a sample from audio on the syllabus, a region with their own voice in their own words, and a region of their choosing. As a companion to each track, students wrote 500 words of liner notes that included a title for the track and their curation and mixing decisions. For the fourth part, students created a second track that was 1.5 minutes in length. The second track was a collaboration with another student in the course; it also featured a sample from the syllabus, a region with their own voices in their own words, and a region of their choosing with liner notes. The fifth part of the assignment was a third audio track that was between 1.5 to 2 minutes in length, and collaboration was optional; the third track had the same criteria for regions and liner notes. For the sixth part, the students posted their three-track mixtape online (often in a web template service such as Wix) with liner notes and included &ldquo;remixed&rdquo; or original cover art. In part seven, students performed the intellectual mixtape. In seven weeks, the intellectual mixtape assignment taught students skills such as audio editing, process writing, audio-visual synchronizing, thematic-website-template building as well as collaboration and performance.</p>
<h2 id="setting-up-the-assignment">Setting-Up the Assignment</h2>
<p>In order to prepare students to create their mixtapes, we made sure each student had the resources that they needed to succeed. It is important to note that it was not enough for students to have access to the material; it was beneficial that the students engaged with the material in homework assignments and class discussions prior to recording their intellectual mixtapes. When students were exposed to the material prior to creating their intellectual mixtapes, they had additional contexts to build upon. Here, we will highlight three strategies that were instrumental to the success of the assignment–syllabus design, a tech survey, and a workshop on audio-editing. First, curating the syllabus required finding sufficient recordings–preferably MP3 for the intellectual mixtape assignment — and making those recordings available on the syllabus. When there was material essential to the study of Afrofuturism that was not available in an audio or video recording, it was kept on the syllabus and paired with audio dealing with the same subject. For instance, Samuel R. Delany&rsquo;s short story &ldquo;aye, and Gomorrah&rdquo; was course reading, and it was paired with an audio interview with Delany. Furthermore, the syllabus only included recordings that were primary sources in which the Afrofuturist artists, scholars, activists, and/or practitioners spoke about their own work. The videos were selected from platforms such as YouTube and Vimeo and then converted to MP3s. There are two distinctions that determined how the audio/video recordings were featured on the syllabus: videos that were &ldquo;live-action&rdquo; of the Afrofuturist were added to the syllabus directly, but videos that were slideshows of still photos were converted to MP3 and put on the syllabus as MP3s.</p>
<p>Second, we began the course with a tech survey to get an inventory of the students&rsquo; tech needs and to assess their exposure and comfort with the technologies we would use in class.<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>  Our survey had eight questions and was distributed on Survey Monkey a week before classes began.<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>  The survey helped us to determine if students would need loaner equipment and whether they would work with Audacity or GarageBand to learn audio-editing or if we would teach both; it also determined our approach to teaching audio-editing so that the students were comfortable enough with the audio-editing software to complete the assignment.</p>
<p>Third, we taught the students GarageBand or Audacity, which was a learning objective for the intellectual mixtape assignment. Teaching an overview of audio-editing required some preparation before class such as requiring that everyone had headphones and GarageBand or Audacity (with Lame encoder) installed on their computers by the start of class, creating a shared drive that was available with MP3/WAV files for students to select audio recordings, and disseminating written instructions of what we were covering in class while facilitating a live demonstration. We also made a screencast of the demonstration available, so students could watch later. We gave the students fourteen tasks to complete after they opened the program.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000516/resources/images/figure01.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000516/resources/images/figure01_hud557f6a5f1a2f0aba85e28231bf565fb_136469_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000516/resources/images/figure01_hud557f6a5f1a2f0aba85e28231bf565fb_136469_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000516/resources/images/figure01.png 796w" 
     class="portrait"
     ><figcaption>
        <p>Handout given to students that chose to use Audacity.
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000516/resources/images/figure02.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000516/resources/images/figure02_hu50c95fd35e604c1dd552625dee0f187f_108340_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000516/resources/images/figure02_hu50c95fd35e604c1dd552625dee0f187f_108340_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000516/resources/images/figure02.png 796w" 
     class="portrait"
     ><figcaption>
        <p>Handout given to students that chose to use Garageband.
        </p>
    </figcaption>
</figure>
<p>The students&rsquo; execution of the audio-editing practice assignment was fast-paced and improvisational. Because of the time constraints, students worked quickly and performatively in a way that they normally would not. They learned and created their practice mixtape tracks on the fly. As a result, we were not concerned if the practice track made sense, and it was not significant if the track stopped abruptly. Instead, when this assignment was assessed, we checked to see if 1) we had received the assignment as an MP3 or as a WAV file, 2) the assignment was 1 minute in length, 3) the track had three different regions in which one was the student&rsquo;s voice, 4) the track included a fade-in or fade-out at some point in the recording, and 5) the track had an effect (it could be a duplicate sound, reverbs, etc.). This assessment gave us an indication of whether or not the students understood the basics of audio-editing and could build on this knowledge to complete the Afrofuturist intellectual mixtape project.</p>
<p>Curating the syllabus with sufficient and appropriate audio recordings that featured the Afrofuturist artists, scholars, activists, and/or practitioners discussing their work was the substratum of the intellectual mixtape assignment. It was the basis of how the students developed flow, sampled, and engaged in internally persuasive discourse and techno-vernacular creativity. Creating the tech survey allowed us to prepare for our students and guarantee that they had access to equipment and software, as well as ample time to finish assignments. Without this kind of preparation, the course could have been bottle-necked by the technology and diverted focus from the course content. Finally, teaching the basics of audio-editing was one aspect that makes the intellectual mixtape assignment a digital humanities project. The assignment was multimodal and encouraged students to develop their own voices.</p>
<h2 id="a-layered-approach-to-the-intellectual-mixtape">A Layered Approach to the Intellectual Mixtape</h2>
<p>In Brandon T. Locke&rsquo;s article &ldquo;Digital Humanities Pedagogy as Essential Liberal Education: A Framework for Curriculum Development&rdquo; in the  <em>Digital Humanities Quarterly</em>  special issue  <em>Imagining the DH Undergraduate: Special Issue in Undergraduate Education in DH</em> , he provides a framework for digital humanities projects or what he calls digital liberal arts curriculum. He argues that the liberal arts are well suited for the integration of digital skills, given the course goals of liberal arts courses. He writes:</p>
<blockquote>
<p>Educators in the liberal arts must continue to grapple with emerging forms of communication and analysis, or we risk leaving our students lacking in critical areas of the liberal arts. Media and information literacies and multimodal and digital writing skills are essential for effective communication and civic engagement now and in the future, and liberal arts courses must engage with them. This flexible and extensible framework offers one fruitful route, by developing digital humanities projects intended to impart such skills while engaging with domain-specific content. <sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup></p>
</blockquote>
<p>Locke uses the term &ldquo;domain&rdquo; to suggest the discipline of a specialist that is taught to students alongside technical objectives. Locke&rsquo;s framework is useful for understanding the intellectual mixtape assignment. This assignment, as presented in the course  <em>Afrofuturism to Vibranium and Beyond,</em>  engaged the domain-specific content of Afrofuturism alongside media and communication literacies. The intellectual mixtape assignment met the course outcomes for many liberal arts courses through teaching skills such as process writing, collaborating, engaging discourses, articulating one&rsquo;s perspective, text and audio-editing, and managing projects.</p>
<p>Furthermore, Nettrice R. Gaskins&rsquo; essay &ldquo;Afrofuturism on Web 3.0&rdquo; in Reynaldo Anderson and Charles E. Jones&rsquo;s  <em>Afrofuturism 2.0: The Rise of Astro-Blackness</em>  provides another useful framework for understanding the intellectual mixtape assignment. Gaskins&rsquo;s methodology of techno-vernacular creativity is reflected in the intellectual mixtape assignment. In the essay, Gaskins describes techno-vernacular creativity as a creation method consisting of appropriation, improvisation, and reinvention. In terms of appropriation, Gaskins writes that Afrofuturists &ldquo;reclaim cultural artifacts, often to counter dominant social or political systems&rdquo; <sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>. Appropriation of a similar kind was evident in the intellectual mixtape assignment. Students appropriated recordings (sample) from the syllabus and used those recordings to create or center their own world views. Gaskins describes Afrofuturists improvisations as &ldquo;performing, creating, problem-solving, or reacting in the moment and in response to one&rsquo;s environment and inner feelings&rdquo; <sup id="fnref1:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>. In the intellectual mixtape assignment, students improvised their audio-editing demonstration by learning and making creative decisions quickly (as noted above), and they also improvised during their performance in the Cube when they interacted with audience members.<sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>  Lastly, Gaskins describes reinvention (of the self) as techno-vernacular creativity due to the way &ldquo;Afrofuturists often use digital and non-digital avatars as tools for transcendence, reinvention, or for existing in and moving between worlds or realities&rdquo; <sup id="fnref2:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>.</p>
<p>Reinvention (of the self) was adopted by most students creating futuristic content who took on a pseudonym such as Grim Reaper, FutureShe, or StarGirl for the intellectual mixtape assignment. This reinvention of the self is significant because students model this Afrofuturists aesthetic without prompting. The use of appropriation (sampling), improvisation, and reinvention within the intellectual mixtape assignment are just some of the ways that the students&rsquo; mixtapes are created from an Afrofuturists framework. Locke&rsquo;s and Gaskins&rsquo;s frameworks in digital humanities and Afrofuturism, respectively, show how this assignment integrates a digital liberal arts curriculum as well as Afrofuturist techno-vernacular creativity.</p>
<p>Our framing of digital humanities and Afrofuturism in the intellectual mixtape assignment was a dialogic process, specifically an internally persuasive discourse. In  <em>Dialogic Imagination</em> , Mikhail Bakhtin defines internally persuasive discourse as &ldquo;more akin to retelling a text in one&rsquo;s own words, with one&rsquo;s own accents, gestures, modifications&rdquo; <sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup>. Bakhtin&rsquo;s concept claims that one&rsquo;s own word is already interwoven with someone else&rsquo;s words, and this interweaving creates new words. As such, the intellectual mixtape was an application of internally persuasive discourse in that students selected audio from the syllabus and synthesized it with and/or juxtaposed it against their own voice. The students&rsquo; mixed/assimilated the recordings and created their own perspectives. Bakhtin&rsquo;s internally persuasive discourse provides a useful framework for understanding how students developed their own discourses in the intellectual mixtape.</p>
<p>Though varied, Locke&rsquo;s, Gaskins&rsquo;, and Bakhtin&rsquo;s frameworks and methods show the layered approach to teaching and executing the intellectual mixtape assignment. The assignment&rsquo;s goals (audio editing, process writing, composing, collaborating, etc.) are prevalent within the liberal arts and incorporate the use of media technologies. This approach to teaching Afrofuturism encouraged students to engage and create using Afrofuturists methods of appropriation, improvisation, and reinvention of the self. Also, through the dialogic process of creating the intellectual mixtape tracks, students had an opportunity to develop their own ideas in conversation with the assigned Afrofuturists recordings.</p>
<h2 id="flowing-through-conversation-and-sampling">Flowing Through Conversation and Sampling</h2>
<p>In &ldquo;Flow as a Metaphor for Changing Composition Practices,&rdquo; David Green addresses the importance of students developing and expressing authority, fluency, and flow with language through hip hop as a model for composition.<sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup>  Specifically, Green defines flow as &ldquo;a construct that helps to clarify and usefully extend discussions about language, diversity, invention, and voice&rdquo; <sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>. Green&rsquo;s articulation of flow comes from a position that critical writing is not limited to academic writing and that students, in particular, can benefit from critiquing standardized English to develop flexible writing practices that include vernacular in order to engage various audiences, traditions, and histories. Green writes that &ldquo;flow provides an interesting way of positioning writing for students by focusing discussions of language and composing on features such as rhythm, vernacular eloquence, layering, and rupture in ways that press for newer considerations of language and literature within English studies&rdquo; <sup id="fnref1:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>. Green&rsquo;s exploration of how flow can be incorporated in English composition courses was a springboard for the intellectual mixtape project, which adopted Green&rsquo;s concept of flow into audio-visual and written compositions in  <em>Afrofuturism: To Vibranium and Beyond</em> .</p>
<p>Flow as a nonstandard, flexible writing practice is expanded when adopted in text and audio compositions. In creating their intellectual mixtapes, students used the techniques of sampling and conversing to contextualize their discourse and world views and to further develop their own meanings — their flow. Moreover, the students who created intellectual mixtapes were composing within the tradition that DJ Kool Herc, a founder of hip hop music, created when he isolated and repeated the breakbeat on records, which allowed for him to create his own meaning through looping part of the song. His technique allowed for other contexts of the beat (how it would be used–sampling), expanded the conversation of the beat (where the beat would be heard), and developed an expression for future flows to emerge.</p>
<p>In an interview on  <em>Fresh Air</em> , Kool Herc describes finding records to expose to his audience and watching his audience&rsquo;s anticipation of a record&rsquo;s breakbeat. He states:</p>
<blockquote>
<p>When we first heard a record called &ldquo;Seven Minutes of Funk.&rdquo; We heard it in a place called (um) at Hunt&rsquo;s Point. And Jay-Z used it, and a few other people used that same record. And that came out of my collection. And when we played that record, or what we did, Coke [La Rock] did it. Coke put the record on, and we all walked off the stage. <sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup></p>
</blockquote>
<p><a href="resources/audio/audio03.mp3"> Interview with DJ Kool Herc on  <em>Fresh Air</em> . </a></p>
<p>What DJ Kool Herc describes above are contexts and conversations that emerge from the record &ldquo;Seven Minutes of Funk.&rdquo; He explains how each iteration of &ldquo;Seven Minutes of Funk&rdquo; flows with its own meaning and authority–whether it is from Jay-Z&rsquo;s &ldquo;Ain&rsquo;t No Nigga&rdquo; or YG&rsquo;s &ldquo;Why You Always Hatin.&rdquo; Furthermore, though DJ Kool Herc&rsquo;s technique is a founding aesthetic of hip hop music, it is improvisational because Herc and Coke La Rock provide an improvisational conversation between the song and the live audience, and because the sample (appropriation) is reinvented in each iteration by artists such as Jay-Z and YG (which is also a conversation).<sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup>  This same process of sampling and conversing was the basis of the intellectual mixtape assignment, and it is what created flow in David Green&rsquo;s articulation of the term. It was in students&rsquo; flow–their intentions, voice, self-acceptance, and diversity–that emerged in this assignment. We will now examine three tracks for how students&rsquo; flow was expressed.</p>
<p>An intellectual mixtape track &ldquo;Xe3&rdquo; (pronounced Chi) by TSaunds featured conversation and flow. In &ldquo;Xe3,&rdquo; TSaunds sampled &ldquo;Tales of Dr. Funkenstein,&rdquo; &ldquo;Venus Fly&rdquo; by Grimes and Janelle Monáe, and &ldquo;Window Licker&rdquo; by Aphex Twin. TSaunds mixing of these audio recordings led to his creation of a story of a young man being teleported from his apartment to Xe3 (a planet in another universe) through a funky Spotify transmission. In the liner notes, TSaunds wrote:</p>
<blockquote>
<p>I used this beat [a sound effect from &ldquo;Window Licker&rdquo;] because it connects to George Clinton&rsquo;s claim that P-Funk is constantly evolving and always present to those who want the funk and its liberty. In fact, the funk and its vibrations [are] what help the young man be connected with Xe3. Additionally, Grimes is one of my favorite artists and her collaboration with Janelle Monáe on the song and music video is pure genius and sci-fi. Layered with this beat are various clips of audio from  <em>Tales of Dr. Funkenstein</em> . <sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup></p>
</blockquote>
<p><a href="resources/audio/audio04.mp3"> Track &ldquo;Xe3&rdquo; created by student artist TSaunds. </a></p>
<p>TSaunds explained that he is evolving P-funk through his sampling of a sound effect from &ldquo;Window Licker.&rdquo; He used the liberty of P-funk to sample Grimes and Janelle Monáe and further develop a sci-fi sound. He also sampled several excerpts from  <em>Tales of Dr. Funkenstein</em> , a documentary on George Clinton, for the additional context of P-funk. TSaunds flow emerged as the unique story he told of a liberating teleportation, and it was a direct result of the conversations with the tracks he sampled.</p>
<p>There are two Afrofuturist tracks where self-acceptance emerged from flow. The first track is titled &ldquo;FAT GYALS&rdquo; by Lauren Garretson and Starg<em>rl. The students who created this track used humor, irony, and pain to challenge stereotypes and assumptions about black women and their bodies. They presented a series of images for the &ldquo;Fat Girl Starter Pack&rdquo; that include &ldquo;cookies before and after dinner,&rdquo; &ldquo;eating just because I can,&rdquo; &ldquo;great singer — must be gospel, must be soul, must be pain,&rdquo; &ldquo;can I do anything for more than 10 minutes?,&rdquo; and &ldquo;arms that won&rsquo;t stretch all the way around.&rdquo; Lauren Garretson and Starg</em>rl strengthened their conversation by speaking at the same time, repeating and/or responding to each other. They also affirmed all body types through the track&rsquo;s intro that featured an audio excerpt from the movie  <em>Phat Girlz</em>  in which the character Jazmin Biltmore (Mo&rsquo;Nique) and her friend Stacey are ordering food at a fast-food restaurant. In the film, Stacey proceeds to order for Jazmin (on the skinny side of the menu), but Jazmin modifies the order and adds &ldquo;the works.&rdquo; Additionally, Lauren Garretson and Starg<em>rl aligned themselves and their images with G/god when they sampled Erykah Badu&rsquo;s lyrics from &ldquo;On and On&rdquo; that &ldquo;If we were made in his image then call us by our names/Most intellects do not believe in god, but they fear us just the same.&rdquo; In their liner notes, Lauren Garretson and Starg</em>rl wrote, &ldquo;all our complexities around body and food and womanhood will not shut us out from a new world, planet, mothership connection or community of aliens.&rdquo; The track &ldquo;FAT GYALS&rdquo; was an Afrofuturist audio conversation about presence, inclusion, and acceptance. <a href="resources/audio/audio05.mp3"> Track &ldquo;FAT GYALS&rdquo; created by student artists Starg*rl and Garretson. </a></p>
<p>Flow was also demonstrated through sonic, rhetorical layering in the track &ldquo;Space Less&rdquo; by FutureShe. FutureShe began her track with a personal understanding of space and place that was framed by one of the most well-known Afrofuturist choruses, &ldquo;space is the place&rdquo; from Sun Ra&rsquo;s song, self-titled album, and film  <em>Space is the Place</em> . While a context of Afrofuturist acceptance is suggested through the canonized song, FutureShe&rsquo;s flow presented otherness and doubt regarding the possibility of experiencing a utopic space outside of herself. She began the track stating, &ldquo;Will there be space for me — for all of me?&rdquo; Her voice also suggested that she must find her own space since elsewhere is inhabited by others. FutureShe then mixed FKA Twigs&rsquo;s &ldquo;How&rsquo;s That&rdquo; into her track. FKA Twigs&rsquo;s recording became a companion for her, initiating her to turn within. In her liner notes, FutureShe wrote, &ldquo;I attempt to exemplify the sort of internal struggles and frustrations with space here on Earth that may eventually lead people to seek space, literally and figuratively, elsewhere.&rdquo; FutureShe&rsquo;s track was a flow that used samples to produce a perspective that the only space where we can truly be accepted is within ourselves. <a href="resources/audio/audio06.mp3">Track  <em>space is the place</em>  created by student artist FutureShe. </a></p>
<p>The creation and performance of the intellectual mixtape provided opportunities for students to engage in popular, academic, and mystical discourses connected to Afrofuturists art and practices. The recordings that the students&rsquo; sampled and mixed provided them with digital media skills used in the digital humanities. The process of audio editing also expanded how the students engaged with language, meaning, and interpretation in an English special topics course. This type of engagement permitted an internally persuasive discourse to be expressed–one in which the students&rsquo; own ideas were accented from the words of others. Through their sampling and conversing with the assigned recordings, the intellectual mixtape assignment opened up opportunities for students to flow and express what mattered to them in terms of sound, perspective, acceptance, and diversity. The students&rsquo; insights about Afrofuturism gained through the intellectual mixtape assignment became an interactive experience of Afrofuturism with empathy at the center of the creation.</p>
<h2 id="performing-the-intellectual-mixtape">Performing the Intellectual Mixtape</h2>
<p>The students also adapted their intellectual mixtapes to a performance environment. This supported our Afrofuturist pedagogical framework for two reasons. First, this assignment requires students to engage in a new multimodal form of performance. This is in line with Locke&rsquo;s vision of a multimodal future for liberal arts education. Second, the assignment requires students to reinvent and improvise as they adapt their mixtapes to a performance environment. While the Afrofuturist performance was on the syllabus and the course schedule, students were not instructed to create their mixtapes with their performance in mind. They had to rethink how to perform their mixtapes after they had already been created. Gaskin argues that improvisation and reinvention are an integral part of Afrofuturist artistic practice. In this way, the performance part of the assignment is also Afrofuturist.</p>
<p>Performing the intellectual mixtape was predominately student-led and took 2-3 weeks. We asked thought-provoking questions and created an environment for students to apply Afrofuturists concepts, practices, and discourses to create their performance.  <em>Sound of Space: An Interactive Afrofuturist Experience</em>  was performed in the Cube, a 42ft high performance and research space in the Moss Arts Center at Virginia Polytechnic Institute and State University. The Cube has a high-density loudspeaker array of 150 loudspeakers. These loudspeakers spread around the entire perimeter, up the walls, and on the ceiling of the Cube to allow for fully immersive sound. The Cube is specially designed for spatial computer music research. Vector Based Amplitude Panning is used so that audience members can precisely pinpoint a sound source from any point in the room <sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup>. The Cube can fit up to 198 people and features the Cyclorama, an optional large 360-degree-panoramic projection wall in the center.</p>
<p>In preparation for the midterm intellectual mixtape performance, we and staff members of the Institute for Creativity, Arts and Technology led discussions with the students about ways to adapt the mixtapes to the space, and we required that they apply their knowledge of Afrofuturism to create the performance. Students decided to create an environment in which the audience could freely navigate the space rather than having a fixed separation between the audience and the stage. This holds roots in an Afrofuturist ideal that the demarcation between artist and audience member results in an imbalanced power relationship that should be actively worked against <sup id="fnref3:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>. Students felt that this would allow the audience to fully embody the space in the spirit of the mixtape creation. They also argued that being able to float freely between different corners of the Cube would allow for improvisational movement that would reflect the Afrofuturist desire to find freedom in space. Students decided to make the Cyclorama the centerpiece of the exhibit. They projected an oscillating image of space and played ambient white noise (similar to that heard on an airplane or spaceship) from overhead. The Cyclorama, filled with the sound of space, served as the meditative home base from which the movement toward the Afrofuturist spatialized sounds took place.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000516/resources/images/figure07.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000516/resources/images/figure07_hu1153642feee713215e66efff58826850_514849_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/15/1/000516/resources/images/figure07_hu1153642feee713215e66efff58826850_514849_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/15/1/000516/resources/images/figure07_hu1153642feee713215e66efff58826850_514849_1200x0_resize_q75_box.jpg 1200w,/dhqwords/vol/15/1/000516/resources/images/figure07_hu1153642feee713215e66efff58826850_514849_1500x0_resize_q75_box.jpg 1500w,/dhqwords/vol/15/1/000516/resources/images/figure07_hu1153642feee713215e66efff58826850_514849_1800x0_resize_q75_box.jpg 1800w,/dhqwords/vol/15/1/000516/resources/images/figure07.jpg 5568w" 
     class="landscape"
     ><figcaption>
        <p>Students preparing in the Cyclorama.
        </p>
    </figcaption>
</figure>
<p>The students then grouped the mixtapes into four categories that were played on a loop during the performance: Flow, Transport, Testimony, and Funk. The students felt that each of these categories was representative not only of their mixtapes as a whole but also of different themes within Afrofuturist discourse. &ldquo;Flow&rdquo; focused on the rhythm and transformation of the self, as in FutureShe&rsquo;s &ldquo;Space Less,&rdquo; in which she explores how to carve out a space for herself as a black body on Earth. &ldquo;Transport&rdquo; focused on the African diaspora, Sankofa (a word in the Twi language and a Ghanian Adinkra symbol meaning &ldquo;go back and get it,&rdquo; but also the title of a Haile Gerima film about abduction in the transatlantic slave trade), and alien abduction in science-fiction narratives (as in the film  <em>Space is the Place</em> , in which Sun Ra is abducted by a group of white scientists working for NASA who hope to uncover how he travels through space). In &ldquo;Testimony&rdquo; students expressed, confessed, and preached their experiences with various forms of oppression, which reflects Sun Ra&rsquo;s testimony of his experience in  <em>Space is the Place</em> . In &ldquo;Funk,&rdquo; students focused on the Afrofuturist connection between creator and consumer, in which a liberating communal dance welcomes all (as is the case with Parliament Funkadelic&rsquo;s &ldquo;We Got the Funk&rdquo;).</p>
<p>Once the students&rsquo; tracks were sorted into groups, each group was mixed, sequenced, and spliced into four singular looping audio tracks. Students utilized the Cube&rsquo;s unique ability to spatialize audio to have each of the tracks played from a different corner simultaneously. As audience members walked around the perimeter of the Cube, the sound of the next corner would slowly grow louder and the previous corner quieter until audience members were fully immersed in the sound of the next group. In this way, they were &ldquo;transported&rdquo; between each audio grouping–transportation, funk, flow, and testimony–spatially. This is reflective of Sun Ra&rsquo;s idea that creation is a form of teleportation as it is in  <em>Space is the Place</em> . This slow gradient of mixed audio recordings was part of the students&rsquo; design as well; the hope was that this blended and improvisational sound would reflect the sampling and mixing of the mixtapes. This balance had to be carefully checked by the GTA to make sure the sounds did not overwhelm and compete with one another.</p>
<p>In addition to designing the space itself, students had to conceive of a title for the performance. We brainstormed the title for the performance over multiple meetings. Students played word association games and constructed &ldquo;word salad&rdquo; in order to think generatively about the language and spaces associated with Afrofuturism. As they continued to conceive of the title in relation to the space, they eventually arrived at &ldquo;Sound of Space&rdquo; to evoke the meditative &ldquo;spaceship&rdquo; sound played overhead in the Cyclorama.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000516/resources/images/figure08.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000516/resources/images/figure08_hua758f598905cb39ec8f40953af5aafa7_505688_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/15/1/000516/resources/images/figure08_hua758f598905cb39ec8f40953af5aafa7_505688_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/15/1/000516/resources/images/figure08_hua758f598905cb39ec8f40953af5aafa7_505688_1200x0_resize_q75_box.jpg 1200w,/dhqwords/vol/15/1/000516/resources/images/figure08_hua758f598905cb39ec8f40953af5aafa7_505688_1500x0_resize_q75_box.jpg 1500w,/dhqwords/vol/15/1/000516/resources/images/figure08_hua758f598905cb39ec8f40953af5aafa7_505688_1800x0_resize_q75_box.jpg 1800w,/dhqwords/vol/15/1/000516/resources/images/figure08.jpg 2550w" 
     class="portrait"
     ><figcaption>
        <p>Poster for Sound of Space performance.
        </p>
    </figcaption>
</figure>
<p>Though sound is the substratum of the  <em>Sound of Space</em>  performance, students wanted to communicate meaning and feeling through more than sound and space imagery in the Cyclorama. The students understood empathic engagement to be a core tenet to the creation of Afrofuturist art <sup id="fnref:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup>. They added props and visual aids to each of the four corners of the Cube as a means of increasing empathic engagement with the audience. While 360-degree-virtual reality has been correlated to positive empathetic response, it was our hope that the 360-degree-panoramic projection wall of the Cyclorama and the interactive-immersive experience in the Cube would foster empathy and embodiment among audience members <sup id="fnref:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup>. To produce this, each person was asked to beat a Garifuna Drum upon entering the Cube.</p>
<p>Additionally, for the &ldquo;Transportation&rdquo; corner, a montage of alien imagery and footage from the  <em>Blair Witch Project</em>  was created through the contribution of an undergraduate videographer. This footage allowed the audience to experience first-hand the jarring nature of alien and colonialist abduction. This footage was played on a loop. Also, the &ldquo;Funk&rdquo; corner also contained a looped montage but of various funk, dance, and Motown musicians, including Michael Jackson and Parliament Funkadelic. This footage was selected by the students, who found videos on Youtube they felt were aesthetically cohesive and sent them to the videographer. These images were meant to celebrate Afrofuturism as a form of embodied, innovative, eclectic, and physical expression.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000516/resources/images/figure09.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000516/resources/images/figure09_hu1153642feee713215e66efff58826850_508754_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/15/1/000516/resources/images/figure09_hu1153642feee713215e66efff58826850_508754_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/15/1/000516/resources/images/figure09_hu1153642feee713215e66efff58826850_508754_1200x0_resize_q75_box.jpg 1200w,/dhqwords/vol/15/1/000516/resources/images/figure09_hu1153642feee713215e66efff58826850_508754_1500x0_resize_q75_box.jpg 1500w,/dhqwords/vol/15/1/000516/resources/images/figure09_hu1153642feee713215e66efff58826850_508754_1800x0_resize_q75_box.jpg 1800w,/dhqwords/vol/15/1/000516/resources/images/figure09.jpg 5568w" 
     class="landscape"
     ><figcaption>
        <p>Transportation Corner of the performance.
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000516/resources/images/figure10.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000516/resources/images/figure10_hu87632fe81efbafa2e401026bbf39454c_531497_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/15/1/000516/resources/images/figure10_hu87632fe81efbafa2e401026bbf39454c_531497_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/15/1/000516/resources/images/figure10_hu87632fe81efbafa2e401026bbf39454c_531497_1200x0_resize_q75_box.jpg 1200w,/dhqwords/vol/15/1/000516/resources/images/figure10_hu87632fe81efbafa2e401026bbf39454c_531497_1500x0_resize_q75_box.jpg 1500w,/dhqwords/vol/15/1/000516/resources/images/figure10_hu87632fe81efbafa2e401026bbf39454c_531497_1800x0_resize_q75_box.jpg 1800w,/dhqwords/vol/15/1/000516/resources/images/figure10.jpg 5568w" 
     class="landscape"
     ><figcaption>
        <p>Funk Corner of the performance.
        </p>
    </figcaption>
</figure>
<p>Next, the &ldquo;Flow&rdquo; corner featured an aromatherapy diffuser with a lavender scent. This corner encouraged audience members to close their eyes and tune into their other senses, rather than fixating on the strict visual presence before them. One of the student tracks in this corner asks, &ldquo;will I be able to find the space within me?&rdquo; In this spirit, this corner encouraged audience members to look inward to find a sense of space. Lastly, the &ldquo;Testimony&rdquo; corner featured a variety of artifacts and instruments from African cultures, which encouraged audience members to participate in the cathartic cleansing of testifying. We adorned a table with a traditional African mud cloth, a bowl (symbolically full of water for cleansing hands and ears), candles, a tambourine, precious stones, an African statue, and an African shield. The students felt that these objects would help bring the audience aesthetically closer to the testimonies in this corner&rsquo;s mixtape tracks, and that &ldquo;the water&rdquo; would help the audience embody the feeling of cleansing.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000516/resources/images/figure11.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000516/resources/images/figure11_hu1153642feee713215e66efff58826850_723186_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/15/1/000516/resources/images/figure11_hu1153642feee713215e66efff58826850_723186_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/15/1/000516/resources/images/figure11_hu1153642feee713215e66efff58826850_723186_1200x0_resize_q75_box.jpg 1200w,/dhqwords/vol/15/1/000516/resources/images/figure11_hu1153642feee713215e66efff58826850_723186_1500x0_resize_q75_box.jpg 1500w,/dhqwords/vol/15/1/000516/resources/images/figure11_hu1153642feee713215e66efff58826850_723186_1800x0_resize_q75_box.jpg 1800w,/dhqwords/vol/15/1/000516/resources/images/figure11.jpg 5568w" 
     class="landscape"
     ><figcaption>
        <p>Flow Corner of the performance.
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000516/resources/images/figure12.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000516/resources/images/figure12_hu1153642feee713215e66efff58826850_493444_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/15/1/000516/resources/images/figure12_hu1153642feee713215e66efff58826850_493444_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/15/1/000516/resources/images/figure12_hu1153642feee713215e66efff58826850_493444_1200x0_resize_q75_box.jpg 1200w,/dhqwords/vol/15/1/000516/resources/images/figure12_hu1153642feee713215e66efff58826850_493444_1500x0_resize_q75_box.jpg 1500w,/dhqwords/vol/15/1/000516/resources/images/figure12_hu1153642feee713215e66efff58826850_493444_1800x0_resize_q75_box.jpg 1800w,/dhqwords/vol/15/1/000516/resources/images/figure12.jpg 5568w" 
     class="landscape"
     ><figcaption>
        <p>Testimony Corner of the performance.
        </p>
    </figcaption>
</figure>
<p>The midterm intellectual mixtape performance took place on March 7th, 2019, in the middle of the afternoon. Members from both the Virginia Tech and the Blacksburg community flowed through the Cube to see the performance. Midway through the hour and fifteen-minute performance, four students from the course  <em>Improvised and Devised Performance</em>  taught by Devair Jeffries and Al Evangelista gave two ten-minute performances in the Cyclorama, which responded to and evolved from the sounds in the space and the audience in the Cube. As intended, many audience members spent significant time seated in the center of the Cyclorama, examining the moving stars. From there, they charted their own courses through the Cube, taking pictures and exchanging experiences. Students in our Afrofuturism course floated through the space to provide guidance and answer questions as needed.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000516/resources/images/figure13.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000516/resources/images/figure13_hu0c1618764907b87f67adeecede502f88_15291_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/15/1/000516/resources/images/figure13_hu0c1618764907b87f67adeecede502f88_15291_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/15/1/000516/resources/images/figure13.jpg 308w" 
     class="landscape"
     ><figcaption>
        <p>Inside of the Cyclorama during the performance.
        </p>
    </figcaption>
</figure>
<p>The  <em>Sound of Space</em>  performance serves as a useful model for exploring embodiment through sound and performance in the digital humanities classroom. Encouraging students to think multimodally and asking them to reinvent and improvise with their own work supports an Afrofuturist pedagogy rooted in the theories of Gaskin and Locke. It also supports a broader digital humanities pedagogy. By asking students to rethink their work in a performance setting, this assignment asks students to form a more intimate and embodied relationship with the creative works studied. Rather than passively quoting from these works, students are asked to envision how these works can be presented so that they will be engaging to a live audience. In doing so, they behave as Afrofuturist creators, many of whom are live performers. Much of this process can be replicated in any digital humanities course in which instructors want students to embody the perspectives of the creators and artists being studied, particularly those courses in which the syllabus contains performance art.</p>
<h2 id="the-challenges-of-copyright-law">The Challenges of Copyright Law</h2>
<p>Given the importance of access to audio, we want to specifically address one major challenge and opportunity for the assignment. During the  <em>Sound of Space</em>  performance, community members from the Virginia Tech and Blacksburg area engaged in conversation with each other, members of the Afrofuturism class, and Afrofuturist art. The result was not a performance in which audience members and artists were sharply demarcated, but rather one in which both parties freely exchanged ideas on a level playing field. This holds its roots in Gaskins&rsquo;s idea that in Afrofuturism, &ldquo;improvisation, call and response, hacking, and tinkering elicit the active engagement and participation of the at-large community (audience)&rdquo; <sup id="fnref4:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>. Similarly, the intellectual mixtape was created as a means of letting students engage in deep conversation with the works of Afrofuturist artists, rather than analyzing Afrofuturist work as critics or passive third-party observers/reviewers. In both cases, the goal was to synthesize and/or juxtapose the interpreter&rsquo;s (student&rsquo;s or audience member&rsquo;s) own artistic voice with that of the artist. However, this collage-style-artistic practice is discouraged by US Copyright Law. Creating the intellectual mixtape may even constitute copyright infringement.</p>
<p>Under the US Code of Laws, a &ldquo;derivative work&rdquo; such as the intellectual mixtape requires a &ldquo;master use&rdquo; license, which can only be obtained by mutual agreement between the owner of the recording and the licensee [17 US Code, § 106]. In the case of the intellectual mixtape, the owners of the recordings were mostly large record labels too difficult to contact in such a short time frame.<sup id="fnref:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup>  The legal penalties for not obtaining proper licenses are unduly severe. Statutory Damages for each case of infringement can be up to $30,000 [17 US Code, § 504c]. This means that in this Afrofuturism course, damages could total $900,000.</p>
<p>US Copyright Law discourages the embodiment of Afrofuturist tracks despite the demonstrated pedagogical benefits. As such, we are left wondering if US Copyright Law ought to undergo revision. US Copyright law privileges research on music over a century old and many films prior to 1964, for which the copyright has already expired.<sup id="fnref:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup>  Music of this age predates most original recorded music, and in particular, all of the audio content that was included on the syllabus was created within the last 60 years.</p>
<p>Furthermore, by limiting the appropriate sample length to 10% of a song, US Copyright Law discourages a serious embodiment of any musical recording, which is a fundamental pillar of the Afrofuturist mixtape.<sup id="fnref:20"><a href="#fn:20" class="footnote-ref" role="doc-noteref">20</a></sup>  In our class, many students had samples that were longer than 10% of a song. This was because students were encouraged to converse with the discourses of Afrofuturism and use them in their own creative process. In the spirit of Bakhtin&rsquo;s internally persuasive discourse, we want to encourage academic work that is &ldquo;affirmed through assimilation, tightly interwoven with &lsquo;one&rsquo;s own word&hellip;&rsquo; half-ours and half-someone else&rsquo;s&rdquo; <sup id="fnref1:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup>. We hope for our students to engage with the material in a way which forces them to converse with and think carefully through the discourses of Afrofuturist creators so that they become creators themselves, rather than limiting themselves to short quotations that could subordinate or elevate their perspectives. In contrast, we argue for the Afrofuturist techno-vernacular creativity identified by Nettrice Gaskins, in which the reappropriation of cultural artifacts is part of &ldquo;counter-dominant social or political systems&rdquo; <sup id="fnref5:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>. We feel this reappropriation should be uninhibited by legal regulation and encourage our students to join the continuum of borrowing and remixing that is so fundamental to Afrofuturist work.</p>
<p>Additionally, in his 2010 Langston Hughes Visiting Professorship Lecture at the University of Kansas, Professor Adam Banks asks:</p>
<blockquote>
<p>Will we stand with a set of copyright and intellectual property codes, laws and conventions, that have pushed more and more severely in the direction of huge corporate interests&hellip;?</p>
</blockquote>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://player.vimeo.com/video/508958639" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="Professor Adam Banks in his Langston Hughes Visiting Professorship Lecture at the University of Kansas." webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div>
<p>He also notes that every course syllabus forms &ldquo;a mixtape compilation of other&rsquo;s text and ideas compiled, ranging, combines, with our own various critical gestures&hellip;&rdquo; <sup id="fnref:21"><a href="#fn:21" class="footnote-ref" role="doc-noteref">21</a></sup>. As such, it seems antithetical to academic practice to have one form of compilation be suppressed while others are allowed to flourish. Why must we limit ourselves to short quotations of these audio text sources when we know full well that academic discourse is a constant resynthesis of existing ideas? Why should the intellectual mixtape be forced to conform to the narrative that sources of knowledge can be easily traced and attributed, when academic discourse often involves a recombination of internally persuasive ideas? We argue instead that in order for the digital humanities to more fully realize an Afrofuturist praxis, copyright law should be challenged just as it has been for decades by practitioners of Afrofuturist art.</p>
<h2 id="conclusion">Conclusion</h2>
<p>The various pedagogical methods here could be realized in any number of digital humanities classrooms, not just in the context of an Afrofuturism course.<sup id="fnref:22"><a href="#fn:22" class="footnote-ref" role="doc-noteref">22</a></sup>  The ideas of remix, flow, and embodied knowledge are increasingly becoming a part of academic and artistic creation today. As such, it makes sense for other courses within liberal arts and social sciences, particularly those that have a substantial corpus of audio materials, to use the intellectual mixtape as a new form of academic discourse. The intellectual mixtape does not necessarily have to be a replacement for traditional academic discourses. It could be used in conjunction with traditional academic writing as a means of getting the student to engage with primary sources in as many ways as possible or as a way of prompting a student to engage empathically with a text before they use it to support their own theories.</p>
<p>While replicating the performance in the Cube is near impossible because of the unique nature of the space, holding a reception for student art-scholarship is very compelling and easy to execute in a digital humanities classroom. In inviting audience members into the classroom, we underscore the importance of having the intellectual mixtapes be a part of a large-networked conversation, rather than a unidirectional discourse. It is imperative that this work is viewed through the lens of community, and not just as an artifact of the academic system. As Brandon Locke notes, &ldquo;Media and information literacies and multimodal and digital writing skills are essential for effective communication and civic engagement now and in the future, and liberal arts courses must engage with them&rdquo; <sup id="fnref1:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>. Any digital humanities classroom could host a reception of any kind as part of the midterm or final. In the case of the intellectual mixtape, community members could gather together to eat food, explore the student work that has been created, and discuss their own reactions to the student work with each other, the teacher, or the students themselves.</p>
<p>The intellectual mixtape assignment is an approach to teaching in liberal arts and digital humanities that promotes multimodal scholarship and artistic creation. The seven-part assignment teaches students skills such as audio-editing, process writing, and performing. Though students are asked to make their mixtape tracks in conversation with the recordings they sample, students are also asked to be authorities on their own tracks. The assignment thus emphasizes the importance of students developing their own voice and developing their own sound in order to develop flow. The conversations they create through these interwoven audio recordings constitute internally persuasive discourse and promote empathic engagement both in the intellectual mixtape assignment and in the  <em>Sound of Space</em>  performance.</p>
<p>The mixtapes and performance were Afrofuturist appropriations and improvisational conversations in which, without being prompted, all students took on other identities in order to express themselves. While this approach to learning and scholarship is primed for delivery in liberal arts and digital humanities courses, Copyright Law in the United States often inhibits legal use of the intellectual mixtape assignment. It is especially dubious when working with contemporary works, like those found in the body of Afrofuturist work. Even so, the intellectual mixtape is a viable assignment that encourages flow and expression of their individual world views. A digital humanities pedagogy with Afrofuturist intellectual mixtapes is one more step towards a more equitable and engaging future for liberal arts education and the digital humanities.</p>
<p>Fair Use, as defined in the Copyright Act of 1976, is a law which permits brief excerpts of copyrighted material to be used under certain circumstances, including when it is for education, scholarship, or research [17 US Code, § 106]. Given this provision, one might hope that the intellectual mixtape would be considered Fair Use. While the US Code provides no strict definition for determining Fair Use, this does not appear to be the case [17 US Code, § 106].</p>
<p>The US Code of Law provides guidelines for determining Fair Use in lieu of a strict definition [17 US Code, § 106]. Whether or not a particular case constitutes Fair Use is left up to the courts to determine. Therefore, any educator can be taken to court for copyright infringement, even if their case constitutes Fair Use. The US Copyright Office attempts to provide some guidance for these court decisions <sup id="fnref:23"><a href="#fn:23" class="footnote-ref" role="doc-noteref">23</a></sup>. However, it is not clear how these guidelines apply, because they were developed by a committee of Music Publishers and Music Educators in 1976 for the music classroom, not for digital humanities classrooms. One provision states that: &ldquo;For other than performance, single or multiple copies of excerpts may be made, provided that the excerpts do not comprise a &lsquo;performable unit as a section,&rsquo; and never more than 10% of the work, and only one copy per pupil.&rdquo; This suggests that the intellectual mixtape may be legal so long as only 10% of each song is used. However, for a typical three-minute song, this is only about 18 seconds.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Anderson, Reynaldo and Charles E. Jones.  <em>In Afrofuturism 2.0: The Rise of Astroblackness</em> . Lanham, MA: Lexington Books, 2016.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Carter, Bryan.  “Bryan Carter: Using Technology to Engage Students.”  <em>CreativeMornings HQ</em> , YouTube, 11 Feb. 2019&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Gallon, Kim.  “Making a Case for the Black Digital Humanities.”  <em>Debates in the Digital Humanities 2016</em> . Minneapolis: U of Minnesota P, 2016.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Tyechia Thompson first used a tech survey in her courses in 2016 at the recommendation of Bryan Carter, Ph.D., Director of the Center for Digital Humanities, which was the first year she taught the intellectual mixtape assignment in its current iteration.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p><a href="https://www.surveymonkey.com/r/ZB7KSPS">https://www.surveymonkey.com/r/ZB7KSPS</a>&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>Locke, Brandon.  “Digital Humanities Pedagogy as Essential Liberal Education: A Framework for Curriculum Development.”  <em>Digital Humanities Quarterly</em> . Vol 11, Num 3. 2018.&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>Gaskins, Nettrice R.  “Afrofuturism on Web 3.0: Vernacular Cartography and Augmented Space <em>.</em> In Afrofuturism 2.0: The Rise of Astroblackness” . Lanham, MA: Lexington Books, 2016.&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref3:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref4:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref5:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>How we taught audio-editing and how the students created the interactive Afrofuturist experience in the Cube at Virginia Tech will be described in more detail later in this essay.&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>Bakhtin, Mikhail. Mikhailovich.  <em>Dialogic imagination: Four essays by M. M. Bakhtin</em>  (Caryl Emerson &amp; Michael Holquist, Trans.). Austin, TX: University of Texas Press. 1991.&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>Tyechia Thompson sampled the intellectual mixtape Assignment from David Green who had assigned the intellectual mixtape as a flexible type of annotated bibliography to his graduate students at Howard University.&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>Green, David.  “Flow as a Metaphor for Changing Composition Practices.”  <em>Changing English: Studies in Culture and Education</em> . <a href="https://www.tandfonline.com/toc/ccen20/24/2">Issue 2: Special Issue: Straight Outta English.</a> Volume 24, 2017 - 28 Jul 2017. 175-185.&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p>DJ Kool Herc.  “DJ Kool Herc and the Birth of the Breakbeat.”  <em>NPR</em> , NPR, 29 Aug. 2005.&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13">
<p>It is important to note that the intellectual mixtape assignment is clearly a part of the jazz tradition, particularly through the aesthetics of improvisation and conversation. This tradition is examined in the course, especially through the music and teachings of Sun Ra. Also, DJ Kool Herc&rsquo;s example of sampling &ldquo;Seven Minutes of Funk&rdquo; is a nod to Afrofuturism through the funk genre&rsquo;s connection to black freedom, utopianism, and the space age.&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14">
<p>TSaunds.  “Xe3.”  <em>Plastic</em> . Afrofuturism to Vibranium and Beyond, 2019.&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:15">
<p>Lyon, Eric, Ico Bukvic, and Caulkins, Terence.  “Genesis of the Cube: The Design and Deployment of an HDLA-Based Peformance and Research Facility.”  <em>Computer Music Journal</em> . 40(4):62-78 · December 2016.&#160;<a href="#fnref:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:16">
<p>Hinton, Anna.  “Bodyminds Reimagined: (Dis)ability, Race, and Gender in Black Women&rsquo;s Speculative Fiction.”  <em>ASAP Journal</em> . August 2018.&#160;<a href="#fnref:16" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:17">
<p>Bertrand, Philippe, Jérôme Guegan, Léonore Robieux, Cade Andrew McCall and Franck Zenasni.  “Learning Empathy Through Virtual Reality: Multiple Strategies for Training Empathy-Related Abilities Using Body Ownership Illusions in Embodied Virtual Reality”  <em>Front. Robot. AI</em> , 22 March 2018.&#160;<a href="#fnref:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:18">
<p>There are two ways of avoiding copyright infringement in creating an Intellectual Mixtape, but all pose problems: 1) Using music for which the copyright has deliberately been waived by the artist; 2) Negotiating master use licenses with independent artists. Both options provide little help because music of these categories generally come from small independent artists who are rarely popular enough to have had a large cultural impact. If students performed a study using only these songs, they would be prevented from exploring the core and popular texts of Afrofuturism.&#160;<a href="#fnref:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:19">
<p>As of Donald Trump&rsquo;s 2018 signing of the Music Modernization Act, music copyright generally lasts a century. However, a significant number of Afrofuturist music was published within the last half a century. This means that most significant Afrofuturist works are still protected under US Copyright Law.&#160;<a href="#fnref:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:20">
&#160;<a href="#fnref:20" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></li>
<li id="fn:21">
<p>Banks, Adam.  “Adam Banks - 2010 Langston Hughes Visiting Professor.”  <em>The University of Kansas</em> , YouTube, 26 Apr. 2010.&#160;<a href="#fnref:21" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:22">
<p>Tyechia Thompson has used the intellectual mixtape assignment in five different courses (including an online course) at three different universities.&#160;<a href="#fnref:22" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:23">
<p><em>United States Copyright Circular 21</em> :  <em>Reproduction of Copyrighted Works By Educators and Librarians</em> . US Copyright Office, August 2016.&#160;<a href="#fnref:23" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry><entry><title type="html">Annotating our Environs with the Sound and Sight of Numbers: The DataScapes Project</title><link href="https://rlskoeser.github.io/dhqwords/vol/15/1/000505/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://rlskoeser.github.io/dhqwords/vol/15/1/000505/</id><author><name>John Bonnett</name></author><author><name>Joe Bolton</name></author><author><name>William Ralph</name></author><author><name>Amy Legault</name></author><author><name>Erin MacAfee</name></author><author><name>Michael Winter</name></author><author><name>Chris Jaques</name></author><author><name>Mark Anderson</name></author><published>2021-03-05T00:00:00+00:00</published><updated>2021-03-05T00:00:00+00:00</updated><content type="html"><![CDATA[<h2 id="our-aim-a-new-form-of-landscape-architecture">Our Aim: A new form of landscape architecture</h2>
<p>There is a singular privilege that accompanies living in this first century of digital computation: the opportunity to discern what it means. The task is nowhere near complete. We know, or think we know, that computation is having a bearing on our capacities to locate pattern, create and replicate pattern, and disseminate pattern. There are even some who believe its advent will ultimately impinge on the trajectories of natural and human history. Welcome to the Noosphere <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup><sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> . The ambition of this exercise in meaning-making is relatively more modest. We seek to explore how computation can be used to support the human penchant to adorn one&rsquo;s surrounds. More specifically, our purpose here is to explore how Augmented Reality (AR) Objects can be used as constituents for Landscape Architecture. In the aftermath of Ivan Sutherland&rsquo;s foundational work in the late 1960s establishing the field of computer graphics, the prospect that users might seek to integrate computer-generated form with their surrounds became a plausible one, one realized in the decades since via the medium of Augmented, and now Mixed, Reality <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>. In the ensuing decades, AR has emerged as an actual or potential support for multiple fields, including education, construction, engineering, computer gaming and firefighting. In this contribution, we present the efforts of  <em>The DataScapes Project</em>  to explore how AR&rsquo;s capacity to situate and register digital content might be leveraged for artistic purposes: Augmented Reality content, combined with constituents from Data Art, can be leveraged as raw materials for Landscape Architecture.</p>
<h2 id="previous-work">Previous Work</h2>
<p>The first challenge faced by members of the project was the fundamental one of determining how to proceed. We were presented with a blank canvass and had to select the location and method that, so to speak, would constitute our &ldquo;paint.&rdquo; Choosing a location proved to be the more straightforward of the two decisions. We were looking for a readily accessible space at one of our two universities, a locale that at once presented a sense of grand scale and a sense of enclosure. We were looking for a tract of land that would serve as a frame and a complement for our AR artwork. Based on that criteria, we selected the traffic circle at Brock University in St. Catharines, Ontario Canada. The circle is 140 metres in diameter, is centrally located on campus, and until recently was bounded by willow trees.</p>
<p>Selecting the method and content that would constitute the matter of our creation, by contrast, proved more difficult. To be sure, there were multiple domains of art that anticipated – in part – what we were seeking to do. The fields of Landscape Art, Earthworks, and Digital Art, through their creation of anamorph and  <em>trompe l&rsquo;oeil</em>  artworks, for example, were suggestive of what an AR-based Landscape Architecture might accomplish. But the three fields also differed significantly in their practice from what we were aiming to accomplish. To start, while Landscape Art and Earthworks both make location a central constituent of artistic production, artists in both fields sculpt material objects – constituents from a local landscape – to generate artistic form, not digital content <sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>  <sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>. In the domain of Digital Art, artists such as Joe Crossley have sought to conflate the digital with the material by transforming static empty spaces – generally building surfaces – into multi-media surfaces using projection mapping. The content that is disseminated, however, is 2D, not 3D <sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>.</p>
<p>Landscape Architecture, the primary field to which we sought to contribute, also did not present an obvious theoretical frame of reference, method or aesthetic to assist our efforts. One dimension of the problem here is that practitioners still tend to view digital tools and content as instruments to support the design of physical landscapes. A second problem is that the field&rsquo;s ontology of place is not sufficiently broad to encompass the fusion of digital and physical that we sought to construct. Since the Renaissance, landscape designers and gardeners, historian John Dixon Hunt argues, have typically divided space into three constructs or &ldquo;natures,&rdquo; with first nature referring to wilderness, second nature to cultivated land, and third nature to landscapes shaped with aesthetic intent <sup id="fnref1:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>. Some designers suggest that Landscape Architects should consider adding a fourth nature to the list, one encompassing reclaimed landscapes and restored habitats, while we in turn wondered if we had stumbled on a fifth: landscapes containing digital annotations, objects or complements.</p>
<p>A third issue centered on the field&rsquo;s self-definition. While Landscape Architecture traces its origins to Landscape Gardening, the profession has maintained a complicated relationship with the arts over the course of the 20th century. Then, many designers, particularly in the wake of the profession&rsquo;s  <em>1966 Declaration of Concern</em>   <sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup> , suggested the focus of landscape architects should be on environmental conservation, not the translation of artistic or philosophical trends, be it transcendentalism or post-modernism, into landscape form. Other practitioners have taken the stance that the primary focus of the field should be on design, here understood as landscape construction that is optimized for the needs of a site&rsquo;s visitors and users. The artistic ambitions of the architect are deemed to be a secondary concern, if they are considered at all. On the 50th anniversary of the 1966 statement, however, the Landscape Architecture Foundation published a  <em>New Landscape Declaration</em> , one in which architects called for a renewed emphasis on aesthetics <sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>. One reason for the call was the realization that aesthetic concerns are not a luxury: poor design of buildings and cityscapes – here understood as the mindless prioritization of utility over beauty or meaning – has had a deleterious social impact. Landscape architects have also stressed artistic concerns because of a widespread sense that the field has never been a site of aesthetic innovation, and that it has missed important opportunities, such as the emergence of avant-garde art in the early 20th century <sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup>  <sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup>  <sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>  <sup id="fnref2:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>. While the field has not been oblivious to important trends such as Modernism and Post-Modernism, &ldquo;there is surprisingly little discussion,&rdquo; Richard Weller writes, &ldquo;of what contemporary landscape aesthetics are and what they might yet become&rdquo; <sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>.</p>
<p>Given this lack of definition, our team opted for an exploratory approach to inform the design of our project, one akin in the Digital Humanities to Stephen Ramsay&rsquo;s Screwmeneutics and Kevin Ferguson&rsquo;s Digital Surrealism. In his seminal essay, Ramsay proposes a method of scholarly activity that eschews the conduct of what he refers to as the &ldquo;search.&rdquo; The &ldquo;search&rdquo; in this context is the practice of identifying a canon – the state-of-the field for a given domain of research – and then identifying a gap in that same canon. As scholars, we contribute to our respective fields by conducting research to fill that gap. But there are times, Ramsay writes, when this formulaic approach does not work, particularly when the researcher is attempting something new. How can the &ldquo;search&rdquo; &ldquo;help me find what I&rsquo;m looking for,&rdquo; he writes, &ldquo;when (a) I don&rsquo;t know what&rsquo;s here and (b) I don&rsquo;t know what I&rsquo;m looking for?&rdquo; <sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup>. For Ramsay, the answer to this dilemma is to engage in what he refers to as &ldquo;browsing.&rdquo; Drawing on the ideas of Roland Barthes, Ramsay proposes that we engage in the construction of the  <em>writerly text</em> . In contrast to the  <em>readerly text</em> , which involves a passive form of reading, engagement with a  <em>writerly text</em>  presupposes a process of composition, an immersion of the reader into the flux of existence, and initiation of a process of discovery and systematization: the connection by the reader of the content from the given text with a context, be it another item of content, a person, or a compelling idea. The theory of the  <em>writerly text</em> , Barthes writes, &ldquo;is a practice (that of the writer), not a science, a method, a research &hellip; this theory can produce only theoreticians or practitioners, not specialists (critics, researchers, professors, students)&rdquo; <sup id="fnref1:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup>.</p>
<p>Via this process, Ramsay argues, the humanist defines a path through culture, thereby providing the Humanist – or, if you will, the Screwmeneuticist – with perspective, the platform necessary to support discovery, desire and questioning. One useful first step in defining such a pathway was undertaken by David Ferguson in his recent visual analysis of Disney animations, where he brings his artistic artefact of interest – the film – into relation with the concept of complex. Any artefact of interest, he suggests, can be dissected and its components re-articulated into new combinations. However, while useful, Ferguson&rsquo;s method for applying Screwmeneutics is not appropriate for our purposes for two reasons. First, its aim is ultimately analytic, not generative. It seeks, like all Structuralist methodologies, to locate the intrinsic properties – the hidden archetypal structures – of a given art-form. Its purpose is not scenario exploration. It does not seek to conduct an exercise exploring the potential ways an art-form  <em>might</em>  exist. It seeks instead to define the way a given form  <em>already</em>  exists. Second, and by extension, Ferguson&rsquo;s methodology, and structuralist methodologies in general, are ones that collapse the dimension of time. They presuppose objects of analysis that operate in a synchronic fashion <sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup>  <sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup>. We, by contrast, were seeking to create Augmented Reality, multi-media landscape complexes that could incorporate the dimension of time, and more specifically would incorporate temporal art forms such as music. Our purpose, in the end, was to generate an artwork that would serve, in Seymour Papert&rsquo;s words, as an object-to-think-with, and in Barthes&rsquo; words, as a  <em>writerly text</em>   <sup id="fnref:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup>. Given that we were aiming to create something that was new, we sought to construct an artefact that would provide definition, suggest potential, and present questions and connections for future artists and humanists to explore.</p>
<h2 id="our-theory-the-oral-tradition-of-harold-innis">Our Theory: The Oral Tradition of Harold Innis</h2>
<p>For these reasons, we opted to employ a different Screwmeneutic method to conceive and develop our landscape complex, namely Harold Innis&rsquo; concept of the Oral Tradition. For those unfamiliar with his work, Harold Innis was a media theorist and one of the founders of the Toronto School of Communication. Between 1940 and 1952 he produced a set of works, including  <em>Empire and Communications</em>  and  <em>The Bias of Communication</em>  that were dedicated to exploring the physical, formal and cognitive effects of communication media. A political economist by training, Innis turned to the study of communications in 1940 because he perceived, not surprisingly, that the world was falling apart. How was it possible, he asked in the final decade of his life, that the world, after enduring the insanity and carnage of the First World War, should so readily plunge itself into a Second? The answer, he believed, was to be found in the grand sweep of global history. What factors, he wondered, enabled societies to thrive? And what, by contrast, led to their dysfunction and eventual collapse? His lived experience, combined with his studies of the past, convinced him that human collectives functioned much like biological organisms. The vitality, adaptability, indeed the sanity, of institutions, nations and empires were dependent on the environmental circumstances in which they found themselves. Since the inputs and outputs of human-environmental interaction were largely mediated by communication devices, Innis set high store on their importance in global history. The political and cultural periodization of history, Innis believed, could be correlated with the invention or adoption of new communication technologies. So could the pathologies that afflicted cultures. There was a price to be paid for the unthinking use of technology, and in his writings Innis pointed to two: cognitive rigidity and cognitive flux <sup id="fnref:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup>  <sup id="fnref:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup>. Technology historically had produced pathological forms of groupthink in which cultures either focused on the wrong thing (due to cognitive rigidity, or bias), or no thing at all (due to cognitive flux). With its judgment impaired by one of the two pathologies, a given culture would lose its ability to discern the opportunities and threats latent in its environment, fail to innovate, fail to compete and as a result fall prey to its competitors <sup id="fnref:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup>.</p>
<p>It was a pessimistic construction of history, but it was not fatalist. While most regimes inevitably went the way of Nineveh and Tyre <sup id="fnref:20"><a href="#fn:20" class="footnote-ref" role="doc-noteref">20</a></sup> , Innis believed it was possible to construct a culture that retained its vitality and resilience via an ethic he referred to as the Oral Tradition. The Oral Tradition, as Innis conceived it, was not a call to forsake writing and adopt pre-literate Greek communication practices that relied on memory and voice. Instead, like Barthes, Innis called on his readers to adopt an  <em>active stance</em>  toward knowledge, a willingness to alter constructs and formalisms to meet the needs and experience of the present. Such a stance, Innis argued, had enabled the Greeks to make their innovations in theology, philosophy, science, politics and art. The Oral Tradition was also distinguished by its reliance on linguistic and aesthetic formalisms that were characterized by internal complexity and hierarchy. The power of such formalisms lay in their latent potential. They contained multiple constituents that could be arranged and re-arranged at will, enabling artists to explore new aesthetic constructs, and philosophers new intellectual possibilities, through the separation, interpolation and translation of content. In its essence, it was a serio-comic method, one that enabled Greece to shift its conception of nature from one based on myth to one shaped by science <sup id="fnref1:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup>.<sup id="fnref:21"><a href="#fn:21" class="footnote-ref" role="doc-noteref">21</a></sup></p>
<p>A third feature of the Oral Tradition, one deriving from its commitment to active manipulation of knowledge, was its use of multi-modal forms of expression. The reason Innis named his communication ethic the Oral Tradition in the first place was due to the influence of linguist Edward Sapir, who noted the &ldquo;formal richness&rdquo; of ancient communication practices, practices that contained &ldquo;a latent luxuriance of expression that eclipses anything known to modern civilization&rdquo; <sup id="fnref:22"><a href="#fn:22" class="footnote-ref" role="doc-noteref">22</a></sup>. Such luxuriance of expression, and with it enhanced expressive potential, could be purchased in the present by creating formalisms that combined written, vocal and visual forms of representation. Such forms in principle presented possibilities for information visualization – which would assist in the location of significant environmental patterns – and information translation – which would enhance viewer understanding of the construct&rsquo;s content <sup id="fnref2:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup>  <sup id="fnref1:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup>. Referring to Italian artistic practice in the 15th and 16th centuries, for example, Innis noted Andrea Alciati&rsquo;s invention of the emblem book, a construct in which poetry, &ldquo;one of the oldest arts, was combined with engraving, one of the newest&rdquo; [Innis 2015, 101] The rationale for so doing was provided by Francis Bacon, who argued that emblems &ldquo;reduce intellectual conceptions to sensible images and that which is sensible strikes the memory and is more easily imprinted on it than that which is intellectual&rsquo;&rdquo; <sup id="fnref:23"><a href="#fn:23" class="footnote-ref" role="doc-noteref">23</a></sup>  <sup id="fnref:24"><a href="#fn:24" class="footnote-ref" role="doc-noteref">24</a></sup>.</p>
<p>The commitment to multi-modality in turn presented a fourth characteristic of the Oral Tradition: a commitment to integrate both spatial and temporal forms of representation. Because of its present and historic relationship with the art of poetry, Innis argued, the Oral Tradition &ldquo;implied a concern with time and religion. &lsquo;The artist represents  <em>coexistence in space</em> , the poet  <em>succession in time</em> &rsquo; (Lessing at the University of Berlin, 1810)&rdquo; <sup id="fnref:25"><a href="#fn:25" class="footnote-ref" role="doc-noteref">25</a></sup>. In his own economic work, Innis would apply this characteristic of the Oral Tradition by interpolating time series of price data with maps to track the emergent patterns of North American economic history, years before the practice become common via Geographic Information Systems <sup id="fnref3:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup>. The Oral Tradition was finally characterized by what in modern parlance can be characterized as an open-source ethic, one in which content and form was routinely lifted from one work and integrated into another. &ldquo;The great epics,&rdquo; Innis writes, &ldquo;were probably developed out of lays constantly retold and amplified. Old ballads were replaced by combinations of a number of episodes into a unity of action. The epic was characterized by extreme complexity and unity&rdquo; <sup id="fnref1:22"><a href="#fn:22" class="footnote-ref" role="doc-noteref">22</a></sup>.</p>
<h2 id="our-mode-and-matter-data-art-protein-and-text">Our Mode and Matter: Data Art, Protein and Text</h2>
<p>In its essence then, the Innis&rsquo; Oral Tradition is a method for intellectual inquiry and aesthetic innovation that rests on a willingness to explore the possibility spaces afforded by complex, spatio-temporal, multi-modal form, and to reconstitute that form through the integration of content brought in from the outside. To assist our exploration of the possibility space associated with digital Landscape Architecture, we opted to use the methods and modes associated with Data Art. Also referred to as Information Art, Data Art rests on the premise that the world is replete with forms that can be harvested for artistic purposes. It chiefly emerged from scientific efforts to use sight and sound analogues as a method to locate significant patterns and relationships in large data sets. From that effort, multiple intriguing genres of art have emerged, particularly in the field of music, such as DNA music <sup id="fnref:26"><a href="#fn:26" class="footnote-ref" role="doc-noteref">26</a></sup>, protein music <sup id="fnref:27"><a href="#fn:27" class="footnote-ref" role="doc-noteref">27</a></sup>, microbial and meteorological music <sup id="fnref:28"><a href="#fn:28" class="footnote-ref" role="doc-noteref">28</a></sup>, and even a music of the spheres, music compositions derived from astronomical data <sup id="fnref:29"><a href="#fn:29" class="footnote-ref" role="doc-noteref">29</a></sup>. With Data Art, we were presented in principle with a method that would enable us to translate a given data set at once into visual and sonic form.</p>
<p>Once we had a mode for our AR landscape, the next, more difficult step was determining its matter, the data that would provide the pattern, the source of serial distinctions for our artworks. The team&rsquo;s decision-making here was determined by the following factors: our decision to produce two works, contingent circumstance, and the intellectual interest of project team members. The contingent circumstance centered on our need to locate sonification software. While Bill Ralph, a mathematician, and Mark Anderson, a computer scientist, respectively possessed the skills required to visualize our data, we had no one with the requisite ability to translate raw data into musical form. After a fairly protracted search (there is not a lot of proprietary or open-source software dedicated to supporting the generation of Data Art) we were able to locate  <em>MusicWonk</em> , an application developed by John Dunn, a pioneer in computer music and art since the 1970s <em>.</em>  While  <em>MusicWonk</em>  is purportedly able to able to work with any data set, the software specializes in supporting the creation of Genetic Music from DNA and Protein Data. The application&rsquo;s website provides tutorials and links to multiple libraries of genetic sequences, including the one we used: the U.S. National Institutes of Health GenBank <sup id="fnref:30"><a href="#fn:30" class="footnote-ref" role="doc-noteref">30</a></sup>. Based on this specialization, we opted to use protein data and to create an artwork called  <em>The Five Senses</em> , a composition of five movements which would respectively be constructed from protein data supporting Sight, Smell, Touch, Taste and Hearing in humans.</p>
<p>Our data selection was also prompted by the team&rsquo;s interest in the construct of change known as self-organization or emergent change, a ubiquitous phenomenon in the natural and social sciences studied by the Science of Complexity <sup id="fnref:31"><a href="#fn:31" class="footnote-ref" role="doc-noteref">31</a></sup>  <sup id="fnref:32"><a href="#fn:32" class="footnote-ref" role="doc-noteref">32</a></sup>. It was also prompted by the observation of scholars such as Werner Jaeger and Harold Innis that different domains of human activity – such as science, philosophy and theology – often produce constructs of change that are very similar <sup id="fnref2:22"><a href="#fn:22" class="footnote-ref" role="doc-noteref">22</a></sup>. It was finally prompted by the observation of John Bonnett that a number of parallels could be found between the core concepts of emergent change – such as positive feedback, the governance of formal cause, and the teleological governance of system attractors – and the philosophy of history associated with the Christian Bible. Economists, for example, often characterize positive feedback as the Matthew Effect, given that the process of cumulative change it describes matches the dynamic Jesus describes in the Parable of the Talents, where the rich became richer and poor poorer <sup id="fnref:33"><a href="#fn:33" class="footnote-ref" role="doc-noteref">33</a></sup>. Given these parallels, we opted to create a second work simply titled  <em>Emergence</em>  based on source text from the Bible. Like  <em>The Five Senses</em> , we planned a composition featuring five movements, with each piece named after a constituent concept of self-organization, respectively &ldquo;Emergence,&rdquo; &ldquo;Differentiation,&rdquo; &ldquo;Regulation,&rdquo; &ldquo;Selection&rdquo; and &ldquo;Attractor.&rdquo; Each piece, in turn, would rest on data taken from the King James version of the Bible, text that provided narrative or mythic analogues to the concepts of positive feedback, formal cause, and so on.</p>
<p>With the terms of composition settled, the team&rsquo;s next step was to identify the specific data sets we wanted to use for each composition and then begin the process of sonification. The proteins selected for  <em>The Five Senses</em>  are shown in <a href="#figure01">Figure 1</a>, while the biblical texts selected for  <em>Emergence</em>  are shown in <a href="#figure02">Figure 2</a>. The process of sonification for  <em>The Five Senses</em>  began by harvesting the letter data associated with each protein record, such as that shown in <a href="#figure03">Figure 3</a>. In many ways, NIH protein records resemble what any researcher might find in a library catalogue for a book listing, with metadata describing authors, key words, versions and the like. However, NIH records also feature a sequence of letters where one typically would find Library of Congress or equivalent subject headings describing a book&rsquo;s contents. Those letters collectively constitute the protein described in the given record. In <a href="#figure03">Figure 3</a>, the highlighted letter sequence refers to the amino acids making Medium Wave sensitive opsin 1 (Homo sapiens), a protein contained in green cone photopigment that we used to generate the work  <em>Sight</em> . These letter sequences provided the patterned sequences that would be translated into notes for  <em>Sight</em> . Similar letter sequences generated the notation for the four remaining movements of  <em>The Five Senses</em> , while the biblical passages provided the initial basis for  <em>Emergence</em> .</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000505/resources/images/figure01.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000505/resources/images/figure01_hu91a120e0a5d78366f009d89c10cc33b0_66062_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000505/resources/images/figure01_hu91a120e0a5d78366f009d89c10cc33b0_66062_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000505/resources/images/figure01_hu91a120e0a5d78366f009d89c10cc33b0_66062_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000505/resources/images/figure01.png 1431w" 
     class="landscape"
     ><figcaption>
        <p>Proteins selected for <em>The Five Senses</em>
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000505/resources/images/figure02.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000505/resources/images/figure02_hudea2adbae274b440cf966eda1fedc12e_48971_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000505/resources/images/figure02_hudea2adbae274b440cf966eda1fedc12e_48971_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000505/resources/images/figure02_hudea2adbae274b440cf966eda1fedc12e_48971_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000505/resources/images/figure02.png 1431w" 
     class="landscape"
     ><figcaption>
        <p>Biblical texts selected for <em>Emergence</em>
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000505/resources/images/figure03.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000505/resources/images/figure03_hu604188573cfd3b7d63c6f32c1edb9339_410908_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000505/resources/images/figure03_hu604188573cfd3b7d63c6f32c1edb9339_410908_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000505/resources/images/figure03_hu604188573cfd3b7d63c6f32c1edb9339_410908_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000505/resources/images/figure03.png 1431w" 
     class="landscape"
     ><figcaption>
        <p>The screenshots show the NIH record for medium-wave-sensitive opsin 1, a protein affiliated with the sense of sight in human beings. The small boxed section of letters on the lower right refers to the protein&rsquo;s constituent amino acids. These letters were the source data for the movement <em>Sight</em> in <em>The Five Senses</em> .
        </p>
    </figcaption>
</figure>
<h2 id="our-method-of-sonification">Our Method of Sonification</h2>
<p>The process of sonification for both compositions was initiated in  <em>MusicWonk</em>  first by converting every letter from every dataset into a counterpart number, with A equaling 1, B equaling 2, Z equaling 26, and so on. Once an entire letter set from a given dataset had been translated, the software then aligned the numeric series with a specified music scale, producing a raw music string, as shown in <a href="#figure04">Figure 4</a>. From that point, the project&rsquo;s two music composers – Erin MacAfee and Amy Legault – had two methods of composition and two modes of operating with their source data open to them. The first method of composition – selected by neither – was to use the algorithmic method of music composition supported by  <em>MusicWonk</em> . There, the raw music string is transformed by feeding it through a set of components that control features such as tempo, chords, instrumentation and the like. The process, shown in <a href="#figure05">Figure 5</a>, is akin to creating an algorithm using a visual programming language, or creating an electronic circuit.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000505/resources/images/figure04.jpeg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000505/resources/images/figure04_hu761da85d059a7d3f3a4af6595c350725_307757_500x0_resize_q75_box.jpeg 500w,
    /dhqwords/vol/15/1/000505/resources/images/figure04_hu761da85d059a7d3f3a4af6595c350725_307757_800x0_resize_q75_box.jpeg 800w,/dhqwords/vol/15/1/000505/resources/images/figure04_hu761da85d059a7d3f3a4af6595c350725_307757_1200x0_resize_q75_box.jpeg 1200w,/dhqwords/vol/15/1/000505/resources/images/figure04_hu761da85d059a7d3f3a4af6595c350725_307757_1500x0_resize_q75_box.jpeg 1500w,/dhqwords/vol/15/1/000505/resources/images/figure04_hu761da85d059a7d3f3a4af6595c350725_307757_1800x0_resize_q75_box.jpeg 1800w,/dhqwords/vol/15/1/000505/resources/images/figure04.jpeg 1950w" 
     class="portrait"
     ><figcaption>
        <p>MusicWonk is shown here generating raw music string from protein data
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000505/resources/images/figure05.jpeg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000505/resources/images/figure05_huc5a135762790b96b2ff749b7caac72d8_138738_500x0_resize_q75_box.jpeg 500w,
    /dhqwords/vol/15/1/000505/resources/images/figure05_huc5a135762790b96b2ff749b7caac72d8_138738_800x0_resize_q75_box.jpeg 800w,/dhqwords/vol/15/1/000505/resources/images/figure05_huc5a135762790b96b2ff749b7caac72d8_138738_1200x0_resize_q75_box.jpeg 1200w,/dhqwords/vol/15/1/000505/resources/images/figure05.jpeg 1350w" 
     class="landscape"
     ><figcaption>
        <p>MusicWonk interface for algorithmic composition of protein music
        </p>
    </figcaption>
</figure>
<p>The second method – selected by both – required the two to export the raw music string to a *.midi file, and then open the exported file in  <em>Finale</em> , a music composition software package. The appeal of  <em>Finale</em>  for Legault and MacAfee is that it presented methods for music notation that were familiar to them, most notably by featuring an interface and tools that supported viewing and direct manipulation of music string notation. With  <em>Finale</em> , the two were able to alter note duration, inscribe chords of their own devising, and repair aberrations in pitch.</p>
<p><em>Finale</em>  also enabled each composer to determine how she wanted to use the musical sequences inherited from her source data. MacAfee&rsquo;s approach in  <em>The Five Senses</em>  was minimalist. While she was willing to alter select attributes in her file, such as rhythm, instrumentation and the duration of individual notes, she was not inclined to alter the tonal sequence inherited from her datasets. Legault&rsquo;s approach, by contrast, was more interventionist. Instead of viewing her inherited notation series as a fixed object, she opted to view her raw music strings as libraries from which she could splice identified musical components. The appropriated sections were – much like the lays in Innis&rsquo; Oral Tradition – placed in new sequences and were integrated into established musical genres selected by Legault, such as the fugue.</p>
<h2 id="our-methods-for-visualization">Our Methods for Visualization</h2>
<p>While Legault and MacAfee were generating their respective sonifications, team members Bill Ralph, a mathematician, and Mark Anderson, a computer scientist, were working on their visual correlates. Ralph is also an algorithmic artist, one who describes his work as &ldquo;an attempt to make a visual connection with the enormous complexity and unity that lies within the rich mathematical objects that inspire my images&rdquo; <sup id="fnref:34"><a href="#fn:34" class="footnote-ref" role="doc-noteref">34</a></sup>. He is particularly interested in mathematical objects that can generate chaotic, dynamic systems, and has used them to generate dynamic and static works distinguished by their complex topologies and colour. From the standpoint of this project, Ralph&rsquo;s approach was ideal because it often relied on source data to drive the generation of a given work. For  <em>The Five Senses</em> , he used proprietary algorithms driven by two inputs. The first source of pattern was the set of distinctions obtained via a sequential progression through the data. The second source was the set of relationships detected by the algorithm between different, non-proximate strings contained in the data. Both were leveraged to provide a visual focus to our landscape composition. For  <em>Emergence</em> , Mark Anderson used a different approach. Instead of deriving his images from the data, he selected Screen Vector Graphic images that related conceptually with concepts such as  <em>Differentiation</em>  and  <em>Attractor</em> , and then used project data and the application  <em>NodeBox</em>  to animate each image and prompt transformations in colour.</p>
<h2 id="_the-datascapes-project_--ar-set"><em>The DataScapes Project</em>  AR Set</h2>
<p>With the sonification and visualization of our two works complete, two further tasks remained, the first being the construction and overlay of a digital set to display  <em>The Five Senses</em>  and  <em>Emergence</em> . Given that we conceived our two works as Landscape Architecture, and that the Brock Traffic Circle – our selected first venue – was 140 metres in diameter, the parameters of our project mandated that the components of our digital set be relatively large in size for the visualizations to be observable and for them in turn to serve as artistic complements to the surrounding landscape. That need was further compounded by the presence of the physical statue, known as She-Wolf, situated in the midst of the circle, and the scaffolding and the four 4.6 metre-high QR code signs we installed to activate and situate our digital works. All three objects are shown in <a href="#figure06">Figure 6</a>, and all three required effacing by the digital set to prevent their disruption of the impact of our AR Landscape works. Accordingly, the set shown in <a href="#figure07">Figure 7</a> was designed by John Bonnett using  <em>SketchUp</em>  to display  <em>The Five Senses</em>  and  <em>Emergence</em> . The set is bounded by eight 9-metre-high monoliths, with the front of each monolith displaying a graphic representing the title of the movement currently under display. <a href="#figure08">Figure 8</a>, for example, shows the graphics for the movement  <em>Touch</em>  on the left and  <em>Differentiation</em>  on the right. The centre of the set is composed of two components. The cylinder at the base is used to cover the statue, scaffold and QR code signs mentioned above. Floating above it is a 9 metre object used to display the given movement&rsquo;s visualization. For  <em>The Five Senses</em> , a cube was used to display the artwork, while a sphere was used for  <em>Emergence</em> .</p>
<h2 id="_the-datascapes-project_--android-app"><em>The DataScapes Project</em>  Android App</h2>
<p>Our final task was the creation of an Android app to support user viewing of the two works via a tablet or phone. Our requirements for the  <em>DataScapes</em>  app were straightforward. We wanted to simultaneously see and hear the respective visualizations and sonifications associated with each movement. We further wanted multiple users to the site to experience the performance in sync. If one viewer was viewing and hearing the movement  <em>Taste</em> , we wanted their counterparts to see and hear the same thing at the same time. We also wanted user entry into the experience to be simple, involving little more than user activation of the app, and direction of his</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000505/resources/images/figure06.jpeg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000505/resources/images/figure06_hu26762530762e03b070a16b15be52ae02_262711_500x0_resize_q75_box.jpeg 500w,
    /dhqwords/vol/15/1/000505/resources/images/figure06_hu26762530762e03b070a16b15be52ae02_262711_800x0_resize_q75_box.jpeg 800w,/dhqwords/vol/15/1/000505/resources/images/figure06_hu26762530762e03b070a16b15be52ae02_262711_1200x0_resize_q75_box.jpeg 1200w,/dhqwords/vol/15/1/000505/resources/images/figure06_hu26762530762e03b070a16b15be52ae02_262711_1500x0_resize_q75_box.jpeg 1500w,/dhqwords/vol/15/1/000505/resources/images/figure06_hu26762530762e03b070a16b15be52ae02_262711_1800x0_resize_q75_box.jpeg 1800w,/dhqwords/vol/15/1/000505/resources/images/figure06.jpeg 1950w" 
     class="landscape"
     ><figcaption>
        <p>Brock University She-Wolf Statue, surrounded by scaffolding and 4.6 meter QR code signs used to situate <em>DataScapes</em> &rsquo; AR content.
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000505/resources/images/figure07.jpeg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000505/resources/images/figure07_hu574715ad154c64bbb117059ff1508923_258477_500x0_resize_q75_box.jpeg 500w,
    /dhqwords/vol/15/1/000505/resources/images/figure07_hu574715ad154c64bbb117059ff1508923_258477_800x0_resize_q75_box.jpeg 800w,/dhqwords/vol/15/1/000505/resources/images/figure07_hu574715ad154c64bbb117059ff1508923_258477_1200x0_resize_q75_box.jpeg 1200w,/dhqwords/vol/15/1/000505/resources/images/figure07.jpeg 1431w" 
     class="landscape"
     ><figcaption>
        <p>The <em>DataScapes</em> AR set, shown as it was featured at Brock University&rsquo;s Traffic Circle.
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000505/resources/images/figure08.jpeg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000505/resources/images/figure08_hu44e026e89d646a0b6130ab502391b5b7_174586_500x0_resize_q75_box.jpeg 500w,
    /dhqwords/vol/15/1/000505/resources/images/figure08_hu44e026e89d646a0b6130ab502391b5b7_174586_800x0_resize_q75_box.jpeg 800w,/dhqwords/vol/15/1/000505/resources/images/figure08_hu44e026e89d646a0b6130ab502391b5b7_174586_1200x0_resize_q75_box.jpeg 1200w,/dhqwords/vol/15/1/000505/resources/images/figure08.jpeg 1430w" 
     class="landscape"
     ><figcaption>
        <p>Graphics for the movement <em>Touch</em> on the left and <em>Differentiation</em> on the right
        </p>
    </figcaption>
</figure>
<p>or her device camera at the QR signs situated in the middle of the traffic circle. The app only worked at a distance of 6 metres or less from the QR code signs, to prevent unsafe viewing of the exhibit by, for example, drivers circumnavigating the circle. The  <em>DataScapes</em>  application was developed using  <em>Vuforia</em> , an SDK (Software Development Kit) designed to facilitate the development of AR applications on mobile devices. Leveraging 3D objects, planar surfaces and patterns situated in the environment,  <em>Vuforia</em>  is able to determine the user&rsquo;s position and camera orientation relative to objects such as the project&rsquo;s QR Code signs, and in turn to situate and register computer-generated 3D objects. These capabilities enabled team programmers Joe Bolton and Mark Anderson to fulfill another requirement for the  <em>DataScapes</em>  app: the capacity to move around and within the artwork.  <em>Vuforia</em> &rsquo;s capabilities, combined with GPS and gyroscopic data, enabled the  <em>DataScapes</em>  app to track the movement of viewers, and in response to change the orientation and rendering of displayed objects in real-time, to make our digital objects seem as if they were integrated and locked into the natural landscape.</p>
<h2 id="results">Results</h2>
<p>The final results of our efforts are shown in <a href="#figure09">Figure 9</a> and <a href="#figure10">Figure 10</a>, along with a video showing the operation of the app in <a href="#figure11">Figure 11</a> (For an earlier treatment of  <em>The DataScapes Project</em> , see Bonnett et al. 2018). The first version of the  <em>DataScapes</em>  app in our estimation was a partial success, both technically, and with respect to audience reception. With respect to the app&rsquo;s functionality, our initial trials of the two works were hindered by two difficulties. The first and perhaps most disappointing setback was the inadequate processing power of the two Asus 12&quot; tablets we had on hand for viewing the display. The tablets did not have the capacity to simultaneously render and register our set while simultaneously displaying our data visualizations in dynamic form. Despite our best efforts to find work-around solutions on site, we had to settle for the necessary compromise of presenting Bill Ralph&rsquo;s and Mark Anderson&rsquo;s respective visualizations in static form. A second difficulty centered on the stability of the set. While the first version of the app adequately anchored our set to the Brock traffic circle, several of its constituents, most notably its monoliths, tended to vibrate or jitter in a way that was at once distracting and unpleasant. These deficiencies have been partially remedied in the second and third versions of our app. Our intention in these two iterations was to develop portable versions that would display  <em>The Five Senses</em>  and  <em>Emergence</em>  indoors, while remedying some of the performance failures identified in the first. Version two, shown in the animation featured in <a href="#figure12">Figure 12</a>, was designed to support Mark Anderson for an Edge Hill University lecture dedicated to the Art of Computing, and presents the project as a wall display. Version three, shown in <a href="#figure13">Figure 13</a>, transformed the orientation and scale of the project yet again, by adapting it for display as a table-top installation. In both, we were able to better stabilize the display of our set. And in both, we were able to add some dynamism to the display, albeit by introducing movement into the set rather than our visualizations. Both iterations required changes to the application, most notably by changing the way  <em>Vuforia</em>  searched for targets and changing the orientation of the 3D content it displayed in relation to those targets. In addition, we also removed the initial application&rsquo;s use of GPS tracking code, as that functionality was specifically designed to support the geo-location of 3D content on the Brock traffic circle, and further designed to hinder the display of content to users not on campus. Finally, we reduced the size of QR code displays used with versions 2 and 3 of the  <em>DataScapes</em>  app and designed each version to generate displays proportional in size to the QR code target. Via this step, the user can print a code on a piece of paper and generate a display proportional in size to the underlying table.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000505/resources/images/figure09.jpeg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000505/resources/images/figure09_huf34d3dd636a32c623649de0bf2bb9770_128214_500x0_resize_q75_box.jpeg 500w,
    /dhqwords/vol/15/1/000505/resources/images/figure09_huf34d3dd636a32c623649de0bf2bb9770_128214_800x0_resize_q75_box.jpeg 800w,/dhqwords/vol/15/1/000505/resources/images/figure09.jpeg 801w" 
     class="landscape"
     ><figcaption>
        <p>Screenshot of <em>Emergence</em> , as it appeared using an ASUS tablet.
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000505/resources/images/figure10.jpeg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000505/resources/images/figure10_hu9c21a79f23e25a495b2c6c68dd19d0ab_181786_500x0_resize_q75_box.jpeg 500w,
    /dhqwords/vol/15/1/000505/resources/images/figure10_hu9c21a79f23e25a495b2c6c68dd19d0ab_181786_800x0_resize_q75_box.jpeg 800w,/dhqwords/vol/15/1/000505/resources/images/figure10.jpeg 1080w" 
     class="landscape"
     ><figcaption>
        <p>Screenshots of <em>The Five Senses</em> , as it appeared using an ASUS tablet.
        </p>
    </figcaption>
</figure>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://player.vimeo.com/video/441090582" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="Video showing the performances of the movements Emergence and Differentiation from the piece Emergence." webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://player.vimeo.com/video/441094016" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="Animation showing version 2 of The DataScapes Project, as it was displayed at Edge Hill University. Version 2 featured the project as a wall display rather than as a component of landscape architecture." webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000505/resources/images/figure13.jpeg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000505/resources/images/figure13_hue4029087d3f14570583ceab12b5f4801_315660_500x0_resize_q75_box.jpeg 500w,
    /dhqwords/vol/15/1/000505/resources/images/figure13_hue4029087d3f14570583ceab12b5f4801_315660_800x0_resize_q75_box.jpeg 800w,/dhqwords/vol/15/1/000505/resources/images/figure13_hue4029087d3f14570583ceab12b5f4801_315660_1200x0_resize_q75_box.jpeg 1200w,/dhqwords/vol/15/1/000505/resources/images/figure13.jpeg 1429w" 
     class="landscape"
     ><figcaption>
        <p>Version 3 of <em>The DataScapes</em> project, here configured for table-top display.
        </p>
    </figcaption>
</figure>
<p>As for audience assessment, we report here anecdotal evidence gathered during our first trial. Our rationale for this approach as opposed to more extensive and expensive methods used in Human Computer Interaction (HCI) and elsewhere is due, in part, to concerns regarding the reliability of user assessment methods pioneered in cognitive psychology and HCI. These methods have been drawn into question due to, on a general level, the replicability crisis in the social sciences and, more specifically, widespread criticisms of the two fields pointing to errors in statistical method, small sample sizes and large confidence intervals <sup id="fnref:35"><a href="#fn:35" class="footnote-ref" role="doc-noteref">35</a></sup>  <sup id="fnref:36"><a href="#fn:36" class="footnote-ref" role="doc-noteref">36</a></sup>  <sup id="fnref:37"><a href="#fn:37" class="footnote-ref" role="doc-noteref">37</a></sup>  <sup id="fnref:38"><a href="#fn:38" class="footnote-ref" role="doc-noteref">38</a></sup>  <sup id="fnref:39"><a href="#fn:39" class="footnote-ref" role="doc-noteref">39</a></sup>. Putting the matter simply, user assessment via this route seemed to impose a good deal of effort for results that did not necessarily convince.</p>
<p>For the type of exploratory exercise that we were pursuing, it seemed more useful to take a craft-based approach to user assessment. In fields such as web design, argues Gerd Waloszek, a former interface designer with SAP, the metric for success for a given design is not its authentication via an HCI analysis of design  <em>consumers</em> . The real metric rests on whether other design  <em>producers</em>  copy the design, making it a convention <sup id="fnref:40"><a href="#fn:40" class="footnote-ref" role="doc-noteref">40</a></sup>. In a different design context, Elizabeth Eisenstein and Hellmut Lehmann-Haupt note a similar process in the history of the print book. Through experimentation and appropriation of designs from competitors, printers such as Peter Schoeffer collectively generated the conventions of the modern book, such as tables of contents, title pages, running heads and footnotes <sup id="fnref:41"><a href="#fn:41" class="footnote-ref" role="doc-noteref">41</a></sup>  <sup id="fnref:42"><a href="#fn:42" class="footnote-ref" role="doc-noteref">42</a></sup>. A craft-based approach, then, suggests that the &ldquo;success&rdquo; of our effort will depend on whether other artists, landscape architects and digital humanists choose to emulate our approach to landscape design, because they like it, and because they believe it will induce a favorable reaction from their respective audiences. The best we can do is report what we learned from our viewers and suggest the questions and implications that stem from those observations. We will be &ldquo;successful&rdquo; if colleagues choose to mimic those efforts and address the questions we have raised.</p>
<p>Based on that framework, we noted two classes of respondents – general viewers and artists – and important questions that arose from each. General viewers for the most part found our various pieces engaging, reporting an intrinsic fascination with the fact that data, particularly protein data, could be translated into art forms, particularly music. That feedback raised an important question for us, however. Is Data Art intrinsically compelling on its own? Is it capable of generating an intellectual or affective response from viewers without annotation, or must it be presented with an accompanying spoken or written context to maintain viewer interest? This is, of course, not a new question in the history of art and Landscape Architecture, where practitioners continue to argue whether art should attempt to communicate a concept or message – such as the need for environmental sustainability – or instead focus on generating an emotional response from viewers <sup id="fnref3:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>. A related question is whether artists and Landscape Architects can generate the sensory &ldquo;languages,&rdquo; be it visual, sonic, olfactory or something else, required to successfully communicate the artist&rsquo;s message to an audience without textual assistance.</p>
<p>The response of artists was, perhaps not surprisingly, more searching and more critical. While most viewers found the project&rsquo;s content and approach very interesting, some took exception to our temporary appropriation of the Brock traffic circle (we used it for one week), particularly since it already contained a physical statue that was surrounded by the project&rsquo;s scaffolding and QR codes. One artist deemed the metal framing and the statue&rsquo;s digital effacement problematic. Another deemed it a violation. This feedback raised fascinating ethical and political questions for us. To be sure, the infrastructure we constructed around the physical  <em>She-Wolf</em>  statute did hinder proper viewing of the statue. Whether it constituted a violation depends on your point of view. Until artists raised the issue with us, the construction of the scaffolding and QR codes prompted no concern from project participants, nor did it for the university officials who granted us permission to use the space. We meant no disrespect. It was not our intention to make a comment on the statue, nor to damage it. But some artists did see our temporary occupation of the space as an unacceptable abridgement on the statue&rsquo;s integrity and the sculptor Ilan Averbuch&rsquo;s rights.</p>
<p>One response to these concerns would be to note that the scaffolding and QR codes will not be a permanent component of AR Land Architecture, or more generally, AR-based artforms in future. We used them because current technology requires their use to situate and register content. In future, artists will be able to use RTK GPS (Real-time Kinematic Geographic Positioning System) data to position content with centimetre and even millimetre level accuracy. That expedient will resolve the issue of the art form&rsquo;s physical intrusion into a given space, but not its digital annotation. Is it ethical for one artist to use digital methods to efface, comment upon, add to, mock, or alter the context of a physical artwork created by another, even if the digital annotation is not visible to the naked eye? To that ethical question we can add a political one: who gets to add that annotation? The complexity and sensitivity of this question was anticipated in 2017 in New York when the statue  <em>Fearless Girl</em>  (sculpted by Kristen Visbal) was placed in proximity to the statue  <em>Charging Bull,</em>  a work situated on Broadway and long associated with Wall Street and the city&rsquo;s financial district. To many,  <em>Fearless Girl</em>  presented an important message of female empowerment.  <em>Charging Bull</em> &rsquo;s sculptor Arturo Di Modica, however, was outraged, arguing that  <em>Fearless Girl</em> &rsquo;s proximity and positioning transformed the meaning of his statue, from one symbolizing prosperity and strength, to that of villain <sup id="fnref:43"><a href="#fn:43" class="footnote-ref" role="doc-noteref">43</a></sup>  <sup id="fnref:44"><a href="#fn:44" class="footnote-ref" role="doc-noteref">44</a></sup>. The controversy raised multiple questions, ones that we now face with our project. When an artist makes an intrusion into a space, does that space become sacrosanct? Artists have long made interventions into spaces that they deem to be playful, even provocative. Are those intrusions to be exempted from three-dimensional annotations that some will label as legitimate super-positions or rejoinders, others as graffiti, and others as outrageous violations? Who gets to decide the issue? These issues will become increasingly central as AR becomes a ubiquitous communication medium.</p>
<h2 id="discussion-and-conclusion">Discussion and Conclusion</h2>
<p>In addition to the questions posed above, what other observations and questions might we offer to conclude this exercise in Screwmeneutics via the Oral Tradition? In our view, the best way to finish would be to explicitly address the questions that constitute the heart of this issue of DHQ.</p>
<h2 id="1-what-are-digital-and-computational-approaches-to-sound-images-and-time-based-media">1. What are digital and computational approaches to sound, images and time-based media?</h2>
<p>For this project, the answer is simple: data translation. Data translation via computation can be used to generate sound (via sonification), images (via visualization), and time-based media (music via sonification). Further, this project is significant in the context of the digital humanities because of its use of approaches typically used for analysis to generate art. In the last 10 to 15 years, visualization has become an integral component of text analysis, while tentative steps are also being taken to leverage sonification <sup id="fnref:45"><a href="#fn:45" class="footnote-ref" role="doc-noteref">45</a></sup>  <sup id="fnref:46"><a href="#fn:46" class="footnote-ref" role="doc-noteref">46</a></sup>. The project also makes a contribution by leveraging text data, typically the grist for DH analysis, as the raw material for art. It finally contributes by using a non-traditional DH data source, protein, as a constituent for art.</p>
<h2 id="2-how-do-these-methods-and-approaches-produce-new-knowledge-and-shift-scholarship-in-a-particular-scholarly-domain">2. How do these methods and approaches produce new knowledge and shift scholarship in a particular scholarly domain?</h2>
<p>In the domain of Landscape Architecture, as indicated above, digital objects have typically been seen as instruments for planning, the basis for the physical transformation of a given landscape. This project suggests that Landscape Architecture – and, by extension, the Digital Humanities – should integrate the digital with the physical when producing new landscape designs, and in so doing should generate a new domain of Landscape Architecture. The reason for so doing rests on a long-standing ambition of the field: to use landscapes as a visual way to say something about the nature of the cosmos in which we find ourselves. For example, pioneers such as Frederick Law Olmsted, the designer of New York&rsquo;s Central Park, were Transcendentalists. They used landscapes to express their sense that there was a latent reality underlying nature, that it was suffused with the divine <sup id="fnref4:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>.</p>
<p>Landscapes populated by latent, invisible without instrument, AR objects could be used to extend a similar, but different, message. In this context, the construction disseminated would not be  <em>transcendence</em> , but  <em>ubiquitous sentience</em> . In the past 20 years, biologists have made numerous remarkable discoveries that suggest many supposedly human distinctives are not unique to our species at all. Trees, for example, form familial and friendship networks, and communicate via something akin to a fungal Internet <sup id="fnref:47"><a href="#fn:47" class="footnote-ref" role="doc-noteref">47</a></sup>. Cetaceans have been shown to use artificial languages and form cultures while musical biologists suggest birds compose music equal in complexity to symphonies <sup id="fnref:48"><a href="#fn:48" class="footnote-ref" role="doc-noteref">48</a></sup>. Perhaps most surprisingly, recent work in molecular biology suggest that cells use molecules in a fashion akin to words, that their communication is possibly akin to language, with a semantics, syntax and pragmatics, that they communicate in more than one &ldquo;language,&rdquo; and that they vote (biologists refer to it as quorum sensing) <sup id="fnref:49"><a href="#fn:49" class="footnote-ref" role="doc-noteref">49</a></sup>  <sup id="fnref:50"><a href="#fn:50" class="footnote-ref" role="doc-noteref">50</a></sup>  <sup id="fnref:51"><a href="#fn:51" class="footnote-ref" role="doc-noteref">51</a></sup>  <sup id="fnref:52"><a href="#fn:52" class="footnote-ref" role="doc-noteref">52</a></sup>. Landscape Architects have long had a commitment to design that reflects an ethic of ecological sustainability and highlighting plant life, stone and other constituents of their local regions. To our knowledge, those same architects have not produced designs that in humanist terminology reflect an  <em>Animal Turn</em> , a commitment to highlighting the agency embedded in environments that individuals in modern, European-derived cultures have traditionally ignored, largely because they – and we – were ignorant of their existence. It would be a worthy challenge for Landscape Architects to derive an AR-based visual language to focus viewer attention on our neighbours. DNA, molecular and Protein Data – the &ldquo;words&rdquo; used by single cell organisms to communicate – would be particularly worthy material upon which to build that language.</p>
<p>For the Digital Humanities, our exercise in Screwmeneutics via the Oral Tradition suggests that scholars have the potential to translate their traditional focus – text data – into visual and sonic forms of art. They also can join their colleagues in the arts and sciences in translating other forms of data for the same purpose. The exercise also suggests that so long as the intrinsic structure of source data is retained, there is no correct, preferred or consistent method for generating Data Art. The source data can be divided into its components, and, again, provided the internal structures of those components are retained, the appropriated sections can be placed in new sequences and contexts. Further, the components of a given data source can be situated with whatever form, medium or context that is consistent with the needs, aspirations and capabilities of the artist. To be sure, most artists in domains such as Protein Music and other forms of Data Music choose to retain the sequential structures of their source data in their entirety. But the insights of the Oral Tradition and the inherent structure of our protein and text source data suggest that this preference need not be so, and our experience, combined with the past behavior of some Data Artists, suggests that some will do so.</p>
<p>With respect to the nature of our data, Douglas Hofstadter notes that proteins like music are composed of smaller components: &ldquo;Music is not a mere linear sequence of notes. Our minds perceive pieces of music on a level far higher than that. We chunk notes into phrases, phrases into melodies, melodies into movements, and movements into full pieces. Similarly, proteins only make sense when they act as chunked units&rdquo; <sup id="fnref:53"><a href="#fn:53" class="footnote-ref" role="doc-noteref">53</a></sup> Biologist Mary Anne Clark similarly notes that &ldquo;I was struck by the parallels between musical structure and the structure of proteins and the genes that encode them. Proteins also seemed to be composed of phrases organized into themes. For years I was haunted by the image, and tried occasionally to interest musicians in making the transformation for me &hellip;&rdquo; <sup id="fnref1:27"><a href="#fn:27" class="footnote-ref" role="doc-noteref">27</a></sup>. Similar observations can and have been made about the complex nature of linguistic and textual data, not the least, as we saw, by Harold Innis in his descriptions of the Oral Tradition.</p>
<p>Our stance that some Data Artists, particularly digital humanist Data Artists, will opt to leverage the &ldquo;phrases&rdquo; and &ldquo;themes&rdquo; of data for artistic purposes, and connect them to the instrumentation, media or objects that suit their purpose, is indicated by the choices made by the artists who collaborated on  <em>The Five Senses</em>  and  <em>Emergence</em> . In the first work, Erin MacAfee and Bill Ralph chose to work within the sequential confines of the data presented to them. In the second, Amy Legault and Mark Anderson did not, collectively producing a work that confined the Data Art to the sonic level, based on selections of phrases detected by Legault in the source data, and the selection by Anderson of non-data images derived from an on-line library. In each case, the artists subjectively chose the extent to which they would subject themselves to the structures contained in their data, and the objects, media and instrumentation they conflated with their data. That is no different in principle from what previous Data Musicians have done, where they have displayed a remarkable degree of ingenuity and freedom in the artistic choices underlying their compositions. In our work, we played with complexity, arranging and re-arranging units contained on a single level of biological organization. In previous work, sonic composers have played with hierarchy, subjectively conflating structure from different levels of biological organization – such as DNA data, amino acid data, and protein folds – to produce a composition. Our work has been an exercise in juxtaposing retrieved structure with selected visualization. Previous work has juxtaposed identified structure with selected instrumentation. The key point here is that strict adherence to a given method is not necessary for Data Art to retain its integrity. So long as some trace of the data&rsquo;s original structure is retained, one should expect artists, as they must, to exercise their own judgment on what method or array of methods is legitimate to fulfill their purpose.</p>
<h2 id="3-what-are-the-challenges-and-possible-futures-for-av-in-dh">3. What are the Challenges and Possible Futures for AV in DH?</h2>
<p>Our combined use of Screwmeneutics, Harold Innis&rsquo; Oral Tradition and Digital Art has explored one potential way that AR-based objects can be leveraged as constituents for Landscape Architecture. The Possibility Space for spatial AR art, however, is much larger, and its exploration and realization would constitute a worthy possible future for the Digital Humanities. One obvious way that space could be explored would be to change the digital content that is used to populate selected landscapes. Data Art is and will remain a central constituent of our future work. Indeed, we intend to further refine our app so that future users can incorporate their own visual and sonic constructs from data into it. But we also intend to incorporate other forms of art. We find ourselves, for example, wondering what the surreal imagination of Salvador Dali might have made of AR as a medium. The sky is a constant backdrop in his paintings, as witnessed in masterworks such as  <em>Dream Caused by the Flight of a Bee</em> ,  <em>Christ of Saint John of the Cross</em> , and  <em>Santiago el Grande</em> . One only has to survey a few of his works before the question arises if other spaces might be filled by AR objects. We are used to thinking of Landscape Architectures. Dali&rsquo;s work suggests AV digital humanists ought to expand their imagination to consider what a Skyscape Architecture or Seascape Architecture might look like.</p>
<p>Two factors, one old, one new suggest why they might want to consider the development of such architectures. To start, it is a commonplace to observe that architects and interior designers, since ancient times, have sought to shape a visitor&rsquo;s experience of interior space – a given room inside a building – by importing select features from exterior space, ranging from the physical to the metaphysical. Whether by landscape or  <em>trompe l&rsquo;oeil</em>  paintings, alignment of windows, doors, and ceiling oculi with the sun and moon, paintings with celestial motifs, or religious paintings such as Michelangelo&rsquo;s  <em>The Creation of Adam</em> , artists have sought to use exterior space to create a sense of interior place. A second factor that should influence digital humanists is the imminent availability of smart glasses, as well as intelligent glass supported by smart film. Both present the possibility for immersive AR that outstrips the constrained field-of-view presently afforded by tablets and head-mounted displays. While smart glasses – which will be light, easy to wear, and feel like regular glasses – suggest the possibility for user access to AR content anywhere at any time, intelligent glass and smart film – sheets of glass and film that can alter glass opacity and display information – suggest the possibility that digital humanists and interior designers will be able to conceive and construct augmented seascapes and skyscapes.</p>
<p>Consider the following storyboard as a possible example of what might result from an architect&rsquo;s attempt to construct a Skyscape Architecture using intelligent glass. Typically, the night-time sky in a city is a rather dull place. Light pollution effaces many if not most of the stars that might be observed, plus the Milky Way. An architect, say, of a restaurant might choose to exploit this gap by creating an A-Frame structure topped by a roof made with glass and smart film. The architect or interior designer, in turn, would then be free to fill the evening sky with whatever object, or array of objects is wished. Drawing on ceiling celestial paintings as a source of inspiration, our architect might opt to create a surreal display conflating the earth and moon with the Andromeda Galaxy, as shown in <a href="#figure14">Figure 14</a>. The restauranteur taking possession of the locale, however, would not be constrained by the initial skyscape provided by the architect, and could supplement it with other designs displaying other astronomical objects and other styles. The next evening our restauranteur might choose to populate the evening sky with an enlarged version of Jupiter juxtaposed with part of the Rosette Nebula. The following day, patrons would be treated to a surreal display of objects floating in the sky, as indicated in <a href="#figure15">Figure 15</a>.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000505/resources/images/figure14.jpeg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000505/resources/images/figure14_huec446fe6cdf59588af3d05653f00d916_314604_500x0_resize_q75_box.jpeg 500w,
    /dhqwords/vol/15/1/000505/resources/images/figure14_huec446fe6cdf59588af3d05653f00d916_314604_800x0_resize_q75_box.jpeg 800w,/dhqwords/vol/15/1/000505/resources/images/figure14_huec446fe6cdf59588af3d05653f00d916_314604_1200x0_resize_q75_box.jpeg 1200w,/dhqwords/vol/15/1/000505/resources/images/figure14.jpeg 1431w" 
     class="landscape"
     ><figcaption>
        <p>Restaurant configured with smart-film supported rooftop windows for display of AR skyscapes. (John Bonnett, 2020)
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000505/resources/images/figure15.jpeg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000505/resources/images/figure15_hu8d03fb865983b2f23a9ec633123d969c_482055_500x0_resize_q75_box.jpeg 500w,
    /dhqwords/vol/15/1/000505/resources/images/figure15_hu8d03fb865983b2f23a9ec633123d969c_482055_800x0_resize_q75_box.jpeg 800w,/dhqwords/vol/15/1/000505/resources/images/figure15.jpeg 1080w" 
     class="landscape"
     ><figcaption>
        <p>Restaurant configured with smart-film supported rooftop windows for display of AR skyscapes. (John Bonnett, 2020)
        </p>
    </figcaption>
</figure>
<p>In such a scenario, building architecture would emerge as an interface to a new domain of art. And in such a scenario, sea, land and sky would emerge as platforms for the very sort of meaning-making that we have sought to promote in this study. Nothing is inevitable, but we would be very surprised if artists and digital humanists do not avail themselves of this opportunity to use digital form to enhance their surrounds. Such a step would simply be a continuation of a long history in which humans have used built form and modified topography for more than functional purposes. &ldquo;From the most immemorial Hindustan pagodas to the Cathedral of Cologne,&rdquo; Victor Hugo writes in  <em>The Hunchback of Notre Dame</em> , &ldquo;architecture was the great script of the human race.&rdquo; Landscapes and buildings have been used by humans to express their yearning for beauty; their conception of history; their power over space; and their belief that humans – in the end – live in a cosmos with intrinsic purposes that can be understood. Now, that conversation, and that quest for meaning, can be continued in digital form, in new locales. And now, that conversation – in visual, sonic and topographic form – can express in a powerful way that the planet is replete with agency, creativity and dignity. We have no idea what our peers and our successors will produce. But we expect it will be replete with all the usual adjectives that we associate with great art: compelling, interesting, at times infuriating, but never boring.</p>
<p>The line is a reference to a famous poem by Rudyard Kipling titled &ldquo;Recessional,&rdquo; which dwells on the temporary and fleeting nature of empires. They key passage in the poem is this: </p>
<p>_       Far-called, our navies melt away;_</p>
<p>_              On dune and headland sinks the fire:_</p>
<p><em>Lo, all our pomp of yesterday</em></p>
<p><em>Is one with Nineveh and Tyre!</em></p>
<p>The entire poem can be found at <a href="https://www.poetryfoundation.org/poems/46780/recessional">https://www.poetryfoundation.org/poems/46780/recessional</a></p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Teilhard de Chardin, Pierre,  <em>The Phenomenon of Man</em> , London: Collins. (1959).&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>The  noosphere  is a philosophical concept developed and popularized by the  <a href="https://en.wikipedia.org/wiki/Biogeochemistry">biogeochemist</a>  Vladimir Vernadsky, and the French philosopher and Jesuit priest Pierre Teilhard de Chardin. See: <a href="https://en.wikipedia.org/wiki/Noosphere">https://en.wikipedia.org/wiki/Noosphere</a>&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Association for Computing Machinery,  “Ivan Sutherland, 1988”  <em>A.M. Turing</em>    <em>Award</em> . Available at: <a href="https://amturing.acm.org/award_winners/sutherland_3467412.cfm">https://amturing.acm.org/award_winners/sutherland_3467412.cfm</a> [Accessed 27 May 2020].&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Thompson, Ian H.,  <em>Landscape Architecture: A Very Short Introduction,</em>  Oxford, UK: Oxford University Press. (2014).&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref3:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref4:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>Herrington, Susan,  <em>Landscape Theory in Design</em> , London: Routledge. (2017).&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>Crossley, Joe,  “How We Can Hack the Surfaces around us with Projection Mapping”    <em>TEDx Talks</em> , <a href="https://bit.ly/2Toc1lm">https://bit.ly/2Toc1lm</a> [Accessed 27 May 2020].&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p><a href="https://www.lafoundation.org/who-we-are/values/declaration-of-concern">https://www.lafoundation.org/who-we-are/values/declaration-of-concern</a>&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>Landscape Architecture Foundation,  <em>The New</em>    <em>Landscape Declaration</em> , Gayle Berens (eds), Los Angeles, CA: Rare Bird Books. (2017).&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>Corner, James,  “Landscape City” ,  <em>The New Landscape Declaration</em> , ed. Gayle Berens, Los Angeles, CA: Rare Bird Books. (2017), pp. 65–68.&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>Fajardo, Martha,  “Manifesto About the Profession&rsquo;s Future” ,  <em>The New</em>    <em>Landscape Declaration</em> , Gayle Berens (eds), Los Angeles, CA: Rare Bird Books (2017), pp. 119 -122.&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>Jellicoe, Geoffrey,  <em>The Landscape of Man: Shaping the Environment from</em>    <em>Prehistory to the Present Day</em>  , 3rd edition, Thames &amp; Hudson. (1995).&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p>Weller, Richard,  “Our Time?”    <em>The New Landscape Declaration</em> , Gayle Berens (ed),Los Angeles, CA: Rare Bird Books. (2017), pp. 5–12.&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13">
<p>Ramsay, Stephen,  “The Hermeneutics of Screwing Around; or What You Do with a Million Books”    <em>PastPlay: Teaching and Learning History with Technology</em>  , Kevin Kee (ed), Ann Arbor, MI: The University of Michigan Press. (2014):111–120.&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14">
<p>Ferguson, Kevin L.,  “Digital Surrealism: Visualizing Walt Disney Animation Studios”    <em>Digital Humanities Quarterly</em> , 11(1) (2017). Available at: <a href="http://www.digitalhumanities.org//dhq/vol/11/1/000276/000276.html">http://www.digitalhumanities.org//dhq/vol/11/1/000276/000276.html</a> [Accessed 27 May 2020].&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:15">
<p>Merrell, Floyd,  “Structuralism and Beyond: A Critique of Presuppositions”    <em>Diogenes</em> , 23(92) (1975): 67–103. Available at: <a href="https://doi.org/10.1177/039219217502309205">https://doi.org/10.1177/039219217502309205</a> [Accessed 27 May 2020].&#160;<a href="#fnref:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:16">
<p>Papert, Seymour,  <em>Mindstorms: Children, Computers and Powerful Ideas</em> , New York: Basic Books. (1980).&#160;<a href="#fnref:16" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:17">
<p>Innis, Harold,  <em>Political Economy in the Modern State</em> , Toronto: The Ryerson Press. (1946).&#160;<a href="#fnref:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:18">
<p>Bonnett, John.  “The Flux of Communication: Innis, Wiener and the Perils of Positive Feedback”    <em>Canadian Journal of Communication</em>  42(3) (2017), 431–446.&#160;<a href="#fnref:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:19">
<p>Bonnett, John,  <em>Emergence and Empire: Innis, Complexity and the Trajectory of</em>    <em>History.</em>  Montreal, McGill-Queens University Press (2013).&#160;<a href="#fnref:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref3:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:20">
&#160;<a href="#fnref:20" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></li>
<li id="fn:21">
<p><a href="http://podcasts.torontoreviewofbooks.com/2013.02/Bonnet.mp3">http://podcasts.torontoreviewofbooks.com/2013.02/Bonnet.mp3</a>&#160;<a href="#fnref:21" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:22">
<p>Innis, Harold,  <em>Empire and Communications</em> , Oxford, UK:The Clarendon Press. (1950).&#160;<a href="#fnref:22" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:22" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:22" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:23">
<p>Innis, Harold  <em>Harold Innis&rsquo;s History of Communications,</em>  William J. Buxton, Michael B. Cheney, Paul Heyer (eds), foreword by John Durham Peters, Lanham, Maryland: Rowman and Littlefield. (2015).&#160;<a href="#fnref:23" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:24">
<p>Bacon, Francis,  <em>De Dignatate et Augmentis Scientarium</em> , Book 5, Chapter 5, Argentorati: Sumptbus, Johan Joachimi Bockenhoferi (1654).&#160;<a href="#fnref:24" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:25">
<p>Innis, Harold  <em>The Bias of Communication</em> , Toronto: University of Toronto Press. (1951).&#160;<a href="#fnref:25" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:26">
<p>Gena, Peter and Strom, Charles,  “Music Synthesis of DNA Sequences”    <em>Sixth International Symposium on Electronic Art,</em>  Montreal (1995): 83–85.&#160;<a href="#fnref:26" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:27">
<p>Dunn, J. and Clark, M.,  “The Sonification of Proteins”    <em>Leonardo</em>  32 (1)(1999): 25–32.&#160;<a href="#fnref:27" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:27" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:28">
<p>Larsen, P and Gilbert, J.,  “Microbial Bebop: Creating Music from Complex Dynamics in Microbial Ecology”    <em>PLoS ONE</em>  8: e58119 (2013).&#160;<a href="#fnref:28" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:29">
<p>Ballora, M. and Smoot, G.S.,  “Sound: The Music of the Universe”    <em>The Huffington Post,</em>  February 23, (2013), <a href="https://bit.ly/2Yx2MAF">https://bit.ly/2Yx2MAF</a> [Accessed 27 May 2020].&#160;<a href="#fnref:29" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:30">
<p>Algorithmic Arts ,  “Interactive Generative Creativity Software for Music, Visuals and Wordplay”    <em>Algorithmic Arts,</em>  (2015). <a href="http://algoart.com">http://algoart.com</a> [Accessed 27 May 2020].&#160;<a href="#fnref:30" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:31">
<p>Waldrop, M. Mitchell,  <em>Complexity: The Emerging Science at the Edge of Order</em>    <em>and Chaos</em> , New York: Simon and Schuster. (1992).&#160;<a href="#fnref:31" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:32">
<p>]Gell-Mann, Murray,  <em>The Quark and the Jaguar</em> , New York: Henry Holt and Company. (1994).&#160;<a href="#fnref:32" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:33">
<p>Rigney, Daniel  <em>The Matthew Effect: How Advantage Begets Further Advantage</em> , New York: Columbia University Press. (2010).&#160;<a href="#fnref:33" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:34">
<p>Ralph, Bill,  “Artist&rsquo;s Statement” (2018),  <em>BillRalph.com</em>  <a href="https://bit.ly/2YvD53D">https://bit.ly/2YvD53D</a> [Accessed 27 May 2020].&#160;<a href="#fnref:34" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:35">
<p>Lundh, Lars-Gunnar,  “The Crisis in Psychological Science and the Need for a Person-Oriented Approach”    <em>Social Philosophy of Science for the Social Sciences</em> , Jaan Valsiner (ed), Cham, Switzerland: Springer. (2019), pp. 203–224.&#160;<a href="#fnref:35" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:36">
<p>Greenberg, S. and Thimbleby, H.,  “The Weak Science of Human-Computer Interaction”    <em>CHI &lsquo;92 Research Symposium on Human Computer</em>    <em>Interaction, Monterey, California,</em>  (May 1992). Available at: <a href="https://bit.ly/2M6nfH3">https://bit.ly/2M6nfH3</a> [Accessed 27 May 2020].&#160;<a href="#fnref:36" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:37">
<p>Cairns, Paul,  “HCI. . . Not as It Should Be: Inferential Statistics in HCI Research”    <em>BCS-HCI &lsquo;07: Proceedings of the 21st British HCI Group Annual Conference on People and Computers</em> . Eds. L.J. Ball, M.A. Sasse, C Sas , Swindon, UK: BCIS Learning and Development, (2007)195–201.&#160;<a href="#fnref:37" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:38">
<p>Gigerenzer, Gerd,  “Mindless Statistics”    <em>The Journal of Socio-Economics</em>  33(2006): 587–606.&#160;<a href="#fnref:38" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:39">
<p>Cohen, Jacob,  “The World is Round (p &lt; .05)”    <em>American Psychologist</em>  49(12) (December 1994): 997–1003.&#160;<a href="#fnref:39" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:40">
<p>Waloszek, Gerd,  “User Interface Design – Is it A Science, An Art, or A Craft?”    <em>SAP Design Guild</em>  (2003). Available at: <a href="https://bit.ly/3gt1Sh3">https://bit.ly/3gt1Sh3</a> [Accessed 27 May 2020].&#160;<a href="#fnref:40" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:41">
<p>Eisenstein, Elizabeth,  <em>The Printing Press as an Agent of Change:</em>    <em>Communications and Cultural Transformations in Early Modern Europe</em> ,  <em>Volumes I and II.</em>  Cambridge, UK: Cambridge University Press. (1979).&#160;<a href="#fnref:41" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:42">
<p>Lehmann-Haupt, Hellmut,  <em>Peter Schoeffer of Gernsheim and Mainz</em> , Rochester, NY: The Printing House of Leo Hart. (1950).&#160;<a href="#fnref:42" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:43">
<p>Dobnik, Verena,  “Will New York invite the &lsquo;Fearless Girl&rsquo; statue to stay on Wall Street?”    <em>USA Today</em> , March 27 (2017). Available at: <a href="https://bit.ly/3gtadBr">https://bit.ly/3gtadBr</a> [Accessed 27 May 2020].&#160;<a href="#fnref:43" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:44">
<p>Mettler, Katie,  “Charging Bull sculptor says Fearless Girl distorts his art, so he&rsquo;s fighting back”    <em>Chicago Tribune</em> , April 12 (2017). Available at: <a href="https://bit.ly/3etBGRS">https://bit.ly/3etBGRS</a> [Accessed 27 May 2020].&#160;<a href="#fnref:44" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:45">
<p>Sinclair, S. and Rockwell G.,  “Text Analysis and Visualization”    <em>A New Companion to the Digital Humanities</em> , Chichester, West Sussex, UK: Wiley- Blackwell. (2016), pp. 274–290.&#160;<a href="#fnref:45" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:46">
<p>Graham, Shawn,  “The Sound of Data (a gentle introduction to sonification for historians)”  <em>The Programming Historian</em> . Available at: <a href="https://programminghistorian.org/en/lessons/sonification">https://programminghistorian.org/en/lessons/sonification</a> [Accessed 28 May 2020].&#160;<a href="#fnref:46" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:47">
<p>Wohlleben, Peter,  <em>The Hidden Life of Trees: What They Feel, How They</em>    <em>Communicate – Discoveries from a Secret World</em> , Trans. Jane Billinghurst, Vancouver, Greystone Books. (2015).&#160;<a href="#fnref:47" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:48">
<p>Conway Morris, Simon,  <em>Life&rsquo;s Solution: Inevitable Humans in a Lonely</em>    <em>Universe</em> , Cambridge, UK: Cambridge University Press. (2003).&#160;<a href="#fnref:48" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:49">
<p>Ahmed, Farooq,  “Profile of Bonnie L. Bassler” in  <em>PNAS</em>  105(13): 4969–4971. April 1, (2008). Available at: <a href="https://www.pnas.org/content/105/13/4969">https://www.pnas.org/content/105/13/4969</a> [Accessed 28 May 2020].&#160;<a href="#fnref:49" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:50">
<p>Bassler, Bonnie,  “How bacteria &rsquo;talk&rsquo;”  <em>Ted Talks</em> , (February 2009). Available at: <a href="https://bit.ly/36C0IeR">https://bit.ly/36C0IeR</a> [Accessed 28 May 2020].&#160;<a href="#fnref:50" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:51">
<p>Ben Jacob, Eshel, Becker, Israela, Shapira, Yoash and Levine, Herbert,  “Bacterial Linguistic Communication and Social Intelligence” ,  <em>Trends in Microbiology</em>  12(8) (2004): 366–372.&#160;<a href="#fnref:51" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:52">
<p>Ji, Sungchul,  “The Linguistics of DNA: Words, Sentences, Grammar, Phonetics, and Semantics”    <em>Annals of the New York Academy of Sciences</em> , 870 (May 18,1999): 411–417.&#160;<a href="#fnref:52" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:53">
<p>Hofstadter, Douglas,  <em>Gödel, Escher, Bach: An Eternal Golden Braid,</em>  Hassocks, Sussex, UK: Harvester Press. (1979).&#160;<a href="#fnref:53" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry><entry><title type="html">Another Type of Human Narrative: Visualizing Movement Histories Through Motion Capture Data and Virtual Reality</title><link href="https://rlskoeser.github.io/dhqwords/vol/15/1/000514/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://rlskoeser.github.io/dhqwords/vol/15/1/000514/</id><author><name>Eugenia S. Kim</name></author><published>2021-03-05T00:00:00+00:00</published><updated>2021-03-05T00:00:00+00:00</updated><content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>Human narratives are one of the most compelling and versatile information sources. One specific category, oral histories, &ldquo;play a unique role in to documenting cultural heritage and in preserving memories of historical and everyday events&rdquo; <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. These verbal narratives have traditionally been recorded in audio, video and as written transcriptions. Based on the importance of how an oral history is performed <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> and of how much information physical gestures and facial expressions can convey <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>, a question arises about whether the media formats listed above sufficiently capture all the nuances of non-verbal information. By extension, there is also a question of whether all memories can be verbalized and if other means of expression are better suited for communicating the content of those memories.</p>
<p>The issue of performance and non-verbal information within an oral history poses a complex translation problem. Video recordings or &ldquo;videohistory&rdquo; helps capture physical movement and nuances that audio recordings and transcripts cannot <sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>. Furthermore, certain kinds of cultural experiences cannot be fully described using only words. A concrete example of this are descriptions of symbolic poses and gestures used for complicated rituals. Similarly, physical interactions between individuals and their reactions to situations also lose their nuance when described verbally or through controlled cinematography. From an ethical standpoint, voice and photographic images can be potentially compromising even if censored or modified. This may be important if a memory includes some form of physical trauma, distress or injury. Based on the notion that an oral history is the verbal recounting of an experience, then it could be posited that a movement history is a kinesthetic rendering of lived human experiences where identifying voice and visuals could be obscured if needed.</p>
<p>With these factors in mind, I propose that the concept of a &ldquo;movement history&rdquo; be considered for memories that are based in physical embodied experiences which can be difficult to describe with only words or depict using photographs and video. This definition reflects Albert Lichtblau&rsquo;s statement that &ldquo;physically active remembering also influences the narration, especially when psychology and mentality can be brought into contact with the past <sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>. Additionally, I recommend that movement histories be captured and presented in a way which enables a viewer to see all angles and details of the movement rather than be restricted to a single angle or framing. It is currently easiest to achieve this effect by combining mocap and VR technology with the understanding that other technologies may be used in the future.</p>
<p>I begin my exploration of this concept by reviewing how the evolution of oral history practices can lead to the creation of movement histories. For this article, I will focus on illness narratives expressed through somatic movement practices with my own movement history,  <em>Lithium Hindsight 360</em>  as a case study. Following that I offer some practical insights from my own experience as a motion capture (mocap) and virtual reality (VR) researcher to offer practical tips and relatively affordable technology solutions. One key issue will be the dissemination and accessibility of the mocap data as there is still a usability barrier in the form of technological devices. Additional resources tailored to content creators helping to maintain the movement dataset and accompanying visualization interfaces are also suggested.</p>
<h2 id="background">Background</h2>
<p>The roots of this article started in my previous research on digital dance preservation. As a choreographer and digital archivist, I was intent on finding a practical way to preserve human movement as accurately as possible. At first I believed very strongly in all the existing preservation practices, whether it was collecting good metadata, using high quality video for documentation or ensuring that a repository followed the Open Archival Information System [OAIS] model. These practices worked well in the archives and libraries where I worked, but were not so feasible as an independent artist working alone. From an archivist&rsquo;s perspective it meant processing haphazardly maintained content. From a choreographer&rsquo;s perspective, it was frustrating to be unable to see and analyze all the nuances of how a dance work was performed.</p>
<p>In 2013, I started to become more invested in the Boston dance community and its history since the 1950s. In an effort to fill gaps in the existing documentation, I was inspired by The Dance Oral History Project created by the New York Public Library <sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000514/resources/images/figure01.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000514/resources/images/figure01_hu49a93b68d2c5e468b6a22a99676bc784_253501_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000514/resources/images/figure01_hu49a93b68d2c5e468b6a22a99676bc784_253501_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000514/resources/images/figure01_hu49a93b68d2c5e468b6a22a99676bc784_253501_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000514/resources/images/figure01_hu49a93b68d2c5e468b6a22a99676bc784_253501_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/1/000514/resources/images/figure01_hu49a93b68d2c5e468b6a22a99676bc784_253501_1800x0_resize_box_3.png 1800w,/dhqwords/vol/15/1/000514/resources/images/figure01.png 1944w" 
     class="landscape"
     ><figcaption>
        <p>&ldquo;Bebe Miller Interview, Excerpt.&rdquo; Source credit: New York Public Library. <a href="https://www.nypl.org/audiovideo/bebe-miller-interview-excerpt">https://www.nypl.org/audiovideo/bebe-miller-interview-excerpt</a>.
        </p>
    </figcaption>
</figure>
<p>By luck, dance scholar Jeffrey Friedman happened to be teaching an oral history workshop in Boston around this time. Hearing him talk about dance-specific examples was very useful as I prepared to conduct my own interviews. Very quickly, however, I encountered the same problem that had plagued me for documenting dance: how does one verbally describe movements that defy words?</p>
<p>This problem eventually formed the basis for my practice-based research on creating movement-based pathographies. My original motivation for using mocap and VR was to give patients and viewers alike a sense of privacy. After constructing my initial prototype, however, I realized another benefit. By using mocap data to create an animation viewable in VR, viewers could have complete freedom to examine the movement from any angle or distance. In some ways this meant a more unbiased form of moving image documentation in that the framing of the content had not been decided for the viewer.</p>
<p>I also began to notice some similarities between illness narratives and pathographies with oral history which I address later in this article. The most obvious commonality is the autobiographical narrative based on an individual&rsquo;s memory. This common trait thus made me start to wonder about the possibility of movement-based equivalent to oral history – that is, a movement history.</p>
<h2 id="oral-history-into-movement-history-mutable-memories">Oral History into Movement History: Mutable Memories</h2>
<p>Oral history was originally considered the domain of historians and has since been adopted by a variety of disciplines such as ethnology, anthropology and sociology as well as used for social work and community-led projects <sup id="fnref1:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. For example, a local historical society may conduct oral histories to help a community learn about its past <sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>. Guidelines and instruction courses for how to conduct an oral history are available but there is technically no single correct way to engage in oral history <sup id="fnref1:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>. In terms of specific activities, Lynn Abrams describes oral history as consisting of three main components: 1) an interview process and product of interview, 2) research methodology and result of research process and 3) an act of recording and final record <sup id="fnref2:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>.</p>
<p>At the core of any oral history is a human memory. These memories usually reflect recollection of and/or commentary on significant events, although they may also be centered on daily life or more private events. The fact that these memories are not always precise <sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup> can be seen as problematic or beneficial in that it can affect the accuracy and presentation of content. To avoid frustration, interviewers are encouraged to think of oral history as a shared responsibility with those whom they are interviewing <sup id="fnref2:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>. To that end, the extent to which an oral historian must gain the trust of their subjects is no small matter. When this trust is well-placed, a potential platform for communicating information through narratives is created. This platform can be useful for bringing awareness to marginalized communities and hidden histories <sup id="fnref3:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>.</p>
<p>Although visual content may help supplement verbal descriptions, the perspectives are often limited or curated by the documenter. Sometimes a lack of aural clarity or a need to adhere to &ldquo;filmic language&rdquo; can also interfere with accurately capturing all the information provided <sup id="fnref1:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>. Given the rapid developments in 21st century audio/video technology, it is increasingly easier to supplement oral histories with additional information or options for interaction. An example of this would be the University of Southern California Shoah Foundation&rsquo;s Dimensions in Testimony exhibition which combines holographic projections, voice recognition and natural-language processing <sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup>. This immersive way of viewing and interacting with a subject speaking their oral history could generate a deeper understanding as it would allow users to change distance, angle and relationship to the subject. At the same time, the use of multimedia technology must make logical sense given the costs, logistics and ethics involved <sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup>.</p>
<p>To address the issues of providing an enhanced viewing experience without being extravagant in one&rsquo;s technology choices, it may be useful to look to the field of dance for performance strategies that can help add nuance. The connection between dance and oral histories is that dances are a part of oral tradition that can be intangible and ephemeral in such a way that challenges the act of documentation itself <sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>. Furthermore, &ldquo;Dance also challenges other experts to think about devising imaginative methods or scoring, notating, annotating, and archiving a processual, somatic, and multisensory practice&rdquo; <sup id="fnref1:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>. Since a movement history is still based on a personal memory, it would be logical to look at how somatic dance forms that embrace the unique attributes of an individual body for strategies on how to recall, express and perform embodied experiences in a physical manner.</p>
<p>Somatic movement practices often require practitioners to explore the idiosyncrasies of their bodies and their relationship to other bodies in a way that generates a sort of autobiography. These movement practices can also be used to literally illustrate life events which leads to a form of double-layered autobiography. Improvisational methods may incorporate internal and/or external questioning which then becomes reminiscent of an oral history interview. The range of somatic movement practices range from the Asian disciplines of yoga and tai chi to more recent systems such as Feldenkrais and Body Mind Centering. Within that spectrum lie somatic dance forms that have become increasingly used for artistic expression: Authentic Movement, Contact Improvisation and Skinner Releasing Technique amongst others. Steve Paxton, founder of Contact Improvisation, was known for his Small Dance ritual which guides the individual to examine their own body. This self-examination was later part of Paxton&rsquo;s documentation about his process which essentially served a sort of autobiography of his body <sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>.</p>
<p>Similarly, Bill T. Jones, who also used Contact Improvisation, created several solos based on both his own life events and his body&rsquo;s mechanics. Jones helped pioneer the combining of somatic movement practices with motion capture technology. The particular nature of Jones&rsquo; works yielded what could be considered a type of movement history. An early example would be the 1999 Bill T. Jones collaboration with the Open Ended Group,  <em>Ghostcatching</em> . Originally intended to challenge the boundaries of the human body <sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup>,  <em>Ghostcatching</em>  was not intentionally autobiographical but in the end took on autobiographical elements specific to Jones <sup id="fnref1:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup>.</p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://player.vimeo.com/video/26407428" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="&#34;After Ghostcatching HD excerpts.&#34; Source credit: The OpenEndedGroup." webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div>
<p>Similarly, the motion capture samples performed by Steve Paxton as part of his research on contact improvisation that started in 1986 and were released on DVD-ROM in 2008 were not necessarily autobiographical <sup id="fnref1:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>.</p>
<p>More recently, somatic movement experts such as Ruth Gibson are more consciously adapting their technique to the limitations of mocap systems <sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup> and taking a phenomenological approach to mocap recording for VR environments <sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup>.</p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://player.vimeo.com/video/150828741" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="&#34;MAN A VR.&#34; Source credit: Ruth Gibson and Bruno Martelli." webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div>
<h2 id="an-overview-of-motion-capture-vr-and-somatic-movement-practices">An Overview of Motion Capture, VR and Somatic Movement Practices</h2>
<p>The integration of new technologies into oral history practice is not unusual. Digital technology in particular can be integrated across all aspects of the process, whether it is in recording, archiving or disseminating content <sup id="fnref:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup>. It is therefore reasonable to assume that oral history can evolve to the next level of incorporating virtuality. This is already present in projects such as the previously mentioned Dimensions in Testimony exhibition. Virtuality can be used for various purposes whether it is to increase access, create a sense of immersion, generate a sense of embodiment or any other number of reasons. For movement histories, the goal is to allow viewers to examine physical memories without being invasive to the source. To that end, mocap and VR can help create a safer way to view and interact with such content.</p>
<p>Mocap data is extremely versatile in that it can be used for everything from scientific motion analysis to mainstream entertainment. Conceptually mocap has its roots in Eadweard Muybridge&rsquo;s animation experiments and Max Fleischer&rsquo;s rotoscope device <sup id="fnref:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup>. Technologically, the systems used at the time of writing descend from motion tracking systems developed in the 1970s and 1980s. One such example are the point light displays developed by Gunnar Johansson <sup id="fnref:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup>. Since then a variety of systems have been released such as sensor-based systems that rely on accelerometers and camera-based optical systems using infrared markers. The latter type of system allows for reliable tracking and orientation of joints <sup id="fnref1:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup> and accurate capture that enables a variety of data interpretation and facings <sup id="fnref:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup>.</p>
<p>The visualization of this mocap data can also help with human movement analysis <sup id="fnref2:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>. Although mocap data can be visualized in both 2D and 3D spaces, the latter option allows viewers full rein over perspective within a 6DOF (degrees of freedom) interactive context. The term &ldquo;6DOF&rdquo; refers to the three directions in which the head can move (3DOF) combined with the three directions in which the body can move through space <sup id="fnref:20"><a href="#fn:20" class="footnote-ref" role="doc-noteref">20</a></sup> An added benefit of VR is that it is fully immersive and therefore visual distractions can be eliminated. Untethered headsets such as the  <em>Oculus Quest</em>  provide additional freedom in movement. This adds to a sense of immersion which can be a useful element for supporting a narrative. Previous concerns about motion sickness and ergonomic issues <sup id="fnref:21"><a href="#fn:21" class="footnote-ref" role="doc-noteref">21</a></sup> are being addressed through improved headset design, increased graphics computing power and a generally better understanding of VR design principles. Researchers such as Michael Madary and Thomas K. Metzinger have released recommendations for more ethical design <sup id="fnref:22"><a href="#fn:22" class="footnote-ref" role="doc-noteref">22</a></sup> and constant updates in software platforms and standards also contribute to an improved experience. Another benefit of using an immersive environment is the ability with which it is possible to blend both visual and aural media with sound playing as vital a role to immersion as visuals. This balance between types of media may help address Mark Tebeau&rsquo;s concerns about overemphasizing visual representations of oral histories and the need for novel ways of data visualization <sup id="fnref1:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>.</p>
<h2 id="illness-narratives-in-oral-history-and-somatic-movement-practices">Illness Narratives in Oral History and Somatic Movement Practices</h2>
<p>The concept of phenomenology or &ldquo;the experience of&rdquo; is very relevant to memories. A specific type of memory rooted in phenomenology is the illness narrative. The purpose of an illness narrative is to share a patient&rsquo;s perspective on experiencing symptoms, treatment, stigma and other aspects of an illness <sup id="fnref:23"><a href="#fn:23" class="footnote-ref" role="doc-noteref">23</a></sup>. This perspective may agree or contrast with a medical professional&rsquo;s. Like oral histories, illness narratives can focus on specific incidents, lifelong stories or become part of a collection of multiple narratives. More importantly, certain types of patients such as the chronically ill can be categorized as part of a marginalized and/or silenced group who are able to regain their voice through sharing their stories <sup id="fnref:24"><a href="#fn:24" class="footnote-ref" role="doc-noteref">24</a></sup>. Abrams states that &ldquo;Oral history was intended to give a voice to the voiceless, a narrative to the story-less and power to the marginalized&rdquo; <sup id="fnref4:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. From this perspective, the collective experiences of patients can potentially reflect hidden truths about society.</p>
<p>Within the history of medicine, the use of oral history has evolved significantly over time. Oral histories were initially conducted for male figures of influence in medicine before taking on a social history role in the 1970s and a supporting evidence role in the 1990s <sup id="fnref:25"><a href="#fn:25" class="footnote-ref" role="doc-noteref">25</a></sup>. It was the course of this evolution that the question of how to provide a patient&rsquo;s history as opposed to that of a doctor&rsquo;s came into existence. Scholars such as Arthur W. Frank and Arthur Kleinman would start to collect and analyze the narratives of patients while G.Thomas Couser would start to bring the term &ldquo;pathography&rdquo; forward into the 20th century.</p>
<p>Some debate would be spurred by Anne Hawkins&rsquo; very specific definition of pathography in that it is limited to a written account of physical illness <sup id="fnref1:24"><a href="#fn:24" class="footnote-ref" role="doc-noteref">24</a></sup>. Her approach would later be challenged by performing arts scholars such as Alex Mermikides and Gianna Bouchard who identified theatrical performance as a type of &ldquo;embodied pathography&rdquo; <sup id="fnref:26"><a href="#fn:26" class="footnote-ref" role="doc-noteref">26</a></sup>.</p>
<p>Kay Jamison&rsquo;s autobiography as a patient with bipolar disorder <sup id="fnref:27"><a href="#fn:27" class="footnote-ref" role="doc-noteref">27</a></sup> and Kerry Davies&rsquo; collection of mental patient narratives would also challenge the focus on physical illness <sup id="fnref:28"><a href="#fn:28" class="footnote-ref" role="doc-noteref">28</a></sup>. Most importantly, the recognition of the invisibility of patients <sup id="fnref1:25"><a href="#fn:25" class="footnote-ref" role="doc-noteref">25</a></sup> and how narratives can help with self-awareness and regaining personal agency <sup id="fnref1:28"><a href="#fn:28" class="footnote-ref" role="doc-noteref">28</a></sup> elevates the importance of enabling patients to communicate their stories.</p>
<p>Several traditional oral history collections currently exist, including several on mental health. They include the oral histories of disability and personal and mental health collections at the British Library <sup id="fnref:29"><a href="#fn:29" class="footnote-ref" role="doc-noteref">29</a></sup>, the Australian Generations Oral History Project <sup id="fnref:30"><a href="#fn:30" class="footnote-ref" role="doc-noteref">30</a></sup> and the Schizophrenia Oral History Project <sup id="fnref:31"><a href="#fn:31" class="footnote-ref" role="doc-noteref">31</a></sup>. There also exist dance and other movement-based works which are based on a patient experiences. As an art form, illness narratives have been created and shared by professional dancers for centuries with classic examples being the madness scene in  <em>Giselle</em>  and a deathbed duet in  <em>Manon</em> .</p>
<p>As a non-dramatized account performed by a patient, however, illness narratives become subject to moral and ethical complications. In some cases, an illness is accompanied by feelings of helplessness, shame and/or stigma as expressed by Havi Carel in her account of struggling with lymphangioleiomyomatosis <sup id="fnref:32"><a href="#fn:32" class="footnote-ref" role="doc-noteref">32</a></sup>.</p>
<p>For attendees of Depression and Bipolar Support Alliance patient support meetings, the confidentiality requirement is a reminder that sharing identifying information about a patient may have negative consequences <sup id="fnref:33"><a href="#fn:33" class="footnote-ref" role="doc-noteref">33</a></sup>. The outcome is reminiscent of other marginalized or endangered communities where survival is dependent on obscuring identity. In a positive light, sharing an illness narrative can lead to greater understanding and advocacy for improvements in social infrastructure for patients.</p>
<p>One of the pioneers in this area would be Anna Halprin. Although her solo works such as  <em>Portrait</em>   <sup id="fnref:34"><a href="#fn:34" class="footnote-ref" role="doc-noteref">34</a></sup> were more intended to heal her own cancer rather than simply depict the experience of cancer, she would later proceed to create works about the experiences of others. One such work,  <em>Intensive Care: Reflections on Death and Dying</em>   <sup id="fnref:35"><a href="#fn:35" class="footnote-ref" role="doc-noteref">35</a></sup>, is based on her husband&rsquo;s hospitalization. Similarly, Bill T. Jones&rsquo; 1994  <em>Still/Here</em>  is based on the stories of actual patients with life-threatening conditions <sup id="fnref:36"><a href="#fn:36" class="footnote-ref" role="doc-noteref">36</a></sup>.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000514/resources/images/figure12.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000514/resources/images/figure12_hu75425f99d2cc43c83f972248d71a368f_34422_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/15/1/000514/resources/images/figure12_hu75425f99d2cc43c83f972248d71a368f_34422_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/15/1/000514/resources/images/figure12.jpg 560w" 
     class="landscape"
     ><figcaption>
        <p>&ldquo;Still/Here.&rdquo; Please see video <a href="https://www.numeridanse.tv/en/dance-videotheque/stillhere">here</a>. Source credit: Numeridanse.
        </p>
    </figcaption>
</figure>
<p>The contrast between  <em>Intensive Care</em>  and  <em>Still/Here</em>  lies in the ethical complications surrounding both pieces. In Halprin&rsquo;s case, she performs in a piece about someone with whom she had a close relationship with.  <em>Still/Here</em> , on the other hand, is Jones&rsquo; interpretation of stories that he collected from others before presenting it on the bodies of professional dancers. This question of whether such a deeply personal story should be depicted by an outsider calls to mind Susan Kozel&rsquo;s challenging of Daniel Dennett&rsquo;s notion of the heterophenomenologist <sup id="fnref:37"><a href="#fn:37" class="footnote-ref" role="doc-noteref">37</a></sup>. While heterophenomenology is not inherently unethical, the value of such a practice becomes more questionable when the experiences of traditionally silenced and/or vulnerable populations are being presented through the lens of an external neutral observer.</p>
<h2 id="case-study-movement-based-pathography">Case study: Movement-based Pathography</h2>
<p>The concept of phenomenology is the central driver for illness narratives as evidenced by the research of aforementioned medical humanists Frank and Carel as well as S. Kay Toombs and others. Within narrative medicine, phenomenology refers to the experience of having an illness, whether referring to the sensation of symptoms, daily living and/or contextual experiences <sup id="fnref1:32"><a href="#fn:32" class="footnote-ref" role="doc-noteref">32</a></sup>. It therefore makes sense to extend the narrative medicine approach to movement-based illness narratives regardless of documentation method. For my own work  <em>Lithium Hindsight 360,</em>  a set of somatic movement-based illness narratives about bipolar disorder, I referenced the VR ballet  <em>[pain]Byte</em>  developed by Genevieve Smith-Nunes <sup id="fnref:38"><a href="#fn:38" class="footnote-ref" role="doc-noteref">38</a></sup>. Both  <em>Lithium Hindsight 360</em>  and  <em>[pain]Byte</em>  are based on the lived experiences of their respective creators with an emphasis on community engagement. Anonymizing the identities of the mocap performers for ethical purposes was another common factor.</p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://player.vimeo.com/video/241495760" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="&#34;pain[byte] VR on Vimeo.&#34; Source credit: Genevieve Smith-Nunes." webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://player.vimeo.com/video/350437712" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="&#34;Lithium Hindsight 360 Video Capture July 2019.&#34; Source credit: Eugenia S. Kim." webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div>
<p>In order to generate the movement histories presented in  <em>Lithium Hindsight 360</em> , I combined several somatic movement practices to translate the experience, structure the movement and perform the memory. I chose this approach because I noticed that universally relatable physical gestures and movements being expressed by mental health patients while sharing their experiences in a group setting. Examples might include hand tremors, moving slowly due to exhaustion or agitated rapid gesticulation with the arms. For translating and structuring memories into movement, I adapted the Life Art Process to help me identify key positions and gestures with specific experiences. To do this, I first compiled a list of commonly known adolescent bipolar disorder symptoms, read the individual testimonials in medical reports and then tried to translate what I had read. Based on that experience, I decided that it was more appropriate to use my own bodily experience as a patient with bipolar disorder. This decision lead to the creation of two different scenes, &ldquo;Symptoms&rdquo; and &ldquo;Maintenance&rdquo;.</p>
<p>In &ldquo;Symptoms&rdquo;, individual symptoms are depicted by a group of mannequins that resemble each other. They are anchored by a mannequin attempting to ask for help but unable to articulate its needs. Presenting the physical movements without an explanation of the symptom can make a viewer pause and rethink their previous understanding of what a mental health condition looks or feels like. For &ldquo;Maintenance&rdquo; viewers are taken through a standard day of waking up, taking medicine, anxiety over interacting with others, exhaustion from feigning good health and then the return to bed. This scene has a more traditional dance sequence as I used to dance to relieve anxiety and stress. The majority of the movement is more pedestrian to the point of almost looking like pantomime. Since the predominant physical feeling was the sensation of constantly falling apart, the avatar is also constantly shifting and appearing to disintegrate before stabilizing again.</p>
<p>To perform and generate all of the movement, I used various forms of improvisation methods including Contact Improvisation and Authentic Movement to sequence the positions and gestures into a narrative. At this level, I was not thinking about using specific vocabulary from a technique. Instead I was adapting concepts such as initiating movement from an internal point or using a video camera as my &ldquo;witness&rdquo; rather than a person. Taking this approach helped me avoid &ldquo;performing&rdquo; or overdramatizing my movements. Finally, my own movement style or &ldquo;voice&rdquo; is influenced by martial arts forms such as tai chi. This results in movement that is alternatively fluid and percussive.</p>
<p>Initially I had thought it would be better to translate the experiences of other patients with bipolar disorder as presented in medical reports similar to what Jones had done with his choreography. When I realized that this type of heterophenomenology exacerbates issues raised by Susan Kozel about agency and representation that could lead to silencing <sup id="fnref1:37"><a href="#fn:37" class="footnote-ref" role="doc-noteref">37</a></sup>, I made the decision to use my own movement history of being a patient. If this type of heterophenomenology is done in conjunction with patients then it may be ethically possible and accurate for a third party to portray the experiences of another even without a close personal relationship.</p>
<h2 id="practical-strategies-and-considerations">Practical Strategies and Considerations</h2>
<p>Two of the first assumptions that might come to mind when beginning a mocap and VR project is that it is very expensive or technically difficult to generate. In the earlier days of mocap and VR this would have been accurate given the experimental nature and limited access to technologies. As of 2020, there is a constantly growing range of consumer-level hardware and software tools that are still capable of producing high quality results. The simplification of user interfaces and procedures enables creators and researchers to think more holistically about their approach rather than having to become expert programmers and technicians. In this section I offer tips generated from discussions with some of the experts referenced in this article as well as my own personal experiences with multiple hardware systems and recording scenarios. For further details on tasks such as selecting a mocap system, data clean-up, preservation and getting started with a project, please see the Appendix.</p>
<p>In terms of performance, the most important factor is to remember that there is a difference between documenting, performing and performing for documentation purposes. Subjects may need to exaggerate their movements slightly to avoid occlusion issues. This too depends on the type of system and the level of subtlety being captured. For example, a motion sensor like the Kinect can capture full-body, facial and finger data with a single device.</p>
<p>Using a marker-based system, however, allows for more detail refinement later in the visualization stage. Extra set-up time and warm up takes are also recommended as it allows the subject to become accustomed to the direct or indirect impact of a mocap system on their body. This is similar to the process of an interviewer engaging with their subject before conducting an oral history interview.</p>
<p>Other useful strategies can be found in the work of dance movement researchers. In her overview of mocap dance, Kim Vincs reminds readers that concepts such as gravity, race, and gender can all be manipulated in post-production <sup id="fnref:39"><a href="#fn:39" class="footnote-ref" role="doc-noteref">39</a></sup>. This manipulation eases the pressure for a &ldquo;perfect&rdquo; performance and provides the option of hiding certain physical traits. More technical factors such as pre-acceleration of the body and thinking in terms of motion trails rather than shapes may help with recording cleaner data. Movement artist-researcher Steph Hutchison conscientiously adapted her practice to the requirements of a mocap system to yield better results <sup id="fnref:40"><a href="#fn:40" class="footnote-ref" role="doc-noteref">40</a></sup>. One practical modification was in thinking of the markers on her body as &ldquo;partners&rdquo; to work with rather than struggle with.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000514/resources/images/figure17.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000514/resources/images/figure17_hudb74eaa81b0b25a153812a740d2a92dc_230105_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/15/1/000514/resources/images/figure17_hudb74eaa81b0b25a153812a740d2a92dc_230105_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/15/1/000514/resources/images/figure17_hudb74eaa81b0b25a153812a740d2a92dc_230105_1200x0_resize_q75_box.jpg 1200w,/dhqwords/vol/15/1/000514/resources/images/figure17_hudb74eaa81b0b25a153812a740d2a92dc_230105_1500x0_resize_q75_box.jpg 1500w,/dhqwords/vol/15/1/000514/resources/images/figure17_hudb74eaa81b0b25a153812a740d2a92dc_230105_1800x0_resize_q75_box.jpg 1800w,/dhqwords/vol/15/1/000514/resources/images/figure17.jpg 3600w" 
     class="landscape"
     ><figcaption>
        <p>&ldquo;Recognition Steph Agent Avatars Blobs Tryptic Emergence.&rdquo; Source credit: Steph Hutchison.
        </p>
    </figcaption>
</figure>
<p>At the time of writing, it is generally recommended to conduct shorter takes of a movement history for several technical reasons. The first is that a system can glitch out which would then affect the system calibration and therefore render the rest of the mocap data unusable. This glitching can happen regardless of whether an inertial, optical or other system is used. Another reason is that most human bodies have a limit for how long they can continuously move. A trained body may be able to move continuously for a long time whereas a less active person may be fatigued after several minutes. Finally a long take will lead to a larger file that may be harder for an animation software to process and visualize. The mocap recording process will usually yield several files per session, so a consistent file naming scheme and stable storage system is essential. This may seem like a routine course of action but these steps can easily be forgotten when caught up in the moment or pressed for time. Finally, if a transcription and analysis of the movement is necessary, it may be useful to use systems such as Labanotation and Laban Movement Analysis. Both Labanotation and Laban Movement Analysis (LMA) were based on the work of modern dance pioneer Rudolf Laban <sup id="fnref:41"><a href="#fn:41" class="footnote-ref" role="doc-noteref">41</a></sup>. Labanotation is a graphical method of notating movement while LMA is a method for describing the quality and motivation of movement. The two systems can be used together or separately.</p>
<p>When recording, it is important to avoid simply documenting the memories through recording them and to remember that access, storage and preservation are part of this process as well <sup id="fnref:42"><a href="#fn:42" class="footnote-ref" role="doc-noteref">42</a></sup>. Although this fact may seem normal to any archivist, librarian or other information professional, it is not always obvious to laypersons. A significant difference between movement histories and artistic works using mocap is that a history focuses on authentically presenting what has been practiced or lived whereas artistic works typically strive to create a certain aesthetic or explore the differences between each performance of said work. Therefore the need to preserve multiple iterations of a history or striving for a technically perfect performance is eliminated.</p>
<p>On a technical level, there are some challenges specific to the preservation of mocap data and VR projects. In 2006 it was already noted that the quantity of mocap data produced from a session could be quite large and was not easily searchable <sup id="fnref:43"><a href="#fn:43" class="footnote-ref" role="doc-noteref">43</a></sup>.  <em>WhoLoDancE</em>  repository tools have optimized searchability of movement to a certain degree <sup id="fnref:44"><a href="#fn:44" class="footnote-ref" role="doc-noteref">44</a></sup> but a mainstream non-textual search engine has still yet to emerge in any discipline.</p>
<p>From a VR perspective, one must decide whether to use an existing game engine such as Unity or engineer a bespoke solution. Further objects such as 3D models, texture maps, supplementary audio and video may also need to be taken into consideration. Fundamentally, the digital curation of movement histories comprised of mocap data and VR projects is the same as for any other set of digital objects in that standardized file formats (i.e. fbx, bvh) used across multiple industries are typically used for the source objects. If anything, the most challenges lie in the lack of a standard file format for gaming engines such as Unity and Unreal. Constant changes in both hardware and software also make it difficult to pinpoint a single ideal long-term (&gt;25 years) file format for any of the objects contained within a VR project.</p>
<p>Given the rapid evolution of mocap and related technologies, the practical solution would be to focus on maintaining and migrating mocap data with the expectation that the visualization of that data will change over time. This is not to belittle the work that goes into creating VR environments. Instead, by focusing on movement as the primary source of information, the content can be made more accessible since it could be visualized in multiple ways. In an ideal setting, digital preservation practices such as using repositories built on the OAIS (Open Archival Information System) reference model and metadata standards would be expected. Implementing such practices, however, also comes with a significant cost <sup id="fnref1:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>. To help relieve the burden on data stewards, it may be useful for movement history creators to reference the strategies of existing artists&rsquo; toolkits <sup id="fnref:45"><a href="#fn:45" class="footnote-ref" role="doc-noteref">45</a></sup> and habits of new media artists <sup id="fnref:46"><a href="#fn:46" class="footnote-ref" role="doc-noteref">46</a></sup>. Furthermore, the previously theoretical concept of networked digital archives <sup id="fnref:47"><a href="#fn:47" class="footnote-ref" role="doc-noteref">47</a></sup> has already materialized in the form of widespread cloud computing services. The need for large and flexible storage space is better served by these services than traditional servers.</p>
<h2 id="conclusion-and-future-evolutions">Conclusion and Future Evolutions</h2>
<p>In the past, the recording of mocap and its subsequent visualization was technically complicated and costly. The process of mocap is still not as simple as making an audio recording, but evolutions in digital technology certainly make it easier. Similar things could be said about VR as new tutorials, hardware sets and software solutions continue to be released. For the immediate future, the most frequent use of movement histories might be as a complementary pairing with oral histories. Examples include a voiceover accompanying an illness narrative using mocap or a written account of a traumatic event accompanied by movement illustrating key moments. A future concern for this type of visualization is that of enabling accessibility for the visually impaired <sup id="fnref1:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup>. In the future, haptics might be able to help resolve vision impairment or reliance on text by conveying a different type of a sensory information. The initial work conducted at Deakin Motion Lab and the Institute for Intelligent Systems Research and Innovation (IISRI) that integrates social haptics with dance to create an embodied experience through kinesthesia <sup id="fnref1:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup> may well lead to being able to completely experiencing the physical memory of another individual.</p>
<p>On a more personal level, I had two major revelations while constructing my own movement history. The first discovery was that the reactions to my LH360 experience often reflected more about a viewer&rsquo;s previous perceptions about mental health and the communities they came from than any truth or understanding about the experience being shown to them. In other words, it was possible to induce reflexivity and intersubjectivity through presenting abstracted movement in an immersive environment. Patients would express relief and motivation that someone else was sharing an experience similar to theirs. Non-patients would question their previous perceptions of mental health patients. As I had never encountered this kind of reaction when similar movement was presented as a live performance, I was extremely surprised by this result. Presenting the VR experience and the publications about it have also brought me into contact with a wide range of artists and researchers working on similar projects.</p>
<p>These kinds of connections and discoveries are why I became an interdisciplinary practice-based researcher in the arts rather than an artist, archivist or other type of humanities researcher. Traditionally, artistic practice is supposed to speak for itself through the artistic work itself and academic research should be presentable through textual publications. Yet there are still forms of human-generated content that are too multi-faceted to be presented using only one kind of media or without contextual information. The  <em>LH360</em>  development process relied on methodologies such as autoethnography, reflective practice and user experience design as well as an understanding of various artistic technique. Furthermore, I received comments that viewing the VR experience enhanced an understanding of its associated publications and vice versa. Based on my experience with creating  <em>LH360</em> , I believe that forms of artistic expression can be utilized for purposes other than creating pure works of art or entertainment. The concept of an artistic practice-based researcher has been acknowledged in academic circles since the 1980s <sup id="fnref:48"><a href="#fn:48" class="footnote-ref" role="doc-noteref">48</a></sup>  <sup id="fnref:49"><a href="#fn:49" class="footnote-ref" role="doc-noteref">49</a></sup> which means that there are pioneers to look up to, a current peer community, and most likely an increase in this type of researcher. Regardless of whether one comes from an academic, artistic or hybrid approach, increasing developments in visual, aural, and haptic technologies should make it more feasible to reproduce non-verbal content that addresses all the senses. Over time, these innovations may lead to more multi-dimensional presentations of individual memories, historical events and entire cultures.</p>
<h2 id="appendix-choosing-hardware-and-software-options">Appendix: Choosing Hardware and Software Options</h2>
<p>The first thing that researchers should consider is their choice of mocap system. Ideally the system should be chosen based on the type of movement recorded, frequently used body part(s), portability and need for anonymity. It may be tempting to choose a system for its cost, convenience or availability but this can later create problems. A comparison of several systems is provided below to illustrate how the attributes of a system could be problematic or simply not useful for a researcher. The information is a combination of general knowledge and my personal experience using the listed system. It is by no means an extensive listing of all systems currently available on the market. For a more detailed look into mocap systems, I recommend reading the white paper released by Epic Games in May 2020.<br>
<strong>System</strong>    <strong>Type/Description</strong>    <strong>Unique Attributes</strong>    <strong>Benefits</strong>    <strong>Limitations</strong>       Microsoft Kinect  Depth camera/Motion sensor device  Originally a gaming device  Excellent for point cloud effects. Very cheap to buy and easy to setup.  May need multiple devices. Combining multiple data sources may require a custom software solution.      HTC Vive Tracker  Inertial/Repurposed trackers  Repurposes part of the HTC Vive virtual reality headset package  Very portable set-up. Can be used with third party software.  The weight of the sensors affected ease of movement      Notch Pioneer  Inertial/Motion sensors strapped to body  Individual sensors are attached to adjustable straps  Easy to put on and customize location of sensors. Reasonably priced and expandable.  Need smartphone or tablet for recording      Perception Neuron  Inertial/Wearable sensors in a suit  Sensors are attached to a &ldquo;suit&rdquo; of straps.  Fairly easy to put on and lightweight. Includes a comprehensive software platform.  May require frequent calibration. Weight of the battery pack can affect movement. Requires wireless router setup.      Rokoko Smartsuit  Inertial/Wearable sensors in a suit  Can be used almost anywhere.  Suit is comfortable and well-constructed. Price is about the same as a high-end laptop. Can be combined with face and hand mocap devices.  Separate suits need to be purchased for each person/size. Same battery pack and wireless router issue as Neuron. Cannot accurately measure changes in levels.      Optitrack Prime  Optical/Passive markers (reflective)  Comes in multiple configuration options.  Very good software and easy calibration. Can do almost any kind of motion with ease. Easy to use data clean-up tool.  The most expensive option. Requires a lot of resources (i.e. computing, electricity, space). Good technician with knowledge of marker setups, calibration, etc. is essential.   <br>
Another aspect to consider about choosing a hardware system is the software platform that accompanies it as well as the external software platforms that the hardware can interface with. Software can affect how cleanly motion is captured, provide easier ways to clean up and manipulate the data, or even enable faster sharing. In some situations, the hardware may not be the most robust, but the software makes the rest of the process easier. Most systems will come with their own proprietary software or require use of a licensed program. Ultimately the final decision should take into consideration whether one is recording full body large movements or smaller subtle movements, whether identity is an issue, if factual accuracy is required or if emotional expressiveness is the main priority.</p>
<p>Just like with any other data set, mocap data clean-up is extremely time consuming. The clean-up process usually consists of manipulating coordinates using either a linear graph editor or actual points in 3D space. While there are tools to help automate the process it is useful for researchers to review the raw capture and the cleaned-up version to ensure that vital details are not accidentally erased. By contrast, mapping the data to a 3D model to generate an animation can be relatively quick so long as the skeleton has been properly mapped.</p>
<p>Based on existing practices from commercial industry as balanced by financial limitations, I would recommend the following hardware, software, digital object and project planning starting points for researchers engaging in mocap and VR for the first time. Factors include cost of renting or purchasing a mocap system, personnel time and compensation and whether the project is being worked on a full or part-time basis.<br>
<strong>Ideal Expensive Setup</strong>        <strong>Moderate Investment Setup</strong>        <strong>Basic Setup</strong>        <br>
Marker-based/optical system with knowledgeable technician (Optitrack is good)  Programmer with knowledge of Unity or other visualization software  3D modeler for custom figure and environment  At least 2 sessions at 2 hours each for a short recording  3 months of dedicated time for visualization</p>
<p>Portable sensor suit system  Unity or software template  A way to customize a stock 3D figure and environment  At least 2 hours for the recording session  3 months of partial time for visualization</p>
<p>Notch, Vive or Kinect type system  WebVR template using frameworks such as A-frame  Stock 3D figure for free and mapped photos  Keep interview very short and shoot as best as possible  No deadline for completion of visualization</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>. Tebeau, M.  “Listening to the city: Oral history and place in the digital era”    <em>Oral History Review</em> , 40.1 (2013): 25-35.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Abrams, L.,  <em>Oral History Theory</em> . Milton Park and New York (2010).&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref3:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref4:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Ritchie, D.A.  <em>Doing Oral History: A practical guide</em> . Second Edition. New York, (2003).&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Williams, B.R.  “Doing Video Oral History.” In Ritchie, D. (ed.),  <em>The Oxford Handbook of Oral History,</em>  New York (2011), pp. 267-276.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>Lichtblau, A.  “Case Study: Opening Up Memory Space: The Challenges of Audiovisual History.” In Ritchie, D. (ed.),  <em>The Oxford Handbook of Oral History</em> , New York (2011), pp. 277-284.&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>New York Public Library.  “Dance Oral History Project”  Available at: <a href="https://www.nypl.org/oral-history-project-dance">https://www.nypl.org/oral-history-project-dance</a> [Accessed 24 April 2020].&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>Baum, W.K.,  <em>Oral History for the Local Historical Society,</em>  Third Edition Revised. Nashville (1978).&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>Matusiak, K. K., Taylor, A., Newton, C., and Polepeddi, P.  “Finding access and digital preservation solutions for a digitized oral history project: A case study”    <em>Digital Library Perspectives</em> , 33.2 (2017): 88-99.&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>USC Shoah Foundation.  “Dimensions in Testimony”  Available at: <a href="https://sfi.usc.edu/dit">https://sfi.usc.edu/dit</a> [Accessed 24 April 2020].&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>Hughes, J.,  “Taking the Leap: Including Video in Audio Oral Histories”    <em>Oral History Australia Journal</em> , 37 (2015): 63-71.&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>Whatley, S.  “Documenting Dance: Tools, Frameworks and Digital Transformation.” In Sant, T. (ed.),  <em>Documenting Performance : The Context and Processes of Digital Curation and Archiving</em> , London and New York (2017a), pp. 283-304.&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p>Corin, F.  “Steve Paxton&rsquo;s Material for the Spine: The Experience of a Sensorial Edition.” In M. Bleeker (ed.),  <em>Transmission in Motion: The Technologizing of Dance</em> , London and New York (2017), pp. 32-40.&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13">
<p>Open Ended Group.  “Ghostcatching”  Available at: <a href="http://openendedgroup.com/artworks/gc.html">http://openendedgroup.com/artworks/gc.html</a> [Accessed 24 November 2019].&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14">
<p>Whatley, S.  “The Poetics of Motion Capture and Visualisation Techniques: The Differences between Watching Real and Virtual Dancing Bodies.” In Reynolds, D. and Reason, M. (eds.),  <em>Kinesthetic Empathy in Creative and Cultural Practices</em> , London (2012), pp. 264-279.&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:15">
<p>Kozel, S., Gibson, R., and Martelli, B.  “The Weird Giggle: Attending to Affect in Virtual Reality”    <em>Transformations</em> , 31 (2018): 1-24.&#160;<a href="#fnref:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:16">
<p>Boyd, D.,  “Achieving the Promise of Oral History in a Digital Age” ,In Ritchie, D. (ed.),  <em>The Oxford Handbook of Oral History</em> , New York (2011), pp. 285-302.&#160;<a href="#fnref:16" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:17">
<p>Boucher, M. , “Virtual Dance and Motion-Capture”    <em>Contemporary Aesthetics,</em>  9. Available at: <a href="https://contempaesthetics.org/newvolume/pages/article.php?articleID=614">https://contempaesthetics.org/newvolume/pages/article.php?articleID=614</a> [Accessed 10 November 2019].&#160;<a href="#fnref:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:18">
<p>Jensenius, A. R.  “Some Video Abstraction Techniques for Displaying Body Movement in Analysis and Performance”    <em>LEONARDO</em> , 46.1 (2013): 53-60.&#160;<a href="#fnref:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:19">
<p>McCormick, J., Hossny, M., Fielding, M., Mullins, J., Vincent, J.B., Hossny, M., Vincs, K., Mohamed, S., Nahavandi, S., Creighton, D. and Hutchison, S.  “Feels Like Dancing: Motion Capture–Driven Haptic Interface as an Added Sensory Experience for Dance Viewing”    <em>LEONARDO</em> , 53.1 (2020): 45-49.&#160;<a href="#fnref:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:20">
<p>Cisneros, R.E., Wood, K., Whatley, S., Buccoli, M., Zanoni, M. and Sarti, A.  “Virtual Reality and Choreographic Practice: The Potential for New Creative Methods”    <em>Body, Space and Technology</em> , 18.1 (2019): 1-32.&#160;<a href="#fnref:20" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:21">
<p>. Cobb, S. V. G., Nichols, S., Ramsey, A. and Wilson, J. R.  “Virtual reality-induced symptoms and effects (VRISE)”    <em>Presence</em> , 8.2 (1999):169–186.&#160;<a href="#fnref:21" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:22">
<p>Madary, M. and Metzinger, T. K.  “Real virtuality: A code of ethical conduct; Recommendations for good scientific practice and the consumers of VR-technology”    <em>Frontiers in Robotics and AI</em> , 3.3 (2016): 1–23.&#160;<a href="#fnref:22" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:23">
<p>Frank, A. W.  <em>The Wounded Storyteller: Body, Illness, and Ethics</em> . Chicago (1995).&#160;<a href="#fnref:23" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:24">
<p>Hawkins, A. H.  <em>Reconstructing Illness: Studies in Pathography</em>  , 2nd edition, West Lafayette (1999).&#160;<a href="#fnref:24" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:24" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:25">
<p>Winslow, M. and Smith, G.  “Medical Ethics and Oral History.” In Ritchie, D. (ed.),  <em>The Oxford Handbook of Oral History</em> , New York (2011), pp. 372-392.&#160;<a href="#fnref:25" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:25" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:26">
<p>Mermikides, A. and Bouchard, G. (Eds.).  <em>Performance and the Medical Body.</em>  London (2016).&#160;<a href="#fnref:26" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:27">
<p>Jamison, K. R.  <em>An Unquiet Mind.</em>  New York (1996).&#160;<a href="#fnref:27" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:28">
<p>. Davies, K.  “Silent and Censured Travellers&rdquo;? Patients&rsquo; Narratives and Patients&rsquo; Voices: Perspectives on the History of Mental Illness since 1948”    <em>Social History of Medicine</em> , 14.2 (2001): 267-92.&#160;<a href="#fnref:28" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:28" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:29">
<p>. The British Library.  “Oral Histories of Disability and Personal and Mental Health”  Available at: <a href="https://www.bl.uk/collection-guides/oral-histories-of-personal-and-mental-health-and-disability">https://www.bl.uk/collection-guides/oral-histories-of-personal-and-mental-health-and-disability</a> [Accessed 01 May 2020].&#160;<a href="#fnref:29" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:30">
<p>Monash University, School of Philosophical, Historical and International Studies.  “Australian Generations Oral History Project”  Available at: <a href="https://www.monash.edu/arts/philosophical-historical-international-studies/australian-generations">https://www.monash.edu/arts/philosophical-historical-international-studies/australian-generations</a> [Accessed 01 May 2020].&#160;<a href="#fnref:30" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:31">
<p>The Schizophrenia Oral History Project. Available at: <a href="https://schizophreniaoralhistories.com/">https://schizophreniaoralhistories.com/</a> [Accessed on 01 May 2020].&#160;<a href="#fnref:31" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:32">
<p>Carel, H., Illness:  <em>The Cry of the Flesh (The art of living),</em>  New York (2013).&#160;<a href="#fnref:32" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:32" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:33">
<p>Depression and Bipolar Support Alliance: DBSA.  “DBSA SUPPORT GROUPS: An Important Step on the Road to Wellness”  Available at: <a href="https://www.dbsalliance.org/wp-content/uploads/2019/02/DBSA_SUPPORT_GROUPS.pdf">https://www.dbsalliance.org/wp-content/uploads/2019/02/DBSA_SUPPORT_GROUPS.pdf</a> [Accessed 13 May 2020].&#160;<a href="#fnref:33" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:34">
<p>Halprin, A.  “Portrait.” Available at: <a href="https://digitalcollections.nypl.org/items/65a512d0-aa03-0133-bcae-60f81dd2b63c">https://digitalcollections.nypl.org/items/65a512d0-aa03-0133-bcae-60f81dd2b63c</a> [Accessed 22 November 2019].&#160;<a href="#fnref:34" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:35">
<p>Halprin, A.  “Intensive Care, Reflections on Death and Dying.” Available at: <a href="https://digitalcollections.nypl.org/items/3bcc1450-342a-0131-a70d-685b35830967">https://digitalcollections.nypl.org/items/3bcc1450-342a-0131-a70d-685b35830967</a> [Accessed 22 November 2019].&#160;<a href="#fnref:35" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:36">
<p>New York Live Arts.  “Still/Here | New York Live Arts”  Available at: <a href="https://newyorklivearts.org/repertory/stillhere/">https://newyorklivearts.org/repertory/stillhere/</a> [Accessed 24 November 2019].&#160;<a href="#fnref:36" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:37">
<p>Kozel, S. Closer:  <em>Performance, Technologies, Phenomenology</em> . Cambridge (2007).&#160;<a href="#fnref:37" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:37" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:38">
<p>Smith-Nunes, G., Shaw, A., and Neale, C.  “PainByte: Chronic Pain and Biomedical Engineering Through the Lens of Classical Ballet &amp; Virtual Reality”  <em>Proceedings of the Twelfth International Conference on Tangible, Embedded, and Embodied Interaction</em> , Stockholm, Sweden (March 2018).&#160;<a href="#fnref:38" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:39">
<p>Vincs, K.  “Virtualizing Dance.” In Rosenberg, D. (ed.),  <em>The Oxford Handbook of Screendance Studies</em> , New York (2016), pp. 263-282.&#160;<a href="#fnref:39" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:40">
<p>Hutchison S. and Vincs, K.  “Dancing in Suits: A Performer&rsquo;s Perspective on The Collaborative Exchange Between Self, Body, Motion Capture, Animation and Audience”  in Leland, K., Fisher, L. and Harley, R. (eds.),  <em>Proceedings of the 19th International Symposium of Electronic Art</em> , ISEA2013, Sydney (2013), pp. 1-4.&#160;<a href="#fnref:40" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:41">
<p>Eddy, M.  “A brief history of somatic practices and dance: historical development of the field of somatic education and its relationship to dance”    <em>Journal of Dance and Somatic Practices</em> , 1.1(2009): 5-27.&#160;<a href="#fnref:41" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:42">
<p>Sant, T. (ed)  <em>Documenting Performance : The Context and Processes of Digital Curation and Archiving</em> . London and New York (2017).&#160;<a href="#fnref:42" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:43">
<p>Hachimura, K.  “Digital Archiving of Dancing”    <em>Review of the National Center for Digitization</em> , 8 (2006): 51-66.&#160;<a href="#fnref:43" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:44">
<p>Cisneros, R.E., Stamp, K., Whatley, S. and Wood, K.  “WhoLoDancE: Digital tools and the dance learning environment”    <em>Research in Dance Education</em> , 20 <em>.</em> 1 (2019): 54-72.&#160;<a href="#fnref:44" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:45">
<p>Skinner, C., Tyas, P., and Street, A.  <em>Artists in the archives toolkit: A toolkit for best practice</em> . Wiltshire (2015). Available at: <a href="https://theartsinwiltshire.files.wordpress.com/2015/04/artist-archive-doc-final.pdf">https://theartsinwiltshire.files.wordpress.com/2015/04/artist-archive-doc-final.pdf</a>. [Accessed 1 October 2019].&#160;<a href="#fnref:45" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:46">
<p>Post, C.  “Preservation Practices of New Media Artists: Challenges, Strategies, and Attitudes in the Personal Management of Artworks”    <em>Journal of Documentation</em> , 73.4 (2017): 716-732.&#160;<a href="#fnref:46" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:47">
<p>MacDevitt, J.  “The User-Archivist and Collective (In)Voluntary Memory: Read/Writing the Networked Digital Archive.” In Gardiner, H. and Bailey, C. (eds.),  <em>Revisualising Visual Culture</em> , London (2012), pp. 109-124.&#160;<a href="#fnref:47" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:48">
<p>Candy, L.  “Practice Based Research: A Guide. CCS Report: 2006-V1.0 November.” Available at: <a href="https://www.researchgate.net/publication/257944497">https://www.researchgate.net/publication/257944497</a> [Accessed 27 February 2020].&#160;<a href="#fnref:48" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:49">
<p>Nelson, R.  <em>Practice as Research in the Arts: Principles, Protocols, Pedagogies, Resistances</em> . Houndsmill, Pelgrave (2013).&#160;<a href="#fnref:49" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry><entry><title type="html">Audiated Annotation from the Middle Ages to the Open Web</title><link href="https://rlskoeser.github.io/dhqwords/vol/15/1/000512/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://rlskoeser.github.io/dhqwords/vol/15/1/000512/</id><author><name>Tanya E. Clement</name></author><author><name>Liz Fischer</name></author><published>2021-03-05T00:00:00+00:00</published><updated>2021-03-05T00:00:00+00:00</updated><content type="html"><![CDATA[<h2 id="annotations-in-textual-theory-paratext-marginalia-metadata">Annotations in Textual Theory (paratext, marginalia, metadata)</h2>
<p>Scholars have long theorized annotation in the creation and analysis of literature as  <em>paratext</em> ,  <em>marginalia</em> , and  <em>mark-up</em>   <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>  <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>  <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>  <sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>  <sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>  <sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>  <sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>. Paratext includes peritext, such as those materials in the interstices of the book, such as chapter titles or notes, and epitext. Epitexts are other texts outside of the central text that influence the ideal or typical reader&rsquo;s interpretation, such as &ldquo;interviews, conversations, and confidences&rdquo; (Genette 10). Revealing the political and social perspectives about the &ldquo;ideal&rdquo; reader that such contexts engage, Genette includes &ldquo;contextual paratexts&rdquo; in this category that include politicized information about the author, such as &ldquo;Proust&rsquo;s part-Jewish ancestry and his homosexuality&rdquo; (8). Not surprisingly then, paratext is authorized by the author or by his (in this case) social demographics or discourse community: it &ldquo;is always the conveyor of a commentary that is authorial or more or less legitimated by the author&rdquo;, Gennette writes (2). Indeed, &ldquo;by definition, something is not a paratext unless the author or one of his associates accepts responsibility for it&rdquo; (9). Authorized by the surrounding discourse community, paratext is also authorized by the text itself through its physical association. Paratext is &ldquo;situated in relation to the location of the text itself: around the text and either within the same volume or at a more respectful (or more prudent) distance&rdquo; (4). Genette&rsquo;s concept of paratext essentializes authorial intention and the immediacy of the textual object.</p>
<p>Marginalia in literary study is also interpreted based on its proximity to the text as well as the extent to which it is authorial or authorized. H.J. Jackson argues that marginalia of significance includes notes inside of books, not outside of books, citing &ldquo;significant differences between notes made on separate sheets of paper or in a notebook and notes made in the book that becomes part of the book and accompany it ever after&rdquo; <sup id="fnref1:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>. Beyond its locative status, marginalia is more or less important if the person creating the notes is &ldquo;authorial.&rdquo; According to Jackson, notes by the authors themselves are the most significant, then marginalia by other authors, of equal or greater literary importance, and finally, granted the least status, are general readers&rsquo; notes: &ldquo;Our own notes we like, or have learned to live with,&rdquo; Jackson writes, &ldquo;those we resist are always written by somebody else&rdquo; (235). Marginalia plays a minor role in textual theory by reflecting an association with other authors in reception theory and in a history of reading or as biography when the author corrects writing about themselves (243), but Jackson generally argues that marginalia has not been considered significant enough to study because marginalia is generally non-authorial and often ephemeral, not physically attached to the authorized text.</p>
<p>Metadata, or data about data, are also significant forms of annotation in recent literary study, deemed less and more important based on their authority and textual proximity. In the digital realm, activities such as searching and retrieving texts in library systems, sharing scholarly or pedagogical work with students and researchers, and using artificial intelligence and machine learning to discover patterns are activities that share a common reliance on metadata. Much like paratext in the publishing industry, metadata has more official functions in libraries (for access) and archives (for context) <sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>. In both cases, metadata is information that is lacking in the &ldquo;information object&rdquo; within a sociotechnical system. In the library setting, metadata might include the author name or genre information, which is gathered in order to facilitate finding that object. In an archive, metadata might include a previous researcher&rsquo;s notes, which can provide important contextual clues for future researchers. In general, Gilliland notes that &ldquo;[i]n all these diverse interpretations, metadata not only identifies and describes an information object; it also documents how that object behaves, its function and use, its relationship to other information objects, and how it should be and has been managed over time&rdquo; <sup id="fnref1:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>. Consequently, metadata&rsquo;s function is entangled with making an &ldquo;information object&rdquo; system-aware, whether that system is a human-readable metadata standard or a technological process. In contrast, unauthorized, &ldquo;user-generated&rdquo; metadata such as community-generated &ldquo;folksonomies,&rdquo; while a nice record of general user&rsquo;s experience, are system-adverse since such metadata often do not fit with the established socio-technical system at hand. Indeed, such out-of-system metadata is &ldquo;idiosyncratic&rdquo; and, therefore, &ldquo;untrustworthy&rdquo;, Gilliland argues, because it can &ldquo;negatively affect interoperability between metadata and the resources it is intended to describe&rdquo; <sup id="fnref2:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>. Like paratextual and marginal annotations, metadata has been considered meaningful in literary study when they are authorized and in proximity to the text. <sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup></p>
<p>In each of these examples (paratext, marginalia, and metadata), the sociotechnical systems in which annotations circulate represent discourse fields where authority is crucial to the significance or signifying capacity. Yet, there are other, under-theorized examples in literary study in which annotations reflect the individual, unauthorized reader&rsquo;s interpretation of an absent text, and the reading experience is part of an unauthorized, distributed, and decentralized system. Below, we discuss two seemingly disparate examples of such annotations across time, in the context of medieval psalm commentary and open web IIIF (International Image Interoperability Framework) standards for annotations. We are calling these  <em>audiated</em>  annotations to emphasize the three principles these kinds of unauthorized and extra-textual annotations share: namely, what we are calling audiated annotations are often (1) self-described and independent, removed from the object of comment itself and reflecting a textual condition <sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup> in which (2) annotations are understood as compound objects that are (3) embedded in a particular, user-generated reading experience rather than an authorized, ideal reading experience.</p>
<h2 id="annotations-in-medieval-literary-culture">Annotations in Medieval Literary Culture</h2>
<p>In pre-print, medieval literary culture, annotations still took place next to full texts, but commentary forms were not reliant on the centrality of an ideal text. In the medieval tradition, orality and aurality were central to literacy, and psalm commentaries circulated in an unauthorized, distributed, and decentralized community of texts, readers, orators, and listeners. For Benedictine monks, in particular, Psalms were a major part of medieval monastic life, and weekly recitation of the psalms was recommended. Like later proteges learning to audiate using Edwin Gordon&rsquo;s theories of musical education <sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>, an ability to memorize the psalms was seen as an early indication of intellect among monks in training <sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>. Psalms were not simply recited like other prayers and readings; psalms were nearly always sung. As a natural consequence of years of daily recitation, monks were expected to have the verse and tune of all 150 psalms memorized.</p>
<p>Consequently, the practice of audiation, of using inner-hearing to imagine what a song sounds like, was key to medieval psalm-singing. In medieval devotional practice, there is a concept of &ldquo;the inner senses&rdquo; which operate separately from, but are related to, the physical senses of sight and sound, an inner sense of sight at the origin of the phrase &ldquo;the mind&rsquo;s eye.&rdquo; Beth Williamson discusses the way the physical sense of sight and the inner sense of hearing work together in medieval music, especially psalm-singing (2013). Psalms often have two sections between which is a pause, represented on the page as a space and musically as a breath. Williamson says this pause is not an absence, but a shift in the site of meaning:</p>
<blockquote>
<p>[A]t such a point, the music may not be sounding, but it has not stopped. The singers are aware, within their own interiority, of its continuation, and though they do not hear in their physical ears they hear it still inwardly. In this moment of silence, music does not disappear, but functions temporarily — and temporally — on a different level. <sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup></p>
</blockquote>
<p>What Williamson describes is similar to Gordon&rsquo;s descriptions of audiation — the presence of meaning (the concept and construct of music) in the absence of sound. While Williamson regards that state of inner hearing as temporary in the moment of performance, the implication is that the singers, like Gordon&rsquo;s students, hear the psalms when reading the text.</p>
<p>Annotations were common in the psalm commentary tradition. The psalter was the most commented upon book of the Middle Ages, and all monks would have had access to at least some kind of commentary in their library (Dyer 1989). There are several ways these commentaries are presented on the page. The most standard presentation of medieval commentary is the way the  <em>Glossa ordinaria</em>  (the standard biblical commentary) is usually written: the main text is in one column in a large script, with commentary in the surrounding margins in a smaller script. Privileging the main text, this layout looks much like texts today. Because the practice of audiation was a common mode of interacting with the psalms, other commentary forms perform audiatated annotations.</p>
<p>The popular psalm commentary of Gilbert of Poitiers, for example, privileges the commentary over the primary text. The Gilbert Psalter comes in two layouts:  <em>cum textu</em>  (&ldquo;with the text&rdquo;) and  <em>catena</em>  (&ldquo;chain&rdquo;). In the  <em>cum textu</em>  format, the page is divided into two columns: the inner column (near the spine) for the main psalm text, and the outer for the commentary. In this format, the relative width of the columns is adjusted, and the main text is sometimes abbreviated to ensure the main text and relevant section of commentary stay in sync (Salomon 2012, 43). This layout puts the main text and the commentary on a more equal status: the main text is still usually larger, but takes up less of the page, is not centered and may be altered to accommodate the commentary. Unlike the  <em>cum textu</em>  format, the  <em>catena</em>  format of the Gilbert Psalter places more emphasis on the commentary. The page is still divided into two columns, but the commentary occupies both. When the commentary for a new verse starts, the first few words of the psalm text are given in extreme abbreviation. Aside from the first new words, the psalm text itself is absent from the page. Theresa Gross-Diaz describes the appearance of this layout in her study of the Gilbert Psalter as follows:</p>
<blockquote>
<p>[T]he first words of the verse given in full, the end of the text sometimes disintegrating into a string of initials in the interest of economy of space, time, and parchment. Despite this interpolated repetition of the psalms in this &lsquo;simple&rsquo; format, one would be hard-pressed to reconstruct each psalm from the lemmata provided, since the order of words and even of verses is often scrambled beyond recognition. <sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup></p>
</blockquote>
<p>Such extreme abbreviation of a commented-upon text is only possible if the reader either has a separate copy of the text to use side-by-side or can call the text to mind with minimal prompting. In the case of the psalms, the latter is more likely: as discussed, readers who memorized the psalms as text and as sound encounter the psalms aurally with the mind&rsquo;s ear. A medieval reader who knows his psalms coming to Gilbert&rsquo;s commentary does not need the psalm to be present on the page or audibly because it is present in the mind.</p>
<p>Gilbert&rsquo;s Psalter offers an early example that demonstrates how commentaries are at a remove from the text through extreme abbreviation, but also how these audiated annotations function as compound objects that reflect a particular, rather than a general, reader&rsquo;s experience. Commentaries in the  <em>catena</em>  layout are &ldquo;chains &ldquo;not only in the sense that they move on the page as an unbroken string of commentary but also in the sense that they link together previous commentaries. Where a  <em>Glossa ordinaria</em> -style commentary isolates the words of each commentator — in one corner what Augustine said, in another corner what St. Hippolytus said — the  <em>catena</em>  makes a new, continuous commentary text by pulling together pieces of existing, multi-authored commentaries. As David Salomon says, a  <em>catena&rsquo;s</em>  author &ldquo;joins the links in the chain but does not necessarily have a hand in constructing those links themselves&rdquo; (47). It is important to note that Gilbert&rsquo;s Psalter is not, according to Saloman and Gross-Diaz, a &ldquo;true&rdquo; or typical  <em>catena</em>  for this reason since the commentary seems to be his own rather than just pieces of existing commentaries. Finally,  <em>catena</em>  psalm commentary is embedded in a particular rather than a general reading experience. While psalm commentaries, like Gilbert&rsquo;s, are sometimes &ldquo;authorized&rdquo; in that they were widely read and copied, some  <em>catena</em>  texts were unauthorized, created by individuals for their private use and not widely copied or, currently, discoverable (Salomon 2012).</p>
<h2 id="annotations-in-iiif">Annotations in IIIF</h2>
<p>Today, the most ubiquitous audiated annotations are web-based. Audiated (unauthorized and extra-textual) annotations in open Web standards such as the IIIF (International Image Interoperability Framework) extend the use, shareability, and accessibility of online cultural artifacts. The IIIF consortium adopts the principles of <a href="http://linkeddata.org/">Linked Data</a> and the <a href="http://www.w3.org/TR/webarch/">Architecture of the Web</a> via a <a href="https://iiif.io/model/shared-canvas/1.0">Shared Canvas data model</a> and the use of <a href="http://www.w3.org/TR/json-ld/">JSON-LD</a> &ldquo;in order to provide a distributed and interoperable framework&rdquo; <sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup> for the presentation of Web content. Essentially, linked data on the Web are interrelated&ndash;they are data that refers to and &ldquo;are aware of&rdquo; other similar data — making semantic queries across platforms more productive and useful. The IIIF standard places particular emphasis on facilitating the creation of links (or references between bits of data) that are unauthorized or user-generated annotations of content because often, such contextual information is not well-described by current metadata schemas, especially in the context of cultural heritage institutions such as libraries, archives, and museums. The &ldquo;Introduction to IIIF&rdquo; <sup id="fnref:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup> claims:</p>
<blockquote>
<p>While a multitude of different standards and practices are expected and even desirable for descriptive metadata, they do nothing for the content itself. There has been no standardized way of referring to a page of a book, or a sentence in a handwritten letter, from one digitised collection to the next. Descriptive metadata standards don&rsquo;t help us. It is not their job to enable us to refer to parts of the work, down to the tiniest detail - interesting marginalia, a single word on a page - and make statements about those parts in the web of linked data. It is not their job to present content, or share it, or refer to it.</p>
</blockquote>
<p>This statement functions as a kind of manifesto for a distributed and unauthorized annotation environment that is not beholden to the kind of authorizl, often ideal-text-centric, metadata standards on which library, archive, and museum systems typically depend.</p>
<p>In IIIF, the manifest is the primary document. The manifest is a plain text file written in JSON that privileges a reader&rsquo;s perception of how an object should be presented on a Web page. Manifests can be created and shared by institutions and read by presentation software, but IIIF manifests can be created or copied and reshared by readers who may wish to reorient how that object is presented online. By referencing or creating links to only the tiniest detail of an object such as an image or an audio file (the brightest star in Van Gogh&rsquo;s &ldquo;Starry Night&rdquo; or one phrase in a poem spoken by Maya Angelou), that reader can create a manifest that reorients completely how an object is read or accessed. In a IIIF manifest, all of the instructions about how the object should be presented are conceptualized as annotations on a canvas. Even what we might consider the main object of study — an image of the page of a book, a photograph of a painting, or a snippet of sound or video — is noted in the JSON manifest as an annotation to this canvas of the reader&rsquo;s mind. In this way, the idea of the idealized text is reoriented toward a privileging of the reader&rsquo;s instructions in the manifest about the presentation of that object.</p>
<p>The IIIF manifest is a capacious document, containing multiple links brought together to create a particular reading, viewing, or listening experience; it reflects the object as constituting many parts, as a composite. For example, the manifest for a particular presentation of a medieval manuscript might include a canvas that links images of every manuscript page and the binding, multispectral images showing text that had been erased and written over, transcriptions, and explanatory notes that refer to each. This textual constellation, linked from the manifest and presented on the Web page seamlessly by software, may or may not be created or owned by the same people. Pieces of manuscripts that were cut apart and sold to different libraries can be reunited virtually on a new page as directed by a IIIF manifest.<sup id="fnref:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup>  If a reader&rsquo;s primary object of interest is the digitized Gilbert Psalter manuscript in the Parker library, they can create annotations describing the large, decorated initials in the book, and present those annotations on the Web using IIIF with or without the manuscript image. Without the image, the reader would not see the illuminated initials, but audiated annotations describing them can still be shown in spatial relation to one another. With IIIF, readers can create audiated annotations for the present absent text. In both Medieval and online cultures, audiated annotations circulate as composite, unauthorized, and decentralized objects for study.</p>
<h2 id="conclusions">Conclusions</h2>
<p>In the digital environment, collections that might include manuscripts or musical, spoken, or bioacoustical artifacts will require audiated annotations to be discoverable. Often, for privacy or copyright reasons, audiovisual cultural heritage objects such as historical audio and film are not freely available online. In the analog world, without annotations, we cannot find or know what is in or on a sound or image artifact unless someone has annotated a name on the back of a polaroid or on a written label on an audio reel or a cassette tape. Similarly, without metadata or descriptive information embedded in or associated with a digital file, we cannot search for or discover that object. As a result, audiated annotations — annotations that are unauthorized, decentralized, and composite — sometimes serve as the only access point into important cultural objects in literary study.</p>
<p>Likewise, annotations on an audio object that may never circulate freely for copyright or privacy reasons can be described in temporal relation to that absent object and shared widely, like a playlist on an old mixed tape or liner notes on an album that points to and tells us more about the present, absent content. Such community-based, unauthorized sharing of scholarly annotations already exists in free and minimally produced scholarly editions using Jekyll to produce GitHub pages emphasized by scholarly editors who follow the tenets of Minimal Computing in DH including Minimal Editions (Minimal Computing n.d.), Wax (with IIIF-based static exhibits mimicking Omeka&rsquo;s functionality) (Nyröp n.d.), and the Versioning Machine (Schreibman 2015). The AudiAnnotate project is developing similar workflows for producing the same kind of community-based and composite annotations for audio (HiPSTAS 2020). This ability to share audiated annotations on an inaccessible object increases discoverability and that object&rsquo;s circulation in our cultural imaginary through scholarship, teaching, and learning. Untethered from a &ldquo;main text&rdquo;, which is decentered as yet another annotated link on the IIIF canvas, readers can compile any Frankenstein canvas, that beautiful corpse.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Audenaert, N. and Furuta, R.  “What Humanists Want: How Scholars Use Source Materials”  in:  <em>Proceedings of the 10th Annual Joint Conference on Digital Libraries, JCDL &lsquo;10. ACM, New York, NY, USA (2010)</em> , pp. 283–292. <a href="https://doi.org/10.1145/1816123.1816166">https://doi.org/10.1145/1816123.1816166</a>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Bernstein, C.  “Close Listening: Poetry and the Performed Word.”  <em>Oxford University Press</em> .&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Bernstein, C.  “Attack of the Difficult Poems: Essays and Inventions.”  <em>University of Chicago Press</em> .&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Bradley, J. and Vetch, P.  “Supporting Annotation as a Scholarly Tool - Experiences From the Online Chopin Variorum Edition.”  <em>Literary and Linguistic Computing</em>  22, 225–241. <a href="https://doi.org/10.1093/llc/fqm001">https://doi.org/10.1093/llc/fqm001</a>&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>Bray, J. Handley, M. and Henry, A.C.  “Ma(r)king the text: the presentation of meaning on the literary page”    <em>Ashgate Pub. Co</em> , Aldershot; Burlington, VT.&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>Hillesund, T. Digital reading spaces: How expert readers handle books, the Web and electronic paper. First Monday 15. <a href="https://doi.org/10.5210/fm.v15i4.2762">https://doi.org/10.5210/fm.v15i4.2762</a>&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>Jackson, H.J. Marginalia: Readers Writing in Books.  <em>Yale University Press</em> .&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>Gilliland, A.J.  “Setting the Stage”  in: Baca, M. (Ed.), Introduction to Metadata.  <em>Getty Publications</em>  (2008), pp. 1–19.&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>One exception is the history of conversations in the Text Encoding Initiative (TEI) surrounding stand-off markup. See <sup id="fnref:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup>, and <sup id="fnref:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup>.&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>McGann, J. J. The Textual Condition.  <em>Princeton University Press</em> , 1991.&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>Gordon, E.  “Learning Sequences in Music: A Contemporary Learning Theory”    <em>GIA Publications</em> , Chicago (2007).&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p>Dyer, J.  “The Singing of Psalms in the Early-Medieval Office”    <em>Speculum</em>  64, 535–578. <a href="https://doi.org/10.2307/2854183">https://doi.org/10.2307/2854183</a>&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13">
<p>Williamson, B.  “Sensory Experience in Medieval Devotion: Sound and Vision, Invisibility and Silence”    <em>Speculum</em>  88, 1–43.&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14">
<p>Gross-Diaz, T. The Psalms Commentary of Gilbert of Poitiers: From Lectio Divina to the Lecture Room.  <em>BRILL</em> .&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:15">
<p>Appleby, M., Crane, T., Stroop, J. and Warner, S.  “IIIF Presentation API 3.0 BETA DRAFT”&#160;<a href="#fnref:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:16">
<p>Crane, T.  “An Introduction to IIIF”    <em>Digirati</em> . <a href="https://resources.digirati.com/iiif/an-introduction-to-iiif/">https://resources.digirati.com/iiif/an-introduction-to-iiif/</a> (accessed 11.30.19).&#160;<a href="#fnref:16" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:17">
<p>For an example of this, see Lisa Fagin Davis&rsquo; work with books &ldquo;broken&rdquo; by Otto Ege in the early twentieth century (Davis 2016).&#160;<a href="#fnref:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:18">
<p>Spadini, E. and Turska, M.  “XML-TEI Stand-off Markup: One Step Beyond.” Digital Philology: A Journal of Medieval Cultures 8, 225–239. <a href="https://doi.org/10.1353/dph.2019.0025">https://doi.org/10.1353/dph.2019.0025</a>&#160;<a href="#fnref:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:19">
<p>“TEI Standoff Markup Workgroup.” Stand-off Markup.&#160;<a href="#fnref:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry><entry><title type="html">Audiovisualities out of Annotation: Three Case Studies in Teaching Digital Annotation with Mediate</title><link href="https://rlskoeser.github.io/dhqwords/vol/15/1/000507/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://rlskoeser.github.io/dhqwords/vol/15/1/000507/</id><author><name>Joel Burges</name></author><author><name>Solvegia Armoskaite</name></author><author><name>Tiamat Fox</name></author><author><name>Darren Mueller</name></author><author><name>Joshua Romphf</name></author><author><name>Emily Sherwood</name></author><author><name>Madeline Ullrich</name></author><published>2021-03-05T00:00:00+00:00</published><updated>2021-03-05T00:00:00+00:00</updated><content type="html"><![CDATA[<h2 id="introduction1">Introduction<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></h2>
<p>It has long been a premise in the study of media that multiple senses are in play when we view movies, watch television, listen to live or recorded music, read or hear poems read aloud, and consume advertising, whether in print or on screens <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>  <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>  <sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>. Since the late twentieth century, the interplay of seeing and hearing has yielded richly variegated writing and thinking about, on the one hand, vision and visuality and, on the other, the acoustic, the audible, and the aural. In this essay, we pursue this interplay into the digital humanities. More specifically, we advance the concept of  <em>audiovisualities</em>  in order to describe that interplay in the context of digital annotation of time-based media. In these media, seeing and hearing are inseparable and our goal is to understand how processes of digital annotation can help scholars and students investigate this entanglement. To do this, we describe a platform we have named  <em>Mediate: An Annotation Tool for Audiovisual Media</em> , which was developed with the Digital Scholarship Lab at the University of Rochester and has been used in undergraduate courses across the humanities and social sciences there. In these settings,  <em>Mediate</em>  has enabled our students and ourselves to see and hear the data of our eyes and ears in reflexively collective and recursively interdisciplinary ways. This collective process in a variety of classrooms is what has brought us — three faculty members (two tenure track, one instructional track), two students (an undergraduate and a graduate student), and two staff members from the library (the programmer and the Director of the Digital Scholarship Lab) — to the concept of audiovisualities we explore in this essay.</p>
<p>That concept cannot be disaggregated from visual studies and sound studies, two fields that have developed alongside their more disciplinary counterparts in art history, film studies, musicology, and music theory. Broadly conceived, visual studies offers a means of understanding the expansive domain of the visual beyond what disciplines such as art history or film studies allow us to see. Wildly diverse in the directions it has taken since emerging in the late twentieth century, one of the central legacies of visual studies has been the concept of &ldquo;visuality,&rdquo; or &ldquo;sight as a social fact,&rdquo; which cannot be disaggregated from the act of &ldquo;vision,&rdquo; or &ldquo;sight as a physical operation&rdquo; <sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>. Adapting W. J. T. Mitchell, we can think of visuality as a &ldquo;dialectical concept&rdquo; in which the study of &ldquo;visual culture cannot rest content with a definition of its object as the social construction of the visual field, but must insist on exploring the chiastic reversal of this proposition,  <em>the visual construction of the social field</em> . It is not just that we see the way we do because we are social animals, but also that our social arrangements take the forms they do because we are seeing animals&rdquo; <sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>.</p>
<p>As in visual studies, scholars working in sound studies, which took identifiable shape in the early twenty-first century, refuse to be content with a definition of its object as solely the social construction of the sonic field, but also account for the sonic construction of the social field <sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>  <sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>  <sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup>  <sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup>. Pushing beyond logocentric and ocularcentric theoretical frameworks in various established disciplines, sound studies treats the audible and the aural, as Jonathan Sterne has put it, as &ldquo;an artifact of the messy and political human sphere&rdquo; <sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>. Pondering that &ldquo;artifact,&rdquo; three researchers exploring what they call &ldquo;digital sound studies,&rdquo; including one of the authors of this article, have posed a question about the assumed modes in scholarship itself. &ldquo;How,&rdquo; they ask, &ldquo;can scholars write about sound  <em>in sound</em> ?&rdquo; <sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>.</p>
<p>A key effect that sound studies has had on visual studies has been to remind those working in the latter that the sights we see often go hand in hand with the sounds we hear. While Sterne&rsquo;s work in  <em>The Audible Past</em>  has systematized and historicized this sight-sound relation, film theorists such as Michel Chion <sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup> and Kaja Silverman <sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup> and media archeologists such as Siegfried Zielinski <sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup>  <sup id="fnref:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup> and Bernard Stiegler <sup id="fnref:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup> have generatively tracked the dialectic of the visual and the audible in their work. We think of this dialectic as producing the audiovisualities — the physically and culturally interpenetrating modes of audiovisual experience and audiovisual inscription where hearing and seeing remediate one another for all of us as sensory and social subjects — that this essay aims to chart in relation to digital annotation in  <em>Mediate</em> .</p>
<p>At the University of Rochester, students have used  <em>Mediate</em>  to annotate cinematic, televisual, musical, literary, and commercial media in courses housed in the Film and Media Studies Program, the Musicology Department, and the Department of Linguistics. In these courses, the audiovisual specificities of a given medium become radically legible to students in the data they yield by annotating in  <em>Mediate</em> . Further, as we will show in this article, in exploring the medium-specific qualities of film or music or advertising in their unique material forms, cultural contexts, and social functions, we have unexpectedly ended up in a broader concept of audiovisuality that cuts across disciplinary differences. To riff on Mitchell one last time,  <em>Mediate</em>  allows us to examine the audiovisual construction of the social field as much as the social construction of the audiovisual field. Through the annotation it supports,  <em>Mediate</em>  provides a platform in which that field is no longer immediately intuited through our senses, but turned into an object of analysis — an audible and visible &ldquo;exteriority&rdquo; <sup id="fnref1:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup> that allows us to grasp the interplay of seeing and hearing beyond the often self-contained ways in which we process sensory data internally and individually.</p>
<h2 id="_mediate_--and-the-audiovisual-state-of-digital-annotation"><em>Mediate</em>  and the Audiovisual State of Digital Annotation</h2>
<p><em>Mediate</em>  arose out of Joel Burges&rsquo;s desire to have a digital tool that would enable the collection of large amounts of data about how time works on television. Originally working with Nora Dimmock, Jeff Suszczynski, and Joshua Romphf by experimenting with digital humanities projects in the classes &ldquo;The Poetics of Television,&rdquo; &ldquo;Film History, 1989-Present,&rdquo; and &ldquo;Clocks and Computers: Visualizing Cultural Time&rdquo; between 2012 and 2016, this desire gave way to the still ongoing project of developing a digital annotation tool for audiovisual media that would be of more general use. As it did, we moved from combining software such as Jubler, DaVinci Resolve, and Adobe Encore to building our own platform, primarily through the labor of Romphf <sup id="fnref:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup>.  <em>Mediate</em>  is a web-based platform that allows users to upload audiovisual media; produce real-time notes; generate automated and manual annotations (which we also call markers) on the basis of customized schema; preserve the annotations as data that can be queried; and export the data in CSV and JSON formats for further exploration, interpretation, and visualization. The platform is built in Python and JavaScript, and it makes use of several open source libraries, including Django, OpenCV, FFMPEG, and React. Through websockets tied together by a REST API,  <em>Mediate</em>  supports concurrent updates of annotations added by multiple users.  <em>Mediate</em>  provides a real-time system for annotating and analyzing myriad genres that yield audiovisualities in which sight and sound come into chiastic interplay in medium-specific ways.</p>
<p>There are other tools like  <em>Mediate</em> , including ELAN and NVivo, the subjects of a recent study of the audiovisual state of digital annotation <sup id="fnref:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup>. The build of ELAN and NVivo, however, make them technologically and methodologically distinct from  <em>Mediate</em> . NVivo is not open source, while ELAN is — we hope  <em>Mediate</em>  will be open source and widely accessible in the future. Both ELAN and NVivo are desktop-based programs and have limited collaborative capabilities, whereas synchronous and asynchronous collaboration are foregrounded in  <em>Mediate</em> . ELAN projects can only contain one media file whereas NVivo has a more multimodal approach and supports a variety of file formats, similar to  <em>Mediate</em> . ELAN provides tiers (like the schema we use in  <em>Mediate</em> , described below) with controlled vocabularies (akin to markers that make up schema in  <em>Mediate</em> , again described below), whereas the categorization in NVivo is done after annotating through a code book approach common in the social sciences. Neither tool offers, as  <em>Mediate</em>  does, automated shot detection, and the ease of querying and exporting data across projects, media, and/or schema. Furthermore, they both feature high learning curves.  <em>Mediate</em>  adopts a more streamlined approach to consuming media in the design of its interface, which echoes familiar interfaces from our historical moment.</p>
<p>When a user logs into the  <em>Mediate</em>  website (username: mediate_guest password: mediate2019!), they encounter the User Interface that displays their research groups. Each group includes the initials of the members along with the media assigned to the group. The media thumbnails include a count in the upper right-hand corner for annotations already generated.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000507/resources/images/figure01.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000507/resources/images/figure01_hud06e42c4bb0500835ae308ee27c58aac_1822687_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000507/resources/images/figure01_hud06e42c4bb0500835ae308ee27c58aac_1822687_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000507/resources/images/figure01_hud06e42c4bb0500835ae308ee27c58aac_1822687_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000507/resources/images/figure01_hud06e42c4bb0500835ae308ee27c58aac_1822687_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/1/000507/resources/images/figure01_hud06e42c4bb0500835ae308ee27c58aac_1822687_1800x0_resize_box_3.png 1800w,/dhqwords/vol/15/1/000507/resources/images/figure01.png 1839w" 
     class="landscape"
     ><figcaption>
        <p>User Interface as seen at login with the research group name, collaborators’ initials, and available media.
        </p>
    </figcaption>
</figure>
<p>Selecting, for example,  <em>I Love Lucy</em> , the user enters the Annotation Interface. Here they can watch the media in an interface akin to familiar streaming services, but with the addition of a vertical column of time coded annotations that scroll as the media plays.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000507/resources/images/figure02.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000507/resources/images/figure02_hu36ba03c9b312b9892a0d557bf0138673_537014_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000507/resources/images/figure02_hu36ba03c9b312b9892a0d557bf0138673_537014_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000507/resources/images/figure02_hu36ba03c9b312b9892a0d557bf0138673_537014_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000507/resources/images/figure02_hu36ba03c9b312b9892a0d557bf0138673_537014_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/1/000507/resources/images/figure02.png 1778w" 
     class="landscape"
     ><figcaption>
        <p>Annotation Interface with a sample of markers generated for an episode of <em>I Love Lucy</em> .
        </p>
    </figcaption>
</figure>
<p>Clicking on the blue marker button in the upper left-hand corner reveals the Schema Pane. Here, users select markers and set short-cuts, which enables the marking process once the user returns to the Annotation Interface. This process can take weeks, especially if a group is marking across multiple objects. The repetitive work of collaborative marking not only helps students comprehend the forms they are analyzing, but also reveals the necessary judgments involved in deciding how and when to mark. As a result, it emerges that each marker and the concepts it represents become legible as a formal construct instead of a natural given.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000507/resources/images/figure03.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000507/resources/images/figure03_hua1471bd0715893b732f368b12cced8d5_62866_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000507/resources/images/figure03_hua1471bd0715893b732f368b12cced8d5_62866_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000507/resources/images/figure03_hua1471bd0715893b732f368b12cced8d5_62866_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000507/resources/images/figure03.png 1242w" 
     class="landscape"
     ><figcaption>
        <p>Schema Pane displays available markers for that schema and allows the user to assign markers to specific short-cuts for ease of marking.
        </p>
    </figcaption>
</figure>
<p>At a high level of frequency across seven years and twelve classes, Armoskaite, Burges, and Mueller have observed that  <em>Mediate</em>  encourages a process of learning that slows down the rapid-fire consumption of everyday media, upending the seemingly intuitive and immediate dimensions of the audiovisual field.<sup id="fnref:20"><a href="#fn:20" class="footnote-ref" role="doc-noteref">20</a></sup>  Recursive and reflexive, digital annotation in  <em>Mediate</em>  has made our students tune into the audiovisualities that construct them as seeing and hearing subjects through a range of material forms that operate in different cultural contexts and with differing social functions. The digital annotation in  <em>Mediate</em>  align with what Liliana Melgar Estrada et al. <sup id="fnref1:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup> conclude is &ldquo;the most significant methodological impact&rdquo; of using ELAN and NVivo: &ldquo;making the analytic procedures more explicit,&rdquo; can generate &ldquo;more self-reflection about scholarly work.&rdquo; The authors suggest that digital annotation&rsquo;s greatest strength is its reflexive ability to draw users&rsquo; attention to various units of analysis that they might not otherwise notice. Similarly,  <em>Mediate</em>  allows users to slowly comprehend what makes a film or poem or composition what it is as a mediated genre unto itself, but also to grasp how this medium-specificity turns on &ldquo;units of analysis&rdquo; <sup id="fnref2:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup> that are subjectively chosen in the first place. Just as all data are capta <sup id="fnref:21"><a href="#fn:21" class="footnote-ref" role="doc-noteref">21</a></sup>, the unit of analysis by which a datum is captured when digitally annotating audiovisual media is itself invented. The invented dimension of these units are revealed whenever students discuss and debate marker definitions, as we have seen in all of our classes.</p>
<p>The collaborative process of digital annotation enabled by  <em>Mediate</em> , starts to address one of the concerns raised by Melgar et al. in their analysis of the current state of digital annotation: that more &ldquo;collaborative&rdquo; and &ldquo;systematic&rdquo; efforts might allow scholars to transcend &ldquo;small scale&rdquo; analyses easily replicated on paper <sup id="fnref3:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup>. In this, they acknowledge what has long been a both celebrated and critiqued feature of the digital humanities: its problematically neoliberal stress on teamwork that we hope might be rescued as a project of collective reading and collaborative curriculum <sup id="fnref1:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup>  <sup id="fnref:22"><a href="#fn:22" class="footnote-ref" role="doc-noteref">22</a></sup>. Significant as the debate over the political economy and research efficacy of the digital humanities is <sup id="fnref:23"><a href="#fn:23" class="footnote-ref" role="doc-noteref">23</a></sup>  <sup id="fnref:24"><a href="#fn:24" class="footnote-ref" role="doc-noteref">24</a></sup>  <sup id="fnref:25"><a href="#fn:25" class="footnote-ref" role="doc-noteref">25</a></sup>, we nonetheless want to stress that in our courses we have found that scaling up — collectivizing and collaborating on digital annotation through groups of students producing and sharing data with one another about audiovisual media — has allowed those students to arrive at analytic findings that go beyond what they are able or willing to do otherwise.</p>
<p>To arrive here, we must not assume that our students are &ldquo;digital natives&rdquo; who are naturally better at studying and thinking with computational devices than with pen and paper. None of us believes this lazy assumption; some of us actively work against it in our classes. As we will show, we have nonetheless seen outcomes that are educationally remarkable, especially due to the collaborative learning that  <em>Mediate</em>  enables, when it comes to how digital annotation in  <em>Mediate</em>  fuels our students&rsquo; grasp of a range of audiovisual media in medium-specific ways. Often such medium-specificity is tethered to the discipline-specific approaches that we use in our courses, as we chart in the next section. But something interdisciplinary has arisen across our courses too: the cross-disciplinary concept of audiovisualities that this essay advances.</p>
<h2 id="three-disciplinary-case-studies-in-digital-annotation">Three Disciplinary Case Studies in Digital Annotation</h2>
<p>In our courses, we position individual mediums as possessing unique material forms that exist in cultural contexts and have some social function, as is reflected in our schemas. These schemas emerge from discipline-specific frameworks, with some of us stressing material form, cultural context, or social function more when teaching with  <em>Mediate</em> . In this section we provide three case studies. The first draws on a number of film and media studies classes where Burges wanted students to understand how a range of audiovisual media — television, poetry, and pop songs in the two cases discussed here — work formally such that they can be materially differentiated from another medium, even if they share certain properties. While Mueller and Armoskaite share this concern to varying degrees, their expertise has driven them to underscore questions of cultural context and social function more prominently in the study of audiovisual media. In his history class for music majors, Mueller asks students to interpret how a range of historical contingencies influences the creation and performance of specific musical sounds. Here,  <em>Mediate</em>  is a prompt to redirect assumptions about music that students already think they &ldquo;know.&rdquo; Bringing together questions of material form and cultural context, Armoskaite uses  <em>Mediate</em>  to spark students to delve into how language in advertising itself is audiovisual — or more precisely, how language has a social function in commercial media that turns on how it activates the interplay of hearing and seeing vis-à-vis linguistic and discursive content meant to induce an action in someone.</p>
<h2 id="case-study-1-material-form-in-film-and-media-studies-burges">Case Study 1: Material Form in Film and Media Studies (Burges)</h2>
<p>Questions of material form are central in the film and media studies classes I teach for the College of Arts, Sciences, and Engineering at the University of Rochester, including the two I discuss here: &ldquo;The Poetics of Television&rdquo; and &ldquo;Introduction to Media Studies&rdquo;. In &ldquo;The Poetics of Television&rdquo;, for example, we spend significant time studying the different ways in which the episode is a form of inscription that organizes the audiovisual experience of television in narratively open and closed ways. In &ldquo;Introduction to Media Studies,&rdquo; we discuss television not only from this narrative perspective, but also from the perspective of television as a historically variable technology for transmitting sounds and images onto a screen that was, for many decades, primarily part of the TV set. These are both material to the form of television, with the latter especially providing the specific audiovisual means by which television mediates and materializes narrative, information, and advertising for its viewers. In my classes, the question of material form — of how a medium is a matter of form — is not reducible to solely inquiring into these means in order to secure that which is, to recall Clement Greenberg <sup id="fnref:26"><a href="#fn:26" class="footnote-ref" role="doc-noteref">26</a></sup>, irreducibly exclusive to it. It instead involves pursuing lines of inquiry with students in which we explore the specificity of a range of media — from television and film to poetry and song — through the shifting constellations of qualities constrained and enabled by diverse audiovisual means in the first place <sup id="fnref:27"><a href="#fn:27" class="footnote-ref" role="doc-noteref">27</a></sup>.</p>
<p>Digital annotation in  <em>Mediate</em>  indelibly contributes to this pursuit, especially through the schemas that provide the basis of the highly collaborative — as we will show — marking that students do over the course of a semester in classes such as &ldquo;The Poetics of Television&rdquo; and &ldquo;Introduction to Media Studies.&rdquo; The schemas we have designed so far try to capture the constellations of qualities that make up any medium one might annotate in  <em>Mediate</em> . In &ldquo;The Poetics of Television,&rdquo; the schemas were designed around aural, visual, and narrative qualities in order to show how sound, image, and story are respectively constructed on TV.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000507/resources/images/figure04.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000507/resources/images/figure04_hu9652475e447a905eec5a9a3f9c054618_75180_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000507/resources/images/figure04_hu9652475e447a905eec5a9a3f9c054618_75180_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000507/resources/images/figure04_hu9652475e447a905eec5a9a3f9c054618_75180_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000507/resources/images/figure04_hu9652475e447a905eec5a9a3f9c054618_75180_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/1/000507/resources/images/figure04.png 1792w" 
     class="landscape"
     ><figcaption>
        <p>Visual Schema.
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000507/resources/images/figure05.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000507/resources/images/figure05_hueeb76f87a20b35f5f4de4723d4c2ba0a_669976_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000507/resources/images/figure05_hueeb76f87a20b35f5f4de4723d4c2ba0a_669976_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000507/resources/images/figure05_hueeb76f87a20b35f5f4de4723d4c2ba0a_669976_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000507/resources/images/figure05_hueeb76f87a20b35f5f4de4723d4c2ba0a_669976_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/1/000507/resources/images/figure05_hueeb76f87a20b35f5f4de4723d4c2ba0a_669976_1800x0_resize_box_3.png 1800w,/dhqwords/vol/15/1/000507/resources/images/figure05.png 1919w" 
     class="landscape"
     ><figcaption>
        <p>A sample of markers generated using the Visual Schema on a scene from Buffy the Vampire Slayer, Season Four.
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000507/resources/images/figure06.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000507/resources/images/figure06_hu565d5e7de845aab36f94e6a312b9a911_46761_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000507/resources/images/figure06_hu565d5e7de845aab36f94e6a312b9a911_46761_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000507/resources/images/figure06.png 1190w" 
     class="landscape"
     ><figcaption>
        <p>Narrative Schema.
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000507/resources/images/figure07.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000507/resources/images/figure07_hu95a79a1bc16af76a81e3ed6daa96167f_1073124_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000507/resources/images/figure07_hu95a79a1bc16af76a81e3ed6daa96167f_1073124_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000507/resources/images/figure07_hu95a79a1bc16af76a81e3ed6daa96167f_1073124_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000507/resources/images/figure07_hu95a79a1bc16af76a81e3ed6daa96167f_1073124_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/1/000507/resources/images/figure07_hu95a79a1bc16af76a81e3ed6daa96167f_1073124_1800x0_resize_box_3.png 1800w,/dhqwords/vol/15/1/000507/resources/images/figure07.png 1910w" 
     class="landscape"
     ><figcaption>
        <p>A sample of markers generated using the Narrative Schema on a scene from Game of Thrones, Season Three.
        </p>
    </figcaption>
</figure>
<p>The schemas for &ldquo;Introduction to Media Studies,&rdquo; were designed with comparison in mind, so one focused on a set of markers for annotating poems read aloud by their authors, the other on a set of markers for annotating pop songs by individual singers and bands.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000507/resources/images/figure08.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000507/resources/images/figure08_hu3c8032eebbf9c9d5a9c57c3f40d06858_36732_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000507/resources/images/figure08_hu3c8032eebbf9c9d5a9c57c3f40d06858_36732_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000507/resources/images/figure08_hu3c8032eebbf9c9d5a9c57c3f40d06858_36732_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000507/resources/images/figure08_hu3c8032eebbf9c9d5a9c57c3f40d06858_36732_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/1/000507/resources/images/figure08_hu3c8032eebbf9c9d5a9c57c3f40d06858_36732_1800x0_resize_box_3.png 1800w,/dhqwords/vol/15/1/000507/resources/images/figure08.png 1817w" 
     class="landscape"
     ><figcaption>
        <p>Poetry Schema.
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000507/resources/images/figure09.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000507/resources/images/figure09_hu58dab7ed0fac50c61cfd92434b3ae29c_544082_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000507/resources/images/figure09_hu58dab7ed0fac50c61cfd92434b3ae29c_544082_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000507/resources/images/figure09_hu58dab7ed0fac50c61cfd92434b3ae29c_544082_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000507/resources/images/figure09_hu58dab7ed0fac50c61cfd92434b3ae29c_544082_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/1/000507/resources/images/figure09_hu58dab7ed0fac50c61cfd92434b3ae29c_544082_1800x0_resize_box_3.png 1800w,/dhqwords/vol/15/1/000507/resources/images/figure09.png 1807w" 
     class="landscape"
     ><figcaption>
        <p>A sample of markers and observations generated using the Poetry Schema on a video of Tracy K. Smith reading “Wade in the Water.”
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000507/resources/images/figure10.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000507/resources/images/figure10_hu5df6e7ddd201b340cb63f9d7aed3bf31_60606_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000507/resources/images/figure10_hu5df6e7ddd201b340cb63f9d7aed3bf31_60606_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000507/resources/images/figure10_hu5df6e7ddd201b340cb63f9d7aed3bf31_60606_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000507/resources/images/figure10_hu5df6e7ddd201b340cb63f9d7aed3bf31_60606_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/1/000507/resources/images/figure10_hu5df6e7ddd201b340cb63f9d7aed3bf31_60606_1800x0_resize_box_3.png 1800w,/dhqwords/vol/15/1/000507/resources/images/figure10.png 1817w" 
     class="landscape"
     ><figcaption>
        <p>Narrative Schema.
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000507/resources/images/figure11.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000507/resources/images/figure11_hua6b04dd8052d7b4bbdc3551f92ea7d80_217558_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000507/resources/images/figure11_hua6b04dd8052d7b4bbdc3551f92ea7d80_217558_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000507/resources/images/figure11_hua6b04dd8052d7b4bbdc3551f92ea7d80_217558_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000507/resources/images/figure11_hua6b04dd8052d7b4bbdc3551f92ea7d80_217558_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/1/000507/resources/images/figure11_hua6b04dd8052d7b4bbdc3551f92ea7d80_217558_1800x0_resize_box_3.png 1800w,/dhqwords/vol/15/1/000507/resources/images/figure11.png 1919w" 
     class="landscape"
     ><figcaption>
        <p>A sample of markers and observations generated using the Narrative Schema on Beyoncé&rsquo;s “Partition.”
        </p>
    </figcaption>
</figure>
<p>Over the course of a semester, students in these two classes worked collaboratively in groups of four to six people to annotate on the basis of the schema or schemas that group was assigned, building toward long papers in which they explored a wide range of topics through distant and close readings performed in writing and through visualizations. Regardless of the topic, these papers almost universally exhibited a deep knowledge of the material form of the audiovisual medium under study; the specific interplay of sight and sound embodied by TV, for instance, became &ldquo;second nature&rdquo; to one student, &ldquo;so much so that when I watch TV now I automatically mark the episode in the back of my mind&rdquo; <sup id="fnref:28"><a href="#fn:28" class="footnote-ref" role="doc-noteref">28</a></sup>. This is the result of what the students characterize as the &ldquo;immersive&rdquo; dimension of  <em>Mediate</em>  in which time spent marking, however &ldquo;irritating&rdquo; and &ldquo;monotonous&rdquo; and &ldquo;tedious&rdquo; in its slowness over the semester, generates a profound concept of the qualities that give material form to their audiovisual experience <sup id="fnref:29"><a href="#fn:29" class="footnote-ref" role="doc-noteref">29</a></sup>. This is most palpable when one looks more closely at final projects students completed, in which their collaborative efforts yielded a remarkable level of quantitative data and qualitative observation worth understanding in greater nuance.<sup id="fnref:30"><a href="#fn:30" class="footnote-ref" role="doc-noteref">30</a></sup></p>
<p>In an essay for &ldquo;The Poetics of Television&rdquo; entitled &ldquo;The Formal Nucleus of Television, and Its Subservience to Narrative&rdquo; <sup id="fnref1:29"><a href="#fn:29" class="footnote-ref" role="doc-noteref">29</a></sup>, the students argued that dialogue is a key element of the &ldquo;formal nucleus&rdquo; of TV by exploring the nexus of sound and story in four historically and generically variable series defined by open narration ( <em>Game of Thrones</em> ,  <em>Dark Shadows</em> ,  <em>Guiding Light</em> , and  <em>Robotech</em> ). On the basis of hundreds of markers the students in this group annotated, they argue that dialogue is an elementally generative feature of the &ldquo;aural design&rdquo; of television that &ldquo;advance[s] the narrative progression of an episode.&rdquo; This is due to dialogue allowing &ldquo;all details pertinent to the comprehension of the narrative, in terms of both plot and character, to be enumerated in explicit, unequivocal, and economical terms.&rdquo; This group further contends that the television camera often obeys the human voice, suggesting that visual design flows from aural design, for instance, on the basis of the 54 on-offscreen and off-onscreen shifts of sound vis-à-vis the image and diegesis that the group marked in the infamous Red Wedding scene of  <em>Game of Thrones</em>  while annotating in the Aural Schema. While both the visible and the audible are subservient to narrative in the argument this group&rsquo;s paper makes, it nonetheless richly charts the interplay of sight and sound within storytelling on TV, revealing how that mediated interplay — that audiovisuality — lets narrative take material form on screen.</p>
<p>&ldquo;Introduction to Media Studies&rdquo; similarly attuned students to material form. But rather than focusing on one audiovisual medium to achieve this end, as in &ldquo;The Poetics of Television,&rdquo; I used a comparative approach in which I asked students to work in groups to annotate poetry and pop songs to which they listened closely and repeatedly in  <em>Mediate</em> . Students were not allowed to pull print versions of the poems they were annotating on the basis of oral renditions by their authors, forcing them to use their ears to grasp the differences between poems and songs marked using schemas developed for each of these genres. What one group observed about those differences will sound obvious: while rhythmically structured and lineated language is the medium of poetry, pop songs are much more musical in their means, depending on instrumentation, chord progression, beat groupings and so on <sup id="fnref:31"><a href="#fn:31" class="footnote-ref" role="doc-noteref">31</a></sup>.</p>
<p>But what is less obvious is how the students in this group came to experience this difference because  <em>Mediate</em>  introduced annotation into audition. In annotating, they  <em>heard</em>  what was material to the form of poetry and pop songs as a fixed and motivated structure of aural notation. The songs went from being an internal experience of sound and music to appearing as an externalized — because now annotated — form of audiovisual inscription. Thus the long paper this group produced focused less on material form in favor of trying to pinpoint what constellation of audible qualities inscribe what they call &ldquo;intensity.&rdquo; For this group, intensity refers to how &ldquo;affective response&rdquo; and &ldquo;aesthetic emotion&rdquo; in a mediated genre of sound turns upon medium-specific features, as in their LP record player-inspired visualizations of two songs, Troye Sivan&rsquo;s &ldquo;Wild&rdquo; and Taylor Swift&rsquo;s &ldquo;This Love.&rdquo; Audiovisualities in their own right, these experimental data visualizations show how distinct kinds of vocal stress, chord changes, back-up singing, and instrumentation not only mark the form of these songs, but also form the possibility of having a musically &ldquo;intense&rdquo; reaction to them as well.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000507/resources/images/figure12.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000507/resources/images/figure12_hu1aacdd5850397e0bef9c535453471bdd_6176118_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000507/resources/images/figure12_hu1aacdd5850397e0bef9c535453471bdd_6176118_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000507/resources/images/figure12_hu1aacdd5850397e0bef9c535453471bdd_6176118_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000507/resources/images/figure12_hu1aacdd5850397e0bef9c535453471bdd_6176118_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/1/000507/resources/images/figure12_hu1aacdd5850397e0bef9c535453471bdd_6176118_1800x0_resize_box_3.png 1800w,/dhqwords/vol/15/1/000507/resources/images/figure12.png 2500w" 
     class="landscape"
     ><figcaption>
        <p>Experimental Visualization of Formal and Intensity Markings for Troye Sivan’s &ldquo;Wild.&rdquo;
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000507/resources/images/figure13.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000507/resources/images/figure13_hu17e4df6e12445db13f5d77f5524978fa_5801577_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000507/resources/images/figure13_hu17e4df6e12445db13f5d77f5524978fa_5801577_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000507/resources/images/figure13_hu17e4df6e12445db13f5d77f5524978fa_5801577_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000507/resources/images/figure13_hu17e4df6e12445db13f5d77f5524978fa_5801577_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/1/000507/resources/images/figure13_hu17e4df6e12445db13f5d77f5524978fa_5801577_1800x0_resize_box_3.png 1800w,/dhqwords/vol/15/1/000507/resources/images/figure13.png 2500w" 
     class="landscape"
     ><figcaption>
        <p>Experimental Visualization of Formal and Intensity Markings for Taylor Swift’s &ldquo;This Love.&rdquo;
        </p>
    </figcaption>
</figure>
<p>In classes such as &ldquo;The Poetics of Television&rdquo; and &ldquo;Introduction to Media Studies,&rdquo; digital annotation in  <em>Mediate</em>  enables students to work together to see and hear the material form of a range of audiovisual media. It is important that they are working together, collaborating to annotate such that both within and across their groups they are able to explore audiovisuality in a collective way that has both quantitative and qualitative effects. On the one hand, the quantitative dimension of their collaborative efforts is visible in the plenitude of markers that each group generates as a working collective, and in how they draw on the audiovisual data produced by other groups marking in the same class to understand the annotation that has occurred in a given group. On the other hand, the repeated marking required to yield thousands of data points they can share with one another engenders a quality of description and interpretation that shows how close they understand the material forms of film, television, poetry, and pop songs from multiple audiovisual angles of seeing and hearing. As we have more fully argued elsewhere <sup id="fnref2:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup>, however, that &ldquo;truth&rdquo; depends upon the discussions that often emerge over how to collectively define and collaboratively mark a unit of analysis within and across groups as they annotate features of material form; these discussions over how to see and hear, though, only further shore up both that no medium is a natural given, and that every audiovisual experience is mediated.</p>
<h2 id="case-study-2-the-cultural-contexts-of-music-history-mueller">Case Study 2: The Cultural Contexts of Music History (Mueller)</h2>
<p>Although connected institutionally, Eastman School of Music (ESM) is quite different from the rest of the university in terms of its student population, educational goals, and curriculum. All of ESM&rsquo;s approximately 500 undergraduate students are music majors, with a primary focus on Western classical music.<sup id="fnref:32"><a href="#fn:32" class="footnote-ref" role="doc-noteref">32</a></sup>  Our students are some of the best young musicians in the world, meaning that most view their classroom activities through the lens of their future careers in performance, composition, and pedagogy. To even gain admittance, they need to have remarkable expertise and years of specialized training in a tradition built around individual composers and master performers. So, while these students bring a strong passion for music into the classroom, doing academic work often forces them to confront viewpoints and approaches that are frequently taken as natural rather than culturally constructed.</p>
<p>Most of the traditions represented at ESM are heavily reliant on musical notation, a highly advanced system of written symbols that has, over many centuries, enabled the development and circulation of music that originated in Western Europe. In many respects, the very presence of notation constitutes the tradition <sup id="fnref:33"><a href="#fn:33" class="footnote-ref" role="doc-noteref">33</a></sup>. Musical notation can also be understood as a form of audiovisual inscription that communicates specific information about both how to perform music and also how an individual piece functions melodically, harmonically, rhythmically, and formally <sup id="fnref:34"><a href="#fn:34" class="footnote-ref" role="doc-noteref">34</a></sup>  <sup id="fnref:35"><a href="#fn:35" class="footnote-ref" role="doc-noteref">35</a></sup>  <sup id="fnref:36"><a href="#fn:36" class="footnote-ref" role="doc-noteref">36</a></sup>. Reading music, as we would say, is a presumed skill that students rely upon as they move through the robust series of classes in music theory and history, both of which differently emphasize score-based analysis of internal (within a piece) and external (within a tradition) musical features. As a result, students are very comfortable working with written music, as well as the specialized language used to describe it. Still, the culture surrounding classical music has calcified certain perspectives, especially on what it means to do analysis. For many students, music analysis is too often conceived of as an action solely in visual terms, rather than an act that takes place in a far more expanded audiovisual realm.</p>
<p>In Fall 2019, I introduced  <em>Mediate</em>  into my &ldquo;Experiments at the Edges of 20th Century Music,&rdquo; a required course within the undergraduate music history core. My goal was to foreground listening, rather than reading, as the primary means of analysis. As exceptional performers already, my students listen with an expertly attuned understanding of musical performance. But while they continually analyze music while listening, they do not always listen historically — that is, attend to how specific musical moments express historically contingent beliefs about culture, society, or the many processes of music making. There is no inherent problem with their modality of listening. However, it is not always congruous with my major pedagogical goal: to examine and interpret how music and its written tradition are both heavily mediated creations, dependent on historically situated actors with different investments and values. By asking students to pay attention differently,  <em>Mediate</em>  not only foregrounds listening but also asks students to translate that listening into specific observations represented visually. In effect, this reverses the standard audiovisual direction of music making. Rather than move from visual inscription (notation) to aural expression (performance),  <em>Mediate</em>  renders what is heard into specific visual markings of that performance. Unsettling the assumed relationships between the visual and the aural — by putting listening first — encourages alternative viewpoints to come into the forefront of the analytical process.</p>
<p>Inspired by previous uses of  <em>Mediate</em>  in the classroom, I had students complete a semester-long collective analysis project. Our work began before I introduced the  <em>Mediate</em>  platform by continually asking students during class discussions and daily responses to listen through a series of five interrelated questions oriented towards cultural contexts:</p>
<p>Who or what shaped this particular music or performance?</p>
<p>What would it be like to perform this music?</p>
<p>What are the musical materials used in the creation of this piece?</p>
<p>How does this musical material transform?</p>
<p>What do you think the creators were trying to say or accomplish with this music?<sup id="fnref:37"><a href="#fn:37" class="footnote-ref" role="doc-noteref">37</a></sup></p>
<p>For their  <em>Mediate</em>  project, students organized into groups of two to four. Each student within those groups picked one or more of these questions and marked up their audio in  <em>Mediate</em>  from those perspectives. I developed specific markers for each question to aid in this process, but also encouraged students to develop others to meet their specific needs.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000507/resources/images/figure14.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000507/resources/images/figure14_hue031ee3b848251e997c50f97322c1c91_109381_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000507/resources/images/figure14_hue031ee3b848251e997c50f97322c1c91_109381_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000507/resources/images/figure14_hue031ee3b848251e997c50f97322c1c91_109381_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000507/resources/images/figure14_hue031ee3b848251e997c50f97322c1c91_109381_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/1/000507/resources/images/figure14_hue031ee3b848251e997c50f97322c1c91_109381_1800x0_resize_box_3.png 1800w,/dhqwords/vol/15/1/000507/resources/images/figure14.png 1911w" 
     class="landscape"
     ><figcaption>
        <p>Categories of Analysis Schema.
        </p>
    </figcaption>
</figure>
<p>As they listened, students would mark everything from the seemingly obvious — what they might otherwise notice without thinking — to those details obscured by the rapid unfolding of any time-based art. After creating several hundred markers, each group began to decipher their efforts and develop a thesis for their written analysis with the same five questions again providing a road map. The class went through this process in two different iterations. First, all groups analyzed the first movement of William Grant Still&rsquo;s Symphony No. 1. Then, each group picked a piece of music or specific performance from a given list of artists and composers.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000507/resources/images/figure15.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000507/resources/images/figure15_hued8f02f4489dae6833bbd5e372e4653b_605443_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000507/resources/images/figure15_hued8f02f4489dae6833bbd5e372e4653b_605443_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000507/resources/images/figure15_hued8f02f4489dae6833bbd5e372e4653b_605443_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000507/resources/images/figure15_hued8f02f4489dae6833bbd5e372e4653b_605443_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/1/000507/resources/images/figure15_hued8f02f4489dae6833bbd5e372e4653b_605443_1800x0_resize_box_3.png 1800w,/dhqwords/vol/15/1/000507/resources/images/figure15.png 1919w" 
     class="landscape"
     ><figcaption>
        <p>A sample of markers and observations generated using the Categories of Analysis Schema on Ida Handel performing Part One of the Carmen Fantasy.
        </p>
    </figcaption>
</figure>
<p>Two general observations emerged out of the individual reflections written at the end of the semester. First, the slow, sometimes-tedious markup process requires active listening. Many students discussed how they came to notice subtle details and complexities precisely because the marking up process made &ldquo;passive&rdquo; or &ldquo;casual&rdquo; listening impossible. The intensity involved with repeated listening did not always change their initial opinions, but rather increased the precision and specificity of their observations. One student remarked with surprise about &ldquo;how much could happen within one tiny second.&rdquo; Second, the processes of collective listening encouraged individuals to consider multiple vantage points. Collaboration through group work is not always smooth or easy, and it is sometimes unpopular. But by learning from or being challenged by their colleagues — perhaps even by becoming a &ldquo;cultural context&rdquo; in miniature — many students reported that the dialogic experience enabled them to make connections that were perhaps not obvious to them before.</p>
<p>The written work of each group also proved how valuable doing analysis away from the score could be. Many groups wrote about meaningful moments that would have otherwise remained hidden by only looking at the written notation — the particular use of vibrato, the background noise in a recording, a reoccurring timbre, or the use of space or a particular texture in the orchestration. Through the analysis of seemingly discrete details in relation to the background of the composer or other cultural influences, students then found ways to relate what happens musically to what that performance might mean more broadly, which is to say historically. As one student commented, the analysis provided a way to understand how music functioned as an interconnected web of historical events, musical influences, and experiences of &ldquo;real life people.&rdquo; In comparison with previous semesters, the written work of the students in this class was at once more precise and bolder in their conclusions.</p>
<p>The slow and collective process of listening through  <em>Mediate</em>  allows students to re-situate their otherwise expert ears towards music as a form of audiovisual inscription. Music is a time-based art that exists in performance, yet it nevertheless remains heavily dependent on the visual realm. The specialized language used in traditional forms of analysis — a Neapolitan sixth chord, for one example — describes both what music sounds and looks like. As a digital platform that creates a method for analysis,  <em>Mediate</em>  makes the audiovisualities of music clear. Musical culture is and has always been an audiovisual culture as well, and new possibilities surface for students about this fact when they experience music through  <em>Mediate</em> .</p>
<h2 id="case-study-3-social-function-in-linguistics-armoskaite">Case Study 3: Social Function in Linguistics (Armoskaite)</h2>
<p>The Department of Linguistics in the College of Arts, Sciences, and Engineering at the University of Rochester, while a part of the social sciences, is a hub of interdisciplinary research with ties to other departments, including Music (with a focus on perception and production of sounds), Brain and Cognitive Science (with a focus on meaning and language processing), Anthropology (with focus on culture and language intersections), and Psychology (with a focus of child language acquisition). As a field, linguistics covers a vast number of topics and methodologies, hence it is impossible to provide a general description that would fit the range. For the purposes of this case study, it will suffice to state (i) that linguistics focuses on of the makeup of grammar, which is a set of sub-systems of sound, form and meaning; (ii) and that these subsystems are used for communication, a function that interacts with social conventions and societal values, a.o. <sup id="fnref:38"><a href="#fn:38" class="footnote-ref" role="doc-noteref">38</a></sup>.</p>
<p>My course &ldquo;Language and Advertising,&rdquo; requires students to consider language use in the context of audiovisual marketing against the backdrop of current social trends. While Linguistics does not have a Business or Marketing track, the course routinely is taken by business majors and consistently is a popular elective among other non-Linguistics majors. I face a diverse group of students with different backgrounds, skills, and assumptions, though united by the three common denominators. First, the ubiquity of advertising in their lives gives them a false sense of familiarity and assumed knowledge of the medium; second, they want to learn the nuts and bolts of the advertising machine; and third, they possess limited knowledge of linguistics. Over the course of the semester, they learn that the social function of language in advertising is to manipulate, with commercial media, working to apply psychological pressure through a mode of audiovisuality that depends on influencing our emotions and circumventing our rational mind, a.o. <sup id="fnref:39"><a href="#fn:39" class="footnote-ref" role="doc-noteref">39</a></sup>  <sup id="fnref:40"><a href="#fn:40" class="footnote-ref" role="doc-noteref">40</a></sup>  <sup id="fnref:41"><a href="#fn:41" class="footnote-ref" role="doc-noteref">41</a></sup>.</p>
<p>Situating a familiar medium — advertising — within a likely unfamiliar field — linguistics — necessarily slows down the students. They learn to analyze the linguistic components that, to call back to the film and media studies case above, &ldquo;make&rdquo; the medium. The material form that interests a linguist, however, includes not only sounds and images of the kind that interest Burges and Mueller in their courses, but also the very structure of language as humans speak, read, and hear it. For example, each of the following posters can be deciphered in the linguistic terms of sound, form, and meaning, which shows that even print advertising functions as an audiovisual medium.</p>
<p><a href="https://www.redcrossblood.org/content/dam/redcrossblood/campaigns/missing-types/Missing%20Types%20Campaign%20Partner%20Style%20Guide.pdf">The American Red Cross &ldquo;Missing Types&rdquo; campaign (2018)</a> presents a sound-based puzzle for the viewer to acoustically fill in, whether in silence or out loud: all the missing elements are vowels. The viewer goes in search of those vowels, enjoying a language game that depends on visual absence engendering audio presence. And because the vowels are associated with types of blood, this audiovisual play becomes a linguistic mechanism for soliciting blood donations.</p>
<p><a href="https://www.adsoftheworld.com/media/print/snickers_satisfectellent">The Snickers &ldquo;Satisfectellent&rdquo; (TBWA 2007)</a> advertisement plays upon another element of grammar, namely, derivation of words. In this case, a word that is possible, but does not exist, is created. The novelty of the coined word is the striking — and strikingly audiovisual — feature of the advertisement: the joy of recognition of the brand of the snack is fused with the unexpectedness of the word.</p>
<p>Finally, the <a href="https://www.adsoftheworld.com/media/print/greenpeace_straws_suck_gull">Greenpeace &ldquo;Straws Suck: Gull&rdquo; (Rethink 2018)</a> advertisement exploits the shades of meaning of the verb &ldquo;to suck.&rdquo; The painful visual is certainly not the first association we have about sucking through straws, which we may think about in sonic and/or tactile terms primarily. But the unexpected visual connotation is meant to shock us into changing our habits of consumption.</p>
<p>However, language in print advertising engenders audiovisual experience in a far more static way than the advertising that flows across our many screens as moving images and dynamic sounds. The latter contains hundreds of speech patterns in addition to innumerable cues for our ears and eyes. Superimposed on moving images, these patterns and cues come at a viewer at a speed that barely allows them to register the component parts, let alone perform a thorough analysis.  <em>Mediate</em>  creates a space for such analysis external to the ephemerally immediate modes in which we normally consume commercial media. In so doing, it makes legible that language plays a constitutive part in the interplay of sight and hearing — that all three of these human abilities contribute to the social function of manipulation that is the raison d&rsquo;etre of often brilliant acts of commodified audiovisuality that want us to buy or buy into something.  <em>Mediate</em>  gets students to treat the audiovisuality of advertising as a constellation of elements variously linguistic, optical, and aural — as an object of analysis.</p>
<p>This takes time over the course of the semester. In &ldquo;Language and Advertising&rdquo; (Fall 2018 and Fall 2019), I spent three weeks teaching students the fundamentals of linguistics using similar examples to the print advertisements discussed above. Building on this print unit, we then turned to the challenge of analyzing commercials using  <em>Mediate</em> . The students began by analyzing an ad without the use of  <em>Mediate</em> . Then I modeled the slow and detailed analysis conducted through  <em>Mediate</em>  by providing them with samples of my own marked-up commercial. We discussed their observations, along with my own, which gently lets them understand how many details they&rsquo;ve missed in their own observations. The students were then trained on the platform and introduced to the schema over two class sessions. After that initial introduction, the students took charge of their own learning with only a light editorial supervision.</p>
<p>Circumventing the top-down didacticism of the traditional lecture,  <em>Mediate</em>  allowed the students to immerse themselves into the material on their own. Working in groups, they were tasked with selecting and analyzing at least two video ads, splitting up the work of marking (what we call &ldquo;coding&rdquo; in the field of linguistics) amongst themselves. Rather than a traditional linguistic analysis of a video ad that might include a transcription of the text divorced from the audio and visual cues,  <em>Mediate</em>  facilitated a holistic approach to the interplay and interdependencies of the audiovisualities commercial media employ.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000507/resources/images/figure16.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000507/resources/images/figure16_hu9b918074bff3051f68c252c3b3cd16d0_472528_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000507/resources/images/figure16_hu9b918074bff3051f68c252c3b3cd16d0_472528_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000507/resources/images/figure16_hu9b918074bff3051f68c252c3b3cd16d0_472528_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000507/resources/images/figure16.png 1476w" 
     class="landscape"
     ><figcaption>
        <p>A sample of markers and observations generated using the Linguistics Schema on Cadillac&rsquo;s &ldquo;The Future is Here&rdquo; 60 second spot.
        </p>
    </figcaption>
</figure>
<p>In slowing down their observations, they are forced to think about each element in the totality of the advertisement such that it emerges as a linguistic object of audiovisual analysis, with the manipulative properties of this object becoming ever clearer as its social function over the time they spend in  <em>Mediate</em> . As one student noted, &ldquo;there is no escape but to analyze.&rdquo; At the end of the semester, the groups presented their analyses of commercial media, with the entire class responding. This collective response included debates over how each group defined certain linguistic, audio, and visual units of analysis, and about the conclusions about manipulation in commercial media that each group reached. These discussions — aided by the carefully coded examples in  <em>Mediate</em>  — were paramount in helping students build an applied understanding of the social function of advertising from an audiovisual angle grounded in linguistics as an interdisciplinary field.</p>
<p>Despite their engagement and increased capacity with audiovisual analysis through  <em>Mediate</em> , there still is room for greater interdisciplinary collaboration in &ldquo;Language and Advertising&rdquo;, especially in light of the cross-disciplinary sense of audiovisuality we are advancing in this article. In my course, I welcome film experts as occasional invited speakers. But through my discussions with Burges and Mueller, I have realized that more sophisticated ways of tuning into the linguistic, visual, and sonic patterns would offer further opportunities to explore the range and means of consumer manipulation. For example, thus far, I have left out musical aspects completely as I lack relevant training. In the future iterations of this course, I think about the potential for more nuanced analysis if I could harness Mueller&rsquo;s expertise in helping students define the auditory components, if Burges could work with students to delve further into the visual form — that is, if students benefitted not only from their collective defining through marking, but also the collective expertise of a more intentionally cross-disciplinary approach to teaching and research.</p>
<h2 id="audiovisualities-out-of-annotation">Audiovisualities out of Annotation</h2>
<p>Across our individual classes, we have seen our students more fully enter the study of audiovisual media as they are defined by material form, cultural context, and social function within our respective disciplinary frameworks. In sharing these experiences with one another across our disciplines, we have been reminded that, when it comes to the audiovisual field, a film and media studies scholar should sometimes see and hear that field  <em>like</em>  a music historian who should sometimes see and hear it  <em>like</em>  a linguist. In noticing that we should see and hear like each other more, even as we explored medium-specific matters with our students, we have arrived at the cross-disciplinary concept of audiovisualities. Pedagogically and intellectually segregated from another due to the division of labor that organizes the modern research university, this concept allows us to think about the interplay of sight and sound more promiscuously and productively, overcoming the binaries that too often divide the audible and the visual and the divides that splinter disciplines from one another institutionally. In working together the last few years on digital annotation, we have learned to think more comprehensively across our respective fields about the (re)mediated sites where the physical and cultural operations of audiovisual experience converge. It is these locations of convergence that construct not only sensory and social subjectivities grounded in seeing and hearing, but also material forms and collective technics that set the conditions of possibility to see and hear to begin with — in short, that construct a manifold of audiovisualities. Our work on  <em>Mediate</em>  has helped us to estrange, even to alienate, the &ldquo;natural order&rdquo; that has been imposed on our experience of that manifold, or what Michel Chion describes as the &ldquo;audiovisual contract&rdquo; <sup id="fnref1:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup>.</p>
<p>As this gesture to Chion telegraphs, we are not the first group of scholars to explore such conditions. The cross-disciplinary concept of audiovisualities on which we have landed already has a genealogy of thinkers — many of them cited at the outset of this essay — associated with visual studies and sound studies, not to mention film and media studies, behind it. Indebted to them, we nonetheless think the practice of digital annotation that  <em>Mediate</em>  provides contributes a collaborative model of learning through collective reading that allows our students to conceptualize audiovisualities beyond their individual selves (and our individual disciplines). It may do this, as well, for any scholars that take it up, especially, if not only in a collective and collaborative form. The collectivity of digital annotation can take that which feels intuitive and internal and remake it as unfamiliar and external; the collective act of exteriorizing that occurs in  <em>Mediate</em>  brings a new awareness to the qualities and characteristics of a given audiovisual medium.</p>
<p>&ldquo;History is nothing but exteriorities,&rdquo; writes Jonathan Sterne in  <em>The Audible Past,</em>  by which he means that we can only know the &ldquo;sonic world&rdquo; of the past through its &ldquo;efforts, expressions, and reactions&rdquo; <sup id="fnref2:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>.  <em>Mediate</em>  embraces this point of view. Digital annotation in  <em>Mediate</em>  asks students to exteriorize their reactions to audiovisual media not only by slowing down their consumption of them, but also by turning what feels subjectively intuitive, immediate, and internal (listening to and even playing music, taking in a poem, consuming an advertisement, watching a TV show) into a mediated object to be analyzed collaboratively and collectively beyond oneself. Vivid examples of this process of exteriorizing abound in our case studies. In &ldquo;Experiments at the Edges of 20th Century Music,&rdquo; students produce digital &ldquo;notations,&rdquo; so to speak, through their use of  <em>Mediate</em> , thus resituating the ocularcentric primacy of musical notation through careful listening and historicizing. Similarly, in &ldquo;Language and Advertising,&rdquo; the interface renders commercials a constellation of elements that act on us linguistically, visually, and musically in ways students can tangibly analyze. And the experimental visualizations of two pop songs for &ldquo;Introduction to Media Studies&rdquo; draw on data produced collectively through digital annotation about the aural experience of intensity to visually represent the material form of that intensity.</p>
<p><em>Mediate</em>  therefore enables a defamiliarized perception of audiovisialities, first and foremost, by challenging the consumption of media as an individual and discrete act atomized from others. The work of collaborative annotation, which sets  <em>Mediate</em>  apart from platforms such as ELAN and NVivo, reveals not only the potential for different experiences of the same mediums, but that the criteria through which we name and identify media — and indeed, our respective disciplines — can, and perhaps should, be subject to the scrutiny made possible by collective re-examination. Our respective fields are built upon often now unspoken agreements about what constitutes film or poetry or music or television or advertising or language.  <em>Mediate</em>  shows how &ldquo;agreeing to disagree&rdquo; on a given medium&rsquo;s properties remains a necessary move within and across disciplines, especially if we are to take into account critiques of both collaboration and computation leveled at the digital humanities. The reverse of the earlier claim, in other words, is also true. Our students often debate what a unit analysis means when marking, mobilizing the differences amongst themselves in collaborating on digital annotation. Similarly, when it comes to the cross-disciplinary concept of audiovisualities, a film and media studies scholar should see and hear the interplay of sight  <em>unlike</em>  a music historian who should sometimes see and hear it  <em>unlike</em>  a linguist as much as we should see and hear it  <em>like</em>  each other.</p>
<p>The case studies recounted in this article reflect this collaborative process of disagreement — the self-aware reflection upon &ldquo;units of analysis&rdquo; — as a pedagogically necessary exercise in understanding the audiovisual world we inhabit in the present. However, such a practice is not limited to the undergraduate classroom alone. The collaborative nature of digital annotation breaks down a process that scholars, at all levels, often take for granted: the terms and tools through which we analyze media, especially within our respective fields. By making us be both like and unlike each other,  <em>Mediate</em>  has allowed us to take hold of those terms and tools anew, discovering audiovisualities out of annotation as a concept that unsettles what we do with the interplay of sight and sound inscribed everywhere into experience at present.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>A 2019 University of Rochester Educational IT Innovation Grant supported the teaching described in and the writing of this article.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Benjamin, Walter.  “The Work of Art in the Age of Its Technological Reproducibility.” In Benjamin, Walter.  <em>Walter Benjamin: Selected Writings, 4: 1938–1940.</em>  Edited by Howard Eiland and Michael W. Jennings. Vol. 4. 4 vols. Selected Writings. Cambridge, MA: The Belknap Press of Harvard University Press, 2006.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>McLuhan, Marshall.  <em>Understanding Media: The Extensions of Man</em> . New York: New American Library, 1964.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Hansen, Miriam Bratu.  “Room-for-Play: Benjamin&rsquo;s Gamble with Cinema.”  <em>October</em>  109 (2004): 3–45.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>Foster, Hal, ed.  <em>Vision and Visuality</em> . Seattle: Bay Press, 1988.&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>Mitchell, W. J.T.  “Showing Seeing: A Critique of Visual Culture.”  <em>Journal of Visual Culture</em>  1, no. 2 (August 2002): 165–81. <a href="https://doi.org/10.1177/147041290200100202">https://doi.org/10.1177/147041290200100202</a>.&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>Bull, Michael, ed.  <em>Sound Studies: Critical Concepts in Media and Cultural Studies</em> . New York: Routledge, 2013.&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>Novak, David, and Matt Sakakeeny, eds.  <em>Keywords in Sound</em> . Durham, NC: Duke University Press, 2015.&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>Trevor Pinch and Karin Bijsterveld, eds.  <em>The Oxford Handbook of Sound Studies</em> . New York: Oxford University Press, 2012.&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>Sterne, Jonathan, ed.  <em>The Sound Studies Reader.</em>  New York: Routledge, 2012.&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>Sterne, Jonathan.  <em>The Audible Past: Cultural Origins of Sound Reproduction</em> . Durham: Duke University Press, 2003.&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p>Lingold, Mary Catton, Darren Mueller, and Whitney Anne Trettien, eds.  <em>Digital Sound Studies</em> . Durham, NC: Duke University Press, 2018.&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13">
<p>Chion, Michel.  <em>Audio-Vision: Sound On Screen</em> . Translated by Claudia Gorbman. New York: Columbia University Press, 1994.&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14">
<p>Silverman, Kaja.  <em>The Acoustic Mirror: The Female Voice in Psychoanalysis and Cinema</em> . Bloomington, IN: Indiana University Press, 1988.&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:15">
<p>Zielinski, Siegfried.  <em>Audiovisions: Cinema and Television as Entr&rsquo;actes in History</em> . Translated by Gloria Custance. Amsterdam: Amsterdam University Press, 1999.&#160;<a href="#fnref:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:16">
<p>Zielinski, Siegfried.  <em>Deep Time of the Media: Toward an Archeology of Hearing and Seeing by Technical Means</em> . Translated by Gloria Custance. Cambridge, MA: The MIT Press, 2006.&#160;<a href="#fnref:16" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:17">
<p>Stiegler, Bernard.  <em>Symbolic Misery, Volume 1: The Hyperindustrial Epoch</em> . Translated by Barnaby Norman. Cambridge, UK: Polity Press, 2014.&#160;<a href="#fnref:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:18">
<p>Burges, Joel, Nora Dimmock, and Joshua Romphf.  “Collective Reading: Shot Analysis and Data Visualization in the Digital Humanities.”  <em>DH and Media Studies Crossovers</em>  3, no. 3 (2016). <a href="http://www.teachingmedia.org/collective-reading-shot-analysis-and-data-visualization-in-the-digital-humanities/">http://www.teachingmedia.org/collective-reading-shot-analysis-and-data-visualization-in-the-digital-humanities/</a>.&#160;<a href="#fnref:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:19">
<p>Melgar Estrada, Liliana, Eva Hielscher, Marijn Koolen, Christian Gosvig Olesen, Julia Noordegraaf, and Jaap Blom.  “Film Analysis as Annotation: Exploring Current Tools.”  <em>Moving Image: The Journal of the Association of Moving Image Archivists</em>  17, no. 2 (2017): 40–70.&#160;<a href="#fnref:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref3:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:20">
<p>Enrollments for courses at University of Rochester using  <em>Mediate</em>  are as follows: &ldquo;Poetics of Television&rdquo; (Joel Burges) 2012 (27 students), 2013 (28 students), 2016 (58 students); &ldquo;Film History 1989-Present&rdquo; (Joel Burges) 2014 (29 students); &ldquo;Introduction to Media Studies&rdquo; (Joel Burges) 2019 (67 students); &ldquo;Clocks and Computers&rdquo; (Joel Burges) 2013 (20 students), 2015 (8 students); &ldquo;Recording 20th Century Music&rdquo; (Darren Mueller) 2018 (8 students); &ldquo;Experiments at the Edges of 20th Century Music&rdquo; (Darren Mueller) 2019 (29 students); &ldquo;Language and Advertising&rdquo; (Solveiga Armoskaite) 2018 (48 students), and 2019 (35 students); &ldquo;Signature Hitchcock/Hitchcock&rsquo;s Signature&rdquo; (James Rosenow) 2020 (24 students); &ldquo;Tourist Japan&rdquo; (Joanne Bernardi) 2020 (4 students); &ldquo;Unwept: Women and Silent film&rdquo; (Clara Auclair) 2020 (3 students); &ldquo;Student Teaching Secondary School Science&rdquo; (April Luehmann) 2020 (9 students).&#160;<a href="#fnref:20" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:21">
<p>Drucker, Johanna.  “Humanities Approaches to Graphical Display.”  <em>Digital Humanities Quarterly</em>  5, no. 1 (2011): 1–21.&#160;<a href="#fnref:21" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:22">
<p>Sebok, Bryan.  “Collaborative Models for Engagement.”  <em>Cinema Jounral Teaching Dossier</em> , Teaching Film and Media Studies in Liberal Arts Colleges, 2, no. 2 (Spring 2014). <a href="http://www.teachingmedia.org/teaching-film-media-studies-liberal-arts-colleges-cinema-journal-teaching-dossier-vol-2-2-spring-2014/">http://www.teachingmedia.org/teaching-film-media-studies-liberal-arts-colleges-cinema-journal-teaching-dossier-vol-2-2-spring-2014/</a>.&#160;<a href="#fnref:22" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:23">
<p>Allington, Daniel, Sarah Brouillette, and David Golumbia.  “Neoliberal Tools (and Archives): A Political History of Digital Humanities.”  <em>Los Angeles Review of Books</em> , May 1, 2016. <a href="https://lareviewofbooks.org/article/neoliberal-tools-archives-political-history-digital-humanities/">https://lareviewofbooks.org/article/neoliberal-tools-archives-political-history-digital-humanities/</a>.&#160;<a href="#fnref:23" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:24">
<p>Da, Nan Z.  “The Digital Humanities Debacle.” The Chronicle of Higher Education, March 27, 2019. <a href="https://www.chronicle.com/article/The-Digital-Humanities-Debacle/245986">https://www.chronicle.com/article/The-Digital-Humanities-Debacle/245986</a>.&#160;<a href="#fnref:24" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:25">
<p>Da, Nan Z.  “The Computational Case against Computational Literary Studies.”  <em>Critical Inquiry</em>  45, no. 3 (March 2019): 601–39. <a href="https://doi.org/10.1086/702594">https://doi.org/10.1086/702594</a>.&#160;<a href="#fnref:25" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:26">
<p>Greenberg, Clement.  “Modernist Painting.” In  <em>The Collected Essays and Criticism, Volume 4: Modernism with a Vengeance, 1957-1969</em> , edited by John O&rsquo;Brian. Chicago: University of Chicago Press, 1995.&#160;<a href="#fnref:26" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:27">
<p>Doane, Mary Ann.  “The Indexical and the Concept of Medium Specificity.”  <em>Differences</em>  18, no. 1 (January 1, 2007): 128–52. <a href="https://doi.org/10.1215/10407391-2006-025">https://doi.org/10.1215/10407391-2006-025</a>.&#160;<a href="#fnref:27" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:28">
<p>Crumrine, Seth, Amber Hudson, Simone Johnson, Sarah Kerecman, Anna Llewellyn, and Kyle Smith.  “&lsquo;That sounds so melodramatic&rsquo;: Theatricality and Realism in the Soap Opera and  <em>Game of Thrones</em> .” Unpublished essay,  “The Poetics of Television,” University of Rochester, Fall 2016.&#160;<a href="#fnref:28" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:29">
<p>Allen, Joseph, Josh Barnes, Arielle Lin, Mark Perilli, and Dean Smiros.  “The Formal Nucleus of Television, and Its Subservience to Narrative.” Unpublished essay,  “The Poetics of Television,” University of Rochester, Fall 2016.&#160;<a href="#fnref:29" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:29" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:30">
<p>We believe it is important not only to include these specifics, but also to attribute the papers to the students themselves. Moreover, as needed, we obtained permission from various students in the groups cited from &ldquo;The Poetics of Television&rdquo; and &ldquo;Introduction to Media Studies&rdquo; to share their work; in the latter, we conducted focus groups where students also signed off on us sharing their feedback and ideas. The comments from students in &ldquo;Experiments at the Edges of 20th Century Music&rdquo; and &ldquo;Linguistics and Advertising,&rdquo; however, have remained anonymous since we had less of this infrastructure in place.&#160;<a href="#fnref:30" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:31">
<p>Colberg, Steven, Kayoung Kim, Hannah O&rsquo;Connor, and Rachel Yang.  “Intensity in Songs: More than a Feeling.” Unpublished essay,  “Introduction to Media Studies,” University of Rochester, Spring 2019.&#160;<a href="#fnref:31" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:32">
<p>At the undergraduate level, ESM also offers robust degree programs in jazz performance and jazz composition. Graduate degree tracks include music leadership, conducting, early music, film composition, opera, music theory, ethnomusicology, and musicology.&#160;<a href="#fnref:32" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:33">
<p>Taruskin, Richard. _  The Oxford History of Western Music_ . New York: Oxford University Press, 2005.&#160;<a href="#fnref:33" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:34">
<p>Moseley, Roger.  “Digital Analogies: The Keyboard as Field of Musical Play.”     <em>Journal of the American Musicological Society</em>   68, no. 1 (2015): 151-228.&#160;<a href="#fnref:34" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:35">
<p>Rehding, Alexander, Gundula Kreuzer, Peter McMurray, Sybille Krämer, and Roger Moseley;  “Discrete/Continuous: Music and Media Theory after Kittler.”  <em>Journal of the American Musicological Society</em>  1 April 2017; 70 (1): 221–256. doi: <a href="https://doi.org/10.1525/jams.2017.70.1.221">https://doi.org/10.1525/jams.2017.70.1.221</a>&#160;<a href="#fnref:35" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:36">
<p>Kittler, Friedrich A.  <em>Gramophone, Film, Typewriter</em> . Translated and with an introduction by Geoffrey Winthrop-Young and Michael Wutz. Stanford, CA: Stanford University Press, 1999.&#160;<a href="#fnref:36" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:37">
<p>I adapted these questions from composer David Kirkland Garner (University of South Carolina).&#160;<a href="#fnref:37" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:38">
<p>Fasold, Ralph W., and Jeff Connor-Linton, eds.  <em>An Introduction to Language and Linguistics</em> . Cambridge: Cambridge University Press, 2014.&#160;<a href="#fnref:38" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:39">
<p>Sedivy, Julie, and Greg N. Carlson.  <em>Sold on Language: How Advertisers Talk to You and What This Says About You</em> . Chichester, West Sussex; Malden, MA: Wiley-Blackwell, 2011.&#160;<a href="#fnref:39" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:40">
<p>Lewis, David.  <em>The Brain Sell: When Science Meets Shopping: How the New Mind Sciences and the Persuasion Industry Are Reading Our Thoughts, Influencing Our Emotions and Stimulating Us to Shop</em> . London; Boston: Nicholas Brealey Publishing, 2013.&#160;<a href="#fnref:40" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:41">
<p>Poels, Karolien, and Siegfried Dewitte.  “How to Capture the Heart? Reviewing 20 Years of Emotion Measurement in Advertising.”  <em>Journal of Advertising Research</em>  46, no. 1 (March 2006): 18–37. <a href="https://doi.org/10.2501/S0021849906060041">https://doi.org/10.2501/S0021849906060041</a>.&#160;<a href="#fnref:41" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry><entry><title type="html">Book Review: Digital Sound Studies (2018)</title><link href="https://rlskoeser.github.io/dhqwords/vol/15/1/000543/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://rlskoeser.github.io/dhqwords/vol/15/1/000543/</id><author><name>Tracey El Hajj</name></author><published>2021-03-05T00:00:00+00:00</published><updated>2021-03-05T00:00:00+00:00</updated><content type="html"><![CDATA[<h2 id="heading"></h2>
<p><em>Digital Sound Studies</em> , edited by Mary Caton Lingold, Darren Mueller, and Whitney Trettien and published by Duke University Press (2018), brings together a variety of voices addressing the potential of digital approaches to sound, practically and theoretically. While most contributors acknowledge the field&rsquo;s novelty, they all agree on its timely interventions and the value of the interdisciplinary work this domain can provide. The field of digital sound studies not only investigates new methodologies but also contributes to — if not provokes — the field of digital humanities (DH). In their introduction, the editors note that &ldquo;[w]hile digital media [&hellip;] create a space of possibility for the study of sound, critical, interpretive labor fulfills this potential, not the technology itself&rdquo; <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. This early statement in the book echoes throughout the collection, particularly as authors discuss their processes in the making and success of a particular project or sonic investigation. The projects emphasize the labour that scholars invest and the initiative they take to advance sound studies and experiment with material, using a multitude of technologies that do not require advanced technical skills or expensive devices. In the afterward, Whitney Trettien asks Jonathan Sterne about &ldquo;the primary engine of change in the academy&rdquo; <sup id="fnref1:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. It is not technology. Sterne argues that institutions, money, and academic fashion drive academic change, as is the case for DH. In this edited volume, scholars demonstrate how individual initiative and labour play a big part in pushing scholarship in innovative directions. Intellectual and experimental labour are predominant in the chapters as the authors question DH&rsquo;s bias for text, all the while exploring pedagogical methods and research techniques that invite scholars — as well as the broader public — to invest in sound and listening as cultural approaches to the humanities.  <em>Digital Sound Studies</em>  demonstrates how sound and listening can advance humanities scholarship, detailing pedagogical methodologies that are often critical of DH while addressing its gaps and proposing ways for sound studies and DH to be complementary.</p>
<p>Contributors to  <em>Digital Sound Studies</em>  mostly come from an academic setting, and pedagogy emerges as a focus. Although not every chapter clearly defines a pedagogical intervention or approach, most do gesture at the importance of digital sound studies in humanities classrooms. The three chapters in &ldquo;Theories and Genealogies&rdquo; — by contributors Richard Cullen Rath, Myron M. Beasley, and Jonathan W. Stone — investigate sound and listening as performance, and highlight the importance of interdisciplinarity in their works. Richard Cullen Rath explores the importance of introducing computers with sound cards to classrooms, and emphasizes how it &ldquo;mak[es] more accessible the experiences of people who are not well represented in traditional documentary sources&rdquo; <sup id="fnref2:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. In &ldquo;The Pleasure (Is) Principle,&rdquo; Aaron Trammell, Jennifer Lynn Stoever, and Liana Silva share their experience as editors of the  <em>Sounding Out!</em>  blog. Aside from their focus on the processes and maintenance that go into the site, the authors explain how  <em>Sounding Out!</em>  aims beyond academia. The team&rsquo;s work toward the accessibility of sound studies is partly a pedagogical gesture.  <em>Sounding Out!</em> &rsquo;s interdisciplinary content and multimodal format allow its audience to access and hear multiple voices instead of a limited corpus of scholars whose work is already being broadly disseminated. Such an approach makes  <em>Sounding Out!</em>  a go-to platform for academics who share an investment in social justice and interdisciplinary conversations. Disseminating knowledge to the broader public, beyond institutional boundaries and inequities, is another timely issue that  <em>Digital Sound Studies</em>  addresses.</p>
<p>W. F. Umi Hsu also focuses on pedagogy in &ldquo;Reprogramming Sounds of Learning,&rdquo; where the readers journey with the author&rsquo;s students as they work on their projects. The author successfully &ldquo;propose[s] a series of experimental approaches that attempt to reprogram sounds back into learning and teaching&rdquo; <sup id="fnref3:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. The chapter argues for the strong impact of sound on engaged learning, centralizing the learning experience on &ldquo;the experience of sounding and listening&rdquo; <sup id="fnref4:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. In that respect, Hsu explores three principles: &ldquo;remediation, reflexivity, and resonance&rdquo; <sup id="fnref5:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. These principles align well with media studies and bring forth key aspects of multimodality in pedagogy. Hsu further highlights the role of reflexivity in bringing pedagogy outside the classroom: &ldquo;Deconstructing the recipe of how digital sound media are made via an act of remaking can afford students [&hellip;] to gain an access to personal and reflexive meanings of technology in their everyday lives&rdquo; <sup id="fnref6:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. Hsu&rsquo;s methodology entails having college students work with elementary school students to fulfill their project requirements. This approach bridges academic work and everyday life through sound and listening, and fosters sonic curiosity and appreciation among elementary students along with their college partners. As Michael J. Kramer argues, &ldquo;[t]he digital &lsquo;remediation&rsquo; of the image [&hellip;] provides an opportunity to open ears as well as eyes more fully to the echoes of the past&rdquo; <sup id="fnref7:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. Bringing together the visual and sonic study of artifacts is another example of active sensorial encounters with the material, proving as an effective means to engage a broad audience with diverse interests.</p>
<p>Beyond its contributions to pedagogy,  <em>Digital Sound Studies</em>  is an excellent resource for scholars looking to explore venues of accessible sound projects and the technological turn in the humanities. In &ldquo;Rhetorical Folkness: Reanimating Walter J. Ong in the Pursuit of Digital Humanity,&rdquo; Jonathan W. Stone urges that &ldquo;[a]s we look toward the future of digital sound studies, [&hellip;] frameworks, from secondary orality to digital humanity, usefully conceptualize the various ways contemporary vernacular culture is embedded within, performed through, and transformed by digital technology&rdquo; <sup id="fnref8:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. In that sense, technological accessibility promotes a broad reach for investigating humanities subjects through sound, beyond the limits of academic institutions. In &ldquo;Becoming OutKasted,&rdquo; Bradley discusses her initiative &ldquo;OutKasted Conversations,&rdquo; a webcast series published on YouTube. This series managed to &ldquo;creat[e] a digital site for teasing out how hip-hop can serve as a catalyst of change in the post-civil rights American South&rdquo; <sup id="fnref9:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. The author highlights the work of the group, OutKast, as it addresses a variety of issues, &ldquo;including race, gender, education, economics, spirituality vs. organized religion, sexuality, and identity&rdquo; <sup id="fnref10:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. The series consists of interviews that engage with the music in question, as it offers a critical framework to exploring issues — listed above — to &ldquo;speak to a wider audience than exists inside the classroom or between the pages of an academic journal&rdquo; <sup id="fnref11:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. Similarly, the HiPSTAS institute is concerned with how new, accessible methods of sound studies and their infrastructure affect scholarship. The project&rsquo;s principal investigator, Tanya Clement, explores modes of tagging sound clips and the politics and histories that standard classifications entail, examining the TEI Transcription for Speech guidelines. Similarly, Joanna Swafford describes the process of creating  <em>Augmented Notes</em>  and how the platform intends to &ldquo;build greater support for MEI (Music Encoding Initiative)&rdquo; because &ldquo;this additional functionality would increase the tool&rsquo;s interoperability and usefulness&rdquo; <sup id="fnref12:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. The platforms and initiatives discussed in  <em>Digital Sound Studies</em>  aim to increase accessibility and provide more functionalities to sound studies scholars, all the while addressing the politics, histories, and cultures inherent to the field. In that sense, contributors raise important questions about DH in an attempt to provide ways for it and sound studies to work together, or at least in parallel.</p>
<p>Another common theme throughout  <em>Digital Sound Studies</em>  is DH and how — or whether — it can encompass sound studies. The main premise for such an inclusion is the use of technology. While DH and digital sound studies involve the use of technology, platforms and initiatives in this book seem to be asking for inclusion and acknowledgment within DH. For instance, Trammell, Stoever, and Silva explain that &ldquo;[b]ecause many bloggers like [them] use a digital platform created by someone else, the question of whether blogging really constitutes &lsquo;making&rsquo; — a key but contested tenet of digital humanities — is a roiling debate. Of course, as this essay argues, [they] definitely think it does&rdquo; <sup id="fnref13:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. Although I agree that making in DH has been the subject of various debates, projects like  <em>Sounding Out!</em>  — which is well established in sound studies — do not need to be acclaimed as DH projects to be scholarly interventions. Nevertheless, in calling for acknowledgment by DH, contributors to  <em>Digital Sound Studies</em>  extend an invitation for DH to be more inclusive. For example, Steph Ceraso &ldquo;proposes several &lsquo;sound practices&rsquo; that are intended to help scholars account for full embodied kinds of sensory engagement; these practices amplify the ecological relationship between sound, bodies, and environments&rdquo; <sup id="fnref14:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. Such approaches promise a well-rounded undertaking of subject matter, beyond previously established intellectual biases. Further, in their introduction, Lingold, Mueller, and Trettien explain that scholars &ldquo;worry that the [DH] field has a far too comfortable relationship with systems of power that cultural criticism has long sought to challenge,&rdquo; adding that &ldquo;the text-centricity of the field [is] a bias that is baked into its institutional history&rdquo; <sup id="fnref15:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. As discussed in this review, contributors to the book communicate the efforts needed for sound and listening to be more appreciated as methodologies in the humanities. For instance, the platforms and initiatives described in the book place great weight on having sound clips be part of scholarship.</p>
<p><em>Digital Sound Studies</em>  invites conversation around sound studies and its relationships with neighboring fields, mostly DH. Contributors explore methodologies, platforms, and initiatives that demonstrate interdisciplinary and inclusive work that centers sound and listening. The chapters also provide examples of how sound scholarship can reach a wider public than is accessible through academic journals. Rebecca Dowd Geoffroy-Schwinden argues that &ldquo;[a] turn to diverse media in the presentation of audible history will encourage a vital rethinking of the performance of archival research as well as scholarly production and reception&rdquo; <sup id="fnref16:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. This statement is yet another invitation for scholars to invest in the affordances of sound as a methodology towards advanced understandings of the humanities. As Sterne asserts in his interview, &ldquo;what we need are deep and multidimensional infrastructures&rdquo; <sup id="fnref17:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>.  <em>Digital Sound Studies</em>  provides models of how to create such infrastructures. While I believe that this collection places more emphasis on DH than is necessary, the contributions provide a balanced critique of DH as a norm and culture while detailing digital sound studies&rsquo; contributions to the humanities and the public. This edited volume is an excellent resource for people interested in non-conventional experiences that defy standard and mainstream methods of learning and teaching within the humanities. It invites critical thought from cultural, social, and artistic frameworks, with a sustained and sustainable focus on the potential of sound and listening.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Lingold, Mary Caton, Darren Muller, and Whitney Trettien, eds. (2018).  <em>Digital Sound Studies</em> . London: Duke University Press.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref3:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref4:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref5:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref6:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref7:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref8:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref9:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref10:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref11:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref12:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref13:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref14:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref15:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref16:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref17:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry><entry><title type="html">Books Aren't Dead: Resurrecting Audio Technology and Feminist Digital Humanities Approaches to Publication and Authorship</title><link href="https://rlskoeser.github.io/dhqwords/vol/15/1/000527/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://rlskoeser.github.io/dhqwords/vol/15/1/000527/</id><author><name>Emily Edwards</name></author><author><name>Robin Hershkowitz</name></author><published>2021-03-05T00:00:00+00:00</published><updated>2021-03-05T00:00:00+00:00</updated><content type="html"><![CDATA[<p>Podcast Audio: <a href="resources/audio/audio01.mp3">audio01.mp3</a>  This podcast is an audio supplement to the article.</p>
<h2 id="heading"></h2>
<p>As an audio medium, podcasts are undergoing a renaissance in both consumption and production <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. Alternatively, as some writers have noted <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>, today&rsquo;s podcast market is saturated with content. Although the mutable form of a podcast may be situated in the historical context of radio broadcasting <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>  <sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>, a podcast is distinctively different from the radio broadcast medium in modes of delivery, archiving, and function. Podcasting is a more accessible audio format that has increasingly enabled a broad public to record conversations, interviews, and fictional narratives as well as disseminate audio recordings on various digital platforms rather than commercial airwaves. This article explores the podcast medium as a form of audio technology within the dynamic field of digital humanities (DH). We argue that the podcast as an audio format has the potential to facilitate the reimagining of an academic publication as feminist praxis. As co-producers of the podcast  <em>Books Aren&rsquo;t Dead</em>  ( <em>BAD</em> ), an affiliate of the  <em>Fembot Collective,</em>  we offer  <em>BAD</em>  as a case study to discuss how the podcast embodies feminist and critical approaches in the field of DH publication and project creation.</p>
<p><em>BAD</em>  features interviews of authors and creators of books, academic works, and games that are at the intersection of feminism, new technology, new media, and digital spaces <sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>.  <em>BAD</em>  was relaunched in the fall of 2018 as part of its mother organization, the  <em>Fembot Collective</em> . The Fembot Collective was founded in 2009 at the University of Oregon <sup id="fnref1:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>. Today, the Fembot Collective is a vibrant, scholarly community and an open-access publishing platform, supported by Manifold, that is home to the peer-reviewed journal  <em>Ada: Journal of Gender, New Media, and Technology,</em>  featuring the involvement of staff from universities across the country <sup id="fnref2:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>. As a part of the Fembot Collection, the  <em>BAD</em>  podcast does not simply mediate and produce content about feminism, technology, and media, but rather,  <em>BAD</em>  directly intervenes in existing debates in the field of DH by producing a feminist audio archive and foregrounding audio interviews as alternatives to written book reviews.</p>
<p>To demonstrate how the podcast format expands our definitions of text and therefore, what counts as scholarship in DH, this article begins by examining the  <em>BAD</em>  podcast in the context of feminist DH and the archival preservation of feminist scholarship. We outline how podcasting as a medium and genre allows for the expression and centralization of feminist values and forms of knowledge production. We then recount how podcasting permits the preservation of feminist voices and the curation of an archive of feminist knowledge evidenced by two recent  <em>BAD</em>  interviews. We discuss the significance of performance and mediation between producer, listener, and interviewee in the podcast format. Next, we contextualize how  <em>BAD</em>  subverts and problematizes existing hierarchies in academic publishing and models a feminist praxis. We conclude with encouragement for other DH practitioners interested in utilizing this audio format through small-scale feminist DH projects and publications.</p>
<h2 id="critical-digital-humanities-debates-tensions-and-approaches">(Critical) Digital Humanities: Debates, Tensions, and Approaches</h2>
<p><em>BAD</em>  is situated within emerging debates in DH scholarship, specifically the potential for DH projects to be transformative, feminist, and intersectional. Today we witness the increased prominence of technological devices in the toolbox of humanities scholars <sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>, which is situated within a broader trend of the new  “datafied”  ways of reading, analyzing, and deconstructing texts. These trends privilege a  “big data”  approach to information <sup id="fnref1:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>  <sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>  <sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>. This  “big data”  turn marginalizes feminist and critical methods and deprivileges close readings, situated case studies, and the subjective and contingent position of knowledge <sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup>. Scholars have noted the propensity of DH projects to at best fetishize digital technologies that curate and catalog mass amounts of information <sup id="fnref2:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>, or at worst, that inadvertently reproduce dominant, oppressive structures and ideologies onto texts and archives <sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup>. The field of DH has been critiqued as male-dominated and aligned with the neoliberal corporatization of academic knowledge <sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>  <sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>.</p>
<p>Despite these important critiques of the field of DH, we situate  <em>BAD</em>  as a DH project in the context of a vibrant tradition of feminist DH scholarship, and we draw attention to how  <em>BAD</em>  actualizes what we view as the main goal of feminist DH work, to  “&hellip;expand our notions of text and context, archive and canon, and code and program”   <sup id="fnref1:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>. Our intervention in this project of expansion is the reimagining of the academic book review as a podcast, which falls within a longer trajectory of feminist scholarship in DH. As Alexis Lothian and Amanda Phillips have noted,  “archives may be the most legible form of digital humanities production”   <sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup>. Our DH project produces the necessary historical archives of feminist scholarship, but also expands upon the use and content of an archive in the podcast format <sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup>. We take a broader view of archives, moving away from the primacy of written texts and draw particular attention to how audio recordings serve as living archives. Therefore, in addition to the technology of the feminist archive,  <em>BAD</em>  also draws upon the affordances of audio technology to expand the definition of text, thereby embracing feminist forms of knowledge production that characterize a feminist DH approach.</p>
<p>Along with the expansion of texts, the importance of affective or mediated connection, collaborative labor arrangements, and centralization of ethical values also shape intersectional feminist DH <sup id="fnref2:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>. Following this call,  <em>BAD</em>  ultimately seeks to produce scholarly knowledge through a transparent, collaborative process, via audio rather than textual means. We host  <em>BAD</em>  in an open-access format — available to stream on Anchor, Spotify, Apple Podcasts, and our website  <em>Books Aren&rsquo;t Dead</em>   <sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup> — to bring authors and reviewers together in a collaborative context to reimagine the traditional concept of a book review. As a podcast, the book review interview facilitates connectivity and collaboration between author and interviewer that is not possible through the process of writing a traditional textual book review.</p>
<p>Our approach is in stark contrast to dominant publication models. Book reviews, in the traditional textual format, often appear in academic peer-reviewed journals, with little interaction between the reviewer and the author. Furthermore, book reviews are often pay-walled and thus inaccessible to a broader public <sup id="fnref:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup>  <sup id="fnref:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup>. Increasingly, academics at smaller institutions and independent scholars struggle to pay for institutional or individual access to expensive journal subscriptions <sup id="fnref1:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup>  <sup id="fnref:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup>. As a result, both the production and consumption of book reviews alienates the reviewer, author, and audience from one another. As Alexis Lothian and Amanda Phillips note, the  <em>Fembot Collective</em>  — through its commitment to collaborative, open peer review and an open-access platform — has  “expanded [the] notion of what  article  means”  and more broadly serves as an  “innovator in scholarly communicative possibilities”  within the field of DH <sup id="fnref1:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup>. Podcasting, therefore, offers an exciting approach that addresses critiques of DH and models how the field can challenge structures and hierarchies of power in the academy.</p>
<p>Additionally,  <em>BAD</em>  attempts to go beyond an additive approach to feminist knowledge production by emphasizing the scholarly value of podcast interviews not as addendums to textual book reviews, but as an alternative format. Roopika Risam, in problematizing Western biases in existing DH scholarship, notes that for scholars committed to principles of social justice, including feminism, such scholars must  “challenge the exclusions in the record of digital knowledge”   <sup id="fnref:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup>. Risam further argues that the simple inclusion of marginalized voices is not enough; rather, we must  “seize control over the means of digital knowledge production”   <sup id="fnref1:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup>. In  <em>BAD</em> , we not only include feminist voices in the digital culture record, but we also seize the microphone. We create a new archive for a new era of DH scholarship that is feminist and intersectional. In terms of content, this includes the preservation of feminist voices; in terms of structure, this includes the facilitation of mediated connection between listener, producer, and interviewee; finally, in terms of practice, this project revises exclusive definitions of authorship within academic publishing. Therefore,  <em>BAD</em>  functions as an intervention by sharing and preserving feminist voices through its interview format that expands DH&rsquo;s notions of text and context as well as archive and canon.</p>
<h2 id="books-arent-dead-archiving-feminist-voices-through-interviews">Books Aren&rsquo;t Dead: Archiving Feminist Voices Through Interviews</h2>
<p>In line with feminist calls for context and close analysis, we turn to the feminist work of two podcasts. One of  <em>BAD&rsquo;s</em>  key aims is to curate an audible archive of feminist voices. Risam, in discussing the possibility of intersectional approaches in the field of DH, emphasizes that the  “relationship between theory and praxis is integral to the digital humanities. Connections between the two appear in the archives built, corpora analyzed, oral histories recorded, and geographies mapped”   <sup id="fnref1:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup>. Our interview with  <em>Fembot Collective</em>  founder Carol Stabile in  <em>BAD</em> &rsquo;s inaugural relaunch interview demonstrates such feminist digital humanities praxis; Stabile is a feminist media scholar and the Associate Dean for Strategic Initiatives of the College of Arts and Sciences at the University of Oregon. Archiving Stabile&rsquo;s work as a key figure in feminist media studies <sup id="fnref:20"><a href="#fn:20" class="footnote-ref" role="doc-noteref">20</a></sup>  <sup id="fnref:21"><a href="#fn:21" class="footnote-ref" role="doc-noteref">21</a></sup>  <sup id="fnref:22"><a href="#fn:22" class="footnote-ref" role="doc-noteref">22</a></sup>, makes the knowledge in Stabile&rsquo;s scholarly monographs accessible through the podcast format. The connection between feminist theory and praxis is realized in the content of  <em>BAD&rsquo;s</em>  archives, and specifically through our interview with Stabile.</p>
<p>In this interview, we discussed Stabile&rsquo;s most recent work,  <em>The Broadcast 41: Women and the Anti-Communist Blacklist</em>   <sup id="fnref:23"><a href="#fn:23" class="footnote-ref" role="doc-noteref">23</a></sup>, which focuses on how forty-one progressive women writers, performers, and creators working in the American television industry between the 1930s and 1940s were forced out of their professions as targets of the  “Red Scare.”  Stabile&rsquo;s book illustrates how this resulted in a television landscape with a profound absence of progressive, feminist voices that we recognize in the white-washed, conservative  “Golden Age of Television”  of the 1950s. In the interview, we connect contemporary forms of exclusion and marginalization within audio and visual industries, such as the #MeToo movement to the early 20th century, as explored in Stabile&rsquo;s work.</p>
<p>In the podcast conversation, we were interested in Stabile&rsquo;s process of connecting the early age of television production to the contemporary media landscape. Stabile noted that  “moments when new media get introduced [are] really important moments for studying the history of struggle”   <sup id="fnref:24"><a href="#fn:24" class="footnote-ref" role="doc-noteref">24</a></sup>. She situates the marginalization and disenfranchisement of progressive women writers in the 1930s and 1940s concerning contemporary trends of racial bias and conservative capture of media institutions, which we have most recently witnessed through the prominence of media figures such as President Donald Trump and conspiracy-theorist Alex Jones <sup id="fnref1:24"><a href="#fn:24" class="footnote-ref" role="doc-noteref">24</a></sup>. Stabile spoke about the importance of highlighting the legacy of progressive women writers in the television industry during the blacklist. She noted that by archiving the underexplored contributions of progressive women in television, this  “makes you understand that you&rsquo;re not the first person to fight against the structures, but that you&rsquo;re part of a history of resistance. And I think that that can be inspiring and sustaining to people who struggle today”   <sup id="fnref2:24"><a href="#fn:24" class="footnote-ref" role="doc-noteref">24</a></sup>.</p>
<p>By curating these conversations with feminist authors such as Stabile, we highlight the existence of a community of feminist media scholars, emphasize the critical work that is being done in our own field, and underscore that resistance scholarship exists, even if not in one&rsquo;s home institution or department. Wernimont and Losh note that within the field of DH, feminist, antiracist work and scholarship are often actively minimized or viewed as superfluous to the field <sup id="fnref3:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>. In this scholarly landscape where there are active questions of whether feminist, critical, antiracist, and postcolonial approaches are welcome in the field DH scholarship <sup id="fnref:25"><a href="#fn:25" class="footnote-ref" role="doc-noteref">25</a></sup>  <sup id="fnref2:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup>, interviewing feminist scholars working in the fields of media, technology, and science is an active response to the erasure of these approaches. Building our archive, we explicitly seek contributions that centralize feminist perspectives to counter institutional and field erasure <sup id="fnref:26"><a href="#fn:26" class="footnote-ref" role="doc-noteref">26</a></sup>. Additionally, we structure the interview process to create an open network of scholarly discussions accessible to those who may not have departmental structures that support feminist research. By archiving interviews with authors writing within this intersection of feminism, media, and technology, we create a new community of feminist scholars who are able to listen and engage.</p>
<p>In our second interview with Polina Kroik about her book  <em>Cultural Production and the Politics of Women&rsquo;s Work in American Literature and Film</em>   <sup id="fnref:27"><a href="#fn:27" class="footnote-ref" role="doc-noteref">27</a></sup>, we discussed the legacy of women who are cultural producers between the 1920s and 1950s. Kroik emphasized the relationship between changes in technology, such as typewriters and the physical structures of offices, and how these material changes inaugurated new modes and forms of authorship, which forced women to engage in forms of affective, emotional labor. These changes continue to inform emerging power imbalances in the workplace that reverberate in cultural industries, such as Hollywood, with Kroik also highlighting the #MeToo movement <sup id="fnref:28"><a href="#fn:28" class="footnote-ref" role="doc-noteref">28</a></sup>. With this interview, we emphasize the significance of not only the preservation of Kroik&rsquo;s voice as a feminist scholar, but the possibility of  <em>BAD</em>  to emphasize the historical marginalization and exclusion of female media producers, creators, and writers in cultural industries.</p>
<p>While we agree with Risam that an additive approach to critical DH is not enough <sup id="fnref2:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup>, the addition of podcast interviews serve as potential models for alternative recording and publishing of feminist histories. The curation of a feminist digital archive through podcasts is only one aspect of the significance of this type of DH work. The interview does not merely comprise a static archive; rather, the interview can be read beyond its status as a text and publication. The connections produced through the mediated performance of producer, interviewee, and listener also underscore the value of podcasts as an alternative form of book review publication that privileges connectivity and affect, key principles in our feminist approach <sup id="fnref4:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>.</p>
<h2 id="podcaster-guest-and-listening-audience-a-mediation-of-performances">Podcaster, Guest, and Listening Audience: A Mediation of Performances</h2>
<p>Podcasts can be a site of performance whereby the interviewer and interviewee share affective, aural, and interpersonal intimacy that shapes knowledge creation and circulation. Thus, we consider performance studies an important context with which to locate podcasting in feminist praxis. In one of the first edited academic collections on podcasting,  <em>Podcasting: New Aural Cultures and Digital Media</em>  (2018), the editors note that podcasts represent  “the possibility, in one  space,  to create a considered yet engaging conversation that merges criticality, scholarship, fandom, and practice, not to mention the possibility of attracting an audience that found value in our conversations”   <sup id="fnref:29"><a href="#fn:29" class="footnote-ref" role="doc-noteref">29</a></sup>. Here, the podcast is more than the mp3 file or an audio recorded sound. The space is one of performance, invention, and even resistance. As performance studies scholars Richard Schechner and Sara Brady state,  “performance must be construed as a broad spectrum or continuum of human actions, ranging from ritual, play, popular entertainments, performing arts and everyday life performances and to the enactment of professional, gender, race, and class roles”   <sup id="fnref:30"><a href="#fn:30" class="footnote-ref" role="doc-noteref">30</a></sup>. As such, we emphasize how the various performances within the production of podcasting underscore the potential for feminist DH praxis.</p>
<p>Regardless of genre, podcasts are a type of performance where podcasters and guests are performing a certain version of themselves in collaborative communication. In emphasizing the performance aspects of podcasting, we draw attention to the possibility of co-creation through performance, which facilitates forms of affective, dynamic communication outside of the removed and supposedly objective tone of the written book review. Podcasting has the potential for collaborative knowledge production between author and interviewee through the shared experience of performance in the audio conversation. Additionally, podcasts provide a staging ground for feminist conversations through the ease of podcast production and distribution on digital platforms. Using the lens of performance studies, we argue for the possibility of viewing podcasts as a medium that allows for the centralization of feminist DH praxis.</p>
<p>As an affective form of communication, the act of podcasting a book review connects to feminist DH praxis. The surplus information that the affect of podcasting invites transforms the value of a book review to align with feminist approaches. Podcasts, such as  <em>BAD</em> , reveal previously hidden elements of academic knowledge, such as the conditions of production, ethics, relationships, and the socio-political context of the work. In our interviews, we are interested not simply in the content of the text but the authors&rsquo; experience in producing the text. We particularly want to emphasize the fluidity of a podcast performance that facilitates feminist forms of communicative engagement. The audience, while listening rather than reading, experiences a form of affective connection that allows for identification with the voices and the centralization of feminist themes. The audience&rsquo;s experience of listening is now included in the creation of knowledge. Each individualized listeners&rsquo; decoding allows for and encourages different places of entry and departure within the podcast.</p>
<p>Just as the listener has the opportunity to engage with the podcast, the author or interviewee has the potential to perform the self as an authoritative voice via co-scriptive interactions and discussions. Typically, the accessibility of an academic publication creates the scholarly authority, specifically if it is behind a paywall and situated within a vocabulary only understood by other academics. However, we contend that this type of scholarly authority is determined by institutional methods that are arbitrary and prohibitive to a broader public. In  <em>BAD</em> , the interviewer and the interviewee are not just performing scholarly inquiry but also engaging in dynamic communication throughout the interview that redefines scholarly authority. The interviewer prepares written questions, but the conversation may change as she switches her line of inquiry to the author. Additionally, the author can lead the conversation in various directions as part of an ongoing dialogue. Stacy Copeland emphasizes that in podcasts, the changing emotional tone of voice and other background audio allows for the podcast to become a form of affective media. Copeland states that in podcasts,  “the material voice becomes a sticky surface to be stuck with multiple decodings, re-workings and affect”   <sup id="fnref:31"><a href="#fn:31" class="footnote-ref" role="doc-noteref">31</a></sup>. An interview between two people can become  “sticky”  in the sense that emotional changes in voice, laughter, and unexpected moments can produce the possibility for multiple points of engagement. Stickiness, then, produces surplus affect and creates the conditions for greater dynamic, dialogic engagement, thus providing a facet of connection that goes beyond the formulaic structure of a book review.</p>
<p>While Stabile and all the authors interviewed on  <em>BAD</em>  are experienced scholars, the relationship between interviewee and interviewer produces a more equitable relational dynamic compared to the structure of the book review in which the reviewer comments on and critiques a text from afar. The ability to ask questions of authors in real-time through the podcast performance for clarification, expansion of concepts, or even constructive critique, produces new knowledge and information about the monographs through dialogue and partnership. The interview not only produces a dynamic product but through the process of the interview performance, the interviewer engages in authorship through performance. This method is directly aligned with feminist emphasis on collaborative forms of knowledge production outside of the confines of the academy and the exclusive academic publishing apparatus <sup id="fnref2:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup>.</p>
<p>Our framing of podcasting as performance is an important contribution to feminist DH praxis. To be included within feminist DH praxis, the podcast ought to produce a unique space for dialogue and engagement that was previously limited in the current state of academic publishing. We situate  <em>BAD</em>  to serve as an aural space for feminist scholarly engagement in the field of DH that is, at times, exclusionary to feminist concerns, projects, and critiques <sup id="fnref5:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>. Another way in which podcasting as performance establishes feminist DH praxis is that podcasts ultimately represent active possibilities. These possibilities include performance, affective connection, staging, learning, and the extension of the professional relationship beyond the interview. Podcasts have the potential to directly archive emotions and experiences, and therefore feminist ways of knowing, thus serving as both affective as well as physical archives. Affective archives preserve performances, experiences, emotions, and interactions. Thus, the performance of podcasting is crucial to subverting academic hierarchies by creating archives beyond the physical and written form.</p>
<h2 id="subverting-academic-publishing-hierarchies-and-podcasting-feminist-futures">Subverting Academic Publishing Hierarchies and Podcasting Feminist Futures</h2>
<p>Subverting academic hierarchies through podcasting offers dynamic new opportunities for feminist DH practitioners. In our professionalization, we have been informed that in the hierarchy of publishing, book reviews may be lower in prestige and often serve as a way for graduate students to ease into publishing. Therefore, the individuals that do engage with book reviews may find their labor delegitimized and seen as  “knowing their place”  and  “doing their time.”  However, we see engaging with the labor of other people&rsquo;s work as incredibly important to elevate and legitimize one another&rsquo;s research, especially for marginalized authors in academia. This is important since the work of individuals who identify as Black scholars, indigenous scholars, scholars of color, and female-identified and non-binary scholars, is underrepresented in academic literature and within the academic publishing industry <sup id="fnref:32"><a href="#fn:32" class="footnote-ref" role="doc-noteref">32</a></sup>  <sup id="fnref:33"><a href="#fn:33" class="footnote-ref" role="doc-noteref">33</a></sup>  <sup id="fnref:34"><a href="#fn:34" class="footnote-ref" role="doc-noteref">34</a></sup>. A feminist perspective shifts the focus on padding the reviewer&rsquo;s CV to supporting the author in the interview space and amplifying feminist scholarship.</p>
<p>Another intervention is using sound. Podcasting can facilitate a more feminist use of the idea of voice, both literally and figuratively. Like many women, we were socialized into thinking our voices were not authoritative both in content and delivery. Just as marginalized scholars confront an academic publishing apparatus that is biased towards white, male forms of authorship and knowledge production <sup id="fnref1:32"><a href="#fn:32" class="footnote-ref" role="doc-noteref">32</a></sup>  <sup id="fnref1:33"><a href="#fn:33" class="footnote-ref" role="doc-noteref">33</a></sup>  <sup id="fnref1:34"><a href="#fn:34" class="footnote-ref" role="doc-noteref">34</a></sup>, feminine voices are audibly policed. This can manifest in the characterization of  “vocal fry”  as unprofessional and unattractive <sup id="fnref:35"><a href="#fn:35" class="footnote-ref" role="doc-noteref">35</a></sup>, or a preference among men to listen to higher-pitched femininized voices over average pitched tones <sup id="fnref:36"><a href="#fn:36" class="footnote-ref" role="doc-noteref">36</a></sup>. However, we maintain that a podcast&rsquo;s performative authorship breaks down barriers to knowledge such as paywalls, and also creates spaces for diverse voices to emerge and claim authority. Podcasts become a way to intervene in the politics of sound.</p>
<p>Along with amplifying voices, the form of podcasting itself challenges existing hierarchies. The technical and financial barriers to entry are lower than many other DH methods. On the distribution side, platforms for producing and distributing podcasts, such as Anchor, have become increasingly user-friendly. Unlike most academic journals that are behind steep paywalls, listeners can access and download podcasts with financial and technical ease. As a result, one can bypass expensive and narrow publishing models while increasing access and discovery of, in our case, feminist scholarship.</p>
<p>Furthermore, DH projects should not be viewed as alternatives to traditional academic practices; rather, the academic hierarchies must be changed to accommodate new ways of knowledge production. One example that both incorporates and challenges existing academic frameworks is  <em>Reviews of Digital Humanities</em> , a journal founded by Jennifer Guiliano and Roopika Risam in 2019. The goal of this open-access journal  “is to foster critical discourse about scholarship in a format useful to other scholars,”  publishing  “project overviews written by project directors alongside peer reviews written by members of digital humanities communities”   <sup id="fnref:37"><a href="#fn:37" class="footnote-ref" role="doc-noteref">37</a></sup>. This project takes the form of incorporating previous academic standards by existing as a peer-reviewed journal but expands upon the idea of what is considered eligible and credible to academic archives. Guiliano and Risam&rsquo;s initiative demonstrates how creators of DH projects can still be legitimized by their peers through rigorous reviews and evaluation, but with DH projects and publications considered on their own terms, rather than trying to fit into the very evaluative structure of academic journal publishing that DH practitioners are trying to subvert.  <em>BAD</em> , therefore, synthesizes traditional academic publishing conventions, such as the book review, with the podcasting format to realize the potential of feminist scholarly engagement. The project models how small-scale feminist DH audio projects can realize a different future for DH.</p>
<h2 id="conclusion">Conclusion</h2>
<p>In conclusion, we have emphasized the significance and transformative potential of utilizing audio technology to produce alternative forms of knowledge. We have situated  <em>BAD</em>  within the scholarly conversation of feminist DH practice, drawing attention to expanding definitions of scholarly texts and the possibility of creating a dynamic archive.  <em>BAD</em>  utilizes podcasting to highlight and preserve the work of feminist scholars writing within spheres of media and technology. Additionally, we have suggested the importance of drawing on performance studies to understand the mediated connection between audience, interviewee, and producer, which is a form of collaborative connection unique to the podcast format. Ultimately, in this article, we have demonstrated how podcasting can allow feminist DH practitioners to produce and publish scholarly knowledge in a way that effectively challenges the hierarchical practices of academic publishing.</p>
<p>This case study contributes to the feminist field of DH and serves as a call to action for other feminist and critical DH scholars to engage with the audio medium of podcasting. There is a burgeoning field of academic podcasts that demonstrate the exciting possibility of producing knowledge from a variety of interdisciplinary perspectives using audio technology, including the podcast  <em>Rocking the Academy</em>  from Mary Churchill and Roopika Risam, the  <em>MFAngle Podcast</em>  from Mirna Palacio Ornelas, Bessie F. Zaldívar, and Blessing Christopher et al., and the podcast  <em>Alt! Ack!</em>  from Robin Hershkowitz and Patrick Felton. These are just a few examples of some exciting additions to the field of critical podcasting.</p>
<p>Ultimately, as a truly interdisciplinary field, DH includes ways of knowing that reject arbitrary disciplinarily divisions of knowledge and structures of hierarchy within the academy. Technical knowledge has been streamlined by accessible programs and applications. Thus, the opportunity to create is increasingly available to the public and individuals with beginner levels of technical acumen. We believe that podcasting can be a radical interdisciplinary intervention that challenges not only the textual form of scholarly discourse but also its content. We have seized the mic at Bowling Green State University as  <em>BAD</em>  co-producers, and we hope this article serves as an impetus to pass the mic to other DH scholars invested in feminist and critical approaches.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Bech, L.  “Is This the Golden Age of Podcasts?,”    <em>Columbia Journalism Review</em>  (2014). Retrieved from: <a href="https://archives.cjr.org/behind_the_news/is_this_the_golden_age_of_podc_1.php">https://archives.cjr.org/behind_the_news/is_this_the_golden_age_of_podc_1.php</a>.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Hempel, J.  “If Podcasts Are the New Blogs, Enjoy the Golden Age While It Lasts,”    <em>Wired</em>  (2015) Retrieved from: <a href="https://www.wired.com/2015/12/if-podcasts-are-the-new-blogs-enjoy-the-golden-age-while-it-lasts/">https://www.wired.com/2015/12/if-podcasts-are-the-new-blogs-enjoy-the-golden-age-while-it-lasts/</a>.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Berry, R.  “Podcasting: Considering the Evolution of the Medium and Its Association with the Word  Radio, ”  <em>Radio Journal: International Studies in Broadcast &amp; Audio Media</em>  14.1 (2016): 7–22.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Menduni, E.  “Four Steps in Innovative Radio Broadcasting: From QuickTime to Podcasting,”    <em>Radio Journal: International Studies in Broadcast &amp; Audio Media</em>  5.1 (2007): 9–18.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>Fembot Collective “Home.” (2019). Retrieved from: <a href="https://fembotcollective.manifoldapp.org/">https://fembotcollective.manifoldapp.org</a>.&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>Dobson, J.E.  <em>Critical Digital Humanities: The Search for a Methodology</em> . University of Illinois Press, Champaign (2019).&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>Kitchin, R.  “Big Data, new epistemologies and paradigm shifts,”    <em>Big Data &amp; Society</em>  1, (2014): 1–12.&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>van Es, K. and M. T. Schäfer.  <em>The Datafied Society. Studying Culture through Data</em> . Amsterdam University Press, Amsterdam (2017).&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>Leurs, K.  “feminist data studies: using digital methods for ethical, reflexive and situated socio-cultural research,”    <em>Feminist Review</em>  115, (2017): 130–154.&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>Risam, R.  “Beyond the Margins: Intersectionality and the Digital Humanities,”    <em>DHQ: Digital Humanities Quarterly,</em>  9.2 (2015).&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>Allington, D., S. Brouillette, and D. Golumbia.  “Neoliberal Tools (and Archives): A Political History of Digital Humanities”  (2016).  <em>Los Angeles Review of Books</em> . Retrieved from: <a href="https://lareviewofbooks.org/article/neoliberal-tools-archives-political-history-digital-humanities/">https://lareviewofbooks.org/article/neoliberal-tools-archives-political-history-digital-humanities/</a>.&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p>Wernimont, J. and E. Losh.  “Introduction.” In J. Wernimont and E. Losh (eds.),  <em>Bodies of Information Intersectional Feminism and the Digital Humanities.</em>  University of Minnesota Press, Minneapolis and London, (2018): ix–xxvi.&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref3:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref4:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref5:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13">
<p>Lothian, A. and A. Phillips.  “Can Digital Humanities Mean Transformative Critique?,”    <em>Journal of E-Media Studies</em>  3.1 (2013).&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14">
<p>Wernimont, J.  “Whence Feminism? Assessing Feminist Interventions in Digital Literary Archives.”  <em>DHQ: Digital Humanities Quarterly</em>  7.1 (2013).&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:15">
<p>Hershkowitz, R., and E. Edwards.  “Books Aren&rsquo;t Dead,”    <em>Books Aren&rsquo;t Dead</em>  (2020). Retrieved from: <a href="https://anchor.fm/booksarentdead/">https://anchor.fm/booksarentdead/</a>.&#160;<a href="#fnref:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:16">
<p>Brienza, C.  “Opening the Wrong Gate? The Academic Spring and Scholarly Publishing in the Humanities and Social Sciences,”    <em>Publishing Research Quarterly</em>  28.3 (2012): 159–171.&#160;<a href="#fnref:16" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:17">
<p>Logan, C. J.  “We Can Shift Academic Culture through Publishing Choices,”    <em>F1000 Research</em>  6.518 (2017).&#160;<a href="#fnref:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:18">
<p>Sample, I.  “Harvard University Says It Can&rsquo;t Afford Journal Publishers&rsquo; Prices,”    <em>The Guardian</em> , (2012). Retrieved from: <a href="http://wavelets.ens.fr/BOYCOTT_ELSEVIER/ARTICLES/2012_04_24_The_Guardian.pdf">http://wavelets.ens.fr/BOYCOTT_ELSEVIER/ARTICLES/2012_04_24_The_Guardian.pdf</a>.&#160;<a href="#fnref:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:19">
<p>Risam, R.  <em>New Digital Worlds Postcolonial Digital Humanities in Theory, Praxis, and Pedagogy</em> . Northwestern University Press, Evanston (2018).&#160;<a href="#fnref:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:20">
<p>Stabile, C.A.  “Shooting the Mother: Fetal Photography and the Politics of Disappearance.”    <em>Camera Obscura Feminism, Culture, Media Studies</em>  10, (1992): 178–205.&#160;<a href="#fnref:20" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:21">
<p>Stabile, C. A.  <em>Feminism and the Technological Fix.</em>  Manchester University Press, Manchester, UK (1994).&#160;<a href="#fnref:21" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:22">
<p>Stabile, C. A.  <em>White victims, black villains: gender, race, and crime news in US culture,</em>  Routledge, New York, NY (2006).&#160;<a href="#fnref:22" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:23">
<p>Stabile, C.A.  <em>The Broadcast 41: Women and the Anti-Communist Blacklist</em> . Goldsmiths Press, London (2018).&#160;<a href="#fnref:23" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:24">
<p>Edwards, E., and R. Hershkowitz.  “Carol Stabile Interview,”    <em>Books Aren&rsquo;t Dead</em>  (2019). Retrieved from: <a href="https://fembotcollective.manifoldapp.org/projects/books-aren-t-dead/resource/carol-stabile-interview">https://fembotcollective.manifoldapp.org/projects/books-aren-t-dead/resource/carol-stabile-interview</a>.&#160;<a href="#fnref:24" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:24" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:24" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:25">
<p>McPherson, T.  “9: Why Are the Digital Humanities So White? or Thinking the Histories of Race and Computation.”  In M.K Gold (ed.),  <em>Debates in Digital Humanities,</em>  University of Minnesota Press, Minneapolis, MN (2012).&#160;<a href="#fnref:25" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:26">
<p>Hershkowitz, R. Call for Submissions: Books Aren&rsquo;t Dead Podcast. (2019).&#160;<a href="#fnref:26" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:27">
<p>Kroik, P.  <em>Cultural Production and the Politics of Women&rsquo;s Work in American Literature and Film</em> . Routledge, New York (2019).&#160;<a href="#fnref:27" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:28">
<p>Edwards, E., and R. Hershkowitz.  “Polina Kroik Interview,”  <em>Books Aren&rsquo;t Dead</em>  (2019). Retrieved from: <a href="https://fembotcollective.manifoldapp.org/projects/books-aren-t-dead/resource/polina-kroik-interview">https://fembotcollective.manifoldapp.org/projects/books-aren-t-dead/resource/polina-kroik-interview</a>.&#160;<a href="#fnref:28" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:29">
<p>Llinares, D., and N. Fox. Eds <em>. Podcasting: New Aural Cultures and Digital Media</em> . Palgrave Macmillan, New York (2018).&#160;<a href="#fnref:29" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:30">
<p>Schechner, R. and S. Brady.  <em>Performance studies: an introduction</em> . Routledge, London (2013).&#160;<a href="#fnref:30" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:31">
<p>Copeland, S.  “A Feminist Materialisation of Amplified Voice: Queering Identity and Affect in  <em>The Heart.</em> ”  In Llinares, D., Fox N., and R. Berry (eds.),  <em>Podcasting New Aural Cultures and Digital Media.</em>  Palgrave McMillan, New York (2018): 209-225.&#160;<a href="#fnref:31" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:32">
<p>Mirza, H. S.  “Decolonizing Higher Education: Black Feminism and the Intersectionality of Race and Gender,”    <em>Journal of Feminist Scholarship,</em>  7–8 (2015): 1–12.&#160;<a href="#fnref:32" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:32" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:33">
<p>Roh, C.  “Inequalities in Publishing,”    <em>Urban Library Journal</em>  22. 2 (2016): 1-16.&#160;<a href="#fnref:33" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:33" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:34">
<p>Souto-Manning, M. and N. Ray.  “Beyond Survival in the Ivory Tower: Black and Brown Women&rsquo;s Living Narratives,”    <em>Equity &amp; Excellence in Education</em>  40.4 (2007): 280–90.&#160;<a href="#fnref:34" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:34" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:35">
<p>Anderson, R.C., C.A. Klofstad, W.J. Mayew, and M. Venkatachalam.  “Vocal Fry May Undermine the Success of Young Women in the Labor Market,”    _PLOS ONE _ 9.5 (2014).&#160;<a href="#fnref:35" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:36">
<p>Feinberg, D.R., L.M. DeBruine, B.C. Jones, and D.I. Perrett.  “The Role of Femininity and Averageness of Voice Pitch in Aesthetic Judgments of Women&rsquo;s Voices,”    <em>Perception</em>  37.4 (2008): 615–623.&#160;<a href="#fnref:36" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:37">
<p>Guiliano, J., and R. Risam.  “Welcome to Reviews in Digital Humanities” ,  <em>Reviews in Digital Humanities</em>  (2020) <em>.</em>  Retrieved from: <a href="https://reviewsindh.pubpub.org">https://reviewsindh.pubpub.org</a>.&#160;<a href="#fnref:37" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry><entry><title type="html">Comparative K-Pop Choreography Analysis through Deep-Learning Pose Estimation across a Large Video Corpus</title><link href="https://rlskoeser.github.io/dhqwords/vol/15/1/000506/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://rlskoeser.github.io/dhqwords/vol/15/1/000506/</id><author><name>Peter Broadwell</name></author><author><name>Timothy R. Tangherlini</name></author><published>2021-03-05T00:00:00+00:00</published><updated>2021-03-05T00:00:00+00:00</updated><content type="html"><![CDATA[<h2 id="1-introduction">1. Introduction</h2>
<p>The ability to derive accurate information about human body poses and movements from arbitrary still images and videos introduces considerable new opportunities for digital humanities scholarship, especially in the realm of dance choreography analysis. Most such inquiries have previously occurred within visual media studies and among what might be considered &ldquo;DH-adjacent&rdquo; communities of dance and performance, with scholars using motion-capture systems to record the movements of small numbers of live dancers in controlled environments. The advent in the past few years of powerful deep learning-based models capable of accurately estimating poses directly from digital images and video footage greatly expands the scope and variety of questions researchers can pursue. We consider the particularly exciting prospect of being able to conduct studies of massive amounts of recorded choreography as another facet of the emergent practice of &ldquo;distant viewing&rdquo; of visual materials — a development that is itself analogous to the foundational digital humanities practice of &ldquo;distant reading&rdquo; of large collections of texts <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>  <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>.</p>
<p>Deep learning-based approaches tend to be faster and more accurate than prior computer vision methods for estimating human poses in standard visual-spectrum single-camera images and videos <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>. The new methods also rival the accuracy of dedicated motion-capture systems and far exceed their potential scope given the physical demands of dedicated motion capture, raising the prospect of applying these deep learning methods to large recorded corpora <sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>. We present the initial stages of such an inquiry, focusing on a domain that is an excellent match for the capabilities of deep learning-based pose estimation: K-pop dance choreography.</p>
<h2 id="2-why-k-pop">2. Why K-pop?</h2>
<p>K-pop gained considerable traction in the South Korean domestic entertainment market in the aftermath of the financial downturn that rocked the South Korean economy in the late 1990s <sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>  <sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>. The pop music genre began to dominate the regular and virtual airwaves in tandem with the rise in online social networks, video and music sharing platforms, and the broader cultural phenomenon of the Korean Wave (Hallyu), a wave that gained its initial impetus with the immense popularity of Korean television dramas throughout East Asia <sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>. It was into this well-primed social media environment that K-pop was launched, and the genre quickly evolved to include several distinguishing features, including: (i) an emphasis on individual songs (as opposed to larger &ldquo;albums&rdquo;) promoted via music videos; (ii) the development of individual &ldquo;idols&rdquo; and largely single-sex musical/dance groups; (iii) a coherent musical style based largely on non-antagonistic Europop and American hip-hop styles; (iv) a heavily produced visual style that emphasized costumes, sets, dramatic lighting, and a kinetic shot vocabulary; and (v) tightly choreographed, frequently energetic, dance.</p>
<p>The global ubiquity of online video sharing and streaming services, which accelerated with the launch of YouTube in 2005, helped make K-pop an international phenomenon, with videos attracting many millions of views and solidifying fan bases throughout the world. Because of the potential for significant financial gain, the K-pop industry quickly began to attract substantial funding from the private sector and public agencies eager to promote South Korean cultural products globally <sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>. As the genre developed and the music market pivoted almost entirely away from album-based sales to video-singles and ad-based revenue, the industry began to internationalize. Consequently, it is not uncommon for producers, videographers, choreographers, music composers, lyricists, musicians, and even the idols and K-pop group members themselves to come from countries other than Korea. This internationalization has resulted in a remarkably productive collaborative environment with creative input coming from people with diverse musical, choreographic and visual backgrounds and traditions. In turn, this creative melting pot feeds a productive tension between the expectations of the broad consensus of what constitutes &ldquo;K-pop&rdquo; developed over the past decade by the industry, performers and their fans on the one hand, and the individualistic creative desires of the various individuals contributing to the production of new K-pop videos on the other. While aspects of K-pop production <sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup>, economics <sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup>  <sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>, musical collaboration <sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>, fandom and international reception <sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup>  <sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup>  <sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup>  <sup id="fnref1:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>  <sup id="fnref:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup> as well as broader considerations of K-pop in the contexts of gender <sup id="fnref:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup>  <sup id="fnref:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup>  <sup id="fnref:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup>  <sup id="fnref1:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup>, political philosophies <sup id="fnref:20"><a href="#fn:20" class="footnote-ref" role="doc-noteref">20</a></sup>, body aesthetics <sup id="fnref:21"><a href="#fn:21" class="footnote-ref" role="doc-noteref">21</a></sup>  <sup id="fnref2:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup>, and hybridity and cultural appropriation <sup id="fnref:22"><a href="#fn:22" class="footnote-ref" role="doc-noteref">22</a></sup>  <sup id="fnref:23"><a href="#fn:23" class="footnote-ref" role="doc-noteref">23</a></sup> have received considerable scholarly attention, far less attention has been paid to the kinesthetic dimensions of the genre <sup id="fnref:24"><a href="#fn:24" class="footnote-ref" role="doc-noteref">24</a></sup>. In particular, a typology of K-pop dance movements and considerations of the overall choreography of K-pop have not been subjected to rigorous analysis, possibly because of the overwhelming size of the ever-growing K-pop corpus.</p>
<p>K-pop dance is marked by the integration of a broad range of popular dance styles, most notably American hip hop genres including b-boying (breakdancing), popping and locking, and other street dance styles; Indian popular dance genres such as bhangra; and borrowings from other coordinated dance traditions such as American cheer and stepping. While not all K-pop videos are dominated by dance, or can even be considered &ldquo;dance forward,&rdquo; those that are tend to include either an individual solo dancer, or highly coordinated, often same-sex, groups featuring 4–9 dancers with break-out solo dances, occasionally set against much larger coordinated ensemble dances. Psy&rsquo;s satirical &ldquo;Gangnam Style&rdquo; music video provides an excellent sampler of the different types of dances that characterize the genre <sup id="fnref:25"><a href="#fn:25" class="footnote-ref" role="doc-noteref">25</a></sup>. Ironically, the video far exceeded the international popularity of any previous K-pop song while parodying the genre&rsquo;s conventions along with the superficial lifestyles of Seoul&rsquo;s nouveau riche.</p>
<p>In the &ldquo;official&rdquo; music video for a K-pop song, dances are often interspersed with narrative video scenes, and presented in a fragmentary form. Such fragmentary dance visualizations are challenging for automated analysis. Fortunately, many groups also release &ldquo;dance practice&rdquo; videos that present the entire dance choreography for the song, supplementing renditions of the choreography in concert and in &ldquo;comeback&rdquo; (new release) performances on the Korean networks&rsquo; weekly live music television broadcasts. These sources allow fans to learn the dance moves and, ultimately, to record their own &ldquo;dance cover&rdquo; of a song. As a result, there is a considerable and growing corpus of variant forms of entire dances, shorter dance sequences, and dance moves that, taken together, represent an intriguing opportunity to explore aspects of K-pop dance reproduction, stability and variation, including considerations of borrowing from other periods, genres, styles or artists; inter-artist influence; and incremental change in dance moves and sequences.</p>
<p>K-pop dance videos provide excellent material for developing approaches to understanding and describing dance moves and sequences in a consistent manner at scale. The growing corpus offers a unique opportunity to develop pose- and movement-oriented analytical techniques and data models that in many ways parallel previous research with text corpora: producing, for example, a kinesthetic search engine that would allow dance poses, moves, or larger sequences to act as the search input, as opposed to text descriptors, and return time-stamped results from the broader corpus. Such a search engine would, in turn, facilitate the study of dance evolution, influence, borrowing, and innovation across not only K-pop but, if scaled to include other dance traditions, potentially across many different dance and movement domains, from those mentioned above to other popular Korean music genres such as trot (트로트), and even martial arts and folk dances. We limit the corpus considered in this study to K-pop music videos produced in Korea or by Korean management groups and production houses between 2004 and 2020, resulting in a full-size corpus of over 10,300 videos, primarily sourced from YouTube. Our analytical case studies draw from a subcorpus of approximately 220 official choreography demonstration/dance practice videos posted to YouTube since 2012.</p>
<h2 id="3-relatedfoundational-computational-approaches-motion-capture">3. Related/foundational computational approaches: motion capture</h2>
<p>The primary contribution of deep learning-based pose estimation to the K-pop research envisioned here, and to similarly AV-oriented digital humanities agendas, derives from its speed and accuracy when detecting and estimating the poses of potentially unlimited numbers of human figures from images and video captured &ldquo;in the wild&rdquo; with a single visual-light camera. Earlier, non deep-learning approaches to this type of pose estimation tended to perform less well overall in the same way and for the same reasons that deep learning-based approaches to automated image analysis tasks such as semantic segmentation, captioning, and object detection and masking decisively surpassed previous computer vision approaches <sup id="fnref1:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>.</p>
<p>It is important to note that motion capture-based pose detection methods have been — and remain — capable of capturing pose and movement data at greater levels of accuracy and comprehensiveness than deep-learning approaches. For example, these methods typically record positions in three dimensions natively, rather than inferring 3-D positions (if done at all) from single-camera 2-D images, as in the case of most deep learning methods. Motion capture, though, tends to work only with a small number of dancers (often just one), imaged live in controlled conditions using dedicated hardware and software systems. Such &ldquo;mocap rigs&rdquo; have over the years included wearable wireless (or wired) tracking devices, setups in which one or more cameras detect reference markers worn on the body and the face, and &ldquo;markerless&rdquo; systems that combine a visual-light camera with infrared laser ranging sensors to build a 3-D map of the objects in front of them. The second of these technologies drove the proliferation of performance capture-based characters in popular films of the early 2000s, and the first and third were the signature innovations of the Nintendo Wii and the Microsoft Kinect interactive gaming systems, introduced in 2006 and 2010 respectively <sup id="fnref:26"><a href="#fn:26" class="footnote-ref" role="doc-noteref">26</a></sup>  <sup id="fnref:27"><a href="#fn:27" class="footnote-ref" role="doc-noteref">27</a></sup>. Despite the broad adoption of such methods and their potential benefits, traditional dance motion capture assumes that one has access not only to the equipment, but also to dancers capable of reproducing the desired moves, sequences and complete choreography.</p>
<p>The partial equivalence of deep learning pose estimation output to motion-capture data means that researchers using deep learning-based techniques can derive inspiration and potentially even analytical techniques from prior motion capture-based dance studies. A rich lineage of motion-capture inquiries exists, with many studies exploring how to record, analyze, and communicate the nuances of full-body choreography — as opposed to previous systems for notating foot movements — that prompted Rudolf Laban and his colleagues to develop Labanotation in the 1920s, to be followed by Benesh Movement Notation in the 1940s as well as several other schemes <sup id="fnref:28"><a href="#fn:28" class="footnote-ref" role="doc-noteref">28</a></sup>. Yet technology-assisted dance analysis efforts to date have been typically somewhat narrow in scope, often focusing on just a single dancer, and primarily intended to contribute to dance pedagogy <sup id="fnref:29"><a href="#fn:29" class="footnote-ref" role="doc-noteref">29</a></sup>, close analysis of the micro-scale nuances of a specific dance or movement <sup id="fnref:30"><a href="#fn:30" class="footnote-ref" role="doc-noteref">30</a></sup>, or oriented towards dance entertainment systems and video games.</p>
<p>One noteworthy motion-capture dance study, highly relevant to the present discussion, was a large-scale, multi-year project at the Korean Electronics and Telecommunication Research Institute (ETRI), which also focused on K-pop dance <sup id="fnref:31"><a href="#fn:31" class="footnote-ref" role="doc-noteref">31</a></sup>. From 2014 to 2017, the ETRI researchers used a Kinect-type system to generate motion-capture recordings of professional dancers re-enacting choreography from a large set of popular K-pop numbers. They subsequently developed techniques for characterizing and comparing the recorded poses, eventually assembling a large database of K-pop dance poses. Potential uses of the comparison techniques and database as envisioned in the project documentation included helping to adjudicate choreography copyright disputes and serving as source data for a mocap-based K-pop &ldquo;dance karaoke&rdquo; platform or a similarly featured K-pop dance pedagogy system <sup id="fnref1:31"><a href="#fn:31" class="footnote-ref" role="doc-noteref">31</a></sup>.<sup id="fnref:32"><a href="#fn:32" class="footnote-ref" role="doc-noteref">32</a></sup>  The project&rsquo;s methods for pose characterization are broadly similar to the distance matrix-based approach used in the present study <sup id="fnref:33"><a href="#fn:33" class="footnote-ref" role="doc-noteref">33</a></sup>. The ETRI researchers&rsquo; technique for comparing short dance moves uses an extended version of a previous study&rsquo;s metric based on &ldquo;dynamic time-warping&rdquo; analysis <sup id="fnref:34"><a href="#fn:34" class="footnote-ref" role="doc-noteref">34</a></sup>. This method, which involves comparing the frames of a given motion to a labeled &ldquo;reference&rdquo; version of the motion, is not applicable to the present study, which endeavors to extract descriptors of K-pop poses and gestures in a largely unsupervised manner (i.e., directly from video sources). Nevertheless, it may prove relevant to future expansions of this work.</p>
<h2 id="4-getting-a-leg-up-with-deep-learning">4. Getting a leg up with deep learning</h2>
<p>The deep learning methodologies underpinning recent advances in pose estimation are fundamentally the same as those that drove the earlier breakthroughs in object detection and segmentation for still images. In brief: large sets of potentially meaningful image features, often derived by applying certain filters, overlays, and &ldquo;convolutions&rdquo; to the images, are fed to an interlinked system of data structures (&ldquo;neurons&rdquo;), which repeatedly applies fairly straightforward mathematical calculations to &ldquo;learn&rdquo; which features are the most effective at helping the entire network discern between different labels for the input images, e.g., &ldquo;dog,&rdquo; &ldquo;cat,&rdquo; &ldquo;arm,&rdquo; &ldquo;leg.&rdquo; Large quantities of such labeled data are needed to train the models. Although this input can be derived from existing digital resources, massive amounts of novel manual efforts, often obtained through low-wage or entirely uncompensated labor, are needed to train state-of-the-art models, (e.g., Google&rsquo;s reCAPTCHA service) <sup id="fnref:35"><a href="#fn:35" class="footnote-ref" role="doc-noteref">35</a></sup>. The resulting models tend to perform quite well at tasks related to isolating, identifying and describing the objects they have been trained to detect. For pose estimation, the first of these tasks often consists of deciding which &ldquo;regions of interest&rdquo; on an image (established by dividing the input image into a grid of regions of varying sizes and dimensions and then applying an &ldquo;ROI&rdquo; evaluation sub-model to each) seems likely to contain a human figure. After finding these ROIs, the pose estimation model is employed to identify and localize discrete portions of the detected figure.</p>
<p>Many pose estimation models operate by collapsing the detected probabilistic &ldquo;field&rdquo; for, say, an arm into discrete keypoints (wrist, elbow, shoulder) until eventually a full pose &ldquo;skeleton&rdquo; of keypoints and connecting linkages is obtained. This approach was used for the first major open-source deep learning-based pose estimation project, OpenPose <sup id="fnref:36"><a href="#fn:36" class="footnote-ref" role="doc-noteref">36</a></sup>. Deep learning-based face detection methods also follow a similar process, and it is not difficult to see how the pose comparison approaches described below, predicated as they are on the notion of a pose &ldquo;fingerprint,&rdquo; resemble some facial recognition/matching techniques. It is also not hard to envision the mass surveillance applications to which both methods lend themselves <sup id="fnref:37"><a href="#fn:37" class="footnote-ref" role="doc-noteref">37</a></sup>  <sup id="fnref:38"><a href="#fn:38" class="footnote-ref" role="doc-noteref">38</a></sup>. Importantly, deep learning-based pose detection has grown to encompass a much wider variety of use cases beyond its most obvious applications in security and retail surveillance and driver-assist technology. Examples include the development of &ldquo;in-bed pose estimation&rdquo; for monitoring of hospital patients using low-cost cameras <sup id="fnref:39"><a href="#fn:39" class="footnote-ref" role="doc-noteref">39</a></sup>, as well as the DeepLabCut software, which facilitates the training of non-human pose estimation models to aid observational studies of a virtually limitless range of animals <sup id="fnref:40"><a href="#fn:40" class="footnote-ref" role="doc-noteref">40</a></sup>.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000506/resources/images/figure01.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000506/resources/images/figure01_hud9a9adbfe206accc149929f12fc47cfd_59465_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000506/resources/images/figure01_hud9a9adbfe206accc149929f12fc47cfd_59465_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000506/resources/images/figure01.png 300w" 
     class="portrait"
     ><figcaption>
        <p>The COCO “Common Objects in Context” pose keypoint set. The numbering of the keypoints may vary between software implementations. The numbers here apply to the other figures in this paper.
        </p>
    </figcaption>
</figure>
<p>When providing output coordinates for detected figures, human pose estimation models usually adhere to a standard set of keypoints, such as the 17 keypoints of the COCO (Common Objects in Context) library <sup id="fnref:41"><a href="#fn:41" class="footnote-ref" role="doc-noteref">41</a></sup>. This particular set truncates the figure&rsquo;s arms at the wrists and the feet at the ankles, which is not ideal for choreographic analysis nor for many other potential applications (<a href="#figure01">Figure 1</a>). Accordingly, developers have developed expanded body keypoint standards (such as BODY_25) or, in the case of OpenPose, simply superimposed other models for detecting hand and face landmarks <sup id="fnref1:36"><a href="#fn:36" class="footnote-ref" role="doc-noteref">36</a></sup>. The resulting figures can have as many as 130 keypoints, though one motivation for using smaller keypoint sets is that detecting and subsequently comparing more keypoints usually requires more processing time, storage and power. The &ldquo;model zoo&rdquo; provided by the developers of a given pose estimation package likewise typically includes a range of tuned models, each prioritizing or deprioritizing speed, accuracy, the number of output keypoints, model size, and resource requirements based upon its expected use. For example, a faster but lower-resolution model might be deployed in a smartphone application offering real-time body tracking. A slower, &ldquo;heavier,&rdquo; higher-resolution model could be the right fit for a well-resourced digital humanities research team seeking to resolve fine details of inter-frame movement within a set of pre-recorded dance videos, especially if the team has ample time to run the pose estimation software on the videos.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000506/resources/images/figure02.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000506/resources/images/figure02_hua18c5615139cd673e0ae30b6b3cfddd9_2136877_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000506/resources/images/figure02_hua18c5615139cd673e0ae30b6b3cfddd9_2136877_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000506/resources/images/figure02_hua18c5615139cd673e0ae30b6b3cfddd9_2136877_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000506/resources/images/figure02.png 1374w" 
     class="landscape"
     ><figcaption>
        <p>A visualization of the full DensePose output: body masks (including hair and clothing), segmented body parts with contours, and keypoint skeletons.
        </p>
    </figcaption>
</figure>
<p>Other pose estimation models actually project the detected probabilistic body part fields onto a 3-D model of the body, so that instead of a simplified skeleton, the output consists of a much larger set of points, regions and connectors that define a full 3-D body surface, similar to how a fishing net thrown over a person would form a body-shaped &ldquo;mesh&rdquo; (<a href="#figure02">Figure 2</a>). This mesh output option is available from Facebook Research&rsquo;s DensePose project <sup id="fnref:42"><a href="#fn:42" class="footnote-ref" role="doc-noteref">42</a></sup>; it gives researchers access to a much less reductionist model of the body (including full hands and feet) which is certainly appealing in some applications. One practical concern is that the resulting per-frame body meshes for a single video can use an enormous amount of storage space if not properly compressed.</p>
<p>Another practical consideration is that even the most heavyweight, non time-constrained pose estimation models may not perform well with imagery that is visually distorted, shot from oblique angles, poorly lit, or involves figures whose features are obscured by costumes and walls, or are simply truncated by the frame. That the previous list resembles a primer on music-video cinematography should suggest one reason why most of the present study&rsquo;s initial investigations used supplemental &ldquo;dance practice&rdquo; videos or fan-produced &ldquo;cover&rdquo; dance videos rather than the original music videos. At the root of these problems is the very limited ability of most deep learning models to extrapolate beyond their training data, so if a pose-estimation model is primarily trained via labeled, segmented images of people in brightly lit environs with visible faces performing mundane activities like walking or standing, its ability to resolve, for example, figures wearing masks or swinging their arms vigorously above their heads is likely to be quite limited.</p>
<p>Most pose estimation projects at present aspire to excel at figure detection and pose estimation during the first &ldquo;pass&rdquo; across an image, which has obvious relevance to core time- and resource-constrained applications such as real-time pedestrian detection systems for automobiles. This emphasis on first pass methods means that, in general, alternative methods involving multi-pass processing and smoothing tend to receive less attention, despite their potential to improve model performance in ways that would be especially beneficial to digital humanities researchers working with recorded media. The PoseFix package, for example, managed to achieve state-of-the-art accuracy simply by applying its statistical &ldquo;pose refinement&rdquo; model to the output of other methods <sup id="fnref:43"><a href="#fn:43" class="footnote-ref" role="doc-noteref">43</a></sup>. Such corrective calculations can be as straightforward as setting limits on how distant a figure&rsquo;s head can possibly be from the shoulders in a non-catastrophic scenario. Similarly promising but heretofore backgrounded efforts involve expanding models to incorporate the causal implications of a figure&rsquo;s previous position (and its future position, if known) to its current one <sup id="fnref:44"><a href="#fn:44" class="footnote-ref" role="doc-noteref">44</a></sup>. Developers acknowledge the importance of tracking multiple poses across frames — especially so when figures may pass each other in the same shot — but this is often, and perhaps erroneously, relegated to a &ldquo;post-processing&rdquo; step, something to be considered after the actual pose estimation has been done <sup id="fnref:45"><a href="#fn:45" class="footnote-ref" role="doc-noteref">45</a></sup>.</p>
<p>The accuracy of pose estimation models continues to improve as more varied training sets and clever algorithms are developed. Recent advances in the speed and accuracy of three-dimensional object detection from single-camera sources promise to offer researchers an even greater wealth of information about figures recorded on video and their relationship to their environment <sup id="fnref:46"><a href="#fn:46" class="footnote-ref" role="doc-noteref">46</a></sup>. Furthermore, software platforms and cloud services continue to emerge that make it much easier to provision and configure the software &ldquo;stack&rdquo; and computing resources necessary to run these models (for example, by providing access to cloud-based graphical processing units, or GPUs, which greatly accelerate deep learning tasks). As a consequence of this rapid pace of development, the decision whether to run pose estimation on raw music videos or on their accompanying dance demonstration videos is already more a question of focusing exclusively on dance choreography versus also examining computationally the many other types of performative uses of the human pose that appear in music videos.</p>
<h2 id="5-getting-down-to-the-features-analytical-methods">5. Getting down to the features: analytical methods</h2>
<p>This section describes the fundamental approaches to analyzing deep learning pose estimation output that we have applied to data from K-pop dance videos in the early phases of the research agenda outlined above, and also outlines some of the more elaborate techniques we may pursue in future work. These methods primarily concern pose characterization and comparison and the exploratory and interpretive affordances they offer when applied to a large number of videos. We also outline potential approaches to pose clustering and time-series analysis of movement and synchronization.</p>
<h2 id="pose-representation-comparison-and-correction">Pose representation, comparison and correction</h2>
<p>The raw output of deep learning-based posed estimation software is not fundamentally different from motion-capture data, so many of our techniques may have appeared in prior mocap-based analyses. Because of the reliance of these earlier studies on closed-source systems and the lack of technical details in their associated publications, such implementation-level aspects are difficult to ascertain fully. In any case, our methods by no means encompass the available techniques. Yet seeking comprehensiveness would undercut the central message of this paper: that the ability to run deep learning-based pose detection at scale across large video corpora empowers researchers to pose new questions and to develop methods for addressing them that probably have never been used before — at the very least, not in the domain of choreography analysis. One need only consider the history of computational text studies as an analogue: methods of linguistic examination and structural analysis certainly existed in the pre-digital era, but the advent of digital texts, and particularly the availability of massive quantities of digital texts, prompted an explosion of computational methods, especially at the level of large-scale, &ldquo;distant&rdquo; reading: topic models (LDA), semantic embeddings, named entity detection, network analysis to name but a few. Choreographic analysis has the potential to follow a similar trajectory.</p>
<p>Despite the relatively porous boundaries of K-pop vis-a-vis other forms of Korea-based popular music and the paucity of meaningful descriptive metadata for YouTube videos, in previous work we showed that it is possible to discover thousands of official K-pop music videos on YouTube by querying the videos uploaded to channels run by K-pop production companies identified via online knowledge bases (Wikidata, MusicBrainz) <sup id="fnref:47"><a href="#fn:47" class="footnote-ref" role="doc-noteref">47</a></sup>. For reasons discussed above, we supplemented this list with a smaller number of official dance practice, demonstration and &ldquo;dance cover&rdquo; videos to facilitate development and evaluation of our choreographic pose analysis techniques.</p>
<p>We obtained the pose estimation data used in the case studies below by processing videos with software from the Open PifPaf project <sup id="fnref:48"><a href="#fn:48" class="footnote-ref" role="doc-noteref">48</a></sup>, which we used due to its accuracy and relative ease of setup. The 17-keypoint COCO pose output data has modest storage requirements — an average of 7 megabytes of uncompressed data for a 4-minute video — and is fairly straightforward to process as CSV or JSON (<a href="#figure03">Figure 3</a>). We also ran pose estimation on the entire video corpus using a DensePose model, a process that took several weeks on a dedicated multiple-GPU system.<sup id="fnref:49"><a href="#fn:49" class="footnote-ref" role="doc-noteref">49</a></sup>  The full DensePose &ldquo;mesh&rdquo; output, which is appealing because it allows for the calculation of additional features such as hands and feet positions and even clothing, can require 30 gigabytes or more of storage per 4-minute video (highly reactive to the number of figures in most shots), and often is neither straightforward to compress nor to interpret.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000506/resources/images/figure03.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000506/resources/images/figure03_hu04dc3e61ca323d372b0280c6f0d5f1eb_731522_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000506/resources/images/figure03_hu04dc3e61ca323d372b0280c6f0d5f1eb_731522_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000506/resources/images/figure03_hu04dc3e61ca323d372b0280c6f0d5f1eb_731522_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000506/resources/images/figure03.png 1326w" 
     class="landscape"
     ><figcaption>
        <p>Keypoint pose estimation output as tabular data (top) and JSON (bottom). Mesh data consists of nested arrays and is not immediately comprehensible outside of visualizations.
        </p>
    </figcaption>
</figure>
<p>At the fundamental level, an estimated pose is defined by its labeled keypoints and the extrapolated linkages between them, expressed as x,y coordinates on the visual plane of the screen (with a third spatial coordinate, z, if depth is also estimated). One obvious method of quantifying the difference between, for example, two 17-keypoint COCO poses is simply to sum the distance between the two instances of each keypoint using a metric such as Euclidean distance. This metric also can be a proxy for the amount of motion between two poses if they derive from the same figure at adjacent time intervals. These methods may suffice in a controlled, single-dancer motion capture studio environment, but generalizing pose characterization and comparison to any &ldquo;in the wild&rdquo; video footage requires more sophisticated approaches. Simply summing raw paired keypoint distances can be an incredibly inaccurate measure when, for example, we wish to compare poses in two different contexts, such as when the figures being compared are viewed at varying distances from the camera, or belong to different-sized people.</p>
<p>Solutions typically require devising a different &ldquo;feature set&rdquo; to describe the pose numerically, which in turn calls for different inter-pose comparison techniques. One rudimentary approach is to consider only the angle of certain linkages, such as in the arms or legs, because their angles are not affected by changes in scale. This approach, however, discards all potentially useful information about the positions of unconnected keypoints. Another, more promising class of solutions involves representing the pose not as a set of keypoints and the skeletal linkages between them, but rather as a &ldquo;distance matrix&rdquo; of the distances from each keypoint to every other keypoint (<a href="#figure04">Figure 4</a>). Comparing two such pose representations to quantify similarity or movement can then be accomplished by applying statistical tests designed to measure the degree of correlation between two matrices — a process that ignores differences in scale between the two poses. For our initial studies, we used the Mantel test, which provides a measure both of the strength of the correlation between the input matrices (this correlation can be expressed as a number between 0 and 1) and the computed probability that this correlation is due to random fluctuations in the data <sup id="fnref:50"><a href="#fn:50" class="footnote-ref" role="doc-noteref">50</a></sup>.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000506/resources/images/figure04.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000506/resources/images/figure04_hu7d54c4769707f9eae9ca7eae54a2c0e8_767064_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000506/resources/images/figure04_hu7d54c4769707f9eae9ca7eae54a2c0e8_767064_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000506/resources/images/figure04_hu7d54c4769707f9eae9ca7eae54a2c0e8_767064_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000506/resources/images/figure04.png 1210w" 
     class="landscape"
     ><figcaption>
        <p>A source image with keypoint overlay, a plot of the detected keypoints plotted separately (center), and the corresponding normalized inter-keypoint distance matrix (right). In the distance matrix, pairs of points that are close together are represented by dark squares, while those that are far apart receive light squares.
        </p>
    </figcaption>
</figure>
<p>Another class of enhanced pose representation and comparison methods considers only whether keypoints are closest to each other. In the simplest form of such an &ldquo;adjacency&rdquo; matrix, the cell for [right elbow, right ear] would record a 1 if the right elbow is closer to the right ear than to any other keypoint, and a 0 otherwise. This approach obviously disregards much of the estimated pose data and sacrifices accuracy as a consequence, but retains the overall spatial organization of the pose and has the advantage of being quite fast, requiring few calculations to characterize a pose and to compare two poses to each other. Our implementation of this method expands it further by calculating the Delaunay triangulation around the detected keypoints <sup id="fnref:51"><a href="#fn:51" class="footnote-ref" role="doc-noteref">51</a></sup>. This technique provides an alternative set of keypoint linkages to the standard body skeleton model, one in which every keypoint is connected to at least three others, producing a set of triangles that covers the shape of the pose in a geometrically efficient manner (<a href="#figure05">Figure 5</a>). We then represent these connections via a graph Laplacian matrix, which quantifies both the adjacency and degree of each point <sup id="fnref:52"><a href="#fn:52" class="footnote-ref" role="doc-noteref">52</a></sup>. Although the distance matrix and graph Laplacian matrix for a pose have the same dimensions (17x17 for the COCO keypoints), because a graph Laplacian contains only positive or negative integers, comparing two poses represented in this manner involves simply subtracting one from the other and summing their differences — a very computationally &ldquo;lightweight&rdquo; operation compared to the Mantel test for the distance matrices. It is also worth noting, however, that a standard Delaunay triangulation discards the right/left labels of the keypoints, meaning that frames of a pose with the figure facing the camera would be scored as identical to the mirror-image of the same pose with the figure turned 180 degrees away from the camera — which may or may not be desirable.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000506/resources/images/figure05.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000506/resources/images/figure05_hu443d36ed8b1ae972292b397ad2c94af6_972517_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000506/resources/images/figure05_hu443d36ed8b1ae972292b397ad2c94af6_972517_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000506/resources/images/figure05.png 1182w" 
     class="landscape"
     ><figcaption>
        <p>Delaunay triangulations of detected figure keypoints.
        </p>
    </figcaption>
</figure>
<p>Pose comparison methods, including those described above, must accommodate the near-certainty of incomplete pose and keypoint data. Even sophisticated pose detection software can lose track of body landmarks and sometimes entire figures for multiple frames. Often, the software detects part of the pose, but its &ldquo;confidence&rdquo; value for a keypoint or the entire figure drops low enough that the data points are removed from the output to avoid spurious results. Especially problematic with single-camera &ldquo;in the wild&rdquo; videos are cases in which even a human observer could only speculate as to the true coordinates of a keypoint, such as when a limb or facial landmark is obscured from view. Most pose comparison methods, including the matrix-based methods used here, do not easily accommodate missing data values. Our software therefore falls back on both spatial and temporal interpolation to fill in missing keypoints. The relatively high frame rates of modern video assists in the latter: the position of a missing keypoint often can be placed somewhere between its last and next known position. Failing that, spatial interpolation often allows us to place with high confidence, for example, an eye that is obscured by a hat brim between its adjacent ear and nose. A last-resort option is to position a missing keypoint at the center of the pose. Both the distance matrix technique and the Delaunay triangulation-based graph Laplacian approach, due to their addition of extra linkages to the base keypoint set, are still able to produce usable results when undefined values are replaced with such a default.</p>
<p>A final obstacle is that most pose detection packages do not attempt to track the figures in a scene, simply numbering the figures in a shot via an arbitrary ordering, e.g., left-to-right, or sorted by the size of their bounding box, regardless of identity. This practice leads to discrepancies in the movement data when, for example, one figure changes places with another. To ameliorate these errors, we also run our corrected pose detection output through the AlphaPose project&rsquo;s &ldquo;PoseFlow&rdquo; software, which attempts to generate a single movement track for each distinct figure throughout the duration of the video <sup id="fnref:53"><a href="#fn:53" class="footnote-ref" role="doc-noteref">53</a></sup>. Even when PoseFlow fails to &ldquo;keep track&rdquo; of a dancer, our current choreography analysis methods benefit from considering each figure only within the frame of reference of its own body landmarks, excluding lateral and backwards/forwards movement relative to the camera and other dancers, and from the fact that we generally examine the simultaneous movements of multiple dancers in aggregate rather than individually. Therefore the failures of the pose tracking software to resolve correctly the trajectories of two overlapping poses generally do not introduce significant discrepancies into our analytical results.</p>
<h2 id="pose-clustering-sequence-detection-and-distant-movement-characterization">Pose clustering, sequence detection, and &ldquo;distant&rdquo; movement characterization</h2>
<p>Having chosen a set of methods that can represent a pose and quantify the similarity and difference between two poses, such that the degree of difference between two poses from the same dancer can serve as a reasonable proxy for motion, it is then possible to pursue a variety of more aggregate, &ldquo;distant&rdquo; analyses that consider groupings and sequences of poses over time. The case studies below describe the techniques that we find particularly promising and relevant for the study of K-pop choreography and its influences, but they are by no means an exhaustive set. Ultimately, we believe that a macroscopic approach that includes a series of these analytical techniques will support researchers as they move toward the &ldquo;thick&rdquo; analysis of dance at scale <sup id="fnref:54"><a href="#fn:54" class="footnote-ref" role="doc-noteref">54</a></sup>  <sup id="fnref:55"><a href="#fn:55" class="footnote-ref" role="doc-noteref">55</a></sup>.</p>
<p>The computational considerations germane to these analyses involve at their fundamental levels many standard details of statistical and numerical computation. These include sampling (e.g., whether to consider every pose in every frame), or whether a subset of poses (chosen randomly or by weeding out repetitions) can suffice to produce significant results at much lower computational cost in time and resources. Of further concern are methods for smoothing and interpolation, already discussed above regarding pose correction: how best to reduce the influence of transient data errors on the results without disregarding legitimate phenomena, and to &ldquo;fill in&rdquo; missing values with a suitable degree of confidence. At the higher levels are issues such as the choice of algorithm for clustering or time-series comparison and their various hyperparameters (e.g., how many clusters we expect to find in the data set).</p>
<p>The example analyses below illuminate how the foundational pose characterization and comparison methods described in the previous section build progressively towards one of our primary long-term goals: isolating a typography not only of K-pop poses but of gestures and ultimately dance &ldquo;moves.&rdquo; Although this work is ongoing, the examples below indicate the planned trajectory: using pose similarity and clustering methods to identify &ldquo;key&rdquo; poses that occur frequently with minor variations; the sequences of key poses and interstitial movements that recur frequently across the time series of one or multiple videos thus identify themselves as significant &ldquo;moves&rdquo; within the dance vocabulary of K-pop.</p>
<h2 id="6-case-studies-with-k-pop">6. Case studies with K-pop</h2>
<p>The following case studies highlight some of the potential applications of the pose characterization and comparison techniques described above. Specifically, the first of these examples involves applying pose similarity calculations in a variety of ways to a single-dancer video performance: comparing time-adjacent poses to establish the movement profile of the dance, and comparing poses across the duration of the dance either exhaustively or selectively (via clustering) to highlight repeated poses and pose sequences. The second example applies many of the same techniques to a video of multi-person choreography, with the addition of inter-dancer comparison of poses and motion to detect and quantify the degree of synchronized posing and movement present across the video. The third study calculates per-video and aggregate values of each previously discussed metric for a test corpus of 20 K-pop choreography videos, divided into equal halves according to the gender of the performing group.</p>
<h2 id="solo-dance-repeated-pose-sequence-discovery">Solo dance: repeated pose sequence discovery</h2>
<p>For a single-person video, our goal was to use the pose comparison methods described above to detect when a dance video is repeating certain poses and motions. Being able to find repeated poses illustrates the suitability of these techniques to building a kinesthetic database of poses that can be searched via a &ldquo;query&rdquo; pose to find similar poses. Given this, detecting repeated motions can be as straightforward as noticing when a &ldquo;query&rdquo; sequence of poses matches a reference sequence of poses within some similarity threshold and across some time window. As this example will show, groups of repeated motions tend to correspond to repeated formal sections and suggest the potential of performing automated computation-based formal analyses across a large dance video corpus.</p>
<p>For this case study, we used an instructional recording by dance cover specialist Lisa Rhee of Blackpink member Jennie Kim&rsquo;s debut single &ldquo;Solo,&rdquo; which was choreographed by the prolific New Zealand-based choreographer Kiel Tutin.<sup id="fnref:56"><a href="#fn:56" class="footnote-ref" role="doc-noteref">56</a></sup>  One way of testing how computational pose comparisons can contribute to detecting dance repetition is via the brute-force expedient of comparing the pose in every frame of a single-dancer video to every other frame, and plotting the results on a correlation heatmap matrix (a type of visualization commonly used to explore patterns of repetition in time-series data). Such heatmaps tend to be difficult to read at first, but provide the initiated with a wealth of visual cues about patterns of correlation and similarity; if desired, these features also can be described numerically by applying established analytical techniques to the correlation matrix.</p>
<p><a href="#figure06">Figure 6</a> shows a correlation heatmap for the entire performance, which lasts just over 2 minutes and 43 seconds, captured in 4,079 frames (25 frames per second). The poses were represented as normalized distance matrices as explained above, and therefore the comparison between any two poses returns a Mantel correlation value from 0 (least similar) to 1 (most similar). On the heatmap, these similarity values are visualized via the color scale, with lighter colors indicating lower similarity. The x and y axes of the heatmap represent the progression of frames of the video, with the start time 0:00 at bottom left, so that the cell at position x, y represents a comparison of the pose at frame x to the pose at frame y. The matrix is therefore symmetric around the diagonal x=y axis, which naturally always has a similarity value of 1 and appears as a dark diagonal line dividing the heatmap into two right triangles.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000506/resources/images/figure06.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000506/resources/images/figure06_hu02b2e24756714e5411f76ceae16e8875_549428_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000506/resources/images/figure06_hu02b2e24756714e5411f76ceae16e8875_549428_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000506/resources/images/figure06.png 919w" 
     class="landscape"
     ><figcaption>
        <p>Time-series correlation heatmap of Lisa Rhee’s dance cover of Jennie’s “Solo.&quot; Darker colors indicate higher degrees of pose similarity. The repeating dance “chorus&quot; sections at 0:38–0:60 and 1:38–2:00 are highlighted.
        </p>
    </figcaption>
</figure>
<p>On a time-series correlation heatmap, repeated sections typically appear as dark lines running parallel to the central diagonal x=y axis, and such a feature is indeed visible here (highlighted in <a href="#figure06">Figure 6</a>). Closer inspection reveals that it does in fact indicate a very close repetition of the choreography originally seen at 0:38 to 0:60 almost exactly a minute later, at 1:38 to 2:00. Not surprisingly, the repeated choreography corresponds exactly to the appearances of the song&rsquo;s chorus, hinting at the likely interpretive payoffs of a truly multimodal audiovisual analysis.</p>
<p>Exhaustively comparing every pose to every other pose across an entire video, let alone multiple videos, quickly becomes a prohibitively cumbersome computational task. A more scalable method is to apply similarity-based clustering to all or to a representative sample of the poses in one or more videos, and to identify when poses in the same similarity cluster, or &ldquo;family,&rdquo; reoccur, and moreover when members of clusters reoccur in sequence — a major step towards identifying both formal sections within a choreographic plan, and also smaller, segmentable dance sections (i.e., moves).</p>
<p>Clustering necessarily sacrifices some precision, and requires judicious selection of parameters to produce a useful partitioning of the entire pose space. For this example, we applied the OPTICS hierarchical density-based clustering algorithm <sup id="fnref:57"><a href="#fn:57" class="footnote-ref" role="doc-noteref">57</a></sup>. Many other types of clustering algorithms may be applied fruitfully to this task, but we found OPTICS suited to the exploratory nature of our analysis because it does not require one to specify how many clusters are to be found in advance, but rather builds clusters based on a user-supplied minimum number of members per cluster. This hyperparameter can be set to a value with some intuitive justification; we chose to use the number of frames per second in the video recording (approximately 25, in this case), reasoning that a pose that is held for longer than one second, or that appears cumulatively for at least this length of time, may be significant and should be considered for cluster membership by the algorithm.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000506/resources/images/figure07.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000506/resources/images/figure07_hu40d88c0df07f2603fa85e087671e3343_89798_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000506/resources/images/figure07_hu40d88c0df07f2603fa85e087671e3343_89798_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000506/resources/images/figure07.png 1024w" 
     class="landscape"
     ><figcaption>
        <p>The representative “key&quot; poses of the 10 pose clusters found by the OPTICS algorithm when it is configured to find a minimum of 25 poses per cluster. Each key pose was constructed by taking the average keypoint locations of all members of a cluster.
        </p>
    </figcaption>
</figure>
<p>The clustering analysis for this video found ten groups of poses using the settings discussed above. To aid in visualization and comparison, we computed a representative &ldquo;centroid&rdquo; pose for each cluster by averaging the relative keypoint positions of each pose in the cluster (<a href="#figure07">Figure 7</a>). The OPTICS algorithm leaves a potentially large number of samples unassigned to any cluster, so we elected to assign each of these to the cluster with a representative centroid pose that was most similar to the unassigned pose. Visualizing the occurrences of these cluster groups on a timeline (<a href="#figure08">Figure 8</a>) makes it possible to detect repeated segments of choreography. The chorus sections identified in the pose correlation heatmap above, as well as the bridge section leading to them, are easily discernible. Key pose #9, with the upraised right arm, is particularly recognizable as a signature pose of the song, even though the COCO 17-keypoint set is unable to resolve the raised index finger that evokes the song&rsquo;s title.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000506/resources/images/figure08.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000506/resources/images/figure08_huce963c91d885ccef44abaa31446175b9_72806_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000506/resources/images/figure08_huce963c91d885ccef44abaa31446175b9_72806_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000506/resources/images/figure08.png 1024w" 
     class="landscape"
     ><figcaption>
        <p>The pose distribution heatmap of the key poses from Figure 7 throughout the duration of the song, with the two appearances of the chorus outlined in red and the two “bridge&quot; sections outlined in blue. Both section types repeat their previous choreography when they reoccur, which is apparent in the similar patterns of their sections on the heatmap. Occurrences of poses that the OPTICS clustering algorithm explicitly assigned to one of the 10 groupings are represented by blocks with a darker shade, while the unmatched poses that we subsequently assigned to their “nearest neighbor&quot; key pose are in a lighter shade.
        </p>
    </figcaption>
</figure>
<p>Thinking more broadly while looking more closely, it might also be appealing to be able to search through a much larger corpus of K-pop videos for occurrences of pose #9, as well as others from the directly mimetic gestures that populate the first 15 seconds of the choreography. Several of these are well enshrined in K-pop iconography such as, for example, the two-handed heart-shaped pose at 0:04, which is quickly and dramatically split in two at the six-second mark — an obvious nonverbal declaration that this is a breakup song — while others are relative newcomers, such as the sharp dismissal of items (messages, in this case) from a smartphone screen at 0:10 to 0:12.</p>
<p><a href="#figure09">Figure 9</a> visualizes the analytical methods described above via accompanying graphics as well as overlays of an excerpt of the video, with displays of summary values and a progress indicator (the moving red vertical line) superimposed on the time-series components. Viewing the changes in the distance matrix visualized this way alongside the actual poses imparts a more intuitive understanding of how it is used to generate similarity and movement values. The general resemblance of the distance matrix-based movement and the graph Laplacian-based movement series is apparent here, as is the graph Laplacian series&rsquo; comparatively  “noisier”  representation of the amount of inter-frame movement.</p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://player.vimeo.com/video/486935721" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="Deep learning-based choreography analysis of an instructional recording by dance cover specialist Lisa Rhee of Blackpink member Jennie’s debut single Solo." webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div>
<h2 id="group-dance-synchronized-movement-detection">Group dance: synchronized movement detection</h2>
<p>Multi-person performances are by far the most common type in K-pop, just as multiple-member idol groups are much more numerous than solo performers. Even solo songs are likely to incorporate backup dancers in live performance as well as in the official music video. Reflecting the eclectic combination of genres that are incorporated into K-pop, the choreography for an idol group single may feature several different types of dance sections, including solo interludes or  <em>pas de deux</em>  by the group&rsquo;s primary dancer(s) as well as gestural punctuations during a showcase of the group&rsquo;s main rapper. Yet the signature so-called &ldquo;point&rdquo; dance segments, which are most often staged with the group performing the same moves while facing towards the camera, tend to receive the most attention, to the extent that mechanistic mass synchronization is arguably the most salient feature of K-pop choreography in the global public imagination. This is largely by design; the didactic nature of these sections&rsquo; staging, in addition to serving other functions within a music video&rsquo;s narrative, furthers the choreography&rsquo;s main contribution to the carefully crafted viral appeal of K-pop singles, which is that fans and sometimes the general public in Korea and occasionally even the global population (as in the case of Psy&rsquo;s &ldquo;Gangnam Style&rdquo;) derive social capital from knowing and being able to re-enact these moves.</p>
<p>Group synchronization is straightforward to detect and to quantify using the pose comparison methods described above, through the simple expedient of observing the computed difference between each pair of figures in a single frame (note that the number of comparisons required per frame is the answer to the well-known &ldquo;handshake&rdquo; problem: n*(n - 1)/2 for n figures), then examining the mean and standard deviation of these per-frame values over time. We would expect that the mean degree of similarity would increase while the standard deviation would decrease during episodes of synchronization, which is exactly what we see when this technique is applied to the official &ldquo;dance practice&rdquo; video for the boy group BTS&rsquo;s single &ldquo;Fire,&rdquo; choreographed (as are several of BTS&rsquo;s other hits) by the American choreographer Keone Madrid.<sup id="fnref:58"><a href="#fn:58" class="footnote-ref" role="doc-noteref">58</a></sup></p>
<p>As visualized in <a href="#figure10">Figure 10</a>, the mean intra-frame pose similarity increases markedly and the standard deviation ranges shrink around 0:10 as the frenetic, popping and locking-influenced group dance begins and the song launches into the introductory hook section, with its accompanying EDM &ldquo;hoover&rdquo; synth effects. Note that this is also the first dance choreography that appears in the official music video for &ldquo;Fire,&rdquo; following its introductory narrative imagery, reinforcing the importance of synchronized group dance sections to the multimedia appeal of K-pop singles. Pose synchronization levels remain high for much of the choreography, with the highest sustained values occurring during the returns of the main hook at 1:07 and 2:04, and again leading to the coda at 2:43. These sections, and particularly the first two, share several moves and poses, although it is somewhat indicative of BTS&rsquo;s unorthodox approach to K-pop conventions that the choreography is generally more varied among these recapitulations than might be expected.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000506/resources/images/figure09.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000506/resources/images/figure09_huf884b1b5f89d6955f64eb1b132a920c3_189135_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000506/resources/images/figure09_huf884b1b5f89d6955f64eb1b132a920c3_189135_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000506/resources/images/figure09_huf884b1b5f89d6955f64eb1b132a920c3_189135_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000506/resources/images/figure09.png 1211w" 
     class="landscape"
     ><figcaption>
        <p>Mean (blue line) intra-frame pose similarity values with the standard deviation plotted above and below (orange and green dotted lines) for each frame of BTS’s official dance practice video for “Fire.” Similarity values were smoothed by computing the moving average of a one-second sliding window around each time value. Sections of high similarity (&gt; 90%) indicate dancing with synchronized movements and poses among the group members.
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000506/resources/images/figure10.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000506/resources/images/figure10_huc2ec71360a8ecf34673a23e63a11f8e3_235900_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000506/resources/images/figure10_huc2ec71360a8ecf34673a23e63a11f8e3_235900_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000506/resources/images/figure10_huc2ec71360a8ecf34673a23e63a11f8e3_235900_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000506/resources/images/figure10.png 1211w" 
     class="landscape"
     ><figcaption>
        <p>Mean (blue line) movement values averaged across all dancers for adjacent frames with the standard deviation plotted above and below (orange and green dotted lines). The movement values were smoothed by computing the moving average of a one-second sliding window around each time value.
        </p>
    </figcaption>
</figure>
<p><a href="#figure12">Figure 12</a> animates the time-series analyses from <a href="#figure10">Figure 10</a> and <a href="#figure11">Figure 11</a> with progress indicators alongside playback of the dance practice video, further accompanied by a visualization of the average inter-frame movement values for each body keypoint computed across all figures in the frames, as well as a time-series heatmap and visualization of the most prominent key poses detected in the video. The &ldquo;individual pose movement&rdquo; timeline superimposes the inter-frame movement values for each of the detected dancers, highlighting some details that the averaging visualization (<a href="#figure11">Figure 11</a>) elides. Note that the analysis terminates prior to the conclusion of the dance practice video itself; the arrival of dozens of backup dancers at 3:00 makes it difficult to compare the conclusion section to what came before.</p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://player.vimeo.com/video/486941563" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="Deep learning-based choreography analysis of the dance practice video for the boy group BTS&#39;s single Fire." webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div>
<h2 id="distant--movement-analysis-early-steps">“Distant”  movement analysis: early steps</h2>
<p>There is a temptation to apply the techniques for pose characterization and synchronization detection described above in an evaluative manner, aggregating numbers to support the conclusion that idol group A is &ldquo;more synchronized&rdquo; than group B, or that group A employs a greater variety of dance poses than their presumably less talented and less dedicated fellows. Our intention, however, is to employ such &ldquo;distant&rdquo; analyses to provide material for a more in-depth consideration of the influences and factors shaping the parameters of creativity and production in K-pop dance. Just as establishing a typology of K-pop dance poses and gestures will aid in identifying influences from other genres, pose and motion analysis can help to investigate some of the cultural and artistic practices being enacted (or subverted) through K-pop dance performances.</p>
<p>As an illustration of the interpretive potential of &ldquo;distant&rdquo; aggregate analyses of choreographic corpora, we inspect the movement-based signatures of one of the most foregrounded structural factors in K-pop, namely the gender divide that results in the vast majority of idols being segregated into groups consisting solely of young men or young women. As mentioned above, a great deal of K-pop scholarship investigates the degree to which the performances, fashion, makeup, comportment, marketing and reception of K-pop idols either conforms to or seeks to blur notions of gender roles and modes of masculinity and femininity, in Korea or internationally. A computationally driven inquiry into dance choreography potentially can contribute much to this discussion. Our initial case study involved selecting an equal number of K-pop dance practice videos from boy and girl idol groups over a limited time period and examining how the aggregate data produced from the time-series analyses described above can be used to answer, and more importantly, to raise questions about the role of gender in K-pop performance.</p>
<p>Our selection method involved surveying the available dance practice videos from boy and girl idol groups from the recent past and excluding videos with aspects that would complicate automated pose detection-based choreography analysis, such as camera angles that obscure dancers or studio mirrors and costumes that might confuse the pose-detection software. Of the remaining videos, we sought, albeit informally, to select a range of group sizes and song types that would be generally representative of the population of recent official dance practice videos. For this initial study, our data set consisted of the 20 videos (10 each from girl and boy groups) from 2017 to the present listed in Table 1. We ran each video through the pose detection, correction, tracking, interpolation, smoothing, movement and synchronization analyses discussed above, producing the summary statistics for each video presented in the table.</p>
<p>Also present in Table 1 as well as Table 2 is an additional statistic that we were interested in investigating: the degree and significance of the correlation between a group&rsquo;s averaged inter-frame movement time series and its average intra-frame pose similarity time series, as visualized in <a href="#figure11">Figure 11</a> and <a href="#figure10">Figure 10</a>, respectively. One could reasonably expect these to be inversely correlated, i.e., a group&rsquo;s inter-pose similarity might become more difficult to maintain when its members are all moving quickly. Following this line of reasoning, we sought here to investigate the intuitive hypothesis that a reduced negative correlation between movement and synchronization might indicate what musicians and dancers (and others) refer to as a &ldquo;tight&rdquo; performance, i.e., one that maintains group cohesion amidst increased difficulty. To assign this notion a quantitative metric, we computed the Pearson correlation coefficient between the movement and pose similarity time series for each video. Under our hypothesis, a &ldquo;tight&rdquo; performance would have a higher correlation value than others. This inquiry is an early exemplar of our aspirations to introduce more discerning analytical criteria into our analytical methods than the overtly reductive quantification of synchronization and movement, as even a casual overview of a few K-pop choreography videos will reveal that there is a lot more going on than rote synchronization: different sub-groupings of the members may perform different moves simultaneously, or enact the same movements in cascading sequence over time, among many other patterns.<br>
Per-video statistics for 20 choreography videos, 10 each from girl and boy groups. The average per-dancer movement (calculated every 1/6 of a second to allow straightforward comparison between videos recorded at 24, 30, and 60 frames per second) and the average intra-frame similarity for all dancers are presented with standard deviation values, which help to indicate whether the movement and similarity values are generally consistent or vary widely across the video.    BOY GROUPS   <strong>Video release date</strong>    <strong>Frames per sec</strong>    <strong>Group members</strong>    <strong>Avg movement per 1/6 sec per dancer</strong>    <strong>Avg intra-frame pose similarity</strong>    <strong>Pct of video with frame pose sim &gt;.9</strong>    <strong>Movement: similarity correlation (Pearson)</strong>       TXT - &ldquo;Can&rsquo;t We Just Leave the Monster Alive?&rdquo;  2020-04-12  29.97  5  5.31 +/- 4.01  0.93 +/- 0.09  72%  -0.29      TXT - &ldquo;Angel or Devil&rdquo;  2019-12-01  29.97  5  4.85 +/- 3.68  0.93 +/- 0.08  71%  -0.32      BTS - &ldquo;DNA&rdquo;  2017-09-24  29.97  7  4.94 +/- 3.58  0.9 +/- 0.11  55%  -0.14      BTS - &ldquo;Idol&rdquo;  2018-09-02  59.94  7  5.66 +/- 3.95  0.92 +/- 0.08  73%  -0.15      X1 - &ldquo;Flash&rdquo;  2019-09-04  29.97  10  4.74 +/- 3.78  0.88 +/- 0.14  56%  -0.37      Cravity - &ldquo;Break All the Rules&rdquo;  2020-04-20  29.97  9  5.71 +/- 3.43  0.93 +/- 0.07  72%  -0.28      Dongkiz - &ldquo;Lupin&rdquo;  2020-03-20  23.98  5  3.94 +/- 2.67  0.94 +/- 0.08  78%  -0.3      EXO - &ldquo;Electric Kiss&rdquo;  2018-01-12  29.97  8  5.74 +/- 3.74  0.93 +/- 0.07  77%  -0.26      SF9 - &ldquo;Good Guy&rdquo;  2020-01-09  23.98  9  4.35 +/- 3.37  0.92 +/- 0.09  70%  -0.35      Stray Kids - &ldquo;Levanter&rdquo;  2019-12-11  23.98  8  4.66 +/- 3.76  0.86 +/- 0.18  44%  -0.32      GIRL GROUPS                    AOA - &ldquo;Excuse Me&rdquo;  2017-01-10  23.98  7  2.59 +/- 1.99  0.96 +/- 0.04  87%  -0.11      Blackpink - &ldquo;As If It&rsquo;s Your Last&rdquo;  2017-06-24  23.98  4  5.9 +/- 3.97  0.96 +/- 0.05  68%  -0.04      EXID - &ldquo;I Love You&rdquo;  2018-11-26  29.97  5  2.6 +/- 2.37  0.96 +/- 0.06  88%  -0.08      GFriend - &ldquo;Crossroads&rdquo;  2020-02-05  59.94  6  6.88 +/- 7.83  0.95 +/- 0.06  81%  -0.31      Itzy - &ldquo;Wannabe&rdquo;  2020-03-11  60  5  7.22 +/- 7.85  0.95 +/- 0.08  80%  -0.6      Momoland - &ldquo;I&rsquo;m So Hot&rdquo;  2019-03-24  29.97  7  4.04 +/- 2.97  0.95 +/- 0.08  82%  -0.7      Oh My Girl - &ldquo;Bungee (Fall In Love)&rdquo;  2019-08-13  29.97  7  3.97 +/- 2.64  0.91 +/- 0.11  74%  -0.28      Red Velvet - &ldquo;Umpah Umpah&rdquo;  2019-08-29  29.97  5  2.98 +/- 1.48  0.96 +/- 0.04  91%  -0.1      Twice - &ldquo;Dance the Night Away&rdquo;  2018-07-10  30  9  5.88 +/- 3.82  0.94 +/- 0.07  79%  -0.18      Twice - &ldquo;Knock Knock&rdquo;  2017-02-25  29.97  9  3.31 +/- 2.59  0.95 +/- 0.06  77%  0.08        Aggregate statistics for the 20 choreography videos from Table 1. To facilitate comparison between the &ldquo;girl group&rdquo; and &ldquo;boy group&rdquo; categories, the movement time series and intra-frame pose similarity time series for the 10 videos in each category were concatenated together. The distributions of the movement and pose similarity values then were compared to each other via a Welch&rsquo;s  <em>t</em> -test, with the resulting  <em>t</em> -statistic and two-tailed  <em>p</em> -value (the probability that the differences between the two are due to chance) provided in the table. The correlation values between the movement and similarity time series for each category&rsquo;s meta-video also was measured by calculating Pearson&rsquo;s correlation coefficient (Pearson&rsquo;s  <em>r</em> ).     <strong>COMBINED VIDEO STATISTICS</strong>               Mean movement per 1/6 sec - boy groups  5.01 +/- 3.68    Welch&rsquo;s  <em>t</em> -test - boys movement vs. girls movement ( <em>t</em> )  9.76      Mean movement per 1/6 sec - girl groups  4.5 +/- 4.61    Welch&rsquo;s  <em>t</em> -test - boys movement vs. girls movement ( <em>p</em> )  1.77E-22      Mean intra-frame pose similarity - boy groups  0.92 +/- 0.09    Welch&rsquo;s  <em>t</em> -test - boys similarity vs. girls similarity ( <em>t</em> )  -33.1      Mean intra-frame pose similarity - girl groups  0.95 +/- 0.06    Welch&rsquo;s  <em>t</em> -test - boys similarity vs. girls similarity ( <em>p</em> )  7.26E-234      Movement:similarity correlation - boy groups (Pearson&rsquo;s  <em>r</em> )  -0.24    Movement:similarity correlation - girl groups (Pearson&rsquo;s  <em>r</em> )  -0.28   <br>
A perusal of the statistics in Table 1 gives the impression that despite considerable variability among videos, the girl group performances in our sample feature a greater amount of intra-frame pose similarity, while the boy groups tend to exhibit a higher degree of overall movement. The difference in pose similarity is especially evident from the fraction of each video in which the intra-frame similarity (calculated using the Mantel matrix correlation test described above) is above 90%. The aggregate statistics in Table 2, derived by concatenating all of each category&rsquo;s videos together at a sample rate of 1/6th of a second (the smallest common subdivision among videos recorded at 24, 30 or 60 frames per second, so that videos with higher frame rates do not have disproportionate effect on the results) and applying a Welch&rsquo;s unequal variances  <em>t</em> -test to compare the two resulting &ldquo;meta-videos,&rdquo; confirm the high statistical significance of these gender-based disparities. Given the small number of videos in this sample relative to the thousands of choreographed K-pop numbers, however, it is entirely possible that these differences are solely an artifact of our video selection process. Even so, it is difficult to resist the impulse to divine the influence of gender-based cultural notions of activity/passivity, conformance/individuality, extroversion/introversion, as well as physical differences, upon the numbers. More productive and less statistically questionable humanistic insights, however, may be derived by examining the exceptions to and outliers of these trends, as the following example briefly demonstrates.</p>
<p>The analysis of the correlation between movement and synchronization via the Pearson correlation test described above produced intriguing but not entirely conclusive results. As with the other statistics, it would benefit from a larger sample size and from controlling for song tempo (e.g., in average beats per minute) as a numerical proxy for song style. Yet also in a similar manner to the other comparisons, there is potentially even more to learn by investigating the outliers rather than merely the aggregated statistics. In this case, the dance performance of the girl group Twice&rsquo;s single &ldquo;Knock Knock&rdquo; is the only video in either category with a positive correlation between movement and synchronization. Watching the video closely reveals that this correlation (or more to the point, the lack of a negative correlation) is due to the choreography&rsquo;s fairly unorthodox alternation of sustained tableaux featuring multiple sub-divisions of the group&rsquo;s members in contrasting poses (low similarity, low movement) with more standard synchronized dance sections (high similarity, higher movement). The use in this performance of a choreographic technique that differs considerably in kind and degree from the rest serves to highlight the diverse range of aesthetic &ldquo;concepts&rdquo; that K-pop groups and their artistic collaborators deploy to differentiate one release from another. The further study of such outliers promises to lead to a more in-depth understanding of the role of dance in the K-pop culture industry. And returning to the question of whether it might be possible to quantify the &ldquo;tightness&rdquo; of a performance, this early experiment suggests that the metric also would need to take into account the speed of the movements (i.e., motion over time), assigning more significance to faster motions.</p>
<p>As the discussion above indicates, a sizable set of other &ldquo;distant&rdquo; analyses remains to be explored, in addition to expanding the size of the analyzed video corpus. Further metrics might include more summational measures, such as calculating an entire dance&rsquo;s average pose-based entropy (roughly indicating the &ldquo;diversity&rdquo; of poses) or its degree of movement autocorrelation (whether classes of similar poses, or sections featuring high-velocity movement, tend to appear in quick succession, or else tend to alternate regularly with contrasting materials), or quantifying the &ldquo;fluidity&rdquo; or &ldquo;jerkiness&rdquo; of movements individually or collectively. The brief analyses presented thus far illustrate some of the potential of these methods.</p>
<h2 id="7-conclusions-and-next-steps">7. Conclusions and next steps</h2>
<p>Our application of pose detection techniques and methods for comparison of the resulting matrices offers a visually rich approach to understanding dance similarity both within single K-pop videos and across multiple videos. While we are still quite far from being able to select a dance move or dance sequence and find all other instances of it in the ever-expanding K-pop video corpus, our experiments have shown that this is no longer a distant hope. The development of increasingly accurate pose detection algorithms coupled to relatively fast algorithms for calculating distances and, thus, similarities across detected poses allows us to identify a clear path forward for dance move and sequence comparison. Already, our work has produced intriguing discoveries, such as the relative infrequency of absolute group coordination in K-pop videos despite the general consensus that this is a key feature of K-pop dance performances.</p>
<p>These initial experiments are a foundational beginning to the complex problem of devising methods for the consistent discovery of dance moves and dance patterns at scale. There is significant room for development, and these preliminary engagements with the complex material of K-pop music videos suggest several productive avenues for future inquiry. The nascent field of &ldquo;big dance&rdquo; studies is already becoming more active: recently, the Google Arts &amp; Culture group has engaged in high-profile collaborations with the choreographer Wayne McGregor with the goal of devising a motion-based search engine for a large, curated corpus of dance recordings. Their recurrent neural network-based next-pose generation project at the very least indicates one direction in which this type of research can be developed.<sup id="fnref:59"><a href="#fn:59" class="footnote-ref" role="doc-noteref">59</a></sup>  More relevant to the present study is the &ldquo;Living Archive,&rdquo; an experimental UMAP embedding visualization web app, which is an excellent example of a big-data choreography analysis project, albeit one focused on a single choreographer&rsquo;s work.<sup id="fnref:60"><a href="#fn:60" class="footnote-ref" role="doc-noteref">60</a></sup>  It is worth noting that the latter project is not more sophisticated technically than our pilot experiments, which thus suggests that relatively small DH research groups such as ours (approximately four people) are able to move the needle considerably without the massive resources of an internet giant.</p>
<p>While our work has focused exclusively on K-pop videos, the encouraging results suggest that these methods can help us address a series of complex questions not only in this corpus, but across many dance traditions that have been filmed. These questions include the discovery and documentation of stylistic similarity across large dance corpora. Given the ability to align videos with dates of production, these discoveries of similarities and, equally importantly, variations in similar dances could help with the characterization of large-scale dance trends over both time and, in the case of geographically linked traditions such as West Coast Hip Hop or K-pop, space. Through the application of neighborhood detection and clustering algorithms, it should also be possible to use the similarity of dance sequences as part of a classification system, allowing for the computational characterization of sub-genres <sup id="fnref:61"><a href="#fn:61" class="footnote-ref" role="doc-noteref">61</a></sup>. Intriguingly, given the ability of pose estimation to label body parts automatically, it may be possible to devise a typology of poses and, with an analysis of sequencing, dance moves, leading to an understanding of the vocabulary of moves and sequences in any video or group of videos. Such a computationally derived morphology of K-pop dance could, in turn, lead to investigations of influence, homage and borrowing (inadvertent or intentional) from other domains such as hip hop, Bollywood, Latin dance, martial arts and many other traditions, including traditional Korean dance genres such as p&rsquo;ungmul performance.</p>
<p>In short, our approach confirms the need for a macroscopic approach to the complex domain of popular dance. In addition to comparing poses, motions, moves, and sequences across thousands of videos, researchers are eager to analyze these dances at many different scales, from the broad domain of an entire genre such as K-pop all the way down to the individual performances of a specific dancer at a specific time. We believe these preliminary investigations and refinements of various techniques, given the productive results derived from their application, provide a clear roadmap for the further development of these methods and will help us answer open questions regarding dance development not only for K-pop but for many other genres as well.</p>
<h2 id="acknowledgements-and-notes">Acknowledgements and notes</h2>
<p>We are grateful to Dr. Paul Chaikin for suggesting the Delaunay triangulation approach to graph Laplacian pose characterization and comparison. Dr. Francesca Albrezzi and the students of the Winter 2019 Digital Humanities capstone seminar at UCLA inspired us to expand the range of research questions and corpora treated in this paper. We also thank the contributors to the K-Pop Database site (dbkpop.com) for maintaining a detailed listing of available K-pop dance practice videos on YouTube.</p>
<p>Source code for the examples presented in the text is available at <a href="https://github.com/broadwell/choreo_k">https://github.com/broadwell/choreo_k</a></p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Arnold, Taylor, and Lauren Tilton.  “Distant Viewing: Analyzing Large Visual Corpora.”  <em>Digital Scholarship in the Humanities</em> , March 16, 2019. <a href="https://doi.org/10.1093/digitalsh/fqz013">https://doi.org/10.1093/digitalsh/fqz013</a>.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Wevers, Melvin, and Thomas Smits.  “The Visual Digital Turn: Using Neural Networks to Study Historical Images.”  <em>Digital Scholarship in the Humanities</em> , January 18, 2019. <a href="https://doi.org/10.1093/llc/fqy085">https://doi.org/10.1093/llc/fqy085</a>.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Elhayek, A., E. de Aguiar, A. Jain, J. Thompson, L. Pishchulin, M. Andriluka, C. Bregler, B. Schiele, and C. Theobalt.  “MARCOnI — ConvNet-Based MARker-Less Motion Capture in Outdoor and Indoor Scenes.”  <em>IEEE</em>    <em>Transactions on Pattern Analysis and Machine Intelligence</em>  39.3 (March 1, 2017): 501–14. <a href="https://doi.org/10.1109/TPAMI.2016.2557779">https://doi.org/10.1109/TPAMI.2016.2557779</a>.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Blok, Dylan, Jacob Pettigrew, Thecla Schiphorst, and Herbert H. Tsang.  “Human Pose Detection Through Searching in 3D Database With 2D Extracted Skeletons.” In  <em>2018 IEEE Symposium Series on Computational Intelligence (SSCI)</em> , 470–76. IEEE, Bangalore, India (2018). <a href="https://doi.org/10.1109/SSCI.2018.8628776">https://doi.org/10.1109/SSCI.2018.8628776</a>.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>Kim, Eun Mee and Jiwon Ryoo.  “South Korean Culture Goes Global: K-Pop and the Korean Wave.”    <em>Korean Social Science Journal</em>  34.1 (2007): 117-152.&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>Choi, JungBong, and Roald Maliangkay.  <em>K-Pop–The International Rise of the Korean Music Industry</em> . Routledge (2014).&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>Lee, Sangjoon, and Abé Markus Nornes.  <em>Hallyu 2.0: The Korean Wave in the Age of Social Media</em> . University of Michigan Press, Ann Arbor (2015). <a href="https://doi.org/10.3998/mpub.7651262">https://doi.org/10.3998/mpub.7651262</a>.&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>Lie, John.  “What Is the K in K-Pop? South Korean Popular Music, the Culture Industry, and National Identity.”  <em>Korea Observer</em>  43.3 (2012): 339–363.&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>Unger, Michael A.  “The Aporia of Presentation: Deconstructing the Genre of K-Pop Girl Group Music Videos in South Korea.”  <em>Journal of Popular Music Studies</em>  27.1 (2015): 25–47.&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>Oh, Chuyun.  “Queering Spectatorship in K-Pop: The Androgynous Male Dancing Body and Western Female Fandom.”  <em>The Journal of Fandom Studies</em>  3.1 (2015): 59–78.&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>Messerlin, Patrick A., and Wonkyu Shin.  “The Success of K-Pop: How Big and Why So Fast?”  <em>Asian Journal of Social Science</em>  45.4–5 (2017): 409–439.&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p>Kim, Jeong Weon.  “행보와 동행:≪ 월간 윤종신≫ 의 매체와 협업에 관한 고찰.” 대중음악 15 (2015): 45–73.&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13">
<p>Han, Benjamin.  “K-Pop in Latin America: Transcultural Fandom and Digital Mediation.”  <em>International Journal of Communication (19328036)</em>  11 (2017).&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14">
<p>Jang, Won Ho, and Jung Eun Song.  “The Influences of K-Pop Fandom on Increasing Cultural Contact: With the Case of Philippine Kpop Convention, Inc.” 지역사회학 18 (2017): 29–56.&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:15">
<p>Epstein, Stephen.  “From South Korea to the Southern Hemisphere: K-Pop below the Equator.”  <em>Journal of World Popular Music</em>  3.2 (2016): 197–223.&#160;<a href="#fnref:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:16">
<p>Otmazgin, Nissim, and Irina Lyan.  “Hallyu across the Desert: K-Pop Fandom in Israel and Palestine.”  <em>Cross-Currents: East Asian History and Culture Review</em>  3.1 (2014): 32–55.&#160;<a href="#fnref:16" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:17">
<p>Laurie, Timothy N.  “Toward a Gendered Aesthetics of K-Pop.” In Chapman, Ian, and Henry Johnson (eds),  <em>Global Glam and Popular Music: Style and Spectacle from the 1970s to the 2000s</em> , 1st ed., Routledge (2016).&#160;<a href="#fnref:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:18">
<p>Manietta, Joseph Bazil.  <em>Transnational Masculinities: The Distributive Performativity of Gender in Korean Boy Bands</em> . University of Colorado Boulder (2015).&#160;<a href="#fnref:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:19">
<p>Ota, Kendall.  “Soft Masculinity and Gender Bending in Kpop Idol Boy Bands.”  In  <em>Cal Poly Pomona Lectures</em>  (2015). <a href="http://hdl.handle.net/10211.3/138192">http://hdl.handle.net/10211.3/138192</a>.&#160;<a href="#fnref:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:20">
<p>Kim, Gooyong.  “Between Hybridity and Hegemony in K-Pop&rsquo;s Global Popularity: A Case of Girls&rsquo; Generation&rsquo;s American Debut.”  <em>International Journal of Communication (19328036)</em>  11 (2017).&#160;<a href="#fnref:20" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:21">
<p>Elfving-Hwang, Joanna.  “K-Pop Idols, Artificial Beauty and Affective Fan Relationships in South Korea.” In  <em>Routledge Handbook of Celebrity Studies</em> , Routledge (2018), pp. 190–201.&#160;<a href="#fnref:21" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:22">
<p>Jin, Dal Yong, and Woongjae Ryoo.  “Critical Interpretation of Hybrid K-Pop: The Global-Local Paradigm of English Mixing in Lyrics.”  <em>Popular Music and Society</em>  37.2 (2014): 113–131.&#160;<a href="#fnref:22" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:23">
<p>Oh, Chuyun.  “Performing Post-Racial Asianness: K-Pop&rsquo;s Appropriation of Hip-Hop Culture.” In  <em>Congress on Research in Dance</em> , Cambridge University Press (2014), pp. 121–125.&#160;<a href="#fnref:23" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:24">
<p>Saeji, Cedarbough.  “Cosmopolitan Strivings and Racialization: The Foreign Dancing Body in Korean Popular Music Videos.” In  <em>Korean Screen Cultures: Interrogating Cinema, TV, Music and Online Games</em> , edited by David Jackson and Colette Balmain, Peter Lang Publishers, Oxford (2016), pp. 257–92.&#160;<a href="#fnref:24" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:25">
<p>Howard, Keith.  “Politics, Parodies, and the Paradox of Psy&rsquo;s &lsquo;Gangnam Style.&rsquo;”  <em>Romanian Journal of Sociological Studies</em> , 1 (2015): 13–29.&#160;<a href="#fnref:25" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:26">
<p>Reilly, Kara.  <em>Theatre, Performance and Analogue Technology: Historical Interfaces and Intermedialities</em> . Palgrave Studies in Performance and Technology. Palgrave MacMillan, Basingstoke (2013).&#160;<a href="#fnref:26" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:27">
<p>Sutil, Nicolás Salazar.  <em>Motion and Representation: The Language of Human Movement</em> . MIT Press, Cambridge, Massachusetts (2015).&#160;<a href="#fnref:27" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:28">
<p>Watts, Victoria.  “Benesh Movement Notation and Labanotation: From Inception to Establishment (1919–1977).”  <em>Dance Chronicle</em>  38.3 (2015): 275–304.&#160;<a href="#fnref:28" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:29">
<p>Rizzo, Anna, Katerina El Raheb, and Sarah Whatley.  “WhoLoDance: Whole-Body Interaction Learning For Dance Education.” In  <em>Proceedings of the Workshop on Cultural Informatics</em>  (2018) Vol. 2235: 41–50. November 3, 2018. <a href="https://doi.org/10.5281/ZENODO.1478033">https://doi.org/10.5281/ZENODO.1478033</a>.&#160;<a href="#fnref:29" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:30">
<p>Dong, Ran, Dongsheng Cai, and Nobuyoshi Asai.  “Dance Motion Analysis and Editing Using Hilbert-Huang Transform.” In  <em>ACM SIGGRAPH 2017 Talks</em> , 75:1–75:2. SIGGRAPH &lsquo;17. New York, NY, USA : ACM (2017). <a href="https://doi.org/10.1145/3084363.3085023">https://doi.org/10.1145/3084363.3085023</a>.&#160;<a href="#fnref:30" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:31">
<p>Kim, Dohyung.  “생체역학적용 K-POP 댄스 안무 검색 및 자세 정확성 분석 기술 개발 (The Development of the Choreography Retrieval System from the K-POP Dance Database Including Biomechanical Information and the Analysis Technology of the Correctness of a Dance Posture).”  Electronics and Telecommunications Research Institute, Daejeon, Korea (2017). <a href="http://www.ndsl.kr/ndsl/search/detail/report/reportSearchResultDetail.do?cn=TRKO201700003705">http://www.ndsl.kr/ndsl/search/detail/report/reportSearchResultDetail.do?cn=TRKO201700003705</a>.&#160;<a href="#fnref:31" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:31" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:32">
<p>Such systems have yet to appear, though similar motion mimicry-based dance games exist for the now-discontinued Microsoft Kinect and Nintendo Wii consoles. The dance move data for specific K-pop singles remains for sale on an online portal associated with the project, <a href="http://shop2.mocapkpop.cafe24.com">http://shop2.mocapkpop.cafe24.com</a>.&#160;<a href="#fnref:32" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:33">
<p>Kim, Yeonho, and Daijin Kim.  “Real-Time Dance Evaluation by Markerless Human Pose Estimation.”  <em>Multimedia Tools and Applications</em>  77.23 (December 2018): 31199–31220. <a href="https://doi.org/10.1007/s11042-018-6068-4">https://doi.org/10.1007/s11042-018-6068-4</a>.&#160;<a href="#fnref:33" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:34">
<p>Raptis, Michalis, Darko Kirovski, and Hugues Hoppe.  “Real-Time Classification of Dance Gestures from Skeleton Animation.” In  <em>Proceedings of the 2011 ACM SIGGRAPH/Eurographics Symposium on Computer Animation - SCA &lsquo;11</em> , 147. ACM Press, Vancouver, British Columbia (2011). <a href="https://doi.org/10.1145/2019406.2019426">https://doi.org/10.1145/2019406.2019426</a>.&#160;<a href="#fnref:34" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:35">
<p>Hara, Kotaro, Abi Adams, Kristy Milland, Saiph Savage, Chris Callison-Burch, and Jeffrey Bigham.  “A Data-Driven Analysis of Workers&rsquo; Earnings on Amazon Mechanical Turk.”  <em>ArXiv:1712.05796 [Cs]</em> , December 28, 2017. <a href="http://arxiv.org/abs/1712.05796">http://arxiv.org/abs/1712.05796</a>.&#160;<a href="#fnref:35" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:36">
<p>Cao, Zhe, Gines Hidalgo, Tomas Simon, Shih-En Wei, and Yaser Sheikh.  “OpenPose: Realtime Multi-Person 2D Pose Estimation Using Part Affinity Fields.”  <em>CoRR</em>  abs/1812.08008 (2018). <a href="http://arxiv.org/abs/1812.08008">http://arxiv.org/abs/1812.08008</a>.&#160;<a href="#fnref:36" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:36" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:37">
<p>S.V. Aruna Kumar, Ehsan Yaghoubi, Abhijit Das, B.S. Harish and Hugo Proença.  “The P-DESTRE: A Fully Annotated Dataset for Pedestrian Detection, Tracking, Re-Identification and Search from Aerial Devices”    <em>arXiv:2004.02782</em> , 2020. <a href="http://arxiv.org/abs/2004.02782">http://arxiv.org/abs/2004.02782</a>.&#160;<a href="#fnref:37" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:38">
<p>Byeon, Yeong-Hyeon, Sung-Bum Pan, Sang-Man Moh, and Keun-Chang Kwak.  “A Surveillance System Using CNN for Face Recognition with Object, Human and Face Detection.” In Kuinam J. Kim and Nikolai Joukov (eds),  <em>Information Science and Applications (ICISA) 2016</em>  376:975–84. Lecture Notes in Electrical Engineering. Springer Singapore, Singapore (2016). <a href="https://doi.org/10.1007/978-981-10-0557-2_93">https://doi.org/10.1007/978-981-10-0557-2_93</a>.&#160;<a href="#fnref:38" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:39">
<p>Liu, Shuangjun, Yu Yin, and Sarah Ostadabbas.  “In-Bed Pose Estimation: Deep Learning With Shallow Dataset.”  <em>IEEE</em>    <em>Journal of Translational Engineering in Health and Medicine</em>  7 (2019): 1–12. <a href="https://doi.org/10.1109/JTEHM.2019.2892970">https://doi.org/10.1109/JTEHM.2019.2892970</a>.&#160;<a href="#fnref:39" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:40">
<p>Mathis, Alexander, Pranav Mamidanna, Kevin M. Cury, Taiga Abe, Venkatesh N. Murthy, Mackenzie Weygandt Mathis, and Matthias Bethge.  “DeepLabCut: Markerless Pose Estimation of User-Defined Body Parts with Deep Learning.”  <em>Nature Neuroscience</em>  21.9 (September 2018): 1281–89. <a href="https://doi.org/10.1038/s41593-018-0209-y">https://doi.org/10.1038/s41593-018-0209-y</a>.&#160;<a href="#fnref:40" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:41">
<p>Lin, Tsung-Yi, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollár.  “Microsoft COCO: Common Objects in Context.”  <em>ArXiv:1405.0312 [Cs]</em> , February 20, 2015. <a href="http://arxiv.org/abs/1405.0312">http://arxiv.org/abs/1405.0312</a>.&#160;<a href="#fnref:41" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:42">
<p>Güler, Riza Alp, Natalia Neverova, and Iasonas Kokkinos.  “DensePose: Dense Human Pose Estimation In The Wild.”  <em>CoRR</em>  abs/1802.00434 (2018). <a href="http://arxiv.org/abs/1802.00434">http://arxiv.org/abs/1802.00434</a>.&#160;<a href="#fnref:42" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:43">
<p>Moon, Gyeongsik, Ju Yong Chang, and Kyoung Mu Lee.  “PoseFix: Model-Agnostic General Human Pose Refinement Network.”  <em>CoRR</em>  abs/1812.03595 (2018). <a href="http://arxiv.org/abs/1812.03595">http://arxiv.org/abs/1812.03595</a>.&#160;<a href="#fnref:43" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:44">
<p>Pavllo, Dario, Christoph Feichtenhofer, David Grangier, and Michael Auli.  “3D Human Pose Estimation in Video with Temporal Convolutions and Semi-Supervised Training.”  <em>ArXiv:1811.11742 [Cs]</em> , March 29, 2019. <a href="http://arxiv.org/abs/1811.11742">http://arxiv.org/abs/1811.11742</a>.&#160;<a href="#fnref:44" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:45">
<p>Andriluka, Mykhaylo, Umar Iqbal, Anton Milan, Eldar Insafutdinov, Leonid Pishchulin, Juergen Gall, and Bernt Schiele.  “PoseTrack: A Benchmark for Human Pose Estimation and Tracking.” In  <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2018)</em> , IEEE, Salt Lake City, UT, USA (2018) p. 5167. <a href="https://doi.org/10.1109/CVPR.2018.00542">https://doi.org/10.1109/CVPR.2018.00542</a>.&#160;<a href="#fnref:45" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:46">
<p>Ahmadyan, Adel, and Tingbo Hou.  “Real-Time 3D Object Detection on Mobile Devices with MediaPipe.”  <em>Google AI Blog</em>  (blog). Accessed March 11, 2020. <a href="https://ai.googleblog.com/2020/03/real-time-3d-object-detection-on-mobile.html">https://ai.googleblog.com/2020/03/real-time-3d-object-detection-on-mobile.html</a>.&#160;<a href="#fnref:46" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:47">
<p>Broadwell, Peter, Timothy Tangherlini, and Hyun Kyong Hannah Chang.  “Online Knowledge Bases and Cultural Technology: Analyzing Production Networks in Korean Popular Music.” In Jieh Hsiang (ed.),  <em>Digital Humanities: Between Past, Present, and Future</em> . NTU Press, Taipei (2016), pp. 369– 94.&#160;<a href="#fnref:47" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:48">
<p>Kreiss, Sven, Lorenzo Bertoni, and Alexandre Alahi.  “PifPaf: Composite Fields for Human Pose Estimation.”  <em>CoRR</em>  abs/1903.06593 (2019). <a href="http://arxiv.org/abs/1903.06593">http://arxiv.org/abs/1903.06593</a>.&#160;<a href="#fnref:48" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:49">
<p>Computational image analysis techniques proceed at speeds much closer to real-time human viewing than, say, computational text analysis tasks.&#160;<a href="#fnref:49" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:50">
<p>Giraldo, Ramón, William Caballero, and Jesús Camacho-Tamayo.  “Mantel Test for Spatial Functional Data: An Application to Infiltration Curves.”  <em>AStA Advances in Statistical Analysis</em>  102.1 (January 2018): 21–39. <a href="https://doi.org/10.1007/s10182-016-0280-1">https://doi.org/10.1007/s10182-016-0280-1</a>.&#160;<a href="#fnref:50" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:51">
<p>Delaunay, Boris.  “Sur La Sphère Vide.”  <em>Bulletin de l&rsquo;Académie Des Sciences de l&rsquo;URSS, Classe Des Sciences Mathématiques et Naturelles</em>  6 (1934): 793–800.&#160;<a href="#fnref:51" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:52">
<p>Chung, Fan R. K.  <em>Spectral Graph Theory</em> . Regional Conference Series in Mathematics 92. Published for the Conference Board of the mathematical sciences by the American Mathematical Society, Providence, R.I. (1997).&#160;<a href="#fnref:52" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:53">
<p>Xiu, Yuliang, Jiefeng Li, Haoyu Wang, Yinghong Fang, and Cewu Lu.  “Pose Flow: Efficient Online Pose Tracking.”  <em>ArXiv:1802.00977 [Cs]</em> , July 2, 2018. <a href="http://arxiv.org/abs/1802.00977">http://arxiv.org/abs/1802.00977</a>.&#160;<a href="#fnref:53" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:54">
<p>Geertz, Clifford.  <em>Thick</em>    <em>Description: Toward an</em>    <em>Interpretive Theory of Culture</em> . Basic Books, New York (1973), pp. 3–30.&#160;<a href="#fnref:54" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:55">
<p>Börner, Katy.  “Plug-and-Play Macroscopes.”  <em>Communications of the ACM</em>  54.3 (2011): 60–69.&#160;<a href="#fnref:55" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:56">
<p><a href="https://www.youtube.com/watch?v=Zu3hBEZ0RvA">https://www.youtube.com/watch?v=Zu3hBEZ0RvA</a>&#160;<a href="#fnref:56" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:57">
<p>Ankerst, Mihael, Markus M. Breunig, Hans-Peter Kriegel, and Jörg Sander.  “OPTICS: Ordering Points to Identify the Clustering Structure.”  <em>ACM SIGMOD Record</em>  28.2 (June 1, 1999): 49–60. <a href="https://doi.org/10.1145/304181.304187">https://doi.org/10.1145/304181.304187</a>.&#160;<a href="#fnref:57" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:58">
<p><a href="https://www.youtube.com/watch?v=sWuYspuN6U8">https://www.youtube.com/watch?v=sWuYspuN6U8</a>&#160;<a href="#fnref:58" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:59">
<p><a href="https://artsandculture.google.com/story/1AUBpanMqZxTiQ">https://artsandculture.google.com/story/1AUBpanMqZxTiQ</a>&#160;<a href="#fnref:59" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:60">
<p><a href="https://artsexperiments.withgoogle.com/living-archive/">https://artsexperiments.withgoogle.com/living-archive/</a>&#160;<a href="#fnref:60" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:61">
<p>Abello, James, Peter Broadwell, and Timothy R. Tangherlini.  “Computational Folkloristics.”  <em>Communications of the ACM</em>  55.7 (2012): 60–70.&#160;<a href="#fnref:61" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry><entry><title type="html">Deformin' in the Rain: How (and Why) to Break a Classic Film</title><link href="https://rlskoeser.github.io/dhqwords/vol/15/1/000521/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://rlskoeser.github.io/dhqwords/vol/15/1/000521/</id><author><name>Jason Mittell</name></author><published>2021-03-05T00:00:00+00:00</published><updated>2021-03-05T00:00:00+00:00</updated><content type="html"><![CDATA[<h2 id="heading"></h2>
<p>On its face, film and media studies would seem to be a natural space for digital humanities to grow and thrive, as it studies cultural objects that are either born digital in origination, or digitized from cinematic or analog video originals for contemporary distribution. Additionally, dedicated departments of film and media studies often include video production in their scope, meaning that there are usually more digital resources and expertise available than in other humanities departments. But in reality, most of the methods that have been adopted in digital humanities pose challenges to apply to moving images and sounds as an object of study, as with tricky technicalities of text mining from audiovisual sources or copyright restrictions curtailing expansive digital editions of films, or seem to work best when analyzing data outside of the media objects themselves, as with mapping exhibition patterns or doing network analyses of actors across films. But the existence of films and media texts as already-digital objects suggests another path for digital humanities research away from quantitative data analysis: computationally manipulating sounds and images to create new audiovisual artifacts whose insights might be revealed through their aesthetic power and transformative strangeness.<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></p>
<p>I have previously written about how we might consider videographic criticism as a digital humanities research method, manipulating moving images and sounds to create new audiovisual texts that convey arguments and ideas that would otherwise be impossible to generate or articulate via the written word <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. In that essay, I extended Mark Sample&rsquo;s  “deformed humanities”  to the videographic realm, proposing three deformative techniques and applying them to a range of films to seek discoveries within my research laboratory of Adobe Premiere. Such deformations apply patterns and parameters to cultural works, looking to  “break”  the bound object of a poem or film and create something new from the broken parts. As Sample argues, the resulting deformations, which typically defy norms of sense and logic, do not need to be returned into the rational realm of analysis to provide insights about the original&rsquo;s design or meaning; instead, the strange deformed version can function as a new media object, with aesthetic power and cultural resonances that are not circumscribed by their relationship to their original. As he writes,  “the deformed work is the end, not the means to the end”   <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>; see also <sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>  <sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>.</p>
<p>Building upon my previous essay as well as other scholars&rsquo; deformative videographic work <sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>  <sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>, this  “sequel”  takes a different approach: subjecting a single film to a broad range of deformations as a way to explore the wide array of deformative practices that might yield interesting results on a single object.  <em>Singin&rsquo; in the Rain</em>  is a particularly apt film to repeatedly deform for a number of reasons. Few films are as well-known, beloved, broadly-taught, and widely regarded as historically significant — if one effect of deformative work is to make the familiar strange, it&rsquo;s useful to start with a film that is quite familiar. Additionally, the film itself is about cinema at a moment of technological transition from silent to sound, and thus it is thematically resonant to place it into a technological laboratory to perform experiments with sounds and images, even if the film&rsquo;s diegetic technologies are resolutely analog.  <em>Singin</em> &rsquo; also features a broad array of reflexive formal techniques and moments, including color, black-and-white, dialogue, singing, dancing, film-within-film, and fantasy sequences, whose range and variety encourages a playful approach via deformative operations, and thus might be instructive and inspirational for other potential case studies. Finally, it is an unusual film because it contains its own internal deformative moment: during the test screening of  <em>The Dueling Cavalier</em> , a synching glitch in the nascent sound film technology causes the dialogue of two characters to swap; the diegetic audience&rsquo;s reaction to this deformation via laughter and surprise highlights the pleasures of strangeness that hopefully some of my deformations will provoke.</p>
<p>In providing an inventory of deformative practices, I do not aim to suggest comprehensiveness nor a progression toward  “better”  methods and revelations. This is a provisional list of techniques that I have found productive in creating new cultural objects that seem meaningful and/or pleasurable, but I fully expect many will find some of them uninteresting or unpleasant. I have thrown many techniques at the proverbial wall, but will not inventory the failures that did not stick — except one that might exemplify what a  “failure”  might mean in this context. Inspired by the film&rsquo;s internal deformation of unsynched sound, I experimented with  “re-synching”  that moment in  <em>Dueling Cavalier</em>  to line up the proper dialogue, and then playing the rest of the film with this new sonic synchronization — effectively making the strange moment normal, and the rest of the normal moments strange. But the result was neither sufficiently strange nor interesting — watching the film&rsquo;s dialogue and musical numbers with the sound lagging around two seconds behind was simply annoying, not revelatory nor pleasurable. Thus the bar that each of my subsequent deformations clears (at least to me) is to create something interesting and/or enjoyable to watch and hear. I hope the same holds for all who proceed through this strange wilderness of a broken classic film.</p>
<p>I fully admit that these resulting works are difficult to categorize. Are they acts of scholarship? Do they contain or provoke arguments? Or are they creative works, more akin to experimental films? Certainly there are many parallels with the latter, where the use of algorithms and parameters on pre-existing footage creates new artworks, as with Martin Arnold&rsquo;s glitchy remixes of cinematic fragments or Cory Arcangel&rsquo;s collages of YouTube musical clips (see <sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>  <sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup>  <sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup>). I present these deformations here not in the context of an art gallery or film festival, but rather embedded within a written scholarly essay, clearly signaling them as academic expressions over artistic works. As I previously wrote regarding the possibilities of videographic criticism, such works can embrace the poetic and experimental scholarly impulse of Robert Ray&rsquo;s writing <sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>, joining  “scientific quantification and artistic poeticization together, creating works that transform films and media into new objects that are both data-driven abstractions and aesthetically expressive”   <sup id="fnref1:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. While each deformation falls short of the bar for a piece of rigorous argument-driven scholarship, taken together I believe they do create an argument for approaching an audiovisual work as a site of experimentation and play, offering the possibility of revealing new dimensions and facets of a highly familiar text.</p>
<h2 id="1-still-frames">#1: Still Frames</h2>
<p>One of the first (and most inspirational) examples of filmic deformations I know of is Kevin Ferguson&rsquo;s  “summed frames”  project. In <a href="https://filmvis.tumblr.com/">a wide-ranging website</a> and associated scholarly publications, Ferguson publishes still images of films that are derived from his deformative manipulations. Using Quicktime, he exports a feature film into an image set, typically a set of still frames sampled every few seconds of a film; Ferguson then repurposes the open-source medical imaging software ImageJ to overlay all of these still frames into a single  “summed frame”  image visual that distills all of the light and color typically experienced flickering in front of our eyes over the course of watching a film <sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000521/resources/images/figure01.jpeg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000521/resources/images/figure01_hu5438825b9b6d1014226d20d231e650c2_45094_500x0_resize_q75_box.jpeg 500w,
    /dhqwords/vol/15/1/000521/resources/images/figure01_hu5438825b9b6d1014226d20d231e650c2_45094_800x0_resize_q75_box.jpeg 800w,/dhqwords/vol/15/1/000521/resources/images/figure01.jpeg 715w" 
     class="landscape"
     ><figcaption>
        <p>Kevin Ferguson sums the frames of <em>Singin’</em> into one still image, with subtle details referring to the original’s composition.
        </p>
    </figcaption>
</figure>
<p>Ferguson&rsquo;s summed frame image of  <em>Singin&rsquo; in the Rain</em>  (see <a href="#figure01">Figure 1</a>) is both beautiful and interesting. It presents an impressionistic residue of the film&rsquo;s composition, lighting, and production design that would be unidentifiable without context. Knowing the source material, we can see details that resonate with significance, as with the outline of the screen-within-a-screen image and red-tinted curtains on the margins that highlight the importance of cinematic presentation within the film. Ferguson&rsquo;s work becomes more meaningful in comparison, as the visual patterns and contrasts reveal insights into a corpus of films, as seen in his collection of summed images of Disney features, Westerns, and gialli films <sup id="fnref1:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>. Thus we could compare this image to those produced by summing the frames of other MGM musicals, Gene Kelly performances, or Hollywood films from 1952. But for the purposes of this project, I am less interested in using deformations to take us outside  <em>Singin&rsquo; in the Rain</em>  than to revel in the strange versions of the film that such methods produce.</p>
<p>Once we have broken the film into its component parts of still frames — in this case, a folder of 925 still images sampling every six seconds of the film — we can perform other operations to see what they might yield. Fellow deformation-minded digital humanist Zach Whalen has developed <a href="http://www.zachwhalen.net/pg/imj/">imj</a>, a tool that allows for straightforward web-based graphical analytics from a corpus of images using three basic tools: plot, montage, and barcode <sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup>. Plot is by far the most analytical tool, offering data visualizations to plot an image corpus on a grid with a range of variables to map onto the X and Y axes, including brightness, hue, saturation, and luminance. Not surprisingly, the results create data-driven charts that reveal tendencies and patterns within the film&rsquo;s imagery, but do not produce compelling aesthetic objects in their own rights. The other two tools are more deformatively effective — the montage tool creates a visual grid of the film&rsquo;s frames laid out sequentially. The resulting distillation offers some analytical insights into the scene and shot breakdowns, but moreover it creates an experience that evokes  <em>Singin&rsquo;</em>  as we have never seen it before, simultaneously watching all scenes at once but with a different visualization of temporality than Ferguson&rsquo;s summed frame (see <a href="#figure02">Figure 2</a>). If you know the film, you can identify scenes, dance numbers, and narrative moments from this image, but you can also admire designs that you could never see while watching the film.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000521/resources/images/figure02.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000521/resources/images/figure02_hu5cbf4313d0f7966a1f207b2b5d78c3c5_837597_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000521/resources/images/figure02_hu5cbf4313d0f7966a1f207b2b5d78c3c5_837597_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000521/resources/images/figure02.png 740w" 
     class="landscape"
     ><figcaption>
        <p>The montage tool in imj creates a simultaneous view of the entire film from a distance.
        </p>
    </figcaption>
</figure>
<p>The third tool in Whalen&rsquo;s software package is the most deformative, presenting the sum of the film&rsquo;s frames, but from a sideways perspective to create a barcode. The resulting image certainly bears markers from the film, and knowing the source allows viewers to identify scenes and even characters. But without knowledge of the source, it becomes a work of abstract art — generated algorithmically from another work of art (see <a href="#figure03">Figure 3</a>).</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000521/resources/images/figure03.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000521/resources/images/figure03_hu79289781f3964a6888f30a0d29494e90_4126366_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000521/resources/images/figure03_hu79289781f3964a6888f30a0d29494e90_4126366_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000521/resources/images/figure03_hu79289781f3964a6888f30a0d29494e90_4126366_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000521/resources/images/figure03_hu79289781f3964a6888f30a0d29494e90_4126366_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/1/000521/resources/images/figure03_hu79289781f3964a6888f30a0d29494e90_4126366_1800x0_resize_box_3.png 1800w,/dhqwords/vol/15/1/000521/resources/images/figure03.png 1851w" 
     class="landscape"
     ><figcaption>
        <p>imj’s barcode offers a way to watch <em>Singin’ in the Rain</em> from the side of the image
        </p>
    </figcaption>
</figure>
<p>Of course, algorithms can be altered to create different results. Whalen&rsquo;s tool can create a barcode that reduces each frame to its average or dominant color instead of maintaining its core image pattern, as with the above image. The resulting barcodes, as with this one below based on average colors of each frame, are much more abstract and completely unidentifiable as a derivative work from one of Hollywood&rsquo;s most iconic films. While such a barcode might be a more attractive pattern for a poster or pillowcase, as <a href="https://www.redbubble.com/people/moviebarcode/works/6987368-moviebarcode-singin-in-the-rain-1952">an online shop does sell merchandise based on movie barcodes</a>, it falls outside of the nexus of aesthetic appreciation and analytic possibility where I contend most effective deformations can be found (see <a href="#figure04">Figure 4</a>).</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000521/resources/images/figure04.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000521/resources/images/figure04_huf9e4e6335a822643fcb96b562944dddf_11791_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000521/resources/images/figure04_huf9e4e6335a822643fcb96b562944dddf_11791_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000521/resources/images/figure04.png 925w" 
     class="landscape"
     ><figcaption>
        <p>The barcode version with average colors becomes more removed from the source film, sacrificing analytic possibilities for a more accessible aesthetic.
        </p>
    </figcaption>
</figure>
<p>A still frame itself could be thought of as a kind of manipulation of a film — even though  <em>Singin&rsquo; in the Rain</em>  was originally composed of approximately 146,000 still images projected sequentially, these frames were all designed to create the illusion of movement. The deformations I&rsquo;ve presented thus far have all excluded movement as a design element, raising the question of how we might deform this set of still images by reintroducing movement into their aesthetic. This question arose in a conversation I had with Ferguson, as we brainstormed how to visualize the process of a summed frame accruing over time. Most processes we explored via video editing programs were underwhelming, as the accumulating image would coalesce into a composite blur after a few seconds, with subsequent developments building up too subtly to notice.<sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup>  It became clear to me that a more compelling version would require the images to slowly fade out, creating the effect of a rolling accumulation of images that was too complicated for me to figure out in Adobe Premiere. My solution, like with Ferguson&rsquo;s use of medical imaging software, involved repurposing a tool designed for far different purposes — StarStaX, an image blending program designed for star trail astronomical photography, proved to be the right tool to create the desired effect, and then screen capture the process to create a videographic deformation (see <a href="#figure05">Figure 05</a>).<sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup></p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://player.vimeo.com/video/206950599" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="A videographic summed frame demonstrates the process of overlaying images using StarStaX, along with a sonic rendering of such accumulation." webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div>
<p>The resulting video summing the film&rsquo;s frames creates a hypnotic tour through the film&rsquo;s images in two minutes, creating an impressionistic collage effect that highlights color and composition over plot and character. But the image-only version seemed lacking without sound, so I tried to determine what the equivalent for  “summed frames”  would be for sound. This challenge highlighted a core differential that creates challenges for deformative videographic work: sound lacks clear units like  “frames”  or  “shots,”  meaning that the entire soundtrack is more continuous and fluid than the image track. My colleague David Miranda Hardy helped me design a soundtrack that mimicked the frame sampling idea by isolating sounds sequentially from the film, and using reverb effects to evoke the blurred overlapping images. Together, the video creates an evocative experience that clearly derives from the sequential film source, but creates far different aesthetic and affective responses. As with any successful deformation, the work builds on the source material in unexpected ways to create something that feels both derivative and original.</p>
<h2 id="2-motion">#2: Motion</h2>
<p>Motion is a powerful transformative element in videographic deformations, whether imposed upon still sources, as with the previous summed frame video, or harnessed from a film&rsquo;s own motion. In thinking how to extract motion from a film as a deformative act, we can look to one of the most widespread formats to disseminate motion: GIFs. As a short repeating distillation of a moving image, a GIF can offer a tremendous tool for film criticism via digital publication, as argued by Michael Newman, where both form and affect can be captured for analytical purposes <sup id="fnref:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup>. For instance, this GIF of one of  <em>Singin&rsquo; in the Rain</em> &rsquo;s most iconic shots provides an opportunity to analyze the coordination between camera movement, dance choreography, props, sets, and special effects to create a breathtaking affective moment (see Figure 5).</p>






















<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000521/resources/images/figure06.gif"><figcaption>
        <p>Gene Kelly’s endless spinning reinforces the GIF’s looping aesthetic.
        </p>
    </figcaption>
</figure>
<p>But just as staring at a repeated word or phrase for long enough will cause meaning and sense to start to break down, the repetition and abstraction of this GIF eventually encourages a sense of disorientation, unfamiliarity, and strangeness that is familiar to the realm of deformations, as Gene Kelly spins endlessly and breathlessly in the rain. Thus choosing looped segments that promote such a sense of the strange can make new deformed objects out of more conventional moments (see <a href="#figure07">Figure 7</a>).</p>






















<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000521/resources/images/figure07.gif"><figcaption>
        <p>Donald O’Connor’s manic dance becomes even more extreme on an infinite loop.
        </p>
    </figcaption>
</figure>
<p>In the midst of his show stopping number  “Make &lsquo;Em Laugh,”  Donald O&rsquo;Connor&rsquo;s roll across the room is one of many entertaining moves that does, indeed, make us laugh. As an endlessly repeating decontextualized GIF, it suggests something more manic and disconcerting — perhaps an enchantment that O&rsquo;Connor is doomed to roll with an astonished grin for all eternity, or a curse on us the viewer to watch his gleeful twirl for an equally long time. Other GIFs can capture a moment that is itself already innately odd (see <a href="#figure08">Figure 8</a>).</p>






















<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000521/resources/images/figure08.gif"><figcaption>
        <p>A moment from “Beautiful Girl” becomes a decontextualized loop of cheer.
        </p>
    </figcaption>
</figure>
<p>A tiny excerpt from  “Beautiful Girl,”  one of the film&rsquo;s most idiosyncratic numbers, creates a GIF that evokes no narrative, performance, or character resonances; instead, it just captures pure style and tone, poised to circulate in today&rsquo;s natural habitat for GIFs: as an affective response on social media, disconnected from its cinematic source.</p>






















<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000521/resources/images/figure09.gif"><figcaption>
        <p>A shot-reverse shot of mock dance at the film’s climax builds on character relationships.
        </p>
    </figcaption>
</figure>
<p>My personal favorite  <em>Singin</em> &rsquo; GIF captures a moment of fake dance, as Don, Cosmo, and Simpson mock Lena&rsquo;s attempt to perform a musical number (see <a href="#figure09">Figure 9</a>). As the interchange between the two shots repeats endlessly, the expressions feel more exaggerated, the mockery more pointed and severe. I can imagine ways to exaggerate and further deform the image, but the effect remains strange and off-putting even as the source material is left intact.</p>
<p>A single GIF can transform a moment by repetition and decontextualization, but combining GIFs offers new deformative possibilities via spatial montage. Back in 2001, Lev Manovich predicted that  “broadband cinema”  of the internet era would foreground the multiplicity of frames and screens by juxtaposing images spatially as an alternative to temporal montage <sup id="fnref:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup>. More recently, Catherine Grant has theorized the function of spatial montage in videographic criticism, exploring how the meanings and signifiers coexist and resonate through juxtaposition via split-screen designs <sup id="fnref:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup>. But applying spatial montage to GIFs pushes away from signification and meaning, as the decontextualized looping images bounce off one another to create affecting, engaging, and surprising compositions; as Jennifer Malkowski has productively analyzed, such compositions are popular in fan communities in the form of GIFsets, where users of sites like Tumblr create and curate carefully arranged groupings of GIFs to express affective connections between looping motion sequences. As Malkowski suggests,  “Spatial montage displays multiple moving images on screen at once, granting the work&rsquo;s maker new powers of artistic juxtaposition and its viewer access to a broader and more complex visual field”   <sup id="fnref:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup>. Thus we might imagine a GIFset as a site of deformative possibility to create new ways to experience  <em>Singin&rsquo; in the Rain</em> &rsquo;s motion.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000521/resources/images/figure10.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000521/resources/images/figure10_hud93e17d3a57937b2f702a4fe62e00758_268307_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000521/resources/images/figure10_hud93e17d3a57937b2f702a4fe62e00758_268307_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000521/resources/images/figure10.png 512w" 
     class="landscape"
     ><figcaption>
        <p>To view this GIFset in motion, follow this <a href="https://videographicsandbox.wordpress.com/2017/03/27/singin-in-the-gifs/">link</a>.
        </p>
    </figcaption>
</figure>
<p>I created a number of GIFs of dancing loops from across the film, each designed to approximate continuous motion (see <a href="#figure10">Figure 10</a>). In arranging them in a three-by-three GIFset, I focused on juxtaposing modes of motion that seemed to interact in interesting ways, considering both camera movement and the choreographed bodies of the performers. One of the interesting effects of the GIFset results from the varying lengths of each GIF, creating a phase effect among the repetitions of the loops that diverges over time. As the loops shift their sync, we can see unexpected resonances between the motions in neighboring frames, making the viewing experience quite variable, as spatial montage structures the productively arbitrary temporality.</p>
<p>Of course, GIFs extract moving images from the film, discarding the soundtrack. Using the logic of the GIFset, I created a simulated GIFset within Adobe Premiere by arranging the video in a grid and copying clips to mimic the looping effect, thus providing an opportunity to add a soundtrack. Given the choreographic content of the GIFs, I excerpted, looped, and layered one of Gene Kelly&rsquo;s  “Gotta Dance”  lines from  “Broadway Melody”  to create what feels like a hypnotic, if arhythmic, sonic collage to accompany the grid of dancers (see <a href="#figure11">Figure 11</a>).</p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://player.vimeo.com/video/210426270" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="A simulated GIFset with accompanying audio looping interweaves unexpected beauty and chaos." webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div>
<p>Following the GIFset logic to its most chaotic conclusion, I created another simulated set of GIFs to loop both video and audio within a three-by-three grid. The opening to the musical number  “Beautiful Girl”  is the film&rsquo;s most chaotic, random, and arbitrary sequence, with snippets of technicolor dancing and singing to a patchwork of early movie musical numbers, created in tribute to 1930s Busby Berkeley musicals. I chose nine different visual moments and created a looped grid, allowing the audio to loop in a cacophonous sonic overlay (see <a href="#figure12">Figure 12</a>). By making the film&rsquo;s strangest moments even stranger, the effect is certainly overwhelming and not necessarily pleasant, but seems to capture something of the original&rsquo;s disorienting tone.</p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://player.vimeo.com/video/375152157" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="The Busby Berkeley aesthetic emphasizes spectacle, taken here to its illogical conclusion." webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div>
<p>There are certainly other means to emphasize and deform motion beyond extracting sequences into GIFs. Kevin Ferguson offers another instructive example in his video  “Edge”  (see <a href="#figure13">Figure 13</a>).</p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://player.vimeo.com/video/208869731" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="Kevin Ferguson’s Edge deforms the still images and reassembles them into a musical number that feels like it came from an abstract animated film." webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div>
<p>Following from his summed frame work, Ferguson exported the frames of the musical number  “You Were Meant for Me”  and the opening credit sequence at twenty-four frames-per-second grouped into folders for each shot from the film. He then experimented with ten different ImageJ algorithms to perform  “edge detection”  operations on each set of stills, creating a variety of effects that resemble hand-drawn animation and other abstractions that emphasized the edges of the characters and set. Ferguson reassembled the video by placing the grouped still frames in sequence on top of the soundtrack in a video editor, literally animating the edge-abstracted images to reconstitute the film. The effect is a marvel of motion and blank space, provoking thoughts on how we perceive and constitute movement via cinema, while creating moments of expressive beauty hidden in the original film. Additionally, knowing the underlying process helps us remember the materiality of moving cinematic images as successive still frames, a digital transformation that evokes the analog original.</p>
<p>Focusing on motion in a dance-centered film also raises questions about what movement in a cinematic dance sequence entails. Certainly much of the beauty in  <em>Singin&rsquo; in the Rain</em> &rsquo;s choreography involves the coordination of moving bodies and moving cameras, as many of the GIFs highlight. But what if we force ourselves to focus more on bodies, and specifically body parts in motion — what will we see differently? With that question in mind, I deformed one of the film&rsquo;s most iconic dance numbers, the title track, by masking the image to only include Gene Kelly&rsquo;s hands and feet (see <a href="#figure14">Figure 14</a>).</p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://player.vimeo.com/video/" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="The film’s iconic title number remains intact, but by only being able to see Kelly’s hands and feet, our attention is directed to the motion of his extremities that is otherwise invisible in the original." webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div>
<p>As the extremities of Kelly&rsquo;s body, his hands and feet explore the staged space most broadly, while eliminating his face and torso directs our attention to these abstracted movements in rhythm to his disembodied voice. To me, the effect of the video is cartoony, bringing out the animation inherent in dance, but in a more playful and almost inhuman way than Ferguson&rsquo;s  “Edge”  video. While we watch these body parts in motion, we see them as animated shapes on a black background, where the edges of the mask become noticeable much like Ferguson emphasizes the edges of the dancers&rsquo; bodies. I find the captivating simplicity of his moving hands and feet to simultaneously capture and deny the beauty of Kelly&rsquo;s mastery of cinematic dance.</p>
<h2 id="3-shots">#3: Shots</h2>
<p>Returning to the fundamental elements of film, we can look at specific shots as another type of raw material to create deformations. In my earlier essay, I suggested some deformative possibilities tied to shots, including a videographic version of Nicholas Rombes&rsquo;s  “10/40/70”  project <sup id="fnref2:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>  <sup id="fnref:20"><a href="#fn:20" class="footnote-ref" role="doc-noteref">20</a></sup>. Rombes&rsquo;s protocol juxtaposes three still frames from a feature film, from the arbitrary 10, 40, and 70 minute marks, to see what meanings and insights might emerge; my videographic version compiles the shots that occur at those minute markers to create a short film composed of three arbitrary but chronological shots (See <a href="#figure15">Figure 15</a>).</p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://player.vimeo.com/video/151845898" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="The videographic 10/40/70 demonstrates the possibilities of arbitrary juxtapositions at the core of deformative work." webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div>
<p>The three shots do present an effective mini-narrative, focused on Don&rsquo;s romantic life starting with the flashback scene of Don initially meeting Lena and her rebuffed attempt to silently court him once he is promoted from stuntman to movie star. The next shot is coincidentally the longest take in the entire film, as Don and Kathy escalate their romance while walking across the studio lot and he assures her that he&rsquo;s not involved with Lena. The sequence ends with a shot of Don gleefully dancing in the rain, celebrating his joy over finding love with Kathy. While this brief deformation lacks the interesting insights that some of the other videographic 10/40/70 examples I discussed in my earlier essay, it does highlight how the combination of arbitrary shots can create meaning and resonances.</p>
<p>A more holistic approach to deformation considers how the film in its entirety is organized into shots. In Premiere, I segmented the entire film into its component 376 shots (splitting dissolves and wipes in the middle of the transition) to create a corpus to experiment with. Although we don&rsquo;t typically think of a video editing platform like Premiere as a database, it effectively is — subclips are actually just metadata entries referencing the original video to indicate timecode in and out points, as well as any other effects or transformations. Because Premiere can include some of this metadata in its view of a folder of clips, it was easy to sort the 376 shots ascending by length with a single click; then I gathered the clips into a sequence that re-organizes the entire film by ascending length of shots (see <a href="#figure16">Figure 16</a>).</p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://player.vimeo.com/video/208873779" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="The entirety of Singin’ in the Rain, rearranged in ascending length of each shot, creates a whirlwind marathon of a viewing experience." webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div>
<p>The resulting edit forgoes narrative coherence for another organizing principle driven by shot length, yielding some intriguing moments. The video certainly starts frenetically, with fifty-nine shots in the first two minutes — many of these come from the  “Beautiful Girl”  number, the film&rsquo;s most disjointed, chaotic, and quick-cut sequence. The early moments are often amusing, with massive tonal shifts and odd juxtapositions, such as the audience reaction shots responding to random dance moves or lines; at around 1:30, a number of shots from  <em>The Dancing Cavalier</em>  premiere create a disjointed version of Lena&rsquo;s attempt to sing in front of an audience. As the shots increase in length, some of the arbitrary juxtapositions become more interesting, as with the cinematically reflexive sequence starting at 9:50, featuring shots of  <em>Variety</em>  newspaper headlines, the closing  “The End”  title, the opening credit of  <em>The Dueling Cavalier</em> , and the MGM lion, all intercut with random shots from the film lasting around seven seconds each. At 43:00, a sequence emerges with shots lasting around twenty-six seconds that sandwich two moments from Kathy and Don&rsquo;s first tempestuous meeting around their final kiss, contrasted with Don performing with the film&rsquo;s other two fictional romantic partners (Lena and the unnamed dancer played by Cyd Charisse), and the film&rsquo;s final shot of the happy couple looking at their own billboard for the film-within-a-film; while not narratively coherent, these arbitrary juxtapositions driven by shot length create a more compelling set of contrasts than the 10/40/70 sequence.</p>
<p>The full recut film is hard to watch except as a detached analyst looking for patterns, as the feeling of random disjuncture overwhelms any sense of continuity. However, for some reason this is by far my most-watched video on Vimeo with over 15,000 views as of February 2021, vastly exceeding full-fledged video essays that have been broadly disseminated and shorter experiments that seem more poised to go viral. It&rsquo;s hard to identify what the appeal of this deformative take on the entire film might be, but seemingly it has attracted an audience — the one theory I can imagine is that people are stumbling upon it while searching for an online version of the full film, although that does not explain how the video registers an average of 20% of the 100 minutes viewed and more than 190  “finished”  viewings.<sup id="fnref:21"><a href="#fn:21" class="footnote-ref" role="doc-noteref">21</a></sup></p>
<p>The biggest insight that I gained from this experiment comes toward the end of the video — prior to sorting by shot length, I had assumed that the longest takes in  <em>Singin&rsquo;</em>  would be within dance numbers, as the moving camera would capture the choreography without frequent editing.<sup id="fnref:22"><a href="#fn:22" class="footnote-ref" role="doc-noteref">22</a></sup>  However, only one of the nine longest shots to conclude this video is from a musical number; the rest are lengthy dialogue sequences. The longest shot of almost two minutes, as mentioned before, follows Don and Kathy through the studio lot and thus feels choreographed without music, but the others (all longer than a minute) include Kathy driving Don in her car, Don and Lena&rsquo;s red carpet interview at the start of the film, the cast rehearsing the dialogue scene in  <em>Dueling Cavalier</em> , and numerous discussions behind the scenes debating the film studio&rsquo;s next moves. I decided to capture this insight by creating another deformation highlighting these nine longest takes in tandem (see <a href="#figure17">Figure 17</a>).</p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://player.vimeo.com/video/320830971" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="The nine longest takes in the film surprisingly include only one dance sequence, with the rest featuring dialogue sequences that foreground other forms of movement." webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div>
<p>When arranged next to each other via spatial montage, organized by when they appear in the film chronologically, we can see shared traits in the film&rsquo;s cinematography and blocking, highlighting how movement is featured within many scenes even without dancing. This grid highlights how  <em>Singin&rsquo; in the Rain</em>  functions as a long-take film, with an average shot length of 16.4 seconds, a cutting rate comparable to many filmmakers renowned for their long-take styles, such Max Ophüls, Stanley Kubrick, and Michelangelo Antonioni.<sup id="fnref:23"><a href="#fn:23" class="footnote-ref" role="doc-noteref">23</a></sup>  Few critics would place Stanley Donen and Gene Kelly in such company as masters of a long-take aesthetic, as their films are best known for cinematic movement more than duration. But as this experiment reveals,  <em>Singin&rsquo;</em>  is anchored by many long-take shots and single-take scenes that run longer than the dance sequences it is most renowned for, suggesting the film and broader genre may be due for a formalist reappraisal.</p>
<h2 id="4-speed">#4: Speed</h2>
<p>As the discussion of shot length suggests, timing is a central facet of film technique, and thus a realm ripe for deformation. Since video editing platforms like Premiere can easily change the duration of a video by speeding it up or slowing it down, this is a useful tool for deforming footage. As I outlined in my 2019 essay, one way that speed can deform a film is by creating an  “equalized pulse”  that remaps the time of every shot to match the sequence&rsquo;s average shot length <sup id="fnref3:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. In that essay, I deformed a number of films and scenes this way to chart a range of outcomes; here we can look at a number of different sequences from a single film whose equalized pulses are suggestive.</p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://player.vimeo.com/video/" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="The entire film equalized to its average shot length of 16.4 creates a truly strange effect that is hard to endure." webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div>
<p>We might start by watching the entire film equalized to its shot length of 16.4 seconds, but this is a true endurance test (see <a href="#figure18">Figure 18</a>). Many of the moments are overwhelmingly strange, such as the opening sequence of the  “Beautiful Girl”  number starting at the thirty-three-minute mark, where the montage of quick shots that runs just fifty seconds in the original film drags out in super slow-motion over five minutes. Other segments provide compelling contrasts, as the sequence starting at the thirty-one-minute mark: Don and Lena are shooting the silent version of  <em>Dueling Cavalier</em>  as Simpson runs in to announce the transition to sound in hyperspeed. The film cuts to Lena speaking in super slow-motion, distorting her already unacceptable voice before lingering on the dismayed reaction of the male characters. Another interesting sequence is at the fifty-three-minute mark, as the test screening deforms the audience members mocking the glitchy film, making their reactions even more strange and unexpected than the failed screening itself. This deformation can also redirect our attention, as at the forty-three-minute mark in the lead-up to the  “You Were Meant for Me”  number — the longest take in the film flies by, which draws our attention to the movement through the studio space rather than the dialogue or performances. In the soundstage, some slow shots highlight Kathy and Don&rsquo;s emotions as they launch into the song and romance, but the sped-up song, compressing a four-minute number into a single minute, directs our attention to the camera movement and blocking within the space, more than the subtleties of the song and dance.</p>
<p>The equalized pulse approach yields the most interesting outcomes when applied to musical numbers, where the tempo and rhythm of visual editing overrides those elements in the music and dance. When each shot conforms to a single length, the music contracts or expands to fill the equalized length, and thus creates strange and often compelling moments of unintended oddity.</p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://player.vimeo.com/video/151095736" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="Broadway Melody equalized makes an already experimental sequence even more unusual." webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div>
<p>In my 2019 essay, I wrote about the equalized pulse version of  “Broadway Melody,”  where stark contrasts between long and short takes result in radical juxtapositions, as with the opening few shots where the opening shot of Don&rsquo;s singing sprints through at 387% speed, followed by a pan through the crowd at a glacial 12% speed (see <a href="#figure19">Figure 19</a>). These strange contrasts highlight the artificiality and lack of narrative coherence already present in this number.</p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://player.vimeo.com/video/151158066" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="The equalized pulse version of Singin’ in the Rain provides moments of insight into movements via their deformed speeds." webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div>
<p>A more typical musical number is the title track, which uses long takes with an average shot length of twenty-nine seconds for the sequence (see <a href="#figure20">Figure 20</a>). When equalized to this rate, the contrasts are less severe, but still highlight how there is some editing variability in the original sequence — the famous shot of Kelly twirling on the street gets drawn out to 35% speed, allowing us to linger on his expressions and command of the space. This shot is followed by Kelly dancing along the curb at 128% speed, highlighting the playful comedy of his joyful splashing. While this deformed version is not particularly compelling on its own, it does allow us to see an incredibly familiar sequence in a new light.</p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://player.vimeo.com/video/152071802" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="Make &#39;Em Laugh equalized to its average shot length reveals details about gesture, expression, and energy." webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div>
<p>The  “Make &lsquo;Em Laugh”  number is driven by Donald O&rsquo;Connor&rsquo;s physical comedy and energetic performance. When equalized to the number&rsquo;s average shot length of twenty seconds, each shot reveals something about the original, with sped up shots capturing O&rsquo;Connor&rsquo;s frantic energy, while the slower moments highlight his more subtle facial expressions (see <a href="#figure21">Figure 21</a>). But what none of these equalized musical numbers do is create a clear rhythm through the equalized pulse defined by the tempo of the shots — the average shot lengths are too long to convey such a pulse through the cutting speed, an outcome that I&rsquo;ve only found through very fast cut sequences, as with the  “Roxanne”  number in  <em>Moulin Rouge!</em> , as discussed in my 2019 essay.</p>
<p>Equalized pulse videos make the video track dominant within the sequence, as the visual edits dictate the speed and pace of each deformation. As discussed with the summed frames approach, sound does not conform as easily to manipulable units like frame or shot, as the audio track is continuous and unbroken, and thus hard to quantify. One way that audio might guide a similar equalization experiment would be to focus on the number of words spoken or sung, and then equalize a sequence to a standard number of words per temporal unit. How might such an audio-driven equalized pulse differ from one based on video edits?</p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://player.vimeo.com/video/206354368" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="Equalizing the pulse based on number of words rather than by visual edits allows us to experience how lyrics are unequally distributed across a musical number." webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div>
<p>I performed such a comparison with  “Make &lsquo;Em Laugh,”  by equalizing twenty-four words to the same pulse as the film&rsquo;s average shot length of 16.4 seconds — twenty-four words was an arbitrary number, but inspired by the film&rsquo;s twenty-four frames per second rate. Thus I broke up the sequence into subclips by cutting after every twenty-four words in the scene (using the caption track to help quantify words), and then treating each subclip as a  “shot”  to be equalized. By juxtaposing these two equalizations, where the speed of each subclip changes at the same time every 16.4 seconds, we can contrast the relative rates of video editing and audio speaking/singing in this number (see <a href="#figure22">Figure 22</a>). Most notably, the word rate is faster than average for most of the song, as every clip is slowed down on the audio equalized pulse for the first three minutes of the video; however, the final three shots compress the final 2:20 of the number into only fifty seconds, with one dance-heavy clip playing at 619% in order to squeeze twenty-four words into 16.4 seconds. While this experiment helps highlight how words are distributed unequally in a musical number, ultimately the deformative impact is not significantly different than the equalized pulse driven by visual editing.</p>
<p>Manipulating speed can be used to deform films in many ways beyond equalized pulses, as changing a clip&rsquo;s speed is a noticeable and often playful deformation. Many deformative possibilities are already widespread in the wilderness of online meme culture, where fans mess with popular culture in transformative and imaginative ways; while such memes may not be designed to lead to scholarly insight, like with GIFs, we can adopt such pre-existing fan strategies to see what deformative possibilities might arise. One meme that I felt could be aptly applied to videographic deformation is what might be called the  “but faster”  video: a film or music video gets faster every time something specific is said or happens onscreen. Allegedly started by a video called <a href="https://www.youtube.com/watch?v=JMG1Nl7uWko"> “The Entire  <em>Bee Movie</em>  but every time it says  bee  it speeds up by 15%” </a>, the trend has been applied broadly to many sources to create a range of playful and often silly deformed videos <sup id="fnref:24"><a href="#fn:24" class="footnote-ref" role="doc-noteref">24</a></sup>.</p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://player.vimeo.com/video/375272867" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="Playing with speed following the logic of an online meme highlights some temporal contrasts as well as creating a distinctly goofy experience." webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div>
<p>I applied this meme logic to  “Broadway Melody,”  but speeding up by 10% every time somebody sang the word  “dance.”  The result is frantic by the end, reaching 350% speed as Kelly and company flail to keep pace with the manic music. The result is definitely more fun and goofy than deep or insightful (not that there&rsquo;s anything wrong with fun and goofy), but at high-speeds, the video highlights the different modes of dance embedded within this long number, as the slow and sultry segments with Cyd Charisse feel almost normally paced compared to the frenetic group dances throughout, even though the lengthy ballet segment is sped up to 300%. This video is certainly among the more frivolous of the deformations I have made, but given that the thirteen-minute  “Broadway Melody”  number is the one part of the film that some viewers find too slow, this six-minute version might be more palatable as an alternative.</p>
<h2 id="5-space">#5: Space</h2>
<p>Just as video editing can easily play with time, manipulating spatial relations can create videographic deformations as well. Catherine Grant is a leader in this work, especially in thinking about the role of spatial montage in videographic expression. I was inspired by her video  “Fated to be Mated: An Architectural Promenade,”  which deforms a Fred Astaire / Cyd Charisse dance number from the film  <em>Silk Stockings</em>  by  “sawing [them] in half”   <sup id="fnref:25"><a href="#fn:25" class="footnote-ref" role="doc-noteref">25</a></sup>.</p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://player.vimeo.com/video/300303270" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="Catherine Grant’s Fated to be Mated creates an innovative effect of spatial montage within a dance sequence." webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div>
<p>Grant&rsquo;s video remakes the space of the dance sequence via a simple manipulation: splitting the frame vertically and reducing the scale of the left half (see <a href="#figure24">Figure 24</a>). She further deforms the sequence by slightly slowing it down and replacing the original music with contemporary electronica, but the core visual deformation of scale has the most notable impact — we see the dance space as fractured and made strange, while the duo navigates the space by transforming their relative scale. The most notable moments are when Astaire and Charisse are on opposite sides of the split, thereby shrinking one of them until they swap sides to rescale themselves, as at the four-minute mark. When I viewed this, I immediately began to apply Grant&rsquo;s concept to  <em>Singin&rsquo; in the Rain</em>  (see <a href="#figure25">Figure 25</a>).</p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://player.vimeo.com/video/307534874" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="The Good Morning number deformed as a scaled triptych highlights some of the ways that the choreography interacts with the setting as well as resonating with various character and performer power dynamics." webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div>
<p>I chose  “Good Morning”  to deform through Grant&rsquo;s scaled approach because it was the dance number that most consistently matched her design, with multiple dancers distributed horizontally across the frame in an interesting architectural space. Because this trio required a triptych approach, I broke the frame into thirds and scaled them proportionately to mimic Grant&rsquo;s effect. The resulting deformative video is particularly satisfying when the three characters line up evenly into the scaled zones, as at the three-minute mark, or when they cross between zones to grow and shrink, as at the one-minute mark. The choreography places Kathy in the center for most of the number, with the two men orbiting around her with more advanced dance moves (as Debbie Reynolds was a novice dancer when cast in the film). Aptly Gene Kelly towers over his co-stars at the largest scale for most of the number, as he was not only the film&rsquo;s star but also its co-director and choreographer — and he was particularly unkind to Reynolds in rehearsing and shooting this number, with harsh criticism of her dancing and a late-night shoot that caused her feet to bleed <sup id="fnref:26"><a href="#fn:26" class="footnote-ref" role="doc-noteref">26</a></sup>. This scaled triptych calls attention to the relative power of the performers in the frame in a way that maps the behind-the-scenes struggles onto the effortless footage.</p>
<p>Grant&rsquo;s scaled approach makes onscreen spatial relations strange to create something new. Another approach seeks to use deformative methods to reconstruct the space as it was during the film&rsquo;s shoot. A version of this deformative method can be seen in Jeff Desom&rsquo;s impressive  “ <em>Rear Window</em>  Timelapse”   <sup id="fnref:27"><a href="#fn:27" class="footnote-ref" role="doc-noteref">27</a></sup>. Whereas Desom reconstructs that film&rsquo;s apartment building as a compiled static image with embedded footage throughout the film&rsquo;s run, I am more interested in simultaneously visualizing the film footage and camera movement to create an original spatial reconstruction. I applied this method to  “You Were Meant for Me,”  the musical number that takes place in a mostly empty soundstage and with fairly simple choreography (but complex camera movement) through that space (see <a href="#figure26">Figure 26</a>).</p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://player.vimeo.com/video/375744576" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="Deforming a dance number to keep the soundstage static with a moving frame reveals how camera movement and choreography are navigating the set." webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div>
<p>I find the effect quite startling, inverting our standard understanding of camera movement and cinematic space. By moving the frame through the cinematic space as it follows the dancers, it disrupts our perception of camera movement — the viewpoint feels fixed with a variable frame, rather than our standard perception of a fixed frame with a moving camera viewpoint. The frame moving through cinematic space directs our attention to the soundstage more than the dancers, as we come to expect the sparse props to appear in the same spot on the screen (which is also how I mapped the shots to reference points like the ladder and fan). The choreography becomes about moving through the space and guiding the frame, rather than the camera following the dancers.</p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://player.vimeo.com/video/376915279" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="Juxtaposing the original and deformed versions of You Were Meant For Me highlights how the deformation works and what is particularly revealed and concealed by each version." webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div>
<p>Based on a number of people I have shared this video with, this spatial deformation is challenging for most viewers to truly intuit that it is remapping the camera movement as a shifting frame visualizing the soundstage. Thus I created a side-by-side version, comparing the original footage with the moving frame deformation (see <a href="#figure27">Figure 27</a>). Although this juxtaposition loses the disorienting power of the standalone video, it does highlight the analytical dimensions of the deformation, revealing the camera movement in ways that are hard to discern in the original. While it is hard to imagine such a deformation working for most scenes, it is well suited to reveal the spatial possibilities of the empty soundstage in this number.</p>
<h2 id="6-sound">#6: Sound</h2>
<p>Most of the deformations I&rsquo;ve documented thus far prioritize the visual channel over audio, whether via camera movement, editing pacing, or single frames. Largely this emphasis stems from the difficulty in breaking the audio channel of a film into discrete units that might be algorithmically manipulated; despite the fact that the production and post-production of a film actually uses separate tracks for audio much more than video, the final product provides only a sophisticated mix of audio elements that are challenging to separate. The mixed audio track is easy to manipulate, as editing platforms offer dozens of effects we might apply to the soundtrack, from equalization to reverb to distortion. However in my experiments, such manipulations do little than create a sonic mess without revealing any insights into the clips or creating a compelling aesthetic object on its own. But I certainly believe that sound is the realm of deformation that needs more expansion and development, and I offer two more experiments with the caveat that these are far from exemplary of the wide-ranging possibilities of sonic deformation that might be explored in the future.</p>
<p>Similar to the  “but faster”  meme, we can take inspiration from online remix culture to deform films. One sound-driven remix protocol focuses on alphabetizing footage by the words spoken and/or sung to create an algorithmic pattern that is both predictable and arbitrary. An early example of this approach is <a href="https://vimeo.com/150423718"> “Of Oz the Wizard” </a>, an alphabetized version of the entire  <em>The Wizard of Oz</em>  which Matt Bucy first made in 2004, but became an online sensation in 2016 <sup id="fnref:28"><a href="#fn:28" class="footnote-ref" role="doc-noteref">28</a></sup>. While watching the entire film this way is an endurance challenge, there are moments where the effect is truly impressive — for instance, the section for W starting at the 1:31:00 mark charts through many iconic words ( “wicked,”    “witch,”    “wizard” ) as well as common ones ( “we,”    “with” ) to create a mosaic of the film that has some coherence but also reveals new rhythmic patterns and intriguing juxtapositions. Similar full-film effects can be seen in <a href="https://www.youtube.com/watch?v=5GFW-eEWXlc"> “ARST ARSW:  <em>Star Wars</em>  alphabetized” </a>, as well as smaller-scale alphabetized songs, such as <a href="https://www.youtube.com/watch?v=82nqsksBH7M">Toto&rsquo;s  “Africa” </a> or, because it&rsquo;s meme culture, <a href="https://www.youtube.com/watch?v=1wN43RaaYhk">Smash Mouth&rsquo;s  “All Star” </a>.</p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://player.vimeo.com/video/375491885" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="Rearranging the song Moses Supposes in alphabetical order creates an even more tongue-twisting version as inspired by meme culture." webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div>
<p>I chose  “Moses Supposes”  as the number to alphabetize, as the tongue-twisting song about wordplay seemed best suited to reorganize in a way that plays with words (see <a href="#figure28">Figure 28</a>). The effect is certainly playful, as common short words like  “a”  and  “his”  blur into a rapid-fire hard to discern sequence, especially because in the original song such words are practically swallowed in the fast-paced patter rhythm; however, longer repeated words like  “erroneously”  or  “supposes”  standout in pleasurable sequences charting their different articulations in the song. To me, the effect of this alphabetized video is to highlight how the words and their performance create the playful tone of the number, deemphasizing the excellent choreography that more obviously drives the energy of the original.<sup id="fnref:29"><a href="#fn:29" class="footnote-ref" role="doc-noteref">29</a></sup></p>
<p>Another model of audio-driven deformation is the <a href="https://soundcloud.com/listentotv"> “Listen to TV” </a> project by Casey McCormick and Eric Powell, which takes advantage of the layering possibilities of sound. McCormick and Powell take a full season of a television series and simultaneously play the audio of all episodes at once, creating a chaotic experience of simultaneity. While most of these sound collages foreground dialogue and sound effects, musical moments shine through the chaos, as with the phased instances of Iris DeMent&rsquo;s theme song  “Let the Mystery Be”  toward the start of their deformation of  <em>The Leftovers</em>  season two. For a comparatively sparse sonic series like  <em>Leftovers</em> , many lines of dialogue or sound effects stand out in this layered deformation, triggering memories for fans who know the season well.  <em>Seinfeld</em>  season nine offers a very different experience, as the sonic layers are far messier, with the iconic bass riffs and audience laughter sufficiently obscuring nearly all recognizable dialogue, until the twenty-three-minute mark when most episodes end, but the extra-long series finale endures alone to be heard clearly.</p>
<p>A film lacks the multiple episodes of a television season to open up such layering possibilities, but I considered the individual musical numbers as a comparable unit to an episode to create a layered version from  <em>Singin&rsquo;</em>  (see <a href="#figure29">Figure 29</a>). With fourteen discrete numbers (including multiple versions of the title song) playing at once, we experience sonic chaos punctuated by some sounds that stand out, such as the high-pitched singing in  “All I Do is Dream of You,”  the loud repeated calls of  “Moses”  at the one-minute mark, or periodic refrains of  “Gotta Dance.”  The instrumentation can also become notable, when brass or drums burst through the chaos to offer recognizable fanfares and beats. The sonic collage becomes more discernible as each number ends — by the three-minute mark, only seven songs remain, allowing us to hear more details from the performances, and creating more specific interplay between numbers. By the four-minute mark, all songs have ended except  “Broadway Melody,”  which I stopped at the next cadence to avoid another eight minutes of that number playing solo.</p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://player.vimeo.com/video/293882022" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="Layering the film’s musical numbers creates an intense sonic collage that becomes clearer as each song ends." webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div>
<p>While  “Listen to TV”  is an audio-only project, I wanted to incorporate visuals into my layered musical number deformation, allowing the sound design to reveal potential visual insights. Given that layering video is much more difficult than audio, and my attempts to create layers of opacity resulted in simply an undistinguished blur, I decided to create video  “ribbons”  of each number&rsquo;s visuals, arranged chronologically across the frame. This approach reveals some of the dancing and physical performance presented within each number, yet makes the individual sequences hard to discern until adjacent numbers end to reveal more of each sequence. There are some fun juxtapositions, as at 1:55 when Kathy and Don appear to be dancing together in adjoining clips from  “Good Morning”  and  “Singin&rsquo; in the Rain,”  but in general the visual effect is less interesting than the sonic collage.</p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://player.vimeo.com/video/376421600" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="The layers of musical numbers can be shifted to be additive over time to conclude all at once, revealing different juxtapositions." webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div>
<p>The  “Listen to TV”  collages all start simultaneously, with the episodes ending in a more staggered timing, an approach I mimicked for my layered numbers deformation; however, I also made an  “additive”  version where the fourteen numbers all end simultaneously instead, and thus appear in order of descending length (see <a href="#figure30">Figure 30</a>). The effect of this version is to start with more comprehensible moments, emphasizing the interplay between  “Broadway Melody”  and  “Beautiful Girl,”  until new layers emerge over the course of the video. At the one-minute mark, we start to hear more from  “Make &lsquo;Em Laugh”  and  “Good Morning”  before the sonic chaos becomes overwhelming. The closing seconds are rewarding, as we hear how many numbers end with boisterous cadences out of the polyrhythmic stew of the final minute. Neither of these two versions is  “correct,”  but highlights how changing one variable or parameter in a deformation can create another iteration that is potentially as impactful as the original.</p>
<h2 id="conclusion">Conclusion:</h2>
<p>This litany of experiments in deforming  <em>Singin&rsquo; in the Rain</em>  are not offered as a comprehensive list of what might be done with the film or any other source text. I offer them each as small (or sometimes long) curiosities that might inspire future experiments or provoke insights into a familiar film. In teaching and presenting videographic criticism, I often suggest that one key impact of any successful videographic piece is to help people see and hear a source film through the eyes and ears of the critic, conveying a new perspective on a text. Deformative videographic work also allows us to see and hear a familiar film in a new way, but one that reveals new perspectives to both the critic and the audience. I did not perform any of these experiments with a clear idea of how they might turn out, and I was usually surprised at what was revealed (or what did not work at all). As critics, we too often approach a case study with our conclusions already drawn, eager to apply existing theoretical paradigms or critical assumptions to a new instance that proves our predetermined point. Deformative criticism is a reminder of the joys of discovery, finding something distinct and refreshing even within the most familiar film, breaking apart our critical preconceptions to point us toward new, strange pathways. That seems like an apt mission for digital approaches to audiovisual criticism.</p>
<h2 id="acknowledgements">Acknowledgements:</h2>
<p>The author would like to thank Kevin Ferguson, Catherine Grant, Christian Keathley, Casey McCormick, Ethan Murphy, Alan O&rsquo;Leary, and Mike Zryd for their feedback and support of this project.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Such deformative videographic works seem to be a clear example of fair use transformations under U.S. copyright law, a topic I&rsquo;ve discussed more in depth in  “But Is Any of This Legal?”   <sup id="fnref:30"><a href="#fn:30" class="footnote-ref" role="doc-noteref">30</a></sup>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Mittell, J.  “Videographic Criticism as a Digital Humanities Method.”  In M. Gold and L. Klein (eds),  <em>Debates in the Digital Humanities 2019</em> , University of Minnesota Press, Minneapolis (2019): 224–42. <a href="https://dhdebates.gc.cuny.edu/read/untitled-f2acf72c-a469-49d8-be35-67f9ac1e3a60/section/b6dea70a-9940-497e-b7c5-930126fbd180#ch20">https://dhdebates.gc.cuny.edu/read/untitled-f2acf72c-a469-49d8-be35-67f9ac1e3a60/section/b6dea70a-9940-497e-b7c5-930126fbd180#ch20</a>.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref3:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Sample, Mark.  “Notes towards a Deformed Humanities,”  Sample Reality (blog) (2012). <a href="http://www.samplereality.com/2012/05/02/notes-towards-a-deformed-humanities/">http://www.samplereality.com/2012/05/02/notes-towards-a-deformed-humanities/</a>.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Samuels, L. and McGann, J.  “Deformance and Interpretation.”    <em>New Literary History</em> , 30:1 (1999): 25–56.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>Ramsay, S.  <em>Reading Machines: Toward an Algorithmic Criticism</em> . University of Illinois Press, Champaign (2011).&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>Ferguson, K.  “Volumetric Cinema.”  In M. Gold and L. Klein (eds),  <em>Debates in the Digital Humanities 2019</em> , University of Minnesota Press, Minneapolis (2019). <a href="https://dhdebates.gc.cuny.edu/read/untitled-f2acf72c-a469-49d8-be35-67f9ac1e3a60/section/a214af4f-2d31-4967-8686-738987c02ddf#ch28">https://dhdebates.gc.cuny.edu/read/untitled-f2acf72c-a469-49d8-be35-67f9ac1e3a60/section/a214af4f-2d31-4967-8686-738987c02ddf#ch28</a>.&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>O&rsquo;Leary, A.  “No Voiding Time: A Deformative Videoessay.”  16:9 filmtidsskrift (2019). <a href="http://www.16-9.dk/2019/09/no-voiding-time/">http://www.16-9.dk/2019/09/no-voiding-time/</a>.&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>Bering-Porter, D.  “The Automaton in All of Us: GIFs, Cinemagraphs and the Films of Martin Arnold,”    <em>The Moving Image Review &amp; Art Journal</em> , 3: 2 (2014): 178–92. <a href="https://doi.org/info:doi/10.1386/miraj.3.2.178_1">https://doi.org/info:doi/10.1386/miraj.3.2.178_1</a>.&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>Zryd, M.  “Alone: Life Wastes Andy Hardy.”  Senses of Cinema (2004). <a href="http://sensesofcinema.com/2004/cteq/alone_life_wastes_andy_hardy/">http://sensesofcinema.com/2004/cteq/alone_life_wastes_andy_hardy/</a>.&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>Enns, C.  “Navigating Algorithmic Editing: Algorithmic Editing as an Alternative Approach to Database Cinema,”    <em>Millennium Film Journal</em> , 56 (2012): 66–72.&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>Ray, R.  <em>The Avant-Garde Finds Andy Hardy</em> . Harvard University Press, Cambridge (1995).&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p>Ferguson, K.  “The Slices of Cinema: Digital Surrealism as Research Strategy.” In C. Acland and E. Hoyt (eds),  <em>The Arclight Guidebook to Media History and the Digital Humanities</em> , Reframe Books, Sussex, UK (2016). <a href="http://projectarclight.org/book/">http://projectarclight.org/book/</a>.&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13">
<p>Whalen, Z.  “Imj: A Web-Based Tool for Visual Culture Macroanalytics.”  Zach Whalen (blog) (2016). <a href="https://www.zachwhalen.net/posts/imj-a-web-based-tool-for-visual-culture-macroanalytics/">https://www.zachwhalen.net/posts/imj-a-web-based-tool-for-visual-culture-macroanalytics/</a>.&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14">
<p>See Ferguson’s video summation of  <em>The Searchers</em>  for an example of this effect: <a href="https://vimeo.com/170473567">https://vimeo.com/170473567</a>.&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:15">
<p>StarStaX is available at <a href="https://markus-enzweiler.de/software/starstax/">https://markus-enzweiler.de/software/starstax/</a> - special thanks to Ethan Murphy for suggesting this tool.&#160;<a href="#fnref:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:16">
<p>Newman, M.  “GIFs: The Attainable Text.”    <em>Film Criticism</em> , 40: 1 (2016): R1–7. <a href="http://dx.doi.org/10.3998/fc.13761232.0040.123">http://dx.doi.org/10.3998/fc.13761232.0040.123</a>.&#160;<a href="#fnref:16" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:17">
<p>Manovich, L.  <em>The Language of New Media</em> . The MIT Press, Cambridge (2001).&#160;<a href="#fnref:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:18">
<p>Grant, C.  “Screen Memories : A Video Essay on  <em>Smultronstället</em>  /  <em>Wild Strawberries</em> ,”  Cinergie – Il Cinema e Le Altre Arti 7: 13 (2018): 21–29. <a href="https://doi.org/10.6092/issn.2280-9481/7914">https://doi.org/10.6092/issn.2280-9481/7914</a>.&#160;<a href="#fnref:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:19">
<p>Malkowski, J.  “Spatial Montage, in Miniature: Movie GIF Sets on Tumblr,”  Society for Cinema and Media Studies conference presentation, Chicago (2017).&#160;<a href="#fnref:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:20">
<p>Rombes, N.  <em>10/40/70: Constraint as Liberation in the Era of Digital Film Theory</em> . Zero Books (2014).&#160;<a href="#fnref:20" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:21">
<p>I also uploaded this video to my YouTube account, curious if it would gain an audience there as well, but it was immediately blocked by copyright bots, highlighting the different approaches the two sites take to copyright enforcement.&#160;<a href="#fnref:21" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:22">
<p>I asked a few other film scholars what they thought would be the longest takes in the film as well, and they all likewise presumed they would be musical numbers, a useful confirmation of my assumptions.&#160;<a href="#fnref:22" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:23">
<p>See <a href="http://www.cinemetrics.lv">http://www.cinemetrics.lv</a> for data on average shot lengths for thousands of films.&#160;<a href="#fnref:23" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:24">
<p>Dahir, I.  “People Are Making Music Videos That Get Faster Every Time A Word Is Said And It&rsquo;s The Best Thing Ever,”    <em>BuzzFeed</em> , (December 4, 2016). <a href="https://www.buzzfeed.com/ikrd/people-are-making-music-videos-that-get-faster-every-time-a">https://www.buzzfeed.com/ikrd/people-are-making-music-videos-that-get-faster-every-time-a</a>.&#160;<a href="#fnref:24" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:25">
<p>Grant, C.  “FATED TO BE MATED: An Architectural Promenade”  (2018). <a href="https://vimeo.com/300303270">https://vimeo.com/300303270</a>.&#160;<a href="#fnref:25" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:26">
<p>Saunders, T.  “Bleeding Feet and Gene Kelly&rsquo;s Tongue: How  <em>Singin&rsquo; in the Rain</em>  Nearly Broke Debbie Reynolds.”    <em>The Telegraph</em>  (December 29, 2016). <a href="https://www.telegraph.co.uk/films/2016/12/29/bleeding-feet-gene-kellys-tongue-singin-rain-nearly-brokedebbie/">https://www.telegraph.co.uk/films/2016/12/29/bleeding-feet-gene-kellys-tongue-singin-rain-nearly-brokedebbie/</a>.&#160;<a href="#fnref:26" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:27">
<p>Desom, J.  “ <em>Rear Window</em>  Timelapse” (2012). <a href="https://vimeo.com/37120554">https://vimeo.com/37120554</a>.&#160;<a href="#fnref:27" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:28">
<p>VanDerWerff, E.  “This Man Remixed  <em>The Wizard of Oz</em>  Word by Word — and the Result Is a Surprising Delight.”    <em>Vox</em>  (January 7, 2016). <a href="https://www.vox.com/2016/1/7/10728184/wizard-of-oz-alphabetical-order">https://www.vox.com/2016/1/7/10728184/wizard-of-oz-alphabetical-order</a>.&#160;<a href="#fnref:28" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:29">
<p>Since the final two minutes of the number are solely dance without any singing until the concluding  “A” , I excised the dance sequence from this deformation, leading to a much shorter version as alphabetized.&#160;<a href="#fnref:29" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:30">
<p>Keathley, C., Mittell, J., and Grant, C.  “The Videographic Essay: Practice and Pedagogy”  (2019). <a href="http://videographicessay.org/works/videographic-essay/index">videographicessay.org</a>.&#160;<a href="#fnref:30" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry><entry><title type="html">Exploring Film Language with a Digital Analysis Tool: the Case of Kinolab</title><link href="https://rlskoeser.github.io/dhqwords/vol/15/1/000515/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://rlskoeser.github.io/dhqwords/vol/15/1/000515/</id><author><name>Allison Cooper</name></author><author><name>Fernando Nascimento</name></author><author><name>David Francis</name></author><published>2021-03-05T00:00:00+00:00</published><updated>2021-03-05T00:00:00+00:00</updated><content type="html"><![CDATA[<h2 id="1-introduction">1. Introduction</h2>
<p>Today, decades after the earliest experiments with DH methodologies, scholars hoping to apply DH approaches to the study of audiovisual media continue to find themselves at somewhat of a disadvantage relative to colleagues working with text-based media. Impediments to computationally assisted analysis of moving images have been well documented and are both technological and legal in nature. In recent years, projects like Dartmouth&rsquo;s Media Ecology Project and the University of Richmond&rsquo;s Distant Viewing Lab, among others, have lowered technological barriers by making inroads into moving image annotation and the application of computer vision to moving image analysis. In 2018, the Library of Congress lowered legal barriers in the United States with the most recent round of exemptions to the Digital Millennium Copyright Act (DMCA), granting increased freedom to excerpt short portions of films, television shows, and videos for the purposes of criticism or comment and thereby removing a hurdle to DH-inflected forms of moving image analysis such as videographic criticism. Despite the advances described above, film and media studies scholars are still unable to analyze the moving images digitally that are the subject of their research with anywhere near the ease of DH practitioners working with text or other forms of data.</p>
<p>One illustration of this predicament is the ongoing lack of a database dedicated to something as seemingly straightforward as the analysis of film language. As Lucy Fischer and Patrice Petro lamented in their introduction to the 2012 MLA anthology  <em>Teaching Film</em> , &ldquo;the scholar of literature can do a keyword search for all the occasions that William Shakespeare or Johann Goethe has used a particular word, [but] no such database exists for the long shot in Orson Welles or the tracking shot in Max Ophüls&rdquo; <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. In response to the improvements to moving image access described above, the authors of this case study set out to develop Kinolab, an academically crowdsourced platform for the digital analysis of film language in narrative film and media (see <a href="https://kinolab.org/">https://kinolab.org/</a>). This case study describes the opportunities and challenges that participants in the project have encountered in our efforts to create, manage, and share a digital repository of annotated film and series clips broadly and deeply representative of film language as the latter has evolved over time and across countries and genres. In this essay, we contextualize our project within related projects, recent efforts to incorporate machine learning into DH methodologies for text and moving image analysis, and ongoing efforts by AVinDH practitioners to assert the right to make fair use of copyrighted materials in their work.</p>
<p>Why should cinema scholars pursue DH approaches when, seemingly, they are so fraught with challenges? One answer to the question can be found in the methodology of a groundbreaking analysis in our field that took place before the first wave of DH scholarship in the 1990s and early 2000s and led to the definition of the group style known as classical Hollywood cinema <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. Associated with narrative films made under the Hollywood studio system between roughly 1916 and 1960 and marked by certain recurrent features of narrative, narration, and visual style, classical Hollywood cinema has come to define our understanding of Golden Age cinema and to serve as a benchmark for scholarly inquiries into film form and style. Remarkably, however, the 100 films that made up the sample for the study comprised just a small percentage (roughly .006%) of the approximately 15,000 films produced by American studios between 1915 and 1960 (10). It is eye-opening to consider that such an axiomatic account of American film style and history excludes over 99% of the films produced in the period under investigation, even if, as Bordwell asserts, Hollywood classical cinema is &ldquo;excessively obvious&rdquo;, having documented its style in its own technical manuals, memoirs, and publicity handouts (3). Today&rsquo;s film scholars may very well wonder how our understanding of this monolithic group style might evolve if we were to radically increase the sample size using DH approaches that didn&rsquo;t yet exist in the mid 1980s.</p>
<p>A related answer to the question of why cinema scholars might seek to incorporate DH methodologies into their work can be found on the IMDb Statistics page (see <a href="https://www.imdb.com/pressroom/stats/">https://www.imdb.com/pressroom/stats/</a>), which at the time of this writing included over half a million film titles in its database. Lev Manovich (2012) has argued that, before the global expansion of digital media represented by these kinds of numbers, &ldquo;cultural theorists and historians could generate theories and histories based on small data sets (for instance, &lsquo;Italian Renaissance,&rsquo; &lsquo;classical Hollywood cinema,&rsquo; &lsquo;post-modernism&rsquo;, etc.)&rdquo; but now we face a &ldquo;fundamentally new situation and a challenge to our normal ways of tracking and studying culture&rdquo; (250). For the Kinolab project, this new situation presents an opportunity to broaden our understanding of how film language works by creating a platform capable of sorting and clustering hundreds of aspects of film language along multiple dimensions such as region, genre, language, or period, among others.</p>
<p>We anticipate that our DH approach to the analysis of film language will allow researchers to move between different scales of analysis, enabling us, for example, to understand how a particular aspect of film language functions in the work of a single director, in a single genre, or across films from a particular time period or geographical region. We also anticipate that decontextualizing and remixing examples of film language in these ways will enable us to see what we might not have seen previously, following Manovich&rsquo;s assertion that &ldquo;Being able to examine a set of images along a singular visual dimension is a powerful form of defamiliarization&rdquo; (276). We argue that the collaborative development of a data model for film language, essential for the creation of a common understanding among cinema and media studies researchers as well as for their collaboration across the Semantic Web, will clarify and extend our knowledge of film language in the process of making its constitutive components and their relationships comprehensible to computers. And, finally, we expect that these efforts, made possible through the adoption of DH methodologies, will enable us to make more confident statements about the field of cinema studies at large.</p>
<h2 id="2-analyzing-film-language-in-the-digital-era-related-projects">2. Analyzing Film Language in the Digital Era: Related Projects</h2>
<p>Our research has found few scholarly, open access projects dedicated to the digital analysis of film language – a situation likely due at least in part to the technological and legal barriers indicated above. Among the projects that do exist is the Columbia Film Language Glossary (FLG) (see <a href="https://filmglossary.ccnmtl.columbia.edu/">https://filmglossary.ccnmtl.columbia.edu/</a>), a teaching tool designed to offer users illustrated explanations of key film terms and concepts <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>. It offers a relatively limited number of clips, with each clip selected to illustrate a single term or concept. This model, while well-suited to the project&rsquo;s pedagogical purposes, precludes users from making significant comparisons between different instantiations of film language. Search options are limited to film language terms and keyword searches, so the FLG does not offer the ability to do advanced searches with modifiers. Finally, it offers no means to research film language diachronically or synchronically. Conversely, Critical Commons (see <a href="http://www.criticalcommons.org/">http://www.criticalcommons.org/</a>) offers an abundant source of user-generated narrative media clips, many of which include tags and commentary to highlight their use of film language. A pioneering project to support the fair use of copyrighted media by educators, Critical Commons accepts moving image media uploads and makes them publicly available on the condition that they are accompanied by critical commentary. This effectively transforms the original clips by adding value to them and protects the users who upload them under the principles of fair use <sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>. Critical Commons was not designed intentionally for the analysis of film language; accordingly, the site lacks a controlled vocabulary or standardized metadata related to film language to facilitate search and retrieval, although users can execute keyword searches. Lastly, Pandora (see <a href="https://pan.do/ra#about">https://pan.do/ra#about</a>) is a non-academic platform for browsing, annotating, searching, and watching videos that allows users to manage decentralized collections of videos and to create metadata and annotations collaboratively.</p>
<p>The efforts described above to make narrative moving image media available digitally for educational and scholarly purposes are complemented by projects developing promising tools for the digital analysis of moving images. Estrada et al. <sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup> identify nearly 30 suitable tools for digital video access and annotation, evaluating in particular the professional video annotation tool ELAN and the qualitative data analysis software NVivo. While Kinolab relies upon a custom-built platform, ELAN and VIAN are two preexisting solutions that can be adapted to a variety of digital film analysis projects. ELAN (see <a href="https://archive.mpi.nl/tla/elan">https://archive.mpi.nl/tla/elan</a>) is an annotation tool for audio and video recordings initially developed for linguists and communications scholars that has been adopted successfully by film studies researchers, whereas VIAN is a visual film annotation system targeting color analysis with features to support spatiotemporal selection and classification of film material by large vocabularies <sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>. The brief overview that follows here concentrates more narrowly on current software and projects we have identified as best suited to work in a complementary way with Kinolab to support its focus on the digital analysis of film language. The <a href="http://mediaecology.dartmouth.edu/wp/">Media Ecology Project</a> (MEP), for example, develops tools to facilitate machine-assisted approaches to moving image analysis. These include, among others, a Semantic Annotation Tool enabling moving image researchers to make time-based annotations and a Machine Vision Search system capable of isolating formal and aesthetic features of moving images <sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>. Similarly, the <a href="https://www.distantviewing.org/">Distant Viewing Lab</a> develops tools, methods, and datasets to aid in the large-scale analysis of visual culture <sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>. The Video Analysis Tableau (VAT) facilitates the automated comparison, annotation, and visualization of digital video through the creation of a &lsquo;workbench&rsquo; – a space for the analysis of digital film – that makes available essential tools for the job but leaves the definition of the job itself up to individual media researchers and their collaborators <sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup>.</p>
<p>Even as machine learning projects like the MEP and Distant Viewing Lab bring scholars of moving images closer to the kind of distant reading now being performed on digitized literary texts, their creators acknowledge an ongoing need for human interpreters to bridge the semantic gap created when machines attempt to interpret images meaningfully. Researchers can extract and analyze semantic information such as lighting or shot breaks from visual materials only after they have established and encoded an interpretive framework <sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup>: this work enables computers to close the gap between the pixels on screen and what they have been told they represent. The digital analysis of film language generates an especially wide semantic gap insofar as it often requires the identification of semiotic images of a higher order than a shot break, for example the non-diegetic insert (an insert that depicts an action, object, or a title originating outside of the space and time of the narrative world). For this reason, analysis in Kinolab for now takes place primarily through film language annotations assigned to clips by project curators rather than through processes driven by machine learning, such as object recognition.</p>
<h2 id="3-from-textual-analysis-to-moving-images-analysis-in-dh">3. From Textual Analysis to Moving Images Analysis in DH</h2>
<p>A frequent topic in digital humanities concerns the balance between data annotation and machine learning. Manovich <sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup> rejects annotation for the purposes of Cultural Analytics (the use of visualization to explore large sets of images and video), arguing that the process of assigning keywords to every image thwarts the spontaneous discovery of interesting patterns in an image set, that it is not scalable for massive data sets, and that it cannot help with such data sets because natural languages lack sufficient words to adequately describe the visual characteristics of all human-created images <sup id="fnref1:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>. Notwithstanding researchers&rsquo; increasing success in using computers for visual concept detection, the higher-order semiotic relationships that frequently constitute film language remain resistant to machine learning. When, then, should one annotate, and for what types of information? Projects and initiatives dedicated to text analysis, which is a more historically developed DH methodology, form an instructive continuum of the many ways in which manual annotation and machine learning techniques can be combined to retrieve information and perform digital corpora analysis. In many cases, digital projects rely solely on manually encoded digital texts to provide their representational and analytical tools. Other models seek to add annotations on higher-level semantic entities such as spatial information <sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>, clinical notes <sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup>, and emotions <sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup>. A brief survey of the relationship between annotation and machine learning in text analysis provides insight into how this relationship may apply to time-based media and specifically to moving image analysis.</p>
<p>In the field of ​​Natural Language Processing (NLP), annotations of parts of speech have greatly assisted in the advancement of text mining, analysis, and translation techniques. Pustejovsky and Stubbs have suggested the importance of annotation to enhance the quality of machine learning results: &ldquo;machine learning (ML) techniques often work better when algorithms are provided with pointers to what is relevant about a dataset, rather than just massive amounts of data&rdquo; <sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup>. In another development of the annotation and machine learning relationship, some unsupervised machine learning models seek through statistical regularities to highlight latent features of text without the extensive use of annotations, such as the Dirichlet distribution-based models, including the model proposed by Blei et al <sup id="fnref:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup> for Latent Dirichlet Allocation. Topic modeling has gained considerable attention over the last decade from the digital textual corpora analysis scholarship community. These models take advantage of the underlying structures of natural language coding forms. Despite its intrinsic semantic ambiguity, the code of natural languages textual structure follows syntactic patterns that can be recognized through algorithms that, for example, try to reproduce how texts are generated, following a generative hypothesis. </p>
<p>Even more recent advances in machine learning, especially in the area of ​​neural networks and deep learning <sup id="fnref:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup>, have opened new perspectives for data analysis with simpler annotation mechanisms. Deep neural networks have shown great success in various applications such as object recognition (see, for example, <sup id="fnref:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup>) and speech recognition (see, for example <sup id="fnref:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup>). Moreover, recent works have shown that neural networks could be successfully used for several tasks in NLP <sup id="fnref:20"><a href="#fn:20" class="footnote-ref" role="doc-noteref">20</a></sup>. One of the most used models in recent years has been word2vec, which represents semantic relations in a multidimensional vector space generated through deep learning <sup id="fnref:21"><a href="#fn:21" class="footnote-ref" role="doc-noteref">21</a></sup>. This method allows the exploration of more sophisticated semantic levels without or with little use of annotations external to the text structure itself. More recently, models that use the attention mechanism associated with neural networks known as transformers <sup id="fnref:22"><a href="#fn:22" class="footnote-ref" role="doc-noteref">22</a></sup>have empowered a new wave of advances in results on several areas of natural language processing such as text prediction and translation <sup id="fnref:23"><a href="#fn:23" class="footnote-ref" role="doc-noteref">23</a></sup>.</p>
<p>These advances of digital text analysis seem to point to a trend toward a diminishing need for annotation to achieve results similar to or superior to those that were possible in the past with annotated data set training alone. However, despite the many advances we have described so far, there are still higher levels of semantic information (such as complex narrative structures or highly specialized interpretative fields) that require manual annotation to be appropriately analyzed.</p>
<p>From this brief exploration of the relationship between annotation and machine learning algorithms in the context of text analysis, we highlight three related observations. First, there has been a continuing and evolving interplay of annotation and machine learning. Second, recent machine learning algorithms have been reducing the need of extensive annotation of textual corpora for some interpretative and linguistic analyses. And thirdly, manual annotation still has a role for higher-level semantic analyses, and still plays an essential role in the training of machine learning models. With these three observations related to developments in text analysis, we are better positioned to understand a similar relationship in the context of time-based media. For this purpose, we take as reference the Distant Viewing framework proposed by Arnold and Tilton, which they define as &ldquo;the automated process of making explicit the culturally coded elements of images&rdquo; (5). The point, well noted by the authors, is that the code elements of images are not as clearly identifiable as the code elements of texts, which are organized into lexical units and relatively well-delimited syntactic structures in each natural language. Indeed, as Metz <sup id="fnref:24"><a href="#fn:24" class="footnote-ref" role="doc-noteref">24</a></sup> argues, film is perhaps more usefully understood as a system of codes that replace the grammar of language.</p>
<p>Thus, digital image analysis imposes the need for an additional level of coding – in Kinolab&rsquo;s case, curatorial annotations – so that the semiotic elements comprising film language are properly identified. As discussed earlier, Arnold and Tilton highlighted the semantic gap that exists between &ldquo;elements contained in the raw image and the extracted structured information used to digitally represent the image within a database&rdquo; <sup id="fnref1:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup>.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000515/resources/images/figure01.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000515/resources/images/figure01_huf1c1a4490745e55d539820d5abb4e052_1373224_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000515/resources/images/figure01_huf1c1a4490745e55d539820d5abb4e052_1373224_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000515/resources/images/figure01_huf1c1a4490745e55d539820d5abb4e052_1373224_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000515/resources/images/figure01_huf1c1a4490745e55d539820d5abb4e052_1373224_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/1/000515/resources/images/figure01_huf1c1a4490745e55d539820d5abb4e052_1373224_1800x0_resize_box_3.png 1800w,/dhqwords/vol/15/1/000515/resources/images/figure01.png 2241w" 
     class="landscape"
     ><figcaption>
        <p>&ldquo;Hannibal and Clarice Meet&rdquo; in <em>The Silence of the Lambs</em> . Directed by Jonathan Demme. Strong Heart/Demme Production, 1991. Kinolab, <a href="https://kinolab.org/FilmClip.php?id=726">https://kinolab.org/FilmClip.php?id=726</a>
        </p>
    </figcaption>
</figure>
<p>Mechanisms to bridge this semantic gap may either be built automatically through computational tools or by people who create a system of annotations to identify these semiotic units. Moreover, these semiotic units can be grouped hierarchically into higher levels of meanings, creating a structure that ranges from basic levels of object recognition, such as a cake, to more abstract levels of meaning, such as a birthday party. Such analysis becomes more complex when we consider time-based media since its temporal aspect adds a new dimension to potential combinations, which adds new possible interpretations of meanings to images considered separately. An example taken from Jonathan Demme&rsquo;s  <em>Silence of the Lambs</em>  (1991) illustrates this challenge. In <a href="#figure01">Figure 1</a>, Anthony Hopkins as the murderous psychopath Hannibal Lecter appears to gaze directly at the viewer, ostensibly &lsquo;breaking the fourth wall&rsquo; that traditionally separates actors from the audience. Both curator and a properly trained computer would likely identify this single shot – a basic semiotic unit – as an example of direct address or metalepsis, &ldquo;communication that is explicitly indicated as being targeted at a viewer as an individual&rdquo; <sup id="fnref:25"><a href="#fn:25" class="footnote-ref" role="doc-noteref">25</a></sup>, often marked by a character looking directly into the camera. But, as <a href="#figure01">Figure 2</a> demonstrates, this single shot or basic semiotic unit is actually part of a more complex semiotic relationship that reveals itself to be  <em>also</em>  or  <em>instead</em>  an embedded first-person point-of-view shot when considered in the context of immediately preceding and subsequent shots. The shot itself is identical in both of these cases, but the film language concept that it illustrates can only be determined in light of its syntagmatic (sequential) relation to the shots that precede and follow it <sup id="fnref1:24"><a href="#fn:24" class="footnote-ref" role="doc-noteref">24</a></sup> or other properties, such as an audio track in which direct address is or isn&rsquo;t communicated explicitly. This semantic ambiguity is a key component of the scene&rsquo;s success insofar as it aligns the viewer with the perspective of Lecter&rsquo;s interlocutor, the young FBI trainee Clarice Starling – an alignment that is felt all the more profoundly through the chilling suggestion that the spectator has lost the protection of the fourth wall, represented here through the metaphorical prop of the plexiglass partition separating the two characters.</p>
<p>The Distant Viewing framework proposes an automatic process to analyze and extract primary semantic elements &ldquo;followed by the aggregation and visualization of these elements via techniques from exploratory data analysis&rdquo; <sup id="fnref2:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup>. Based upon the evolution of digital text analysis following the new advances brought about by machine learning techniques described above, we predict that such evolving techniques will also allow the recognition and automatic annotation of more complex semiotic units, further narrowing the semantic gap for meaningful image interpretations.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000515/resources/images/figure02.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000515/resources/images/figure02_hu248b99abf9db059e5a58225609a7dd4e_177264_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000515/resources/images/figure02_hu248b99abf9db059e5a58225609a7dd4e_177264_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000515/resources/images/figure02_hu248b99abf9db059e5a58225609a7dd4e_177264_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000515/resources/images/figure02.png 1305w" 
     class="landscape"
     ><figcaption>
        <p>Timeline showing embedded first-person point-of-view shot in &ldquo;Hannibal and Clarice Meet&rdquo; clip.
        </p>
    </figcaption>
</figure>
<p>Kinolab creates a framework to explore the intermediate levels in this semiotic hierarchy by defining annotations that form a set of higher-level semiotic units of film language relative to basic units such as the cut or other types of edits and allows the description of common categories for understanding time-based media characteristics. Such semiotic units form the basis of a film language that describes the formal aspects of this type of digital object.</p>
<p>Kinolab is structured to help researchers reduce the semantic gap in digital film language analysis in three distinct ways. The most basic form is through a collaborative platform for consistent identification of semiotic units of film language in film clips, allowing sophisticated searches to be done immediately utilizing them. The Kinolab software architecture is also designed for integrating distant viewing plugins so that some film language forms can be automatically recognized by machine learning algorithms from the scientific community. This plugin would also allow subsequent exploratory data analysis based on Kinolab&rsquo;s archive. Finally, Kinolab can serve as a resource for applying, validating, and enhancing new distant viewing techniques that can use the database with information about film language to develop training datasets to validate and improve their results. Given Kinolab&rsquo;s architecture, it can produce a standard machine-readable output that supplies a given clip URL with a set of associated tags that a machine learning algorithm could integrate as training data to learn examples of higher-level semantic annotations, such as a close-up shot. What is lacking in Kinolab towards this goal is specific timestamp data about when a certain film language form is actually occurring (start/stop) which, combined with automatically extracted basic sign recognition (e.g. objects, faces, lighting), would be extremely valuable for any machine learning processes. The existing architecture could be expanded to allow this with the addition of a clip-tag relationship to include this duration information, however the larger work would be identifying and inputting this information into the system. One possible way to address this limitation is to integrate a tool like the aforementioned Media Ecology Project&rsquo;s Semantic Annotation Tool (SAT) into Kinolab. The SAT can facilitate the effort to create more finely grained annotations to bridge the gap between full clips and respective tags, providing a more refined training dataset. </p>
<p>With these extensions and within this collaborative ecosystem of complementary tools we believe that Kinolab could serve as an ideal platform for exploring the full spectrum of combinations between manual annotations and machine learning techniques that will foster new interpretative possibilities of time-based media in a manner analogous to advances in the area of ​​digital text analysis.</p>
<h2 id="4-kinolab-a-dedicated-film-language-platform">4. Kinolab: A Dedicated Film Language Platform</h2>
<p>Kinolab is a digital platform for the analysis of narrative film language yet, as previous discussion has suggested, &lsquo;film language&rsquo; is a fluid concept that requires defining in relation to the project&rsquo;s objectives. The conceptualization of film as a language with its own set of governing rules or codes has a rich history that dates back to the origins of the medium itself. This includes contributions from key figures like D.W. Griffith, Sergei Eisenstein <sup id="fnref:26"><a href="#fn:26" class="footnote-ref" role="doc-noteref">26</a></sup>, André Bazin <sup id="fnref:27"><a href="#fn:27" class="footnote-ref" role="doc-noteref">27</a></sup>, and Christian Metz <sup id="fnref2:24"><a href="#fn:24" class="footnote-ref" role="doc-noteref">24</a></sup>, among many others. Broadly speaking, film language serves as the foundation of film form, style, and genre. Kinolab focuses on narrative film, commonly understood as &ldquo;any film that tells a story, especially those which emphasize the story line and are dramatic&rdquo; <sup id="fnref1:25"><a href="#fn:25" class="footnote-ref" role="doc-noteref">25</a></sup>. To tell a story cinematically, film language necessarily differs in key ways from languages employed for storytelling in other mediums. As the example drawn from  <em>The Silence of the Lambs</em>  demonstrates, this is particularly evident in its treatment of modalities of time (for example, plot duration, story duration, and viewing time), and space (for example setting up filmic spaces through framing, editing, and point of view) <sup id="fnref:28"><a href="#fn:28" class="footnote-ref" role="doc-noteref">28</a></sup>. Film language can also be understood as the basis for, or product of, techniques of the film medium such as mise-en-scene, cinematography, editing, and sound that, when used meaningfully, create distinctive examples of film style such as classical Hollywood cinema or Italian neorealism. Finally, film language is a constitutive aspect of genre when the latter is being defined according to textual features arising out of film form or style: that is, an element of film language such as the jump cut, an abrupt or discontinuous edit between two shots that disrupts the verisimilitude produced by traditional continuity editing, can be understood as a characteristic expression in horror films, which make effective use of its jarring effects. Kinolab adopts a broad view of film language that includes technical practices as well as aspects of film history and theory as long as these are represented in, and can therefore be linked to, narrative media clips in the collection.</p>
<p>Our primary objective in developing Kinolab was to create a rich, DMCA-compliant platform for the analysis of narrative media clips annotated to highlight distinctive use of film language.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000515/resources/images/figure03.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000515/resources/images/figure03_hu5323b9a1b7be5f620c998ce2a8943987_210897_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/15/1/000515/resources/images/figure03_hu5323b9a1b7be5f620c998ce2a8943987_210897_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/15/1/000515/resources/images/figure03.jpg 864w" 
     class="landscape"
     ><figcaption>
        <p>Principal entry points to the Kinolab clip collection.
        </p>
    </figcaption>
</figure>
<p>The platform we envisioned would facilitate comparisons across clips and, to this end, feature advanced search options that could handle everything from simple keyword searches to searches using filters and Boolean terms. A secondary objective was to develop an easy-to-use contribute function so that users wishing to add their own legally obtained narrative media clips to the collection could do so with relative ease, thereby building into Kinolab the capacity for academic crowdsourcing. Ultimately, the simple design that we settled on invites verified academic users into the collection through four principal entry points accessed via the site&rsquo;s primary navigation (see <a href="#figure03">Figure 3</a>): Films and Series, Directors, Genres, and Tags. The terminus of each of these pathways is the individual clip page, where users can view a clip and its associated film language tags, which link to other clips in the collection sharing the same tag, and, if desired, download the clip for teaching or research purposes. Additional entry points accessed via the primary navigation bar include the Contribute (see <a href="#figure04">Figure 4</a>) and Search (see <a href="#figure05">Figure 5</a>) functions. Users can contribute their own narrative media clips via a simple interface designed to facilitate the curatorial process for project members working in Kinolab&rsquo;s back end. Academic crowdsourcing is standardized via a controlled vocabulary of film language terms (discussed further in Section Five: Working Toward a Data Model for Film Language). The Search function queries all of the fields associated with a clip in Kinolab&rsquo;s database, including informational metadata akin to what one would find in an IMDb film or series episode entry and content metadata supplied by Kinolab curators and contributors. Kinolab curators – project faculty, staff, and students – have access to the back end of the Contribute function, where they can evaluate and edit submitted clips and their metadata (informational and content metadata including film language tags) and approve or reject submissions to the collection.   </p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000515/resources/images/figure04.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000515/resources/images/figure04_huf76bd5471558cd362172bc44ec8e0ed3_130717_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000515/resources/images/figure04_huf76bd5471558cd362172bc44ec8e0ed3_130717_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000515/resources/images/figure04.png 1000w" 
     class="landscape"
     ><figcaption>
        <p>Kinolab Contribute page.
        </p>
    </figcaption>
</figure>
<p>The vast majority of Kinolab&rsquo;s file system overhead goes to storing audiovisual clips. Accordingly, we built the first implementation of Kinolab on a system that could handle most of the media file management for us. Our priority was finding an established content management system that could handle the intricacies of uploading, organizing, annotating, and maintaining digital clips. To meet this goal, we initially adopted Omeka, a widely used and well-respected platform with a proven record for making digital assets available online via an easy-to-use interface (see <a href="https://omeka.org/">https://omeka.org/</a>). Built to meet the needs of museums, libraries, and archives seeking to publish digital collections and exhibitions online, Omeka&rsquo;s features made it the most appealing out-of-the-box solution for our first release of Kinolab. These features included: an architecture stipulating that Items belong to Collections, a relationship analogous to clips belonging to films; almost limitless metadata functionality, facilitating deep descriptive applications for film clips; a tagging system that made applying film language identifiers simple and straightforward; a sophisticated search interface capable of performing complex searches; and, finally, a built-in administrative backend capable of handling a significant part of the project&rsquo;s file and database management tasks behind the scenes.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000515/resources/images/figure05.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000515/resources/images/figure05_huf76bd5471558cd362172bc44ec8e0ed3_135479_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000515/resources/images/figure05_huf76bd5471558cd362172bc44ec8e0ed3_135479_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000515/resources/images/figure05.png 1000w" 
     class="landscape"
     ><figcaption>
        <p>Kinolab Search page.
        </p>
    </figcaption>
</figure>
<p>Omeka&rsquo;s ease of use came with some significant restrictions, however. Its functionality for describing Collections through metadata was far more limited than that for Items. This limitation makes sense for the cultural heritage institutions that are Omeka&rsquo;s primary users, which need extensive descriptive metadata for individual items comprising a collection rather than for the collection itself. In Kinolab&rsquo;s case, however, an Omeka &lsquo;Collection&rsquo; was analogous to an individual film, and we struggled with our inability to attach key metadata relevant to a film as a whole at the Collection level (for example, cinematographer, editor, etc.). The constraints of Omeka&rsquo;s model became more pronounced as the project expanded beyond films to include series. This expansion entailed moving from a relatively straightforward Film-Clips relationship to the more complicated relationship between collections and items Series-Seasons-Episodes-Clips, which Omeka&rsquo;s generic model couldn&rsquo;t represent. The inclusion of series also confounded Omeka&rsquo;s search operation, which did not operate in a way that could factor in our increasingly complex taxonomies. As Kinolab grew, so did our need for functionalities that Omeka could not provide, ranging from the ability to select thumbnail images from specific video frames to the ability to specify extra relational concepts. Omeka&rsquo;s rich development community and plugins could have moved us toward some of these goals, but as we continued to add plugins and to customize the core feature set of Omeka, we were forced to recognize that the time and cost of the alterations were outweighing the benefits we gained from a pre-packaged system. Indeed, we had altered the base code so much that we could no longer claim to be using Omeka as most people understood it. That meant that upgrades to Omeka and its plugins could prove problematic as they could potentially affect areas of code we had modified to meet our goals.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000515/resources/images/figure06.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000515/resources/images/figure06_hu2591d9ded1b4ccfa9eeec119fa0fcc14_45511_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/15/1/000515/resources/images/figure06_hu2591d9ded1b4ccfa9eeec119fa0fcc14_45511_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/15/1/000515/resources/images/figure06.jpg 863w" 
     class="landscape"
     ><figcaption>
        <p>Basic object-relational schematic of Kinolab films and series.
        </p>
    </figcaption>
</figure>
<p>Moving away from Omeka gave us the freedom to take the Kinolab concept back to the data modeling phase and define a database backend specifically for our project. We were able to implement the user interface collaboratively, module by module, with all team members, which helped flush out additional requirements and desirable features in easy-to-regulate advances. The system we ended up building used many of the same tools as Omeka.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000515/resources/images/figure07.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000515/resources/images/figure07_hu48d5bf9e86f42b0a7a7b5000b0c06bc3_437954_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/15/1/000515/resources/images/figure07_hu48d5bf9e86f42b0a7a7b5000b0c06bc3_437954_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/15/1/000515/resources/images/figure07_hu48d5bf9e86f42b0a7a7b5000b0c06bc3_437954_1200x0_resize_q75_box.jpg 1200w,/dhqwords/vol/15/1/000515/resources/images/figure07.jpg 1245w" 
     class="landscape"
     ><figcaption>
        <p>UML diagram of Kinolab&rsquo;s database.
        </p>
    </figcaption>
</figure>
<p>The system requirements for Kinolab read much like those for Omeka and include a Linux operating system, Apache HTTP server, MySQL, and PHP scripting language.</p>
<p>Perhaps the most significant change that we made in the move from Omeka to a platform of our own design concerns metadata collection. In the first, Omeka-based implementation of Kinolab, project curators manually gathered informational metadata for films and series from IMDb.com and physical DVDs, subsequently uploading that metadata into Omeka&rsquo;s back end as part of a labor-intensive curatorial workflow. We eventually understood the project to be less about collecting media data than about aggregating annotations in service of film language analysis. We recognized that, if we were to continue attempting to collect and store all of the significant metadata describing films and series ourselves, we would be spending considerable energy duplicating efforts that existed elsewhere. This realization led us to partner with a third party, <a href="https://www.themoviedb.org/?language=en-US">TMDb (The Movie Database)</a> to handle the project&rsquo;s general metadata needs. For our new Kinolab implementation, we do store some descriptive data particular to the project in order to seed our search interface, but for the most part we rely on TMDb to be the actual source data and direct our users to that source whenever possible, enabling us to focus more narrowly on clip annotation.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000515/resources/images/figure08.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000515/resources/images/figure08_hu3f0535746ab8936e4aa4391aae2e55f0_152487_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000515/resources/images/figure08_hu3f0535746ab8936e4aa4391aae2e55f0_152487_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000515/resources/images/figure08.png 681w" 
     class="landscape"
     ><figcaption>
        <p>Kinolab metadata collection
        </p>
    </figcaption>
</figure>
<p>Unlike IMDb, TMDb has a clear message of open access and excellent documentation. In testing, it offered as much and sometimes more information than one could access on IMDb. We have concerns about the long-term reliability of a less established source like TMDb over a recognized entity such as IMDb, but since we only make use of this data tangentially we decided that it is provisionally the best option. The metadata that TMDb provides is important for helping to locate and contextualize Kinolab clips, but the project is not attempting to become a definitive source for providing information about the films and series from which they are excerpted. Consequently, we simply reference this kind of metadata via TMDb&rsquo;s APIs or direct Kinolab users to the TMDb site itself. The lack of an accessible, authoritative scholarly database dedicated to narrative films and series is an ongoing problem shared by the entire field of media studies <sup id="fnref1:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. In the case of the Kinolab project, it has represented a challenge almost as significant as the legal and technological ones outlined elsewhere in this case study.  </p>
<h2 id="5-working-toward-a-data-model-for-film-language">5. Working Toward a Data Model for Film Language</h2>
<p>Early in Kinolab&rsquo;s development, we confronted a tension between the expansive concept of film language and the need to define it methodically for computational purposes. Problematically, clips initially contributed to the project, for example, could illustrate the same cinematographic concept using synonymous but different terms, complicating the indexing and retrieval of clips. For example, a shot in which the camera frame is not level with the horizon was defined differently (and correctly) by contributors as either dutch angle, dutch tilt, or canted angle. Alternatively, a clip might be identified with a single form of film language but not with its parent form. For example, the sequence shot, in which an entire sequence is rendered in a single shot, is a child of the long take, a shot of relatively lengthy duration, thus identifying the one ought to also identify the other.</p>
<p>Though different in kind, these and other related issues we encountered demonstrated the need to situate individual film language concepts within a broader, machine-readable model of film language such as a thesaurus or ontology. The first case cited above, involving the interchangeability of dutch angle, dutch tilt, or canted angle, is a straightforward problem of synonymy, resolvable through the adoption of a controlled vocabulary for film language spelling out preferred and variant terms and including synonym ring lists to ensure Kinolab&rsquo;s ability to return appropriate clips when queried. The second case cited above, however, demonstrates the need to conceive of film language hierarchically. Both problems reveal how Kinolab could benefit from a data modeling approach capable of explicitly defining the &ldquo;concepts, properties, relationships, functions, constraints, and axioms&rdquo; of film language, akin to those proposed by the Getty Research Institute for art, architecture and other cultural works <sup id="fnref:29"><a href="#fn:29" class="footnote-ref" role="doc-noteref">29</a></sup>.</p>
<p>Our research revealed the lack of preexisting, authoritative models for film language. The International Federation of Film Archives (FIAF), for example, offers a &ldquo;Glossary of Filmographic Terms&rdquo; designed to assist film catalogers in the consistent identification and translation of credit terms, as well as a &ldquo;Glossary of Technical Terms&rdquo;, for terms used in film production and the film laboratory, but neither resource could provide the kind of guidance we sought in organizing and deploying film language consistently. The Large-Scale Concept Ontology of Multimedia (LSCOM, see <a href="http://www.ee.columbia.edu/ln/dvmm/lscom/">http://www.ee.columbia.edu/ln/dvmm/lscom/</a>) is, for now, limited to concepts related to events, objects, locations, people, and programs and therefore lacking labels related to film form. The AdA Ontology for Fine-Grained Semantic Video Annotation (see <a href="https://projectada.github.io/">https://projectada.github.io/</a>) is promising for its focus on film-analytical concepts, but remains only partially complete. This led us to take an exploratory first step in that direction in the form of a controlled list of film language terms, drawn primarily from the glossaries of two widely adopted cinema studies textbooks, Timothy Corrigan and Patricia White&rsquo;s  <em>The Film Experience</em>   <sup id="fnref:30"><a href="#fn:30" class="footnote-ref" role="doc-noteref">30</a></sup> and David A. Cook&rsquo;s  <em>A History of Narrative Film</em>   <sup id="fnref:31"><a href="#fn:31" class="footnote-ref" role="doc-noteref">31</a></sup> (see <a href="https://kinolab.org/Tags.php">https://kinolab.org/Tags.php</a> for a complete list of terms). The controlled list currently includes approximately 200 aspects of film language and their accompanying definitions and serves to regulate Kinolab&rsquo;s academic crowdsourcing by ensuring that concepts are applied consistently across the platform. All metadata and particularly the application of film language tags are reviewed by Kinolab&rsquo;s curators before being added to the Kinolab collection. Annotation for Kinolab works by allowing a curator to define a one-to-many relationship of a clip to a limitless number of tags, bounded only by the number of available tags in our controlled list. Tags are linked to the clip by reference only, so if there is a need to change the name or description of a tag, it can be done without having to resync all tagged clips. So, for example, if it were decided that a dutch angle should be called a canted angle that could be updated at the tag level and would automatically update wherever tagged.</p>
<p>This is a modest solution that notably excludes specialized terms and concepts from more technical areas of film language such as sound, color, or computer-generated imagery. Moreover, relying upon authoritative introductory texts like  <em>The Film Experience</em>  and  <em>A History of Narrative Film</em>  threatens to reproduce their troubling omissions of aspects of film language like &lsquo;blackface&rsquo;, which doesn&rsquo;t appear in the glossary of either book despite being a key element of historical film language and narrative in the United States and beyond. Our flat list is admittedly a makeshift substitute for a more robust form of data modeling that could, for example, deepen our understanding of film language and provide further insight into which aspects of it might be analyzable via artificial intelligence, or enable us to share Kinolab data usefully on the Semantic Web. We have, however, anticipated the need for this and built into Kinolab the possibility of adding hierarchy to our evolving controlled vocabulary. For example, tags like</p>
<ul>
<li>color</li>
<li>color balance</li>
<li>color contrast</li>
<li>color filter</li>
</ul>
<p>will eventually allow a user to drill down to</p>
<ul>
<li>
<p>color</p>
</li>
<li>
<p>color balance</p>
</li>
<li>
<p>color contrast</p>
</li>
<li>
<p>color filter</p>
</li>
</ul>
<p>Our experience thus far in developing Kinolab has demonstrated that there is a genuine need for development of a film language ontology with critical input from scholars and professionals in film and media studies, information science, computer science, and digital humanities. Beyond the uses described above, this kind of formalized, machine-readable conceptualization of how film language works in narrative media is also a logical information-age extension of the critical work that has already been done on film language and narrative by the figures cited earlier <sup id="fnref1:26"><a href="#fn:26" class="footnote-ref" role="doc-noteref">26</a></sup>  <sup id="fnref3:24"><a href="#fn:24" class="footnote-ref" role="doc-noteref">24</a></sup> as well as contemporary scholars such as David Bordwell <sup id="fnref1:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>, among others.</p>
<h2 id="6-fair-use-and-the-digital-millennium-copyright-act">6. Fair Use and the Digital Millennium Copyright Act</h2>
<p>A robust, well-researched body of literature exists in support of U.S.-based media scholars wishing to exercise their right to assert fair use <sup id="fnref:32"><a href="#fn:32" class="footnote-ref" role="doc-noteref">32</a></sup>  <sup id="fnref:33"><a href="#fn:33" class="footnote-ref" role="doc-noteref">33</a></sup>  <sup id="fnref:34"><a href="#fn:34" class="footnote-ref" role="doc-noteref">34</a></sup>  <sup id="fnref:35"><a href="#fn:35" class="footnote-ref" role="doc-noteref">35</a></sup>  <sup id="fnref:36"><a href="#fn:36" class="footnote-ref" role="doc-noteref">36</a></sup>  <sup id="fnref:37"><a href="#fn:37" class="footnote-ref" role="doc-noteref">37</a></sup>. Simultaneously, legal exemptions permitting this kind of work have broadened in the United States over the past two decades. Notwithstanding these developments, aspiring DH practitioners interested in working with moving images may be put off by a complex set of practices and code that necessitates a clear understanding of both the principles of fair use and the DMCA. They may also encounter institutional resistance from university or college copyright officers who reflexively adopt a conservative approach to fair use claims made by faculty and students, especially when those claims relate to the online publication of copyrighted moving images. Kinolab&rsquo;s policy regarding fair use and the DMCA builds upon the assertive stances toward fair use and the DMCA adopted by fellow AVinDH practitioners, especially those of Anderson <sup id="fnref1:32"><a href="#fn:32" class="footnote-ref" role="doc-noteref">32</a></sup> in the context of Critical Commons and Mittell <sup id="fnref1:33"><a href="#fn:33" class="footnote-ref" role="doc-noteref">33</a></sup> in the context of videographic criticism. Kinolab&rsquo;s policy also reflects (and benefits from) loosening restrictions authorized by the Librarian of Congress in triennial rounds of exemptions to the DMCA. These have shifted gradually from the outright ban described above to broader exemptions in 2015 for &ldquo;college and university faculty and students engaged in film studies classes or other courses requiring close analysis of film and media excerpts&rdquo; <sup id="fnref:38"><a href="#fn:38" class="footnote-ref" role="doc-noteref">38</a></sup> and, in 2018, for &ldquo;college and university faculty and students [&hellip;] for the purpose of criticism, comment, teaching, or scholarship&rdquo; <sup id="fnref:39"><a href="#fn:39" class="footnote-ref" role="doc-noteref">39</a></sup>. The 2018 exemption should be of particular interest to the AVinDH community in that it does away with the earlier rule that capturing moving images (or motion pictures, in the language of the Register of Copyrights) be undertaken only in the context of &ldquo;film studies classes or other courses requiring close analysis of film and media excerpts,&rdquo; replacing that language with the more expansive &ldquo;for the purposes of criticism, comment, teaching, or scholarship.&rdquo;</p>
<p>The Kinolab team authored a comprehensive statement detailing the project&rsquo;s adherence to the principles of fair use as well as its compliance with the DMCA in order to secure critical institutional support for the project, which was granted after vetting by Bowdoin College&rsquo;s copyright officer and legal counsel (see <a href="http://kinolab.org/">http://kinolab.org/</a> for Kinolab&rsquo;s Statement on Fair Use and the Digital Millennium Copyright Act). Essential as this kind of work is, it is time-consuming and somewhat peripheral to the project&rsquo;s main goal. Moreover, our confidence about finding ourselves on solid legal footing is tempered by the knowledge that that footing does not extend outside of the United States, where Kinolab would fall under the jurisdiction of diverse and, in some cases, more restrictive copyright codes. For now, we echo colleagues whose work has paved the way for Kinolab when we observe that the right to make fair use of copyrighted materials is a key tool that will only become more vital as audiovisual work in DH increases, and that members of the AVinDH community should continue to exercise this right assertively. For our part, we make Kinolab&rsquo;s work available under a Creative Commons Attribution-NonCommercial 4.0 International License (CC BY-NC), which gives users permission to remix, adapt, and build upon our work as long as their new works acknowledge Kinolab and are non-commercial in nature.</p>
<h2 id="7-conclusion">7. Conclusion</h2>
<p>This case study highlights several of the challenges and opportunities facing DH practitioners who work with audiovisual materials: in particular, the recent shift in digital text analysis (and, to some extent, in moving image analysis) away from annotation as a basis for data set training in favor of newer forms of machine learning; the ongoing need for an authoritative data model for film language; and the changing legal terrain for U.S.-based projects aiming to incorporate AV materials under copyright. The fact that each of these challenges is simultaneously an opportunity underscores just how dynamic AVinDH is in 2021. It also explains why this case study describes a project that is still very much  <em>in medias res</em> .</p>
<p>As of this writing, the Kinolab team is testing its new platform and seeking user feedback on ways to improve it. We are also taking steps to ensure the thoughtful, intentional growth of Kinolab&rsquo;s clip collection and the project&rsquo;s long-term sustainability. These include, among others, 1) expanding the project&rsquo;s advisory board to include members broadly representative of an array of scholarly interests in film language and narrative, including sound, color, and computer-generated imagery (the use of 3D computer graphics for special effects), but also animated media, national and regional cinemas, horror, ecocinema, science fiction, silent cinema, television, queer cinema, classical Hollywood cinema, transnational cinema, and/or issues related to diversity and inclusion, among others; 2) independently developing and/or contributing to existing efforts to create a robust data model for film language; 3) encouraging colleagues to contribute to Kinolab by supporting the ongoing work of clip curation at their home institutions, either by internally funding undergraduate or graduate student clip curation or through student crowdsourcing in their classrooms; 4) testing and implementing where appropriate machine vision technologies such as those in development at the Media Ecology Project and the Distant Viewing Lab; 5) developing relationships with likeminded groups such as Critical Commons, Domitor, the Media History Digital Library and the Alliance for Networking Visual Culture, among others; and 6) developing national organizational partnerships with the Society for Cinema and Media Studies and/or the University Film and Video Association. Through these and other strategies, we hope to become a genuinely inclusive platform for the analysis of narrative media clips, built from the ground up by the scholars and students using it.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Fischer, Lucy and Patrice Petro.  <em>Teaching Film</em> . The Modern Language Association of America, New York (2012): 6.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Bordwell, David et al.  <em>The Classical Hollywood Cinema: Film Style and Mode of Production to 1960</em> . Columbia University Press, New York (1985).&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Columbia Center for Teaching and Learning.  <em>About the Columbia Film Language Glossary</em>  [Online]. The Columbia Film Language Glossary. Available at: <a href="https://filmglossary.ccnmtl.columbia.edu/about/Columbia">https://filmglossary.ccnmtl.columbia.edu/about/Columbia</a> Center for Teaching and Learning (Accessed: 30 November 2019).&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p><em>How To</em>  [Online]. Critical Commons. Available at <a href="http://www.criticalcommons.org/how-to">http://www.criticalcommons.org/how-to</a> (Accessed: 30 November 2019).&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>Estrada, Liliana Melgar, Eva Hielscher, Marijn Koolen, Christian Gosvig Olesen, Julia Noordegraaf, and Jaap Blom.  “Film Analysis as Annotation: Exploring Current Tools and Their Affordances” In  <em>The Moving Image</em>  17.2: 40-70 (2017).&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>Halter, Gaudenz, Rafael Ballester-Ripoli, Barbara Flueckiger, and Renato Pajarola.  “VIAN: A Visual Annotation Tool for Film Analysis.” In  <em>Computer Graphics Forum</em> , 38: 119-129. (2019)&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>Media Ecology Project. Available at <a href="http://mediaecology.dartmouth.edu/wp/">http://mediaecology.dartmouth.edu/wp/</a> (Accessed: 30 November 2019).&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p><em>Analyzing Visual Culture</em>  [Online]. Distant Viewing Lab. Available at <a href="https://distantviewing.org/">https://distantviewing.org/</a> (Accessed: 30 November 2019).&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>Kuhn, Virginia Alan Craig et al.  “The VAT: Enhanced Video Analysis.” in  <em>Proceedings of the XSEDE 2015 Conference: Scientific Advancements Enabled by Enhanced Cyberinfrastructure.</em> , a11, ACM International Conference Proceeding Series, vol. 2015-July, Association for Computing Machinery, 4th Annual Conference on Extreme Science and Engineering Discovery Environment, XSEDE 2015, St. Louis, United States, 7/26/15. <a href="https://doi.org/10.1145/2792745.2792756">https://doi.org/10.1145/2792745.2792756</a>.&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>. Arnold, Taylor and Lauren Tilton.  “Distant Viewing: Analyzing Large Visual Corpora.” Digital Scholarship in the Humanities. (2019) <a href="https://academic.oup.com/dsh/advance-article-abstract/doi/10.1093/digitalsh/fqz013/5382183">https://academic.oup.com/dsh/advance-article-abstract/doi/10.1093/digitalsh/fqz013/5382183</a>.&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>Manovich, Lev.  “How to Compare One Million Images?” In  <em>Understanding Digital Humanities</em> . Ed. David Berry. Palgrave Macmillan, New York (2012). 249-278.&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p>Pustejovsky, James, Jessica L. Moszkowicz, and Marc Verhagen.  “ISO-Space: The Annotation of Spatial Information in Language.” In Proceedings of the Sixth Joint ISO-ACL SIGSEM Workshop on Interoperable Semantic Annotation, 6:1–9. (2011) pdfs.semanticscholar.org.&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13">
<p>Tissot, Hegler, Angus Roberts, Leon Derczynski, Genevieve Gorrell, and Marcus Didonet Del Fabro.  “Analysis of Temporal Expressions Annotated in Clinical Notes.” In Proceedings of the 11th Joint ACL-ISO Workshop on Interoperable Semantic Annotation (ISA-11) (2015) aclweb.org. <a href="https://www.aclweb.org/anthology/W15-0211">https://www.aclweb.org/anthology/W15-0211</a>.&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14">
<p>Alm, Cecilia Ovesdotter, Dan Roth, and Richard Sproat.  “Emotions from Text: Machine Learning for Text-Based Emotion Prediction.” In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, 579–86. HLT &lsquo;05. Stroudsburg, PA, USA: Association for Computational Linguistics. (2005).&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:15">
<p>Pustejovsky, James, and Amber Stubbs.  <em>Natural Language Annotation for Machine Learning: A Guide to Corpus-Building for Applications</em> . O&rsquo;Reilly Media, Inc. (2012)&#160;<a href="#fnref:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:16">
<p>Blei, David M., Andrew Y. Ng, and Michael I. Jordan.  “Latent Dirichlet Allocation.” In Advances in Neural Information Processing Systems 14, edited by T. G. Dietterich, S. Becker, and Z. Ghahramani, (2002) 601–8. MIT Press.&#160;<a href="#fnref:16" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:17">
<p>Young, Tom, Devamanyu Hazarika, Soujanya Poria, and Erik Cambria.  “Recent Trends in Deep Learning Based Natural Language Processing.” (2017) arXiv [cs.CL]. arXiv. <a href="http://arxiv.org/abs/1708.02709">http://arxiv.org/abs/1708.02709</a>.&#160;<a href="#fnref:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:18">
<p>Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton.  “ImageNet Classification with Deep Convolutional Neural Networks.” In  <em>Advances in Neural Information Processing Systems</em>  25, edited by F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger. (2012) 1097–1105. Curran Associates, Inc.&#160;<a href="#fnref:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:19">
<p>Sainath, Tara N., Brian Kingsbury, George Saon, Hagen Soltau, Abdel-Rahman Mohamed, George Dahl, and Bhuvana Ramabhadran.  “Deep Convolutional Neural Networks for Large-Scale Speech Tasks.” Neural Networks: The Official Journal of the International Neural Network Society 64 (April) (2015) 39–48.&#160;<a href="#fnref:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:20">
<p>Cho, Kyunghyun, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio.  “On the Properties of Neural Machine Translation: Encoder-Decoder Approaches.” (2014) arXiv [cs.CL]. arXiv. <a href="http://arxiv.org/abs/1409.1259">http://arxiv.org/abs/1409.1259</a>.&#160;<a href="#fnref:20" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:21">
<p>Mikolov, Tomas, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeff Dean.  “Distributed Representations of Words and Phrases and Their Compositionality.” In Advances in Neural Information Processing Systems 26, edited by C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger (2013) 3111–19. Curran Associates, Inc.&#160;<a href="#fnref:21" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:22">
<p>Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin.  “Attention Is All You Need.” (2017) arXiv [cs.CL]. arXiv. <a href="http://arxiv.org/abs/1706.03762">http://arxiv.org/abs/1706.03762</a>.&#160;<a href="#fnref:22" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:23">
<p>Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.  “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding.” (2018) arXiv [cs.CL]. arXiv. <a href="http://arxiv.org/abs/1810.04805">http://arxiv.org/abs/1810.04805</a>.&#160;<a href="#fnref:23" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:24">
<p>Metz, Christian.  <em>Film Language: A Semiotics of the Cinema.</em>  Oxford University Press, New York (1974).&#160;<a href="#fnref:24" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:24" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:24" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref3:24" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:25">
<p>. Chandler 2011.  <em>A Dictionary of Media and Communication</em> . Available at: <a href="https://www.oxfordreference.com/view/10.1093/acref/9780199568758.001.0001/acref-9780199568758-e-1041">https://www.oxfordreference.com/view/10.1093/acref/9780199568758.001.0001/acref-9780199568758-e-1041</a> (Accessed 30 November 2019).&#160;<a href="#fnref:25" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:25" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:26">
<p>Eisenstein, Sergei.  <em>Film Form: Essays in Film Theory.</em>  Harcourt, Inc., San Diego, New York, London (1949).&#160;<a href="#fnref:26" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:26" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:27">
<p>Bazin, André.  <em>What is Cinema?</em>  Volume 1. University of California Press, Berkeley (2004). Available at: ProQuest Ebook Central (Accessed 30 November 2019)&#160;<a href="#fnref:27" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:28">
<p>.  <em>A Dictionary of Film Studies</em> . Available at: <a href="https://www.oxfordreference.com/view/10.1093/acref/9780199587261.001.0001/acref-9780199587261-e-0460">https://www.oxfordreference.com/view/10.1093/acref/9780199587261.001.0001/acref-9780199587261-e-0460</a> (Accessed 30 November 2019).&#160;<a href="#fnref:28" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:29">
<p>Harpring, Patricia.  <em>Introduction to Controlled Vocabularies: Terminology for Art, Architecture, and other Cultural Works.</em>  Getty Research Institute, Los Angles (2010).&#160;<a href="#fnref:29" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:30">
<p>Corrigan, Timothy and Patricia White.  <em>The Film Experience.</em>  Fifth edition. Bedford/St. Martin&rsquo;s, Boston (2018).&#160;<a href="#fnref:30" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:31">
<p>Cook, David A.  <em>A History of Narrative Film.</em>  W.W. Norton &amp; Company, Inc., New York (2016).&#160;<a href="#fnref:31" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:32">
<p>Anderson, Steve.  “Fair Use and Media Studies in the Digital Age.”  <em>Frames</em> , 1 [Online]. Available at: <a href="https://framescinemajournal.com/article/fair-use-and-media-studies-in-the-digital-age/">https://framescinemajournal.com/article/fair-use-and-media-studies-in-the-digital-age/</a> (Accessed 30 November 2019).&#160;<a href="#fnref:32" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:32" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:33">
<p>Keathley, Christian, Jason Mittell, and Catherine Grant.  <em>The Videographic Essay: Criticism in Sound &amp; Image.</em>  Caboose (2019): 119-127.&#160;<a href="#fnref:33" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:33" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:34">
<p>Mittell, Jason.  “Letting Us Rip: Our New Right to Fair Use of DVDs.”  <em>The Chronicle of Higher Education</em> , 27 July [Online]. Available at: <a href="https://www.chronicle.com/blogs/profhacker/letting-us-rip-our-new-right-to-fair-use-of-dvds/25797">https://www.chronicle.com/blogs/profhacker/letting-us-rip-our-new-right-to-fair-use-of-dvds/25797</a> (Accessed 30 November 2019).&#160;<a href="#fnref:34" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:35">
<p>Center for Social Media.  <em>Code of Best Practices in Fair Use for Online Video</em> . Available at: <a href="https://cmsimpact.org/code/code-best-practices-fair-use-online-video/">https://cmsimpact.org/code/code-best-practices-fair-use-online-video/</a> (Accessed: 30 November 2019).&#160;<a href="#fnref:35" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:36">
<p>Society for Cinema and Media Studies.  <em>The Society for Cinema and Media Studies&rsquo; Statement of Best Practices for Fair Use in Teaching for Film and Media Educators</em> . Available at: <a href="https://cmsimpact.org/code/society-cinema-media-studies-statement-best-practices-fair-use-teaching-film-media-educators/">https://cmsimpact.org/code/society-cinema-media-studies-statement-best-practices-fair-use-teaching-film-media-educators/</a> (Accessed 30 November 2019).&#160;<a href="#fnref:36" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:37">
<p>College Art Association.  <em>Code of Best Practices in Fair Use for the Visual Arts</em> . Available at <a href="https://www.collegeart.org/programs/caa-fair-use">https://www.collegeart.org/programs/caa-fair-use</a> (Accessed: 30 November 2019).&#160;<a href="#fnref:37" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:38">
<p>. Available at: <a href="https://www.federalregister.gov/documents/2015/10/28/2015-27212/exemption-to-prohibition-on-circumvention-of-copyright-protection-systems-for-access-control">https://www.federalregister.gov/documents/2015/10/28/2015-27212/exemption-to-prohibition-on-circumvention-of-copyright-protection-systems-for-access-control</a> (Accessed 30 November 2018).&#160;<a href="#fnref:38" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:39">
<p>. Available at: <a href="https://www.federalregister.gov/documents/2018/10/26/2018-23241/exemption-to-prohibition-on-circumvention-of-copyright-protection-systems-for-access-control">https://www.federalregister.gov/documents/2018/10/26/2018-23241/exemption-to-prohibition-on-circumvention-of-copyright-protection-systems-for-access-control</a> (Accessed 30 November 2018)&#160;<a href="#fnref:39" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry><entry><title type="html">Fostering Community Engagement through Datathon Events: The Archives Unleashed Experience</title><link href="https://rlskoeser.github.io/dhqwords/vol/15/1/000536/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://rlskoeser.github.io/dhqwords/vol/15/1/000536/</id><author><name>Samantha Fritz</name></author><author><name>Ian Milligan</name></author><author><name>Nick Ruest</name></author><author><name>Jimmy Lin</name></author><published>2021-03-05T00:00:00+00:00</published><updated>2021-03-05T00:00:00+00:00</updated><content type="html"><![CDATA[<p>“If you build it, they will come.”  Unfortunately, this does not apply when developing digital humanities tools and infrastructure. Creating an open-source tool and fostering a user community around it requires concerted efforts in the realm of community engagement and outreach. It means building a community, which involves scoping, involvement, and ongoing engagement. To support our web archive analysis tools, our project team has run a series of  “Archives Unleashed”  datathons, to help engage users not only just with our tools, but with each other in an attempt to build a sustainable web archiving analysis community.</p>
<p>This article explores the impact that our series of datathon events have had on community engagement both within the web archiving field, and more specifically, on the professional practices of attendees. To do so, we draw on and adapt two leading community engagement models, combining them to introduce our new understanding of how to build and engage users in an open-source digital humanities project. Our model illustrates both the activities undertaken by our project, as well as the related impact they have on the field and can be broadly applied to other digital humanities projects seeking to engage their communities. Ultimately, the six-stage community engagement model emphasizes scoping, informing, consulting, involving, collaborating, and empowering, in an iterative cycle where one can work to continually expand one’s community.</p>
<p>We wanted to explore the following questions: how has our community been built? What activities and approaches at these events had the most impact, and which ones could be improved? What have been the lasting impacts of community engagement? And, finally, what are the broader and long-term impacts of creating a community within the web archiving community? These would be critical for both our project but also for others within the broader digital humanities and library communities interested in similar issues around community building and engagement.</p>
<p>Our specific focus is on our datathons, which were modeled on the broader hackathon movement. Hackathons emerged in the early 2000s, primarily at first to rapidly develop new computer software <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. The term itself combines the terms hacking and marathon, implying an  “intense, uninterrupted period of programming”   <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. Over two decades, hackathons have grown to encompass groups as varied as cultural organizations, government agencies, venture capitalists looking for new ideas, and other forms of innovation <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>  <sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>. A growing body of literature explores how hackathons have been adopted within fields as varied as academia, medicine, and civic hacking. Indeed, the model is well-positioned for  “enriching social networks, facilitating collaborative learning, and workforce development”   <sup id="fnref1:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>.</p>
<p>While our Archives Unleashed Project drew on the hackathon model, as it allowed for short focused, yet intensive work periods, we made an early shift to use the term datathon instead. Our first two events (in 2016 and 2017) used the term hackathon, but our project team worried the nature of the term itself would preclude bringing together a wide array of participants. Our project wanted to engage with individuals in diverse roles within the web archiving field: not just computer scientists and developers, but digital humanists, librarians, and curators. The unifying feature of the events would not be hacking on a particular technology, but rather exploring the implications of new data.</p>
<p>Our article begins with a brief background into the broader Archives Unleashed Project and the ecosystem that our datathons exist in. We then define the communities we engage with, both specifically and more broadly. Following this, we introduce the field of community engagement and note the two main frameworks that we draw from, as well as how they are combined. Then, through a series of eight interviews with datathon participants, as well as related evidence from our events, we discuss how our participants saw the datathons as dramatically impacting both their professional practices as well as the broader web archiving community. We conclude this article with lessons learned and how these models can be adapted to work for the digital humanities.</p>
<h2 id="background-web-archives-and-the-archives-unleashed-project">Background: Web Archives and the Archives Unleashed Project</h2>
<p>The world wide web, made public in 1991 by Tim Berners-Lee after being developed as an internal tool at the European Organization for Nuclear Research (CERN), has grown exponentially over the past three decades. Since its inception, the web has become a significant site of social, cultural, economic, and political activity. Our lives are increasingly mediated through technology, a current trend which has grown even clearer with widespread remote working and social distancing amidst the COVID-19 pandemic. As one of the co-authors of this article has argued,  “without using the web, histories of the 1990s will be incomplete for the most part. Ignoring the web would be like ignoring print culture”   <sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>. Recognizing the significance of the web to the future historical record, beginning in 1996, the Internet Archive as well as national libraries in Sweden and Australia, began to carry out the widespread preservation of web content. This process, web archiving, can be understood as  “any form of deliberate and purposive preserving of web material”   <sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>. Web archiving has increasingly become part of research agendas for national libraries and archives, as well as memory institutions around the world.</p>
<p>As of writing, the Internet Archive holds over 900 billion URLs and 60 petabytes of unique data (a petabyte being 1,000 terabytes). This figure is probably already dated, as the Internet Archive roughly doubles in size every two years. Despite this sheer amount of data, or perhaps because of it,  <em>access</em>  to this data has lagged. The sheer amount of data, coupled with the lack of research tools, means that scholars have, in many cases, not been able to carry out fruitful research with this material. Given the importance of web archives to carrying out histories of the 1990s and beyond, this is a serious problem. In other words, the data is there – and considerable expertise has been developed in the collection of web archival data – but the ability to work with it is not. In addition to the problems of scale, there are technical challenges of working with Web ARChive (WARC) files, in which much of this data is saved. We will return to WARC files shortly. Working with web archival data requires an understanding of both high-performance computing and the command line. This is, for the most part, out of reach for many scholars. They do not have the time, the resources, the support, or the training to work with data at scale, meaning that many research questions from the 1990s onwards cannot benefit from web data. In other words, this data is increasingly important for research, but it is too inaccessible for any research at scale.</p>
<p>Our Archives Unleashed Project thus provides scholars and researchers with tools to explore historical internet content and reduce access barriers to large-scale web archival data. Established in 2017 with funding from The Andrew W. Mellon Foundation, the project grows out of a recognition that web archives are critical to understanding the world around us, and that the scholarly community accordingly needs approachable and user-friendly tools to access born-digital cultural heritage. The Archives Unleashed Project exists in several scholarly communities as it is located at the intersection of researchers, tools developers, and libraries. Our goal is to improve scholarly access to web archives through a  “multi-pronged strategy involving tool creation, process modeling, and community building - all proceeding concurrently in mutually-reinforcing efforts”   <sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>. This multi-pronged process manifests itself in three primary ways. First, the Archives Unleashed Toolkit, an Apache Spark libaray for working with web archives directly. Secondly, the Archives Unleashed Cloud, a cloud-hosted infrastructure and web-based front-end to run Toolkit jobs on WARC data <sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>. Finally, the project co-hosts Archives Unleashed datathons with local partners. These two-day events bring an interdisciplinary group of people together to collaborate and gain hands-on experience with web archive data.</p>
<p>At the datathons, users are encouraged to use our project tools – they structure the sorts of projects that are undertaken. The Archives Unleashed Toolkit and Cloud were created to provide complementary approaches to working with web archives at scale. While digital content can exist in a variety of formats, both tools specifically work with WARC files, as well as their ARC format predecessor. WARC files, an ISO Standard (28500:2017), are essentially a container-file format that holds collected web resources together. As the web archiving community has standardized around WARCs, this has also made the development of a tool and analytics infrastructure possible.</p>
<p>However, as WARC files are inaccessible to the majority of researchers, so much of the work of the Toolkit and the Cloud revolves around extracting derivative files from web archives: familiar formats such as extracted text files, network graphs, or statistical information. For example, when exploring the text of a WARC, several filters can be applied including date, language, keyword, domain, or URL patterns. This also means that projects can be carried out on a wide variety of languages; we have seen successful examples of users working with collections in French and German.</p>
<p>The datathons are the centerpiece of the Archives Unleashed Project’s community engagement strategy. We want to make sure that the Toolkit and the Cloud both reach users and are responsive to their needs. As Niels Brügger has argued, there is great value in  “cooperation between web-archiving institutions and Internet research communities”   <sup id="fnref1:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>. Our approach to community building has taken several shapes, from providing robust open-source code documentation, running a Slack group with open sign-up for sharing and discussion, regularly blogging, providing a quarterly e-mail newsletter, and – crucially, hosting the datathons discussed here. We also strive to develop educational resources for attendees, building relationships with like-minded projects and institutions (from the Internet Archive to university libraries to national libraries in North America and Europe), and also, participate in scholarly activities that support and foster information sharing.</p>
<p>Our first datathons predated the Archives Unleashed Project. Between March 2016 and June 2017, an earlier project team (including two of this paper’s authors) carried out an initial sequence of four events. These were broader events, primarily focused on networking and building capacity in the web archiving community (described in <a href="#milliganetal2019">Milligan et al. [2019]</a>), and included 148 attendees from thirteen countries broadly drawing from web archive curators, digital humanists, and computer scientists.</p>
<p>This paper focuses on the subsequent four datathons as part of our Mellon-funded project. These events were co-hosted with and held at the University of Toronto Library (April 2018), Simon Fraser University Library (November 2019), George Washington University Library (March 2019), and Columbia University Library (hosted online due to COVID-19, March 2020). Collectively, these four datathons have engaged with over 70 participants from seven countries and fifty unique institutions. Participants were predominantly from Canada and the United States, with participants from five additional countries including the United Kingdom, Germany, New Zealand, Egypt, and Hungary. While datathon events were open to a broad spectrum of information professionals, the majority of participants came from the higher education sector: university faculty and graduate students, librarians, and archivists. We had some smaller representation from national archives, non-profit organizations, museums, and independent researchers. In terms of diversity, we did not collect information on racial backgrounds, gender identities, or educational backgrounds; we are accordingly reluctant to make assumptions about our attendees or interviewees.</p>
<p>These datathons had three primary goals. First, they were designed to introduce individuals to tools and methodologies of working with web archives at scale. Secondly, would allow attendees to engage in conversations to facilitate knowledge sharing and scholarly collaborations. Finally, the events aimed to foster community around open-source Archives Unleashed tools and web archive practices. While datathon participants bring a diversity of intellectual and personal perspectives to the events, in general, they can be categorized as access providers, tool builders, and data explorers. For the Archives Unleashed Project, focused on fostering an open-source community, these events would be pivotal for our community engagement strategy.</p>
<h2 id="community-engagement">Community Engagement</h2>
<p>When thinking of community engagement, we were principally informed by the broad definition advanced by Liz Weaver and colleagues in a paper written for the Canadian Tamarack Institute, an organization dedicated to engaging with citizens to grapple pressing community issues. Weaver et al. define community engagement as  “people working collaboratively, through inspired action and learning, to create and realize bold visions for their common future”   <sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup>. For organizations and institutions, community engagement is an opportunity to build active relationships with individuals and other entities for mutually beneficial exchanges. Community engagement is vital as it builds a relationship that actively seeks to understand the goals, needs, aspirations, concerns, and values held by a community <sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup>. Without engagement and understanding of a community’s composition, there can be misunderstanding, misrepresentation, miscommunication, and missed opportunities.</p>
<p>Before discussing community engagement models, we need to briefly define what we mean by community. Community is a surprisingly difficult term to define, as meta-reviews of scholarly definitions suggest (in addition to those below, see <a href="#macqueen2001">MacQueen et al. [2001]</a>). We use the definition suggested by Cobigo et al.:  “A community is a group of people that interact and support each other, and are bounded by shared experiences or characteristics, a sense of belonging, and often by their physical proximity”   <sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>. Indeed, while the web archiving community is largely virtual, our datathons (apart from our one held online due to COVID-19) assembled people together in a physical setting to foster community as well. Accordingly, when we speak of the larger web archiving community, we refer to individuals, organizations, groups, and institutions that have a shared focus on experience and engagement with web archiving activities and research.</p>
<p>Within this broad community there is our Archives Unleashed community, composed of those who who engage with our platforms and support project work. While the web archiving community is multidisciplinary, it can be a resource-intensive process. Accordingly, national libraries and post-secondary educational institutions are overrepresent within the web archiving field (along with the non-profit Internet Archive), providing significant contributions in the form of education, professional development opportunities, services, and tools. The professional backgrounds of these sources are reflected in the demographics and backgrounds of most of our participants.</p>
<p>Community engagement is critical for many other domains, including business and urban development <sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>  <sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup>, medical <sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup>, environmental <sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup>, open-source and technology development <sup id="fnref:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup>  <sup id="fnref:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup>, education <sup id="fnref:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup>, libraries <sup id="fnref:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup>  <sup id="fnref:20"><a href="#fn:20" class="footnote-ref" role="doc-noteref">20</a></sup>, and social sciences such as archaeology <sup id="fnref:21"><a href="#fn:21" class="footnote-ref" role="doc-noteref">21</a></sup>. Across these studies, however, there are few canonical community engagement models or frameworks; rather, most of the studies reflect on specific case studies or activities, and broad understandings of how engaging communities can benefit a specific group or organization. We did, however, identify two models that could be adapted as a framework to better understand the goals and approaches of the Archives Unleashed Project when it came to community building and engagement. We primarily base our work on the first model, although we draw on elements of the second as well.</p>
<p>The first model is the Open Community Engagement Process (OCEP) model, developed and operationalized by the Water Science Software Institute (WSSI). The OCEP model draws on development methods from Agile Software Development and the Open-Source software development community <sup id="fnref:22"><a href="#fn:22" class="footnote-ref" role="doc-noteref">22</a></sup>. This model approaches community engagement both iteratively and incrementally. Their vision is illustrated as an infinity symbol, with communities continuing to gain knowledge through all steps. Crucially, for our own purposes, the WSSI model features hackathons, finding barriers, and disseminating ideas through publications; all of these are critical aspects of the Archives Unleashed Project, especially the first. The limitation of the model, however, is its complexity. OCEP includes fifteen stages, is three dimensional which makes for complicated diagramming, and is difficult to explain. OCEP is also arguably, for our purposes, too focused on software development and does not have the wide range of applications that we want to provide.</p>
<p>The second model that we draw on is the International Association for Public Participation’s (IAP2)  “Spectrum of Public Participation.”  IAP2’s model describes five critical stages of community engagement (IAP2 2018). The first is to inform, or explain an opportunity available to the community. The second is to consult the community. The third is to involve the community in planning, implementing decisions, project design processes, and ensure widespread understanding. The fourth is to collaborate, or to work together to find solutions. And finally, the spectrum closes with empower, or providing the community with resources and skills to make their own decisions. While community engagement looks different in each sector, the processes are broadly applicable and work well for the Archives Unleashed Project. Understandably, there is no one-size-fits-all-model, as each sector and discipline have unique sets of needs. While we found the stage model of IAP2 attractive for being action-oriented, it lacked the stage and flow organization of OCEP. Accordingly, we combine them as seen in Figure 1.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000536/resources/images/figure01.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000536/resources/images/figure01_hu7927fb223d0b967d65522e0d3e70eace_125709_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000536/resources/images/figure01_hu7927fb223d0b967d65522e0d3e70eace_125709_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000536/resources/images/figure01_hu7927fb223d0b967d65522e0d3e70eace_125709_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000536/resources/images/figure01_hu7927fb223d0b967d65522e0d3e70eace_125709_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/1/000536/resources/images/figure01_hu7927fb223d0b967d65522e0d3e70eace_125709_1800x0_resize_box_3.png 1800w,/dhqwords/vol/15/1/000536/resources/images/figure01.png 1920w" 
     class="landscape"
     ><figcaption>
        <p>Archives Unleashed Community Engagement Model, adapted from IAP2
        </p>
    </figcaption>
</figure>
<p>From the IAP2 model, we drew from the spirit of the five-stages of community engagement: informing, consulting, involving, collaborating, and empowering. In this, we are not alone. The IAP2 model has been applied to  “project and program development in fields ranging from health care to environmental planning, particularly in Australia and Canada”   <sup id="fnref:23"><a href="#fn:23" class="footnote-ref" role="doc-noteref">23</a></sup>. As IAP2 focuses on an engagement dynamic originally rooted in government-civic relations, we have adjusted the categories to fit with Archives Unleashed Project priorities, our governance structure, and our activities. For example, our project goals were already defined, so while we do consult with our community for feedback and input on development processes, many overall decisions are made by the project team, not by community vote. This would be the case for many digital humanities projects. It is also important to note, that while we’ve adapted OCEP’s infinity shape for our community engagement framework, stages do not necessarily happen independently or one at a time.</p>
<p>We present the six stages of our model below. Note that the datathon process only appears at stage three of this six-stage process – when we begin to “consult” with the community – but as the first two stages provide invaluable context, we felt it was valuable to provide an overview of all of them.</p>
<p>The first stage is to scope. If we look to the aforementioned OCEP model, we can describe our first stage of community engagement where we scope or identify a problem or challenge, which becomes  “the driving question [which] serves as an incentive for a specific subset of the community to participate in the OCEP Open Community Engagement Process] process”   <sup id="fnref1:22"><a href="#fn:22" class="footnote-ref" role="doc-noteref">22</a></sup>. This stage drives purpose and objectives. Once a problem or challenge has been defined, it is critical to identify the scope of stakeholders that will collectively make up the community we are engaging. In this stage, we ask: who is our target audience, what types of individuals, organizations, or groups are we trying to represent and reach? Who is affected by our driving question? With our Archives Unleashed Project, we leveraged previous work on the Warcbase project (an Archives Unleashed Toolkit predecessor) to identify needs and barriers of scholars within the digital humanities and social sciences, when working with web archives <sup id="fnref:24"><a href="#fn:24" class="footnote-ref" role="doc-noteref">24</a></sup>. Through this experience and outreach at diverse conferences and workshops, we gained a sense of who was being affected by high barriers of access to web archives. Scoping and identifying allowed our project to focus on three user-types: academics and scholars (specifically within the humanities, social sciences, and the specific area of the digital humanities); digital access content providers (primarily librarians and archivists), and tool builders (with a focus on those in computer science). Note that this scoping provided the background for the basic structure of our datathons.</p>
<p>The second stage is to inform. The inform stage, as defined by IAP2, is  “to provide the public with balanced and objective information to assist them in understanding the problems, alternatives and/or solutions”   <sup id="fnref:25"><a href="#fn:25" class="footnote-ref" role="doc-noteref">25</a></sup>. Throughout this stage, it was important for the Archives Unleashed team to share information and summaries around the project goals, objectives, and road mapping, as well as bring awareness to web archiving access barriers, and raise an understanding of why our project was important for the community. One of our initial major activities was to develop a presence on several information-sharing platforms. This was to both deliver information and grow a supportive community. Specifically, we used Slack as a way of supportive two-way communication between the team and the community, as well as encouraging peer-to-peer discussions and information sharing practices. An accessible signup form allowed for quick access to our Slack space, and crucially, we could add additional targeted channels for specific aspects of our project, as well as spaces for general discussions. We also set up a quarterly newsletter and regularly-published blog posts.</p>
<p>The third stage is to consult. In our project, this laid the groundwork for the datathons. As IAP2 identified, the purpose of consulting with a community is to open dialogue in which individuals can provide  “feedback on analysis, alternatives and/or decisions”   <sup id="fnref1:25"><a href="#fn:25" class="footnote-ref" role="doc-noteref">25</a></sup>. This stage identifies the importance of both asking and listening to voices within the community, and to inform development cycles and approaches by the Archives Unleashed team. We achieved this in several key respects: an advisory board, discussions at datathons (as described in this paper), as well as formally through user surveys and interviews. Indeed, much of the research behind this paper exemplifies our consultation process.</p>
<p>The fourth stage is to involve, which for us, centered around the datathons discussed in this paper. This stage speaks to an active mode of participation from the community. IAP2 defines this stage as a way  “to work directly with the public throughout the process to ensure that public concerns and aspirations are consistently understood and considered.” <sup id="fnref:26"><a href="#fn:26" class="footnote-ref" role="doc-noteref">26</a></sup>  While our project, as an academic one funded by a granting agency, does not involve community participation at the ultimate decision-making level, involvement has taken the shape of working collaboratively with our community on tools development.</p>
<p>Involving our community has been the primary goal of our datathon events, intending to build a community around the tools that would complement and contribute to the broader web archiving scholarly community. As attendees participate in our datathons, they create connections that can support future research collaborations and the sharing of skills and practices with their broader networks. As the OCEP model suggests, by participating in events like our datathons, attendees are exposed to ideas, methods, approaches, skills to address web archiving challenges that may not have  “emerged using traditional disciplinary methods and that require synthetic knowledge”   <sup id="fnref2:22"><a href="#fn:22" class="footnote-ref" role="doc-noteref">22</a></sup>. In other words, the datathon model we adopted as a key activity in our involvement stage draws perspectives and approaches from the community that we otherwise would not have encountered. Notably, involvement overlaps with the consultation stage.</p>
<p>The fifth and penultimate stage is to collaborate and establish partnerships. Here, too, the datathons represented a building of both collaborations with our community and amongst them. The Archives Unleashed Project aims to foster interdisciplinary collaborations as we sit at the intersection of technology, cultural heritage, and researchers. Interdisciplinary and multi-institutional collaboration also provides opportunities for information and resources to be shared more widely. The datathon structure has provided a glimpse into the ways Archives Unleashed has stepped into a role of an intermediary for peer-to-peer collaborations and peer-to-institutional partnerships. As one participant (R4, introduced later in this article) suggested, Archives Unleashed became a broker of data, which for some participants created a necessary connection between awareness and knowledge by bridging a gap to accessing web archives data. This has been an active element of our project, and includes formal partnerships with academic institutions to explore their collections, co-host datathons together, and crucially build relationships with other projects and institutions within the field, such as the Internet Archive. Crucially, it has been critical to identify and develop relationships with projects and organizations that could help us foster interoperability between projects (such as data transfer APIs or adapting web-based notebooks).</p>
<p>The final stage, then, is to empower. The IAP2 model defines the empower stage as,  “To place final decision-making in the hands of the public”   <sup id="fnref2:25"><a href="#fn:25" class="footnote-ref" role="doc-noteref">25</a></sup>. However, the Archives Unleashed model approaches empowerment in a way which encourages and supports the confidence and skills of an individual to work with web archives, and addresses the objective of lowering barriers to web archives. Beyond the active participation of empowering our users through the datathons, we also seek to give resources to the community through learning guides (text and image-based documents that provide a tutorial-based approach to working with data), videos, and publications which explain our workflows and approaches.</p>
<p>Why the infinity symbol? Ultimately, it reflects our approach to long-term sustainability. After empowering users, we need to consider the scope and think about what the subsequent projects and iterations will look like, and the opportunities to further community engagement and enrichment; we then begin to move through the model again. Cognizant of long-term use of our tools and projects, we are reluctant to post a firm ending to the model.</p>
<p>This was thus our project’s landscape. We wanted to explore the impact of the Archives Unleashed datathon events on community engagement through a robust understanding of these models and the broader scholarly landscape. We return to these two models later in the paper, as we explore how they meaningfully integrate to help make sense of the Archives Unleashed Project’s approach to community engagement.</p>
<h2 id="methods">Methods</h2>
<p>To understand the impact of our events and inform our proposed community engagement model, we turned to interviews with participants from our event. To do so, we compiled a list of all participants who attended our Mellon-funded datathons between 2017 and 2019 (notably our events in Toronto, Vancouver, and Washington DC). Our primary goal was to craft interview questions and discussions to shed light on steps 3 – 6 (consult, involve, collaborate, and empower). By applying the model to our datathon, we aimed to validate our proposed model.</p>
<p>Our datathons sought to bring together individuals across three discrete categories, as identified by our scoping phase. First, attendees from libraries and archives, – predominantly from post-secondary institutions- or whose professional role was largely related to curating web archives. Secondly, attendees from the tool-building community, or those whose professional roles largely related to developing software for archiving or analysis. And, finally, researchers, or those whose professional role mostly revolved around using web archives for scholarly research. As our datathons are roughly structured to include a third of their attendees from each group, we wanted to interview across these three categories <sup id="fnref:27"><a href="#fn:27" class="footnote-ref" role="doc-noteref">27</a></sup>. Some basic information on the flow of the datathon can inform the discussion that follows. Our datathons involve teams working in small groups of between four and six individuals. These groups ideally spanned the three main participant groups (tools builders/library and archives/researchers) and indeed, represented diversity within those sub-categories, spanning the range from libraries, archives, sociology, social media and society scholars, humanists, software developers, government documents librarians, and so forth.</p>
<p>Datathons provided an open and collegial environment that allowed for the organic formation of new collaborations and connections. For some attendees, it was a chance to finally meet people they knew about or follow in the web archiving community but had never had a chance to meet face-to-face with; while for others, it was an opportunity to forge an entirely new network. The datathon also afforded participants the opportunity to meet individuals they would not have met in any other context.</p>
<p>These networking opportunities are especially challenging as there are significant barriers within the web archiving community, especially as it comprises of so many institutions, organizations, projects, and individuals from diverse backgrounds. As the community developed organically over two decades, there is no one overarching body or group to look to, but rather clusters of groups have formed around specific organizations (such as meetings at the Society of American Archivists, or digital humanities conferences, or regional professional groups, or national libraries).</p>
<p>Within the structure of the datathon event, there are various elements and activities geared towards fostering an open and welcoming community, and helping individuals develop a sense of belonging. To encourage team formation, we run a sticky note exercise at the event’s beginning to bring people together into teams. Participants were encouraged to write down research questions on one coloured sticky note, web archive collections of interest on another, and finally, tools and infrastructure on another one. They place their sticky notes on surfaces as organizers physically cluster the notes to identify emerging themes, and encourage physical groups of people around those themed clusters. After three or four iterations, teams would be in place <sup id="fnref1:27"><a href="#fn:27" class="footnote-ref" role="doc-noteref">27</a></sup>. This method was adapted from the field of participatory design <sup id="fnref:28"><a href="#fn:28" class="footnote-ref" role="doc-noteref">28</a></sup>.</p>
<p>The sticky note exercise was critical as teams are, of course, the core of the event. Indeed, the teams that emerged out of this were fundamental in fostering group dynamics and belonging. As there were no prescribed research problems or questions, teams were free to decide on the datasets, methods, and research questions – identified during the sticky note exercise and refined while in their groups. Some groups did have a technical expert who they could rely on, but many others would need to work together to approach some of the daunting technical barriers presented by WARC files and difficult tools. Cognizant of limitations of individual’s personal laptops or bandwidth constraints at events, we provide cloud-hosted virtual machines which participants could access remotely.</p>
<p>After working on their projects for two days, the event culminated with final presentations. Teams would have up to five minutes to present their findings, usually through a slide show or dynamic demonstration, and then answer one or two questions from the audience. Judges, selected by the host organization, would then pick one winner who received a token prize of Starbucks gift cards (chosen mainly because of their cross-border accessibility).</p>
<p>For this paper, a total of eight semi-structured interviews were conducted via Skype, and subsequently transcribed for reviewing and coding to identify emerging themes and relationships. All interviewees consented to have the interview recorded and agreed to be identified in any ensuing publications with a description of their professional role. While all interviewees had attended at least one of the three Mellon-funded datathons, four of them had also attended pre-Archives Unleashed Project datathon. We will introduce each interview subject again below in prose as they appear in results, but for convenience we use abbreviations for subsequent mentions. They are listed in Table 1.<br>
Interviewees    Code  Description  Category      R1  Librarian, large national library  Library/Archives      R2  Librarian Developer, large U.S. private university  Tools Developer      R3  Faculty Researcher, large U.S. public university  Researcher      R4  Graduate student and developer, large U.S. public university  Tools Developer      R5  Librarian, mid-sized Canadian university  Library/Archives      R6  Archivist, large U.S. private university  Library/Archives      R7  Graduate student and developer, mid-sized U.S. public university  Tools Developer      R8  Graduate student researcher, large U.S. private university  Researcher   <br>
The interview questions revolved around three general areas. First, we were interested in interviewees’ professional background and experience, as well as their scholarly interests. We wanted to understand their knowledge and experience with web archiving prior to attending the datathon. Secondly, we explored their datathon experience, focusing on their overall experience and impressions from the event, their familiarity with similar events, and crucially whether their scholarly community or practice has changed as a result of the datathon. Finally, we concluded with general questions around their thoughts on the future of web archiving, including challenges, opportunities, and gaps that they deemed relevant to the future of the community. These questions were developed as a means to understand the degree to which Archives Unleashed had successfully created a scholarly community, how the datathons impacted community formation, and what, if any, were the impacts on the broader web archiving community.</p>
<h2 id="results">Results</h2>
<p>The following section is divided into our main themes: backgrounds and pre-existing knowledge of web archiving, the impact of the datathon experience (notably skill-building, exposure to diverse practices, community formation, and inculcation of a sense of belonging), the datathon within the web archiving community context, and finally, general reflections on the future of web archiving and barriers in the field. As noted, these all primarily shed light on steps 4 – 6 (involve, collaborate, and empower) of the community model. For convenience, an overview of findings is provided in Table 2.<br>
Summary of interview results    Theme  Summary of Findings      Knowledge of Web Archiving and Backgrounds</p>
<ul>
<li>
<p>Knowledge of archiving was primarily based on and accumulated through professional experience and employment, not formal training.</p>
</li>
<li>
<p>Datathon increased knowledge of web archiving on both a high level (theoretical aims, goals, systems) and a granular level (technical skills).</p>
<pre><code> Impact of the Datathon on Professional Practice    
</code></pre>
<p><em>Datathon contributed to skill-building</em></p>
</li>
<li>
<p>All agreed their technical knowledge and skills increased as a result of their participation.</p>
</li>
<li>
<p>It was a unique opportunity to work directly with web archival data and specific analysis tools.</p>
</li>
<li>
<p>Final projects allowed participants to share information, approaches, and methods for working with web archival data.</p>
</li>
<li>
<p>Helped participants make a stronger case for web archiving within their institutions and communities.</p>
<p><em>Exposed attendees to diverse interdisciplinary perspectives</em></p>
</li>
<li>
<p>Small group collaboration allowed participants the opportunity to work with a diverse range of perspectives.</p>
</li>
<li>
<p>Participants had to adapt and communicate across disciplinary lines.</p>
<p><em>Fostered community formation</em></p>
</li>
<li>
<p>Created a space in which individuals would not have otherwise met.</p>
</li>
<li>
<p>Participants are equipped with skills to be ambassadors within the broader web archiving community. </p>
</li>
<li>
<p>The datathon also influenced a shift for participants who attended multiple Archives Unleashed datathons, as they organically grew into a mentorship’s role both at the event and within the larger community.</p>
<p><em>Fostered a sense of belonging</em></p>
</li>
<li>
<p>Event activities and elements were designed to foster an open and welcoming atmosphere.</p>
</li>
<li>
<p>The sticky note exercise, adapted from participatory design, helped individuals find colleagues and topics they most identify with. </p>
</li>
<li>
<p>Working together in organically formed groups, individuals felt like their frustrations, limitations, and struggles with technical barriers could be empathised by others due to this shared experience.</p>
<pre><code> Areas for Improvement    
</code></pre>
</li>
<li>
<p>Two-day time limit and diminished energy on projects after the physical meeting is a constraint of the datathon model.</p>
</li>
<li>
<p>Limited discussions on broader issues relating to policy, ethics, or theory.</p>
</li>
<li>
<p>Technical expertise within each group varied.</p>
<pre><code> The Road Ahead    
</code></pre>
</li>
<li>
<p>Overall optimism for future directions of web archiving: increased access to web archives and promise of scholarship.</p>
</li>
<li>
<p>Consensus about the value and significance that web archives bring to a wide range of disciplines.</p>
</li>
<li>
<p>Time lag between archival dataset creation and their use.</p>
</li>
</ul>
<h2 id="knowledge-of-web-archiving-and-backgrounds">Knowledge of Web Archiving and Backgrounds</h2>
<p>The first step in understanding a community has to be understanding its composition. Where do members come from? What technical or social knowledge of web archives did they have? Accordingly, we asked participants about their varied professional backgrounds and scholarly interests. As attendees at a datathon, the participants interviewed all in some way worked with web archives in their current or most recent position: whether they were conducting crawls for institutional collections, providing access to researchers, or creating tools to interact with web archival data. While working in the field, however, when asked about the ways in which they had become familiar with web archiving, many pointed to an accumulation of knowledge based on practice through work experience. Knowledge, in a sense, was a result of trial-and-error and through professional positions, rather than through formal higher education training. This was seen across the board from those with degrees in the LIS, humanities, engineering, and computer science disciplines. Indeed, experience was often obtained outside of the field but applied to it. For example, one librarian working at a large national library (R1) had previously worked with special media formats at scale, an experience which lent itself well to web archiving. Another librarian working at a large research library (R2) had been working with social media data. With both examples, we see a pattern where previous experience with diverse digital formats carried over to their work in web archiving.</p>
<p>All eight participants agreed that the Archives Unleashed datathon had increased their knowledge of web archiving (generally related to higher-level thinking – theoretical aims, goals, processes) outside of any technical skills they learned. One respondent, a researcher at a large U.S. public university (R3), noted that the datathon provided foundational knowledge of some of the working parts and concepts within web archiving, for instance, WARC files, derivatives, as well as the possibilities of web archiving analysis. R2 emphasized that the datathon validated for them the connection between web archiving and scholarly research. For R2, as well as two additional interviewees – one a graduate student and digital humanities developer (R4) and another a digital scholarship librarian at a Canadian university (R5) – the event brought an understanding of how researchers may want to use and look at web archive data. This was important for both tool developers as well as librarians to help inform their ways to support users and colleagues. Finally, another participant – an archivist at a large U.S. private university (R6) – remarked that the event had helped them to think about “what we might change on our end as the staff members creating these collections, to allow for this other kind of research that we hadn&rsquo;t really supported before at our institution.”</p>
<h2 id="impact-of-the-datathon-on-professional-practice">Impact of the Datathon on Professional Practice</h2>
<p>With backgrounds established, we wanted to explore the degree to which our events – as an exercise in community engagement – had meaningful impacts on professional practices. We can cluster their responses into four main themes. First, the datathons contribution to skill-building. Secondly, how the event exposed attendees to diverse, interdisciplinary perspectives. Thirdly, how the events fostered community formation; and fourthly – and finally – how our events fostered for some a sense of belonging. We explore each of these in turn.</p>
<p>Despite the varying levels of experience that participants brought to the datathon, all agreed there were several important lessons and skills they took away from their datathon experience. This meant building or adding to their technical repertoires, even amongst those with technical expertise coming in. For those who work with web archive collections as part of their professional practice, such as R3 and R4, their primary focus during their everyday work is to collect information. This means that they rarely have opportunities to, really dig in and work with the data up close. Indeed, this ability to work with the data itself is where the datathon model shined for participants. By digging into the mechanics of data and the intricacies of tools in a small, supportive environment, group collaborations led to discussions around the challenges, limitations, and other structural practices of web archiving. Notably, the wrap-up activity – where the groups presented their final projects – was noted by participants as an opportunity to showcase final projects and to share information.</p>
<p>A number of specific tools were learned, as well. All respondents mentioned the Archives Unleashed Toolkit. Social media analysis tools such as twarc (<a href="https://github.com/DocNow/twarc">https://github.com/DocNow/twarc</a>), APIs, and approaches to visualization were also common answers. Notably, R4 noted the importance of learning Spark:  “prior to the second workshop, I really had never used Spark before. I kind of knew what it was but just had never spent the time to learn how to use it,”  and argued that future work and collaborations would not have emerged if they had not attended the datathon and learned how to use Spark. The overall impact of the event on lowering barriers was also noted by R8, a graduate student at a large private U.S. university, who expressed ongoing efforts to  “help archivists to feel more confident or less afraid of the technology of command line and things like that so that they could use Archives Unleashed and the tools that you&rsquo;re developing. Don&rsquo;t be afraid of it. This is great. That&rsquo;s the great thing about the datathon, right?”</p>
<p>Another crucial dimension of the datathon was helping participants make a stronger case for why professions needed to adopt these methods and approaches to web archiving. In other words, it allowed them to better advocate for the value of web archiving within their institutions and communities. As R1, the librarian from a national library described, institutions and organizations are  “not as likely to put money towards it or they’re not as likely to just believe in it”  without tangible outputs or use cases like one might see coming out of a datathon. This was echoed by R6, an archivist at a large U.S. private university, who indicated that it is hard to advocate for web archiving  “when administrators often want statistics and usage numbers and quantitative reasons to support the work, we can’t necessarily say that people are using web archive yet.”  As R5, the librarian at a mid-sized Canadian university considered, institutions may not also  “realize the commitment that’s needed to do this well.”  Through tangible examples, R5 expressed that though their mandate as a digital scholarship librarian,  “I find that explaining to them why this is important, how they can use this technology, all of a sudden it becomes less of an abstract fear and more of an oh, I see why this is important and why I want to learn it.”  By being able to showcase actual projects from the datathons, participants were able in turn to bring lessons and approaches back to their institutions.</p>
<h2 id="exposure-to-diverse-perspectives">Exposure to Diverse Perspectives</h2>
<p>Similarly, participants regularly commented on the benefit of working in small groups (discussed in the previous section) with a diverse range of perspectives unified only in some cases by a shared interest in web archiving. Given the event, this was sufficiently cohesive. Indeed, as R7 (a graduate student and developer at a mid-sized U.S. public university) noted, it represented an  “opportunity to network with like-minded people.”  The range of perspectives and background groups was in and of itself a real boon. R7 spoke at this at length, highlighting that:</p>
<blockquote>
<p>You don’t realize how much you don’t understand until you try to explain something that you think is obvious. So that’s something I think really came out when I was working with teams, was trying to convey things that seemed obvious to me, or intuitive to me, but then other folks were rightly questioning why would you do it that way? and I would say, well how else would you do that? Basically, going back and forth was really educational.</p>
</blockquote>
<p>This back-and-forth would be represented in the final projects as well, as R7 stressed how this creativity was borne out in the final presentations. For example, a researcher with a flair for visualization could make a concept come alive. R7 reflects, in these final presentations, participants could  “transform[ing] topics that you never really thought you could take a look at and making them into visual representations.”  Similarly, connections between different disciplines could pay off in the long term. R8, a graduate student, was able to connect with an archivist as part of their datathon team, leading to critical insights around data brokering, collections strategies, and beyond; as she expressed,  “a lot of things sort of came together just through that one interaction with her. And that led to a lot of insights for me,”  noting that they corresponded after the event as well.</p>
<h2 id="community-formation">Community Formation</h2>
<p>While us organizers had the initial goal of building community around the specific Archives Unleashed Tools (i.e., community  <em>with</em>  our project), coincidently, we were able to encourage connections  <em>within</em>  the community that did not involve our tools. Indeed, part of this was simply breaking down disciplinary barriers. One of our attendees, R6, an archivist at a large private university, saw the event as helping to  “break down the librarian, archivist versus scholar researcher dichotomy &hellip; all of [these] people are in the same room equally as peers.”</p>
<p>Indeed, through the interviews, we saw community formation taking shape as participants emerged as ambassadors within the web archiving community, their home institutions, and professional communities. For example, R3 – a faculty researcher at a large U.S. university – drew a direct relationship between the datathon and their current scholarly practices. For them, the skills and connections learned at the event allowed them to expose colleagues to web archiving practices and demonstrate the possibilities that WARC files provided. Another, R2, the librarian developer at a private U.S. university, noted that it informed their own development work on a related project that used WARCs and some of the web archiving APIs. When it came time to work on this project, the datathon  “gave us confidence that when we were trying to do the same thing, that it was the right way to go.”</p>
<p>Within the series of events that were run, participants who attended multiple events matured in their participation as they could mentor new attendees. R7 noted that the first time they attended the event, they were  “still learning what a WARC was,”  and R3 reflected that  “the projects that [they] worked on became increasingly more technical over multiple datathons.”  Crucially, these repeat attendees shifted organically into a mentorship role, having the confidence to help other group members based on the technical skills and expertise developed iteratively over datathons.</p>
<h2 id="sense-of-belonging">Sense of Belonging</h2>
<p>In general terms, the world of web archiving is unfamiliar and unintuitive to researchers, librarians, archivists, and even some tool builders. At the same time, those who work in the field are quickly confronted with an overwhelming and intimidating amount of data in a relatively idiosyncratic file format (in that WARCs are seldom encountered outside of the web archiving field). To lower these barriers, we want to inculcate a sense of belonging.</p>
<p>A sense of belonging has been defined in a critical article by Hagerty et al. as  “the experiences of personal involvement in a system or environment so that persons feel themselves to be an integral part of that system or environment”   <sup id="fnref:29"><a href="#fn:29" class="footnote-ref" role="doc-noteref">29</a></sup>. Peer support and a sense of belonging have been identified as two critical factors for overall mental health as well amongst students, and are essential goals of our events as well (drawing on our experience as educators as well) <sup id="fnref:30"><a href="#fn:30" class="footnote-ref" role="doc-noteref">30</a></sup>. A sense of belonging is critical to whether an individual participates in a broader community.</p>
<p>The importance of the sticky note exercise in creating a sense of belonging came across in the interviews. As R7 explained,  “we did the exercise on the board with the Post-Its and things like that, and I think that really helped me find a group and also find something of interest.”</p>
<p>This was also nurtured through sustained group work. As R3 noted, there was a recognition that the  “majority of people were in the same boat because working with web archives is difficult.”  By being able to work together, the events for many  “normalize[d] the experience with the tools and the technology and limitations”  as R3 explained. R4, a graduate student and developer at a large U.S. university, saw the event as akin to a  “travelling web archive or reading room,”  and by making data accessible to teams (many of whom who could now talk to the curating librarian responsible for the data) we were akin to a broker of data. Indeed, these partnerships were mutually beneficial as participants were able to access web archive data, at the same time, attention was brought to unique institutional datasets that exist, but may not be widely known about.</p>
<h2 id="areas-for-improvement">Areas for Improvement</h2>
<p>While participants were positive around the overall impact – conveyed through post-event surveys and interviews– ideas did arise around how this event model could either be better refined or perhaps would not fit all of our community-building goals. Some of these, notably the diverse range of technical expertise or disciplines, as well as the slowed momentum of collaboration after the event <sup id="fnref1:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>, were expected. Other suggestions led us to engage more deeply with pedagogical literature.</p>
<p>The focus on data, implicit in the name datathon itself, had some downsides. It led to an emphasis on technical issues and working with discrete datasets themselves, which had the effect of preventing discussion on broader issues to do with policy, ethics, or theory. R3 gave an important example of  “issues of representation and what this means and what these gaps mean that aren&rsquo;t data specific”  as something that did not feel welcome at the event as a topic of discussion. One could look at a dataset, but this left out questions that might (as R3 again noted) allow us to  “look at the whole [web archiving] life cycle.”</p>
<p>With such technical questions and the overall ethos, issues were also raised around the diversity of technical expertise. As teams largely formed themselves through the collaborative team building exercise, some teams ended up without much in the way of technical strength. R6, an archivist at a large U.S. private university, found themselves in such a group and wished that there had been more of an expectation for peers (even if they were in another group) to help  “peers who are struggling to get them more towards a middle ground rather than identifying the people who are doing a really stellar job and pushing them farther ahead from the folks who are struggling.”  We were inspired by this, and in future grant-funded work, are pursuing a formal mentorship program to help pair expertise with researchers in a more targeted and inclusive way.</p>
<p>Finally, the loss of momentum after the event was a shortcoming identified by interviewees and our project team alike. The short two-day nature of the event means that it largely relies heavily on exploratory projects that may not have a readily apparent route forward to a larger study or publication. It also reflects the difficult nature of the tools, and how outside of the resources provided by the datathon, they may continue to be too difficult. However, we need to consider that engagement is not just a short-term milestone, nor is it just using a particular toolkit or approach. Rather, it is again  <em>community</em> . Several participants noted that while they do not use the tools, they do have them tucked away as a resource to recommend when a patron or a colleague might want to work with web archival data.</p>
<h2 id="the-road-ahead">The Road Ahead</h2>
<p>Participants were all asked about their thoughts on the future of web archiving. While broad in nature, this question was met with optimism from respondents who were generally excited about the direction of web archiving and the opportunities for scholars, access providers, and tool builders (and everyone in between). On the collection side, R2 (librarian developer at a large national library) saw a shift from national to institutional collecting, which would notably involve  “large growth in subject-based collecting that is likely to be of more value for both the historical record and for scholarly work.”  As well, there was a consensus that web archive collections hold immense value for current and future researchers, and web archives have – as R3 explained –  “a potential to add a lot more rigor and also allow us to share data and in lots of other spaces.”  We also saw that most participants were happy about the Archives Unleashed Project; R7, for example, noted that they felt  “feel very good about the tools that you’ve been developing, like those are exactly the right sorts of tools and tearing down the right sorts of technical barriers that are going to make the adoption happen faster.”</p>
<p>Yet there was also a sense that the process of web archiving analysis might be ahead of where researchers are right now; that we are, perhaps, along with the web archiving field more generally, laying the groundwork for future work. As R2 notes, while there is undoubtedly  “long-term massive significance to [this] scholarly activity &hellip; [it will] happen in a timeframe that will be frustrating long &hellip; [but the pace of adoption] should not at all be taken as a reflection on the significance of the work.”  R1, a librarian at a national library, remarked that Archives Unleashed  “opens the door to (those computational) conversations because it starts to break down [barriers to] the technical part.”  Indeed, use of archival collections often lags behind their creation; as R6 argued,  “in archives we take a pretty long view of time and history, and just because (few) are using it now doesn’t mean that people aren’t going to want it (web archive data).”</p>
<h2 id="conclusions-an-emerging-community-model">Conclusions: An Emerging Community Model</h2>
<pre><code>Summary of the Archives Unleashed Community Model approach    Stage  Archives Unleashed Activities      Engagement Preparation        
</code></pre>
<p>_Stage 1: Scope _</p>
<ul>
<li>
<p>Identify the problem or question.</p>
</li>
<li>
<p>Identify stakeholders that comprise the community.</p>
</li>
<li>
<p>Leveraging experience from our previous Warcbase project.</p>
</li>
<li>
<p>Identifying needs and barriers of digital humanities scholars.</p>
</li>
<li>
<p>Categorize stakeholder groups: researchers; digital access content providers; tool builders.</p>
<p><em>Stage 2: Inform</em></p>
</li>
<li>
<p>Provide information so that the community can understand present problems and potential solutions</p>
</li>
<li>
<p>Share information and summaries around our project objectives, and proposed pathway.</p>
</li>
<li>
<p>Establish information sharing platforms with regular engagement. </p>
</li>
<li>
<p>Offer opportunities for contributions.</p>
</li>
<li>
<p>Provide regularly monitored lines of communication.</p>
<p><em>Stage 3: Consult</em></p>
</li>
<li>
<p>Conduct an open dialogue with the purpose of gathering feedback from the community.</p>
</li>
<li>
<p>This stage involves the project leadership asking questions and actively listening. </p>
</li>
<li>
<p>Assembling and consulting with an Advisory Board.</p>
</li>
<li>
<p>Conducting surveys and interviews.</p>
<pre><code> Community Outreach        
</code></pre>
<p><em>Stage 4: Involve</em></p>
</li>
<li>
<p>Work with the community to ensure community concerns and wishes are considered.</p>
</li>
<li>
<p>For the Archives Unleashed Project, we interpreted this stage as a way of working collaboratively with the community on tool development.</p>
</li>
<li>
<p>A primary undertaking has been directly involving community members in our Archives Unleashed datathons. </p>
</li>
<li>
<p>Sharing skills and practices with the community, growing out of datathon experiences.</p>
<p><em>Stage 5: Collaborate</em></p>
</li>
<li>
<p>Collaborate and establish partnerships.</p>
</li>
<li>
<p>Foster interdisciplinary collaborations, both peer-to-peer and peer-to-institution.</p>
</li>
<li>
<p>Partnerships with academic institutions for the purpose of resource sharing and exposure, specifically web archive collections.</p>
</li>
<li>
<p>Datathons offered a significant connection between institutions who created web archive collections and researchers who wanted to use them.</p>
</li>
<li>
<p>Broader collaboration and partnerships fostered interoperability with other web archiving projects and tools.</p>
<p><em>Stage 6: Empower</em></p>
</li>
<li>
<p>Encourage and support scholars in building confidence and skills needed to work with web archives.</p>
</li>
<li>
<p>Datathons empowered individuals through learning opportunities.</p>
</li>
<li>
<p>Project invested in developing accessible learning-based resources, for instance learning guides, video tutorials, and documentation. </p>
</li>
<li>
<p>Resources aimed at empowering users to feel comfortable and confident when exploring web archives</p>
</li>
</ul>
<p>Through this, an emerging community model takes shape for both the Archives Unleashed Project as well as our role within the broader ecosystem; this has implications for other digital humanities projects. It is summarized in Table 3. Archives Unleashed is located within the web archiving ecosystem, with important links to the digital humanities, historical, computational social sciences, and computer science fields. From these interviews and our own experiences, we understand that  “if you build it, they will come”  mentality does not work, and active engagement to build relationships and communities is essential. The goal through datathons was to build a community around our tools that would complement and contribute to the other connections taking shape.</p>
<p>From our earlier discussion, we identified two community engagement frameworks – the Water Science Software Institute’s Open Community Engagement Process (OCEP) and the International Association for Public Participation’s (IAP2)  “Spectrum of Public Participation,”  and brought them together in our Archives Unleashed community model. We can see that our combination of the OCEP and IAP2 models works reasonably well, capturing the breadth of activities taking place through our datathons. IAP2 captures the main categories, more or less; and OCEP captures the need to see the cycle as an infinite loop, where we continually navigate and improve as we move through the community engagement model.</p>
<p>Overall, the Archives Unleashed Project has successfully built a community around the scholarly practice of exploring web archives and created unique opportunities for individuals with a wide variance in backgrounds, skills, and experience to connect and collaborate. All participants identified at least one, and in many cases several, tangible skills that they learned. They also spoke to the important aspect of having the opportunity to forge new collaborations, which have positively impacted their professional practices and research collaborations. It also bears out the hackathon literature, which suggests the ability for these events to  “enable building a community of users and strategic networks,”   <sup id="fnref2:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> seen in our discussions around community formation, a sense of belonging, and skills acquired by participants through the Archives Unleashed datathons.</p>
<p>Without community engagement, projects would live and work in silos. The models we engage with here, as well as the modified version that we advance, are all part of the broader ways in which our projects and others in the digital humanities create a space for and recognize collaborative and interdisciplinary work.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Briscoe., G. and Mulligan, C.  “Digital Innovation: The Hackathon Phenomenon.”  Working paper No.6, Creativeworks London (2014). Available at: <a href="http://www.creativeworkslondon.org.uk/wp-content/uploads/2013/11/Digital-Innovation-The-Hackathon-Phenomenon1.pdf">http://www.creativeworkslondon.org.uk/wp-content/uploads/2013/11/Digital-Innovation-The-Hackathon-Phenomenon1.pdf</a>.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Komssi, M., Pichlis, D., Raatikainen, M., Kindström, K., and Järvinen, J.  “What are Hackathons for?”    <em>IEEE Software</em> , 32(5) (2015): 60–67.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Concilio, G., Molinari, F. and Morelli, N.  “Empowering Citizens with Open Data by Urban Hackathons,”  Conference for E-Democracy and Open Government (CeDEM), Austria 2017.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Pe-Than, E. P. P., Nolte, A., Filippova, A., Bird, C., Scallen, S., and Herbsleb, J.  “Designing Corporate Hackathons with a Purpose: The Future of Software Development,”    <em>IEEE Software</em> , 36(1) (2019): 15–22.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>Milligan, I.  <em>History in the Age of Abundance? How the Web is Transforming Historical Research</em> . McGill-Queen’s University Press, Kingston and Montreal (2019).&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>Brügger, N.  “Web Archiving: Between Past, Present, and Future.”  In R. Burnett, M. Consalvo, and C. Ess, (eds),  <em>The Handbook of Internet Studies</em> . Wiley-Blackwell, Malden (2010), pp. 24–42.&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>Ruest, N., Lin, J., Milligan, I., and Fritz, S.  “The Archives Unleashed Project: Technology, Process, and Community to Improve Scholarly Access to Web Archives,”    <em>Proceedings of the Joint Conference on Digital Libraries, Proceedings of the 19th Joint Conference on Digital Libraries</em> , Wuhan, China, August 2020.&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>Ruest, N., Fritz, S., Deschamps, R., Lin., J., and Milligan, I.  “From Archive to Analysis: Accessing Web Archives at Scale Through a Cloud-Based Interface,”    <em>International Journal of Digital Humanities</em>  (2021). <a href="https://link.springer.com/article/10.1007/s42803-020-00029-6.">https://link.springer.com/article/10.1007/s42803-020-00029-6</a>.&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>Whaley, L., Born, P. and Whaley, D.  <em>Approaches to Measuring: Community Change Indicators</em> . Tamarack Institute, (2010). Available at: <a href="https://www.tamarackcommunity.ca/hubfs/Resources/Publications/Approaches%20to%20Measuring%20Community%20Change%20Indicators.pdf?hsCtaTracking=b162a8d6-6083-4f31-9d02-16b532e25e82%7C60a4dede-75a7-4edb-a91a-a5c0819b3c69">https://www.tamarackcommunity.ca/hubfs/Resources/Publications/Approaches%20to%20Measuring%20Community%20Change%20Indicators.pdf?hsCtaTracking=b162a8d6-6083-4f31-9d02-16b532e25e82%7C60a4dede-75a7-4edb-a91a-a5c0819b3c69</a> (Accessed: 21 May 2020).&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>Moore, T., McDonald, M., McHugh-Dillo, H., and West, S.  “Community Engagement: A Key Strategy for Improving Outcomes for Australian Families,”    <em>Australian Institute of Family Studies</em> , Melbourne, Victoria. Available at: <a href="https://trove.nla.gov.au/version/226801100">https://trove.nla.gov.au/version/226801100</a> (Accessed: 21 May 2020).&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>Cobigo, V., Martin, L., and Mcheimech, R.  “Understanding Community,”    <em>Canadian Journal of Disability Studies</em> , 5(4) (December 2016): 181-203.&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p>Fredericks, J., Caldwell, G. A. and Tomitsch, M.  “Middle-Out Design: Collaborative Community Engagement in Urban HCI,”   Proceedings of the 28th Australian Conference on Computer-Human Interaction, Association for Computing Machinery (OzCHI ’16), Tasmania, Australia 2016.&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13">
<p>Mitra, N.  “Community Engagement Models in Real Estate — A Case Study of Tata Housing Development Company Limited,”    <em>Asian Journal of Business Ethics</em> , 5(1) (2016): 111–138.&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14">
<p>Joosten, Y., Israel, T., Williams, N., Boone, L., Schlundt, D., Mouton, C., Dittus, R., Bernard, G., and Wilkins, C.  “Community Engagement Studios: A Structured Approach to Obtaining Meaningful Input from Stakeholders to Inform Research,”    <em>Academic Medicine: Journal of the Association of American Medical Colleges</em> , 90(12) (2015): 1646–1650.&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:15">
<p>Fernandez, M., Piccolo, L., Maynard, D., Wippo, M., Meili, C., and Alani, H.  “Talking Climate Change via Social Media: Communication, Engagement and Behavior,”    <em>Proceedings of the 8th ACM Conference on Web Science, Association for Computing Machinery (WebSci ’16)</em> , Hannover, Germany, May 2016.&#160;<a href="#fnref:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:16">
<p>Decker, A., Eiselt, K. and Voll, K.  “Understanding and Improving the Culture of Hackathons: Think Global Hack Local,”    <em>IEEE Frontiers in Education Conference (FIE)</em> . El Paso, Texas, 2015.&#160;<a href="#fnref:16" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:17">
<p>Link, G. J. P. and Jeske, D.  “Understanding Organization and Open Source Community Relations through the Attraction-Selection-Attrition Model,”    <em>Proceedings of the 13th International Symposium on Open Collaboration, Association for Computing Machinery (OpenSym ’17)</em> , Galway, Ireland 2017.&#160;<a href="#fnref:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:18">
<p>Gribb, W. J.  “Field Experience through Community Engagement: A Model and Case Study,”    <em>The Professional Geographer</em> , 70(2) (2018): 298–304.&#160;<a href="#fnref:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:19">
<p>Reid, H. and Howard, V.  “Connecting with Community: The Importance of Community Engagement in Rural Public Library Systems,”    <em>Public Library Quarterly</em> , 35(3) (2016): 188–202.&#160;<a href="#fnref:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:20">
<p>Tharani, K.  “Shifting Established Mindsets and Praxis in Libraries: Five Insights for Making Non-Western Knowledge Digitally Accessible through Community Engagement,”    <em>Canadian Journal of Academic Librarianship</em> , 4 (2019): 1–13.&#160;<a href="#fnref:20" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:21">
<p>Leiuen, C. D. and Arthure, S.  “Collaboration on Whose Terms? Using the IAP2 Community Engagement Model for Archaeology in Kapunda, South Australia,”    <em>Journal of Community Archaeology &amp; Heritage</em> , 3(2) (2016): 81–98.&#160;<a href="#fnref:21" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:22">
<p>Ahalt, S., Minsker, B., Tiemann, M., Band, L., Palmer, M., Idaszak, R., Lenhardt, C., and Whitton, M.  “Water Science Software Institute: An Open Source Engagement Process,”    <em>5th International Workshop on Software Engineering for Computational Science and Engineering (SE-CSE)</em> , San Francisco, California, May 2013.&#160;<a href="#fnref:22" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:22" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:22" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:23">
<p>Powell, D., Gilliss, C., Hewitt, H., and Flint, E.  “Application of a Partnership Model for Transformative and Sustainable International Development,”    <em>Public Health Nursing</em>  (Boston, Mass.), 27(1) (2010): 54–70.&#160;<a href="#fnref:23" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:24">
<p>Lin, J., Milligan, I., Wiebe, J., and Zhou, A.  “Warcbase: Scalable Analytics Infrastructure for Exploring Web Archives,”    <em>ACM Journal of Computing and Cultural Heritage</em> , 10(4) (2017): 22:1-22:30.&#160;<a href="#fnref:24" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:25">
<p>“IAP2 Spectrum of Public Participation”  (2018). Available at: <a href="https://cdn.ymaws.com/www.iap2.org/resource/resmgr/pillars/Spectrum_8.5x11_Print.pdf">https://cdn.ymaws.com/www.iap2.org/resource/resmgr/pillars/Spectrum_8.5x11_Print.pdf</a>.&#160;<a href="#fnref:25" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:25" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:25" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:26">
<p><a href="https://cdn.ymaws.com/www.iap2.org/resource/resmgr/pillars/Spectrum_8.5x11_Print.pdf">https://cdn.ymaws.com/www.iap2.org/resource/resmgr/pillars/Spectrum_8.5x11_Print.pdf</a>&#160;<a href="#fnref:26" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:27">
<p>Milligan, I., Casemajor, N., Fritz, S., Lin, J., Ruest, N., Weber, M., and Worby, N.  “Building Community and Tools for Analyzing Web Archives through Datathons,”    <em>Proceedings of the 18th Joint Conference on Digital Libraries</em> , Champaign, Illinois, June 2019.&#160;<a href="#fnref:27" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:27" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:28">
<p>Walsh, G., Foss, E., Yip, J., and Druin, A.  “FACIT PD: A Framework for Analysis and Creation of Intergenerational Techniques for Participatory Design,”    <em>Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, Association for Computing Machinery (CHI ’13)</em> , Paris, France.&#160;<a href="#fnref:28" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:29">
<p>Hagerty, B., Lynch-Sauer, J., Patusky, K., Bouwsema, M., and Collier, P.  “Sense of Belonging: A Vital Mental Health Concept,”    <em>Archives of Psychiatric Nursing</em> , 6(3) (1992): 172–177.&#160;<a href="#fnref:29" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:30">
<p>McBeath, M., Drysdale, M. T. and Bohn, N.  “Work-Integrated Learning and the Importance of Peer Support and Sense of Belonging,”    <em>Education &amp; Training</em> , 60(1) (2018): 39–53.&#160;<a href="#fnref:30" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry><entry><title type="html">Founding the Special Interest Group Audio-Visual in Digital Humanities: An Interview with Franciska de Jong, Martijn Kleppe, and Max Kemman</title><link href="https://rlskoeser.github.io/dhqwords/vol/15/1/000542/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://rlskoeser.github.io/dhqwords/vol/15/1/000542/</id><author><name>Stefania Scagliola</name></author><published>2021-03-05T00:00:00+00:00</published><updated>2021-03-05T00:00:00+00:00</updated><content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>The special issue was initiated by the <a href="https://avindhsig.wordpress.com">Special Interest Group Audio/Visual</a> in Digital Humanities, a group that brings together community participants who share an interest in audio and/or visual data. Yet, how did the SIG come into formation? The following is an interview about why and how the AVinDH SIG was founded. The interview was conducted on June 20th 2020 via the online application Microsoft Teams.</p>
<p><strong>Stefania:</strong>  How did you sense the need for such a SIG in the context of DH?</p>
<p><strong>Franciska:</strong>  We were interested in a form of dialogue with scholars working with audiovisual data and computational methods. As a linguist who had built up expertise in search technology to open up spoken word archives, I had already expanded my focus from data related to the study of language to audio and video collections that are part of the cultural heritage domain. Both Erasmus University Rotterdam and the Netherlands Institute for Sound and Vision were keen on presenting their work stemming from two international projects, AXES and Post-Yugoslav Voices. The best way to engage with peers, in our view, was to organise a workshop for the audience that we envisioned. However, the overwhelming interest and enthusiasm from the side of the presenters and the audience was a surprise to us. We were clearly filling a gap. After having heard about the possibility to set-up a Special Interest Group that could be endorsed by ADHO (Alliance of Digital Humanities Organisations), we immediately took action and submitted a <a href="https://avindhsig.wordpress.com/background">proposal</a>.</p>
<p><strong>Stefania:</strong>  I specifically recall how one of the presenters involved in research on sound archives confided to the audience that &ldquo;at last she did not feel as the odd one out at a DH conference&rdquo;. Does it mean that DH approaches to audiovisual data had a backlog compared to textual data?</p>
<p><strong>Franciska:</strong>  I don&rsquo;t think so. I think they operated in different circles. They presented their work at conferences on media studies, sound studies, oral history, and computer science. They published their work in journals that stemmed from these specific networks. If I think of the first projects that were set up in the Netherlands to apply state-of-the-art software to open up cultural heritage archives, we published in proceedings of conferences about software development for the study of language, or journals about digitisation.</p>
<p><strong>Max</strong> : I agree, but at the same time, when compared to the tools that existed at the time to search for patterns in massive amounts of text, the audiovisual realm did have a backlog. In the workshop proposal that we put together for DH2014, we claimed that it was a matter of urgency to develop new tools for extracting information from audiovisual archives in the same way as could be done with text, particularly given the prospect of the exponential growth of (moving) images on the web that was envisioned in 2014. We could, at the time, still refer to the audiovisual as a &ldquo;blind&rdquo; medium for retrieval, quoting Sandom and Enser, because of the need for sequential viewing to extract knowledge from the source (Sandom and Enser, 2001). It is remarkable how in only six years time this claim seems no longer valid given the progress in Computer Vision and Speech Retrieval.</p>
<p><strong>Stefania:</strong>  The central themes of the subsequent workshops seem to reflect this progress. We tackled obstacles for the integration of AV in DH at DH2014 in Lausanne, formally installed the SIG in DH 2015 in Sydney, discussed multimodality at DH016 in Krakow, explored computer vision at DH2017 in Montreal, hosted a tutorial on Distant Viewing at DH2018 in Mexico City, and set-up hands-on sessions with audiovisual research infrastructures at DH2019 in Utrecht. Could you say that the SIG had a considerable role in this progress?</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000542/resources/images/figure01.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000542/resources/images/figure01_hucae853d9729427dfc421b81f523cdfed_42036_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000542/resources/images/figure01_hucae853d9729427dfc421b81f523cdfed_42036_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000542/resources/images/figure01.png 768w" 
     class="landscape"
     ><figcaption>
        <p>Number of works accepted at the ADHO DH conferences between 2013-2018 related to A/V and Multimedia. Figure provided by Scott Weingart.
        </p>
    </figcaption>
</figure>
<p><strong>Martijn:</strong>  I think that gives too much credit to the SIG, as if we had formulated specific objectives from the start and were consciously selecting topics and papers that reflected the state-of-the-art. Our approach was actually quite pragmatic. We wanted to create a structure for people with similar research interests to meet, either through the mailing list, or via the yearly conference. I think what was key for the role of the SIG, was its institutional embedding. Being endorsed by ADHO meant a secured slot in each pre-DH conference program that could draw the attention of scholars who would otherwise primarily share their work within their traditional mono-disciplinary scholarly realm.</p>
<p><strong>Franciska:</strong>  Computational approaches to audiovisual data were gaining momentum around that time. You could say that the SIG presented itself at the optimal moment, in contingency with other important developments, such as national and cross national funding for opening up audiovisual heritage for the general audience and scholars.</p>
<p><strong>Stefania:</strong>  Could you give some examples of these initiatives, and were key people in these projects involved in our workshops?</p>
<p><strong>Martijn:</strong>  Well the first key lecture was given by prof. Andreas Fickers, who was already a central figure in the EU project on cultural heritage of television, EU Screen, and now runs the Centre for Contemporary and Digital History (C2DH) in Luxemburg. Scholars working on building up the large scale research infrastructure for humanities research such as CLARIAH in the Netherlands, including professor of Digital Culture Heritage Julia Noordegraaf, were well represented at all the SIG&rsquo;s workshops. This applies also for the Media Ecology Project of Mark Williams from Dartmouth, and for Lauren Tilton and Taylor Arnold&rsquo;s work on Distant Viewing. Yet, there is no way of telling whether our program really reflected the state-of-the-art in this field. I recall that at some point at DH2017 in Montreal I approached a researcher who I expected would have had an interest in presenting at our SIG pre-conference workshop, but the reply was that he preferred to present at a slot within the conference. This gave me food for thought about whether our SIG served as a kind of incubator, for scholars that in a next stage moved on to a slot in the main conference.</p>
<p><strong>Stefania:</strong>  With regard to representativeness, you are right that we cannot make any claim, but I can offer a rough sketch of the contributions from 2014 until now, based on a review of the workshops at the annual DH conference. What I see is that most papers or short talks were about innovations in technologies to extract visual or aural features either from a single collection, from a homogenous archive, or from an archive with collections in different formats. Next in line was the topic of annotation in music and audiovisual collections. The innovation in these types of papers were represented by the possibility to automatically enrich manual annotations with computer generated links by applying the principles of Linked Open Data. There were some papers that focused less on tool development and more on the analysis of content, like the study of changes in voices in news coverage or the use of colours in film. Specifically at the workshop in Krakow, the theme of multimodality featured contributions that evolved around education and pedagogy. There was also some interest in restoration, preservation, and loss of data on the web. Other topics across the workshops included copyright, oral history, art, metadata and interface design. The most striking shift in the programme of the SIG, that in a way reflects the increasing maturity of the field, is from a space with papers presentation to hands-on tutorials. In Mexico 2018 and Utrecht 2019, the workshop followed more closely a traditional workshop format and offered opportunities to try out the tools that had been envisioned in talks in the previous workshops.</p>
<p><strong>Max:</strong>  This may sound like the field is constantly growing within DH, but when you step out of our AVinDH bubble and look at the statistics that Scott Weingart collected of the previous DH conferences, you can see that it is still a niche, well established, but with an abundance of alternatives to present work on computational approaches to AV. There are a lot of factors that play a role in determining the interest of scholars working with audiovisual data to profile themselves as digital humanists.</p>
<p>It would really be worthwhile to conduct a more detailed study about the type of contributions, the affiliations, the platforms where work is published, and the professional trajectories of scholars involved in this type of DH work to get a better sense of how AV in DH researchers are advancing the field.</p>
<p><strong>Stefania:</strong>  To conclude, I would like to pay tribute to our late Canadian colleague Clara Henderson, an ethnomusicologist working at Indiana University, who joined the SIG as a representative of the American hemisphere in Sydney DH 2015. This was where the SIG AvinDH was formally approved by ADHO. Her background in ethnomusicology helped widen the scope of the SIG beyond the realm of media studies. We were deeply saddened when we received the news in Autumn 2016 that Clara had passed away after a short illness. Thinking of all the plans with the SIG that she had in mind and tasks that she was willing to take up, I would like to dedicate this little thread in the immense tapestry of DH scholarship that brings people together across the continents to her memory.</p>




























<figure ><img loading="lazy" alt="Picture of Clara Henderson." src="/dhqwords/vol/15/1/000542/resources/images/figure02.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000542/resources/images/figure02_hu439103422a6f16f788db72fa1559579b_30093_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/15/1/000542/resources/images/figure02_hu439103422a6f16f788db72fa1559579b_30093_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/15/1/000542/resources/images/figure02.jpg 400w" 
     class="landscape"
     ><figcaption>
        <p>Clara Henderson&rsquo;s photo on her Twitter account.
        </p>
    </figcaption>
</figure>
<p><em>With special thanks to Scott Weingart and Max Kemman for providing the visualisation of AV in DH conferences and to the members of the former Steering Group who were willing to share their memories and insights.</em></p>
]]></content></entry><entry><title type="html">From close listening to distant listening: Developing tools for Speech-Music discrimination of Danish music radio</title><link href="https://rlskoeser.github.io/dhqwords/vol/15/1/000522/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://rlskoeser.github.io/dhqwords/vol/15/1/000522/</id><author><name>Iben Have</name></author><author><name>Kenneth Enevoldsen</name></author><published>2021-03-05T00:00:00+00:00</published><updated>2021-03-05T00:00:00+00:00</updated><content type="html"><![CDATA[<h2 id="1-introduction">1: Introduction</h2>
<p>In the collective research project,  <em>A Century of Radio and Music in Denmark</em>  financed by the Independent Research Fund Denmark (2013-2018), eleven researchers investigated the present status and the historical development of Danish public music radio. The project demonstrated, among many other things, how digitization has changed the flow of music radio in several ways. Competition from music streaming services like Spotify and iTunes challenges traditional playlist radio, and the dissemination of software-generated playlists in public service radio stations in the 1990s has superseded the passionate music radio host <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>  <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>  <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup> (see also <sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>). As opposed to music radio, talk radio is experiencing an increasing popularity in synergy with on-demand podcast formats. And while music radio channels are bleeding listeners, these years national governments and public debates around the world discuss whether music radio should still be viewed as a public service and receive public funding <sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>.</p>
<p>As a part of the Danish research project, Have investigated the hypothesis that there has been a development from recorded music being the most important content to an increasing emphasis on spoken words (chattering hosts, news etc.) on Danish public service music radio. This hypothesis was tested, discussed, and to some extent also confirmed, in a qualitative study of five case studies of the most popular Danish morning music show on the radio channel P3 in the period 1989-2016 <sup id="fnref1:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. P3 is a public service music radio channel broadcasted nationwide in Denmark and the equivalent of, for example, the Swedish P3, the Norwegian P3 and the British BBC Radio 1. In this article we investigate this hypothesis from a quantitative perspective, by extending the data from five case studies to 65,000 hours of music radio at P3.</p>
<p>Digitization is not only changing radio but it is also paving the way for new areas of radio research. Radio archives are being digitized all over the world as a part of a preservation policy regarding cultural heritage and some of these archives allow access for researchers. Even if archive politics are becoming more user-friendly, there is still a need for developing tools for engaging with the overwhelming amounts of material in the audio archives <sup id="fnref1:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>. Access to digital archives has changed radio studies and media studies in general. First, by making it possible to actually listen deeply to the archive and investigate how radio content and expression have changed historically and thereby also enabling corrections and rewriting of existing radio history <sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>. Humanities scholars are skilled and have many tools at hand in doing deep hermeneutic and aesthetic content analysis. But the existing digital archives and improvement of metadata also opens open up new possibilities of large-scale analysis and enables us to listen both closely and at distance to the archive.</p>
<p>The terms close and distant listening are inspired by Franco Moretti&rsquo;s concept of distant reading <sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>; in this article these terms will refer to two different methodological approaches to audio analysis. By close listening we understand a human sensorial listening, manual registering, and hermeneutic interpretation of audio material. The strength of this method is the possibility to dive deeply and detailed into the data, which require a limited amount of material, which in this context means a few selected radio programs. Distant listening, on the other hand, refer to a methodology where computational models are developed to do the  “listening”  for you, so to speak, because the amount of data is too big for human sensorial processing. The strength is here the width in listening to a huge amount of radio programs in the archive focusing on simple categories such as music and speech.</p>
<p>In sound studies the term  “deep listening”  has previously been used to describe a concentrated and contemplative listening mode <sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup> and Charles Bernstein has used the term in relation to poetry analysis (1998). Tanya Clement has introduced the term distant listening, as we use it here, as a methodological term in digital humanities to describe large-scale machine-processing of audio data <sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup>  <sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup>  <sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>  <sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>. Large-scale analysis of audio data is still quite rare in Digital Humanities compared to textual and visual analysis. Most of the existing audio analysis deal with recordings of text associated with the field of literary studies. In Musicology the field of Music Information Retrieval (MIR) has been growing the last few years as a part of the broader field of Audio Content Analysis (ACA)<sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup> , but in Media Studies large-scale audio analysis are still rare.</p>
<p>The present study&rsquo;s point of departure is the Danish digital radio archive and digital infrastructure <a href="https://www.larm.fm">LARM.fm</a>, which gives access to almost two million digitized Danish radio programs. No tool has yet been developed for large-scale analysis of the archive, and one of the ambitions behind the project presented in this article is to demonstrate a way to do just that. Thus, the aim of the article is twofold: 1) To describe the methodological process and challenges for developing a model for large-scale Speech-Music discrimination analysis on audio data to answer the research question  <em>How has the distribution of music and talk on the Danish Broadcasting Corporation&rsquo;s radio channel P3 developed (1989-2019)</em> ? 2) To discuss and critically compare the methods, results, strengths and shortcomings of the qualitative case study and the large-scale analysis, respectively.</p>
<p>In the following we will first present the LARM.fm archive that serves as the basis for both studies. After that we shortly present the analogue hand-held case study first presented in the article  “A Lost Link between Music and Hosts: The Development of a Morning Music-Radio Show”   <sup id="fnref2:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> — we will call it  <em>the case study</em>  in this article. Hereafter we present the new big data study —  <em>the large-scale study</em>  — in relation to methodological processes and challenges and analytical results. In the section that follows we discuss and compare the two studies and the strengths and weaknesses of  <em>close listening</em>  and  <em>distant listening</em>  methodologies, and the article ends with a short conclusion.</p>
<h2 id="2-larmfm-a-digital-archive-and-infrastructure">2: LARM.fm a digital archive and infrastructure</h2>
<p>LARM.fm was originally developed by the research project LARM Audio Research Archive (2010-2014), which was a collaboration between 10 research and cultural institutions funded by the Danish Ministry of Higher Education and Science. Since 2015 LARM.fm has formed part of the national project DIGHUMLAB, <a href="https://www.dighumlab.org">www.dighumlab.org</a>. The infrastructure was then recreated on the present HTML5 platform, and the platform was expanded to also include TV <sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup>.</p>
<p>Since 1987 Denmark have had legal deposit of all broadcast material to the Royal Danish Library which is included in the Danish Media Collection. From 2005 onwards, this material has been exclusively born-digital and almost all analogue programs from 1989 to 2005 have been digitized. Due to legal protection the Media Collection is only available to the public through on site computers at the libraries. However, an agreement between The Danish Agency for Culture and the copyright holders allows university faculty and students to stream, but not to download, the material through the library&rsquo;s platform Mediestream or LARM.fm.</p>
<p>As opposed to Mediestream LARM.fm also contains older material from the archives of the Danish Broadcasting Corporation (Danmark&rsquo;s Radio or DR in the following), dating back to 1925, when the national radio was launched in Denmark. The old material is incomplete and consists mostly of text documents, as many programs were not saved, and others (still) have not been digitized <sup id="fnref1:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup>. Besides access, LARM.fm also provides various search tools and opportunities for organizing, annotating and sharing material.</p>
<p>The amount of available material grows proportionally as new Danish radio and TV programs are broadcasted in Danish media. In November 2019 the collection consisted of more than three million radio and TV programs and more than 200,000 OCR-scanned PDF files .</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000522/resources/images/figure01.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000522/resources/images/figure01_hu3acae3185490888ed230a92a7008bf2e_175504_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000522/resources/images/figure01_hu3acae3185490888ed230a92a7008bf2e_175504_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000522/resources/images/figure01_hu3acae3185490888ed230a92a7008bf2e_175504_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000522/resources/images/figure01.png 1473w" 
     class="landscape"
     ><figcaption>
        <p>A segment of the interface of LARM.fm (November 2019) with searching tools and list of content at the left and a list of the files at the right side.
        </p>
    </figcaption>
</figure>
<p>As shown in <a href="#figure01">Figure 1</a> (left column) the material is arranged according to type: TV programs, Radio programs, Radio news reports, Radio news manuscripts, Program guides, Radio news guides. Radio- and TV programs are available for streaming the other types of material consist of PDF files containing OCR-scanned documents — some handwritten and others typed or printed.</p>
<p>The oldest radio program in the archive is from 22 May 1931 and lasts just over five minutes. As illustrated in <a href="#figure02">Figure 2b</a> there are 10 or fewer programs from the first five years and between 35 and 61 programs from the remaining years in the 1930s.<sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup></p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000522/resources/images/figure02.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000522/resources/images/figure02_hua93042d4f4903105bdbd580980034493_712913_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000522/resources/images/figure02_hua93042d4f4903105bdbd580980034493_712913_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000522/resources/images/figure02_hua93042d4f4903105bdbd580980034493_712913_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000522/resources/images/figure02_hua93042d4f4903105bdbd580980034493_712913_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/1/000522/resources/images/figure02.png 1664w" 
     class="landscape"
     ><figcaption>
        <p>Overview of the collection of radio programs available for streaming in the LARM.fm archive, by November 9 2019, divided by decades and number of files shown in the black areas. Notice the relatively low number of the 2000s compared to the 1990s, reflecting a lack of files in this period. Bottom: the result of clicking at the 1930s showing the distribution of years. And the drill down process can continue to months and days.
        </p>
    </figcaption>
</figure>
<p>LARM.fm has expanded Danish radio research towards deep and detailed content analysis of first-hand sources, which has been requested not only in Danish radio research but internationally within media studies, where radio studies used to be characterized by institutional and media systemic analysis <sup id="fnref2:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>. Research access to archives and digital tools such as LARM.fm makes it possible to do deeply and broadly contextualized synchronic analysis because it enables us to study what was broadcasted simultaneously, before and after a given event or a specific program. However, archives especially call for historical diachronic analysis of longer time-spans and affords unique possibilities for coupling qualitative and quantitative methods and analysis. The following is an example of such a study combining an already existing qualitative study <sup id="fnref3:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>, with a large-scale study presented for the first time in this article. The combination of human close listening to the LARM.fm-archive and distant machine listening to the very same archive also prepare the ground for discussions of methodological strengths and challenges in Digital Humanities.</p>
<h2 id="3-the-case-study">3: The case study</h2>
<p>The close listening case study of the development of the share of music and speech focuses on the most popular music radio program  <em>Go&rsquo; Morgen P3</em>  (Good Morning P3) broadcasted on the national radio channel P3. The channel has existed on the frequency modulation band (FM) since 1963 and  <em>Go&rsquo; Morgen P3</em>  was launched in November 1989 playing music from 6 to 9 a.m. on weekdays. Except from a shifting variety of hosts and host constellations the program appears rather stable during the decades.</p>
<p>The aim of the case study was to test the hypothesis that from 1989 to 2016  <em>Go&rsquo; Morgen P3</em>  developed from having recorded music as the most important content of the programs (qualitatively as well as quantitatively) to have increasingly emphasis on speech and entertaining hosts. In Have (<a href="#have2018">2018</a>) this hypothesis was discussed in the light of the introduction of computerized playlists and rotation systems in Danish public service radio in the 1990s, as well as the increasing competitive situation, involving commercial and digital radio stations and, not least, digital music streaming services. In this article we will only present selected results and methods from the case study relevant to the large-scale study.</p>
<p>We listened closely to five selected  <em>Go&rsquo; Morgen P3</em>  programs broadcast between 6 and 9 a.m. on Wednesdays close to 1 November covering a period of 25 years. First, the programs were roughly annotated by Have using the categories music, speech, other to get a first impression of the results. The category &lsquo;speech&rsquo; includes only speech of the hosts and interviewees, while the category &lsquo;other&rsquo; covers news, sport, weather forecasts, jingles, channel advertisements etc. Then the counting was independently validated and fine-tuned by student assistants.<sup id="fnref:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup>  Choosing the same weekday at the same time of the year renders yearly comparisons more valid and Wednesdays around 1 November are not influenced by bank or festive holidays. The aim was to ensure that the five programs were evenly distributed throughout the time span, but due to few accessible programs from the first years of the millennium in the LARM.fm archive it ended up with five programs from 1992, 1998, 2006, 2011 and 2016 respectively <sup id="fnref4:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>.</p>
<p>To ensure that the program from 1992 was representative for the early 1990s and not affected by the institutional restructuring and formatting of radio channels in DR that year, registrations from the previous years were included for comparison: 31 October, 1990 and 30 October, 1991. As shown in <a href="#figure03">Figure 3</a>, the distribution between the categories was rather even, which confirmed that 1992 was not different from the previous two years. Unfortunately, the program from 2006 appeared to be unusual because it has technical problems in the studio, which gave rise to the use of some unplanned  “rescue music”  during the first 15 minutes, and therefore the music takes up more time than expected. However, that was discovered so late in the process that we kept it in the case study.</p>
<p>The registrations of the content of the five/seven programs point to some stable as well as evolving elements during the period. The basic format remained fairly stable over the years, but there were significant changes in the content of  <em>Go&rsquo; Morgen P3</em>  and the relations between music and hosts. During the 25 years there was a change from one host&rsquo;s talking filling out the gaps between the musical tracks to music as something filling out the gaps between a group of 3-4 hosts. However, as shown in <a href="#table02">Table 2</a> there were only a slight decrease of recorded music but in general more host speak during the years. The analysis showed that the amount of speech has increased, but not so much to the detriment of music (as expected) as to  “other”   <sup id="fnref5:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>.</p>
<p>However, a close listening to the programs revealed that the emphasis on music  <em>in</em>  the talk has gradually been disappearing. The time spent on introducing the music decreases radically and the link between the music and the hosts has correspondingly weakened during the past 25 years, finally disappearing from the program in 2016. The five/seven examples illustrate that in the early 1990s there was a strong connection between the music and the host, who chose the music and to some extent scripted speech of him- or herself. And almost everything said between the tracks was about the music. After the introduction of playlists in the 1990s, this link was terminated, and the presentation is minimized in 2011 from  “Chris Brown,  <em>With You</em> ”  to a total disappearance in 2016.</p>
<p>Some elements of content are identical in all five programs namely the recorded music, the time announcements and channel/program adverts, but new, entertaining elements that are not related to music (satire, quizzes, and everyday news and actuality) are gradually introduced. This development together with the hosts&rsquo; ignorance towards the music is pushing the music further into the background.<br>
Go&rsquo; Morgen P3 programs in minutes divided between content categories.    Date/Category  4 Nov. 1992  28 Oct. 1998  25 Oct. 2006  2 Nov. 2011  2 Nov. 2016      Speak  10.28 minutes  26.48  46.3  55.52  51.5      Music  116.52  92.5  100.32  91.45  101.95      Other  42.22  51.07  28.63  26.15  22.5   <br>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000522/resources/images/figure03.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000522/resources/images/figure03_huffcda2416c518a7efb66b6861fb7307f_190125_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000522/resources/images/figure03_huffcda2416c518a7efb66b6861fb7307f_190125_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000522/resources/images/figure03.png 1172w" 
     class="landscape"
     ><figcaption>
        <p>Same as Table 1, but here visualized in percentages and including 1990 and 1991.
        </p>
    </figcaption>
</figure></p>
<p>On the basis of the findings from the case study, it was concluded that  <em>Go&rsquo; Morgen P3</em>  evolved from being a radio program with a strong focus on music or at least with music as the most important content, to a program focusing on light, entertaining news and chatter of the hosts. This changed the status of the program from being a clear music-radio format where the recorded music is clearly significant and essential to being a more blurred soft-news format, where music takes many different roles, not only as musical pieces but also as underscore music and jingles important to the flow of the program. These observations enabled by a close listening to the programs combined with knowledge of Danish media politics and institutional changes in DR are of relevance for the large-scale analyses presented in the following section. Instead of relying on a small, curated and hand-annotated sample, this study utilizes thousands of radio programs to test the qualitative observations on data from LARM.fm. The study represents the first large-scale audio analysis in the Danish Media Collection.</p>
<h2 id="4-the-large-scale-study">4: The large-scale study</h2>
<p>In this study we take the existing study further by zooming out and letting a computer do the  “listening”  and  “counting”  — not on five/seven case studies but on a sample of the whole morning programming of P3 1989-2019 including the Go&rsquo; Morgen P3 schedule.<sup id="fnref:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup>  The study has been guided by the following research questions: How has the distribution of music and talk developed 1989-2019? How much music and how much speech? (classification). How long are the sequences with speech/music?</p>
<p>The corpus for analysis is all radio programs broadcasted at P3 1989-2019 between 6 a.m. and 12 a.m. and available as audio files in the Danish Media Collection and accessible through LARM.fm. The data in the corpus is divided in two corpora, resulting from different digitization processes: 1) Digitized radio files 1989-2005. 20,660 programs from 1989-01-01 to 2005-06-30 and 2) Born digital radio files 2005-2019. 24,456 programs from 2005-07-20 11:05:00 to 2019-05-28 14:03:00.<sup id="fnref:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup>  Since the material is copyright-protected, we made a processing agreement with the Royal Danish Library which allowed us to retrieve digital copies of the audio-files from the Media Collection needed for the project.</p>
<p>First, we repeated the manual coding of the five programs from the case study — this time through the open source multi-track audio editor and recorder Audacity — to be able to test the performance of a large-scale analysis. Doing the coding on the same material twice enhanced the reliability and as you can see by comparing <a href="#figure04">Figure 4</a> with <a href="#figure03">Figure 3</a> the diagrams are quite consistent from 2006-2016. However, the comparison also revealed some errors, since there is a discrepancy in 1992 and 1998. So, we went back to LARM.fm to deep-listen to the programs and found some external program segments with more speech than usual in Go&rsquo; Morgen P3 were included in the data. Those smaller inconsistencies further add to the need for quantitative analysis in cohort with a qualitative approach.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000522/resources/images/figure04.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000522/resources/images/figure04_huaaa10b7137688f404ec6f7fd2e52e21d_51158_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000522/resources/images/figure04_huaaa10b7137688f404ec6f7fd2e52e21d_51158_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000522/resources/images/figure04_huaaa10b7137688f404ec6f7fd2e52e21d_51158_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000522/resources/images/figure04.png 1474w" 
     class="landscape"
     ><figcaption>
        <p>Result of coding in Audacity. Percentage of music and speech at the y-axis and years at the x-axis. The blue overlap “Speech and Music” counts as speech.
        </p>
    </figcaption>
</figure>
<p>In the next step we utilize the trained model by Papakostas and Giannakopoulos (2018)<sup id="fnref:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup>  for Speech-Music discrimination, in which it is shown that a convolutional neural network (CNN) image-based classification on spectrograms obtain state-of-the-art performance. This model was trained by applying transfer-learning from a CNN trained on subset of the Imagenet <sup id="fnref:20"><a href="#fn:20" class="footnote-ref" role="doc-noteref">20</a></sup>. In this approach each audio file is broken into overlapping mid-term segments of 2.4 seconds with a step of 1 second. A spectrogram is then derived for each segment after which the CNN trained by Papakostas and Giannakopoulos (<a href="#papakostas2018">2018</a>) is used to discriminate between speech and music. Followingly, a median filter was used to smooth the signal. This could e.g. remove 1-second segments of speech in music, which is naturally very unlikely.<sup id="fnref:21"><a href="#fn:21" class="footnote-ref" role="doc-noteref">21</a></sup>  During testing, this image classification approach yield accuracy of approximately 93-98% on 11 hours of radio stream <sup id="fnref:22"><a href="#fn:22" class="footnote-ref" role="doc-noteref">22</a></sup>, while state-of-the-art audio-based classifiers obtain a performance of approximately 85-94%. For more on how the model was trained, its performance and audio to spectrogram preprocess we encourage the interested reader to examine the original study by Papakostas and Giannakopoulos (<a href="#papakostas2018">2018</a>).</p>
<p>As seen in <a href="#table02">Table 2</a> and <a href="#figure05">Figure 5</a> these performance levels generalize well to Danish when comparing it with our coded dataset. For a comparison with an audio-based classifier we used the support vector machine (SVM) by Giannakopoulos (<a href="#giannakopoulos2015">2015</a>).<br>
Performance using only the Speech and Music categories. Positive class is set as speech for reference.       <strong>Audio-based Classifier (SVM)</strong>    <strong>Image-based Classifier (CNN)</strong>       Accuracy (95% CI)  0.92 (0.920, 0.925)  0.98 (0.979, 0.982)      Balanced Accuracy  0.94  0.98      Sensitivity  0.98  0.99      Specificity  0.90  0.97      Pos Pred Value  0.80  0.94      Neg Pred Value  0.99  0.99      No Information Rate  0.70  0.70   <br>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000522/resources/images/figure05.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000522/resources/images/figure05_hua9739009ebf129dfedfb603c6332b9f6_37909_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000522/resources/images/figure05_hua9739009ebf129dfedfb603c6332b9f6_37909_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000522/resources/images/figure05.png 500w" 
     class="landscape"
     ><figcaption>
        <p>ROC Curve showing the performance of the CNN when evaluated on the coded corpus. The area under the curve (AUC) is 0.99. The ROC curve shows the relation between the sensitivity and specificity. Sensitivity, also called true positive rate, is the proportion of speech segments correctly classified as speech, while specificity, also called the true negative rate, is the proportion of music segments corrected classified as music. The diagonal indicates a model which would guess at chance level. Note that it is always possible to get a specificity or sensitivity of 1, by always guessing one of the two categories.
        </p>
    </figcaption>
</figure></p>
<p>These performance levels include only pure categories of speech and music, which does not represent the music radio programs, especially not after 2006 (see <a href="#figure04">Figure 4</a>) in which it is seen that the proportion of the mixed category speech and music is increased. Including  “Speech and music”  in the  “speech”  category and  “jingles”  in the  “music”  category, we see that the CNN obtains an accuracy of 0.96 (95% CI: 0.954, 957). Note that this includes categories which are unpredictable by the CNN, such as noise and silence. Therefore, this is the likely performance which we will see over all categories, given the assumption that jingles are music and  “speech and music”  is speech.</p>
<p>This level of performance allows for large-scale analysis of the relationship between speech and music and while our approach focuses on speech and music classification in radio, it can be generalized to other audio media and audio recordings such as podcasts, audiobooks and music performances. It is also possible to alternate predictors and, for instance, include gender and mood and allow for multiple outcomes, such as speech, music and jingles.</p>
<p>The results of the analysis are shown in the following four Figures, which will be further discussed in the following section.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000522/resources/images/figure06.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000522/resources/images/figure06_hua3714e1ddfba2deab70b50e1e7b94293_328641_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000522/resources/images/figure06_hua3714e1ddfba2deab70b50e1e7b94293_328641_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000522/resources/images/figure06_hua3714e1ddfba2deab70b50e1e7b94293_328641_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000522/resources/images/figure06_hua3714e1ddfba2deab70b50e1e7b94293_328641_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/1/000522/resources/images/figure06_hua3714e1ddfba2deab70b50e1e7b94293_328641_1800x0_resize_box_3.png 1800w,/dhqwords/vol/15/1/000522/resources/images/figure06.png 1888w" 
     class="landscape"
     ><figcaption>
        <p>Shows proportion of speech over time as predicted by the model. Shaded area is uncertainty in 95% confidence interval. The data is smoothed using Kolmogorov-Zurbenko filter with a window size of 4 for easier interpretation.
        </p>
    </figcaption>
</figure>
<p>In <a href="#figure06">Figure 6</a> we measured the changes in the proportion of speech during the thirty years and notice significant changes around 1992 and 2001 and again in 2005. We also see an upward trend from 2016-2019.</p>
<p>Apart from the Music-Speech discrimination we also have an interest in how the length of the sequences with speech/music might change during the years. This interest came from the findings in the case study showing that the length of a musical track has been standardized, especially after the introduction of playlists, rotates systems and musical clock-schemes <sup id="fnref:23"><a href="#fn:23" class="footnote-ref" role="doc-noteref">23</a></sup>.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000522/resources/images/figure07.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000522/resources/images/figure07_hu874bd5c2ff4c61cab257c90f81982789_485959_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000522/resources/images/figure07_hu874bd5c2ff4c61cab257c90f81982789_485959_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000522/resources/images/figure07_hu874bd5c2ff4c61cab257c90f81982789_485959_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000522/resources/images/figure07_hu874bd5c2ff4c61cab257c90f81982789_485959_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/1/000522/resources/images/figure07_hu874bd5c2ff4c61cab257c90f81982789_485959_1800x0_resize_box_3.png 1800w,/dhqwords/vol/15/1/000522/resources/images/figure07.png 1886w" 
     class="landscape"
     ><figcaption>
        <p>Average length in seconds over time as predicted by the model. Shaded area is uncertainty in 95% confidence interval. The data is smoothed using Kolmogorov-Zurbenko filter with a window size of 6 for easier interpretation. Note that if a section is interrupted, e.g. if a radio host interrupt a segment of music, with a segment of speech, this will result in the music segment being cut in half. Interpret the results accordingly.
        </p>
    </figcaption>
</figure>
<p>Because we in <a href="#figure07">Figure 7</a> present the mean length in seconds, the distribution is rather a Poisson distribution for speech and for music — a distribution which resembles a bimodal Poisson distribution (see <a href="#figure08">Figure 8</a> and <a href="#figure09">Figure 9</a>). Note the increase in the average length of music in 1992 and the decline in music and speech length from 2011, which seems to indicate that there are shorter segments of both speech and music and consequently they must have switched more often. Additionally, we observe a rise in average speech length from 1992 until 1999.</p>
<p>To elaborate the findings from <a href="#figure07">Figure 7</a> we grouped the numbers in seven-years-periods to make comparison of these segments in relation to speech and music, separately.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000522/resources/images/figure09.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000522/resources/images/figure09_huc8a4c5611fbbf05f49d748b911d90ce8_128170_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000522/resources/images/figure09_huc8a4c5611fbbf05f49d748b911d90ce8_128170_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000522/resources/images/figure09_huc8a4c5611fbbf05f49d748b911d90ce8_128170_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000522/resources/images/figure09_huc8a4c5611fbbf05f49d748b911d90ce8_128170_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/1/000522/resources/images/figure09_huc8a4c5611fbbf05f49d748b911d90ce8_128170_1800x0_resize_box_3.png 1800w,/dhqwords/vol/15/1/000522/resources/images/figure09.png 2500w" 
     class="landscape"
     ><figcaption>
        <p>Proportion of music section length over time as predicted by the model. It can be seen that segments of music are typically short (less than 30 seconds) or approximately 200 seconds. Note the increase in segments of music around 200 seconds after 1995.
        </p>
    </figcaption>
</figure>
<p>In <a href="#figure08">Figure 8</a> and <a href="#figure09">Figure 9</a> we pay attention to the first peak of very short segments of 10-20 seconds and the second peak around the 200 seconds - especially significant in relation to music.</p>
<h2 id="5-discussion-and-comparison-with-qualitative-study">5: Discussion and comparison with qualitative study</h2>
<p>The study has analyzed some general tendencies in the development of the morning programming 6-12 a.m. at P3 1989-2019 by measuring the proportional distribution of music and speech and by measuring the length of music and speech segment respectively. When we look at the most visually striking changes in Figures 6 - 9, some years across the analysis seems note-worthy: 1992, 2001, 2005 and 2016.</p>
<p>Some important institutional and media political changes must be taken into account to explain the development. The official end of the monopoly in Danish radio came in 1983 when local radio stations were allowed for the first time; however, with regard to nationwide channels DR&rsquo;s monopoly was not fully broken until 2003. That means that P3 has been in an increasing competitive condition during the thirty years, which is further intensified by the competition from music streaming services like Spotify and iTunes from the latter half of the 2000s.</p>
<p>To meet the competition from the local and commercial music radio channels, P3&rsquo;s profile was strengthened with the formatting of DR&rsquo;s four radio channels in 1992 (P1, P2, P3 and  <em>Danmarkskanalen</em>  (from 1997: P4). Together with a more market-oriented strategy, a controversial playlist tool for music control was introduced and slowly implemented together with rotation systems during the 1990s, meaning that the music presenter no longer chooses the music for the program. This systematization and a standardization of the lengths of musical tracks might explain the increase in the average length of music in 1992 supported by the increase in segments of music around 200 seconds after 1995 in <a href="#figure09">Figure 9</a>.</p>
<p>If we look at <a href="#figure06">Figure 6</a>, we see significant changes around 1992 and 2001 and again in 2005. This generally confirms the case study in relation to the tendency of less speech in the early 1990s before the restructuring and formatting of DR&rsquo;s radio channels in 1992, and a growth peaking around 2011 and hereafter a decreasing tendency towards 2016. What is new in the large-scale study is, that we here see an upward trend in the proportion of speech from 2016 to 2019 reflecting a corresponding decrease in musical content.</p>
<p>The rise in the proportional amount of speech from 2005 to 2011 in Figure 6 can be explained by the increasing number of hosts in the studio (Figure 10). More hosts entail more talking. Have (<a href="#have2018">2018</a>) found that there were many different hosts in the early years but only one host in the studio at a time (1989-2000), after which there was a relatively stable group of hosts of three to four who were all co-present in the studio (2005-2015). In the intervening years there was a short period with one or two hosts in the studio (2001-2004) <sup id="fnref6:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000522/resources/images/figure10.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000522/resources/images/figure10_hub3b6aa0dd7f5a6d24018fd76a2c14c60_22509_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000522/resources/images/figure10_hub3b6aa0dd7f5a6d24018fd76a2c14c60_22509_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000522/resources/images/figure10.png 499w" 
     class="landscape"
     ><figcaption>
        <p>Overview of the development of the changing host constellations in DR&rsquo;s morning music-radio show Go&rsquo; Morgen P3 <em>1989-2015.</em> [^have2018].
        </p>
    </figcaption>
</figure>
<p>The changes in 2005 might also be explained by a change in file format in the dataset. That year The Danish State Media Collection&rsquo;s procedure changed from digitizing the incoming material from the Danish media providers, to receiving it as born-digital as described above. While this change is positive, it leads to analysis on individual programs rather than the entire morning section. This leads to files which predominantly is music or speech hereby leading to increased uncertainty. To normalize across corpora each file was further split into segments of approximately 1,000 seconds leading less effect of outliers, however a noticeable difference still remains between the corpuses. As seen in <a href="#figure08">Figure 8</a> and <a href="#figure09">Figure 9</a>, hardly any sections of music or speech is above 1,000 seconds. But we must still take an increase in uncertainty after 2005 into account.</p>
<p>In <a href="#figure07">Figure 7</a> we see an increase in the average length of music from 1992 and a decline in music and speech length from 2011, which seems to indicate that there are shorter segment of both speech and music in the early years which might to some extend be explained by the practice of the host overlapping with the music and consequently music and speech are switching more often. The shift in 2011 might be explained by a general use of jingles and segments like quizzes and DR&rsquo;s own commercials. However, that does not correspond with <a href="#figure04">Figure 4</a> showing a more stable category of  “Other”  between 2011 and 2016. The rise in average speech length from 1992 until 1999 can be explained by the fact that DR in relation to the reorganization in 1992 introduced more entertaining and chatty news communication as a part of Go&rsquo; Morgen P3, which occupy half of the morning programs during the thirty years. In general, P3 after 1992 turned from being a purer music channel towards a more entertaining and communicative channel.</p>
<p>In <a href="#figure08">Figure 8</a> and <a href="#figure09">Figure 9</a> we pay attention to the first peak of very short segments of 10-20 seconds, which again can be explained by the overlapping host talk in the early years, and the second peak around the 200 seconds — especially significant in relation to music. This observation is interesting because it confirms a standardization in the lengths of musical tracks played in the radio after the restructuring in 1992. It is an example of what Have (<a href="#have2018">2018</a>) calls a  <em>radiotization</em>  of music, which means that the logic of radio dictate parts of the creative processes of the musicians. In other words: If you want to be played in the radio, you must compose tracks close to 3 minutes and 20 seconds to fit into the flow of the musical clock structure. The clock format is based on the structure of an hour divided in segments of speech and music (see e.g. <sup id="fnref:24"><a href="#fn:24" class="footnote-ref" role="doc-noteref">24</a></sup>, 94ff for an introduction to the so-called clock format <sup id="fnref1:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>  <sup id="fnref1:23"><a href="#fn:23" class="footnote-ref" role="doc-noteref">23</a></sup>). The structure of well over 3 minutes per segment has proven to be the best to maintain the listeners on the channel — a perfect balance between recognizability and renewal. As the playlists in the case study show, the variation in the music became narrower after the introduction of the playlist tool, and as shown in <a href="#figure09">Figure 9</a> the length of the individual sections of music have become more stable during the years confirming a more controlled and computer-calculated playlist and structure.</p>
<p>Apart from the specific results of the changes in the Danish radio channel P3, this article also has an interest in the methodological discussions of deep and distant listening. The opportunity to move back and forth between the existing case study and the large-scale study — you might call it a meso scale listening approach — has made the analysis not just more solid in relation to the existing findings but also in relation to filling the gaps of each method. Risks of cherry picking in relation to the qualitative case study have been dismissed, and <a href="#figure06">Figure 6</a> added a more nuanced perspective to the case study by showing a quite variable amount of speech during the years. However, with an upward trend from 2016-2019, which is the period not included in the case-study. So, the large-scale study also gave rise to new questions, such as, why do we see an increasing amount of speech from 2016 and onwards? An explanation could be the increasing competitive situation, both from commercial digital music radio stations and digital music streaming services. In this competitive field of musical content DR and P3 turn towards one of their core competences as a public service institution, to offer professional journalistic content presented by well-known young personas in an entertaining way. That strategy turns P3 even more away from being the music channel it once was towards an entertaining and communicative radio channel.</p>
<p>Many of the eye-catching changes in Figures 6-9 can be explained from institutional changes in DR, but the significant change in 2005 cannot. That gave rise to questioning not the classifier but the data, which changed format in 2005. This is valuable knowledge not only in this study but in future large-scale studies of the LARM.fm archive. Working with the LARM.fm archive as reverse-engineering Humanists has given insights in some of the challenges of large-scale archive analysis calling for critical attention to significant oscillation, which is not always anchored in the actual changes in the content of the data but in the formats providing this content.</p>
<p>A main aim of this study in total has been to analyze whether there has been a development from recorded music being the most important content to an increasing emphasis on spoken words. The close listening approach enabled a study of how the music was presented by the hosts in the program and confirmed the qualitatively change from a host filling out the gaps between the musical tracks by talking about the music in the early 1990s to music as something filling out the gaps between a group of hosts, who do not relate to the music at all. However, the large-scale study enabled us to correct or at least nuance the findings further by demonstrating that the general amount of speech has actually not increased but is varying during the 30 years period, as shown in <a href="#figure06">Figure 6</a>. From the large-scale study we learned that the diachronic changes in the share of music and speech are less significant than initially expected. This points to the fact that picking single case studies (even if sampled deliberately) can lead to doubtful conclusion if not reflected on a background of the full radio programming. At this point our study clearly shows strengths and weaknesses of the two approached and why it generates more valid answers to combine them.</p>
<h2 id="6-conclusion-combining-close-to-distant-listening">6: Conclusion: Combining close to distant listening</h2>
<p>This study has compared a case study of five Danish music radio programs (1990-2016) to a large-scale study of the whole morning programming (6 to 12 a.m) of the music channel P3 1989-2019. Both studies are anchored in the data from Danish digital archive and infrastructure LARM.fm, which is offering new paths for radio and media studies by affording deep listening studies as well as large-scale distant listening studies. The study is the first to present a large-scale analysis of the huge amount of data from the Danish radio archive. Inspired by Papakostas and Giannakopoulos we applied a convolutional neural network (CNN) image-based classification on spectrograms, which was obtaining state-of-the-art performance. For a comparison with an audio-based classifier we used the support vector machine (SVM) by Giannakopoulos (<a href="#giannakopoulos2015">2015</a>). As demonstrated in <a href="#table02">Table 2</a> and <a href="#figure05">Figure 5</a> the classifier tools developed from Papakostas and Giannakopoulos performed with high accuracy and the performance levels generalized well to Danish. This is useful insight to bring into future studies of automated speech recognition in Digital Humanities: For instance, how the analytical results found in a small amount of data can successfully be generalized to a large amount of data, and how models of speech-music discrimination can successfully be transferred across languages (Danish and English in this case). We also expect that the model developed in this article can be trained in relation to other tasks such as gender detection or regional accent detection.</p>
<p>The findings in the study confirms that the political and institutional changes in Danish music radio leave their imprint in the content of the programs. For instance, when we register more standardized formats and segments after the restructuring in 1992. But the most eye-catching protrusion of <a href="#figure06">Figure 6</a> in 2005 must be explained by a change of format caused by a shift from digitized to born digital files in the archive. Thereby the study also contributes with an example of how it is important to include reflections on the constitution and possible changes in the data when doing large-scale analysis.</p>
<p>In general, we can conclude that the combination of a close listening and a distant listening approach has given us a more saturated picture of the development of the morning music radio programming at P3 from 1989 to 2019. The combination both enables us to give more valid answers to how the distribution of music and talk has changed during the period but it also brings forward some strengths and shortcomings of the qualitative case study and the large-scale analysis, respectively. Finally, we hope that this study can contribute to encouraging Digital Humanities scholars to include more audio content analysis in relation to the field of Media Studies.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Have, I. (2018).  “A Lost Link Between Music and Hosts: The Development of a Morning Music Radio Programme”  In Michelsen, M., Krogh, M, Have, I. and Nielsen S. K. (ed.),  <em>Tunes for All: Music on Danish Radio</em> , Aarhus: Aarhus University Press.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref3:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref4:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref5:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref6:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Krogh, M. (2018).  “Non/Linear Radio Genre, Format and Rationalisation in DR Programming”  In Michelsen, M., Krogh, M, Have, I. and Nielsen S. K. (ed.) (2018).  <em>Tunes for All: Music in Danish Radio.</em>  Aarhus: Aarhus University Press.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Wallevik, Katrine (2018).  “To Go with the Flow and to Produce It”  In Michelsen, M., Krogh, M, Have, I. and Nielsen S. K. (ed.) (2018).  <em>Tunes for All: Music in Danish Radio.</em>  Aarhus: Aarhus University Press.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Dubber, A. (2014).  <em>Radio in the digital age</em> . Cambridge: Polity.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>Michelsen, M., Krogh, M, Have, I. and Nielsen S. K. (ed.) (2018).  <em>Tunes for All: Music in Danish Radio</em> . Aarhus: Aarhus University Press.&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>Michelsen, M., Have, I., Lindelof, A. Sivertsen, H. S. and Larsen, C. R. (ed.) (2018a).  <em>Stil nu ind …: Danmarks Radio og Musikken</em> . Aarhus: Aarhus University Press.&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>Moretti, Franco (2013).   <em>Distant Reading</em> . Verso.&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>Bull, M. and Back, L. (ed.) (2003).  <em>The Auditory Culture Reader</em> . Oxford, New York: Berg.&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>Clement, T. (2012).  “Distant Listening: On Data Visualisations and Noise in the Digital Humanities”    <em>Text Tools for the Arts</em> , special issue of  <em>Digital Studies</em>  /  <em>Le champ numérique</em> , 3(2), 2012.&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>Clement, T. (2016).  “Towards a Rationale of Audio-Text”    <em>Digital Humanities Quarterly,</em>  10(2).&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>Clement, T. et al. (2013).  “Distant Listening to Gertrude Stein&rsquo;s  Melanctha : Using Similarity Analysis in a Discovery Paradigm to Analyze Prosody and Author Influence”    <em>Literary and Linguistic Computing</em> , 28(4), 582-602.&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p>Mustazza Chris (2018).  “Machine aided close listening, Machine-aided close listening: Prosthetic synaesthesia and the 3D phonotext”    <em>Digital Humanities Quarterly</em>  12(3).&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13">
<p>See for instance the activities at International Society for Music Information Retrieval (<a href="https://ismir.net/">https://ismir.net/</a>), including the journal Transactions of the International Society for Music Information Retrieval (<a href="https://transactions.ismir.net/">https://transactions.ismir.net/</a>).&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14">
<p>Have, I. and Nielsen, J. (2016),  <em>User Manual for LARM.fm. A Digital Radio and TV Research Infrastructure for Researchers, Teachers and Students</em> , Aarhus: DIGHUMLAB.&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:15">
<p>For a more thorough description of LARM.fm we refer to Have and Nielsen 2016.&#160;<a href="#fnref:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:16">
<p>The registrations for the case study was done by the help of student assistant Caroline Skov. The registrations of the same programs for the large-scale study was done by the help of student assistant Martha Jepsen.&#160;<a href="#fnref:16" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:17">
<p>The computational part of the project is supported by DeIC&rsquo;s National Cultural Heritage Cluster and Royal Danish Library providing HPC-facilities, and developed together with Center for Humanities Computing Aarhus.&#160;<a href="#fnref:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:18">
<p>The timespan of the material from 2005-2019 shows that it will take four years, one month and fourteen days to listen to it all.&#160;<a href="#fnref:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:19">
<p>The model along with code is available at Papakostas&rsquo; <a href="https://github.com/MikeMpapa/CNNs-Speech-Music-Discrimination">Github</a>.&#160;<a href="#fnref:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:20">
<p>Deng, J., Dong, W., Socher, R., Li, L. J., Li, K., &amp; Fei-Fei, L. (2009, June). Imagenet: A large-scale hierarchical image database,  <em>2009 IEEE conference on computer vision and pattern recognition</em>   (pp. 248-255). Ieee.&#160;<a href="#fnref:20" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:21">
<p>Notably if the CNN was retrained to predict other classes, this might result in short segments such as jingles being smoothed out, which would be undesirable.&#160;<a href="#fnref:21" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:22">
<p>Papakostas, M. and Giannakopoulos, T. (2018).  “Speech-Music Discrimination Using Deep Visual Feature Extractors”    <em>Expert Systems with Applications</em>  114, 334-344.&#160;<a href="#fnref:22" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:23">
<p>Russo, A. (2013).  “Tick Tock goes the Musical Clock”  In Loviglio and Hilmen (ed.)  <em>Radio&rsquo;s New Wave</em> . New York: Taylor and Francis.&#160;<a href="#fnref:23" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:23" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:24">
<p>Hendy, D. (2000).  <em>Radio in the global age</em> . Cambridge, Oxford, Boston, New York: Polity.&#160;<a href="#fnref:24" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry><entry><title type="html">From the Presupposition of Doom to the Manifestation of Code: Using Emulated Citation in the Study of Games and Cultural Software</title><link href="https://rlskoeser.github.io/dhqwords/vol/15/1/000501/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://rlskoeser.github.io/dhqwords/vol/15/1/000501/</id><author><name>Eric Kaltman</name></author><author><name>Joseph Osborn</name></author><author><name>Noah Wardrip-Fruin</name></author><published>2021-03-05T00:00:00+00:00</published><updated>2021-03-05T00:00:00+00:00</updated><content type="html"><![CDATA[<h2 id="1-the-changing-archive">1 The Changing Archive</h2>
<p>As many institutions contemplate migrating their software collections to digital repositories and expanding their born-digital holdings, scholars find new opportunities and use cases that leverage these records’ born-digital nature. At the same time, as recent surveys of the digital humanist landscape make abundantly clear, the future of historical scholarship will be tied to reconciling past, print-based, qualitative practices with newer, networked, quantitative ones <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>  <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>  <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>  <sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>  <sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>. The meaning of the historical archive is changing, and the ways to enable and maximize its exploration must also adapt.</p>
<p>We provide in this article a way forward for the particular case of scholarship in video game history — by designing new means for historical citation, reference, and source retrieval. As we discuss below, these new means support a new approach to game scholarship (including, but not limited to, the history of games). Moreover, this new way of understanding game citation generalizes to other computational artifacts that will likely be of increasing concern to scholars, such as systems and application software, user interfaces, and more.</p>
<p>Specifically, we outline the current practice and requirements of citation in historical game studies works and then suggest a route beyond that practice through the description of a tool for the reference and resurrection of game software data. This resurrection is used as a shorthand for the re-execution of legacy software data inside a new computational context.<sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup></p>
<p>Those investigating humanistic citation and authority in digital humanities, like Burdick et al. in  <em>Digital_Humanities</em> , call out the coming dissolution of humanistic and preservationist foundations.</p>
<p>As concepts of authorship, document, argument, provenance, and reference become increasingly unstable, concepts that are fluid, iterative, and distributive, but less  “authoritative,”  are taking their place <sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>.</p>
<p>Game citation directly confronts — and calls for answers to — many of these issues.<sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>  In this work, we highlight such problems of  “authorship, document, argument, provenance, and reference.”</p>
<p>As a motivating example, consider the following text from Pinchbeck&rsquo;s detailed account of the 1993 computer game  <em>Doom</em> :<sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup></p>
<blockquote>
<p>The color palette of browns and grays shouts a level of realism that  <em>Wolfenstein 3D</em>  didn’t get anywhere near. A barrel positioned temptingly in the center of the screen just aches to be shot at and delivers a meaty crunching explosion that scatters debris across the screen. . . . There are a number of linked secrets, establishing that opening up new areas involves not just finding and triggering buttons and trip wires but triggering things in sequence. In this case, we have a different-colored wall panel, dropping down into a passage that takes us to the lake of waste with the superarmor, then a trip wire in the final room that lowers the Imp platform, announcing dynamic vertical-level adjustment and opening up a little area with a shotgun and shells. . . . In the space of a few short and small secrets, the game trains the observant player or completionist to watch for wall discoloration, lines of light/shadow and new sectors as trip wires, raising and dropping platforms, and linked sequences.<br>
<sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup> This passage presupposes comparative knowledge of other early first-person shooters and assumes that the reader either accepts Pinchbeck&rsquo;s descriptions at face value or will do the legwork required to find a copy of the game, load it into emulation software (or find a computer capable of running it!), and reproduce Pinchbeck&rsquo;s referenced objects, sites, and narrative arc. This also puts a greater obstacle in the reader&rsquo;s path: Is the reader sufficiently dextrous and committed to complete the demo chapter and find the secrets? Whom does this type of citation include and exclude? Moreover, for game studies more broadly, it is time-consuming, tedious, and space-intensive to give detailed accounts of even a single linear playthrough of a game.</p>
</blockquote>
<p>Consider, as an alternative, this interactive, augmented version of a longer excerpt from the same passage, in which the highlighted text may be clicked to launch the game in a specific saved state.        The color palette of browns and grays shouts a level of realism that Wolfenstein 3D didn&rsquo;t get anywhere near. [Editorial note: This first Wolfenstein 3D example unfortunately does not function on some browsers.] A barrel positioned temptingly in the center of the screen just aches to be shot at and delivers a meaty crunching explosion that scatters debris across the screen&hellip;The lights vary in this room - we can see the light from the large window to the right. When we pause by this window, the open areas seem huge, and we can see a lake of animated green goo. A glowing piece of armor sitting in the lake tells us straight away that we can leave the corridors and rooms and actually get outside&hellip;We skirt around to the left toward the pillars (which are throbbing and glowing with light) and head up the staircase to a plinth with animated scrolling textures, to collect some armor. On the way, a shaven-headed goon with a shotgun bellows at us. We fire, and he flies backward in a gush of blood, dropping the shotgun. We collect it and scoop up some blue health vials and some archaic metal helmets for an armor bonus, and we’re ready to go.       Game Emulator      Toggle audio  Example 1. This window contains an active emulation context that runs the cited states (clickable links) in the surrounding paragraphs. When you click on the emulation, your mouse will be captured, use the &ldquo;esc&rdquo; key to release your mouse. Controls for Doom and Wolfenstein 3D are as follows: Ctrl key to shoot, arrow keys to move, numbers to change weapons, mouse to look (when captured), &ldquo;shift&rdquo; key to run and &ldquo;esc&rdquo; key to bring up game menu. Sometimes a state may load inconsistently, just click on the citation again to reload it. A Doom state may hit an internal &ldquo;LUMP&rdquo; error, this is due to internal consistency checking with the Doom engine. We recommend using Google Chrome to run this page, if possible, as it produced the fastest emulation speed in our tests.         Other innovations introduced in the first level include multiple vertical levels included in the same area. In the third room we enter, Imps stand on a raised platform in the far corner, while Zombie soldiers advance along a walkway that zigzags over green radioactive waste. There are a number of linked secrets, establishing that opening up new areas involves not just finding and triggering buttons and trip wires but triggering things in sequence. In this case, we have a different-colored wall panel, dropping down into a passage that takes us to the lake of waste with the superarmor, then a trip wire in the final room that lowers the Imp platform, announcing dynamic vertical-level adjustment and opening up a little area with a shotgun and shells. Finally, moving back out of this area and toward the second room opens a timed lift in the corner of the secret shotgun area, which we can run back to before it raises again (and it only does this once; some secrets are nonrepeatable). The lift leads to a short corridor with a couple of small armor bonuses before delivering the real reward, a one-way wall with a view over the walkway room. In the space of a few short and small secrets, the game trains the observant player or completionist to watch for wall discoloration, lines of light/shadow and new sectors as trip wires, raising and dropping platforms, and linked sequences.          It is worth noting that the shaven-headed goon referenced in the first quoted paragraph above only appears at this point at certain difficulty levels, so a player selecting easier difficulties will not encounter it. The previous citation, regarding the plinth, is the same position in the level on an easier difficulty setting. Emulated citations in this paragraph therefore reflect more than just the level position and player state, but also macro-level changes to game settings. In addition, the citation referencing the  “timed lift in the corner of the secret shotgun area”  loads with the game menu activated. This is due to the difficultly of catching the lift while simultaneously triggering a save state. If the menu is removed with the esc key, the lift will be shown rising up the wall. Therefore, there appears to be an art inherent to capturing evocative states that well-align with each descriptive sentence.</p>
<p>In preparing this augmentation, the authors used a new browser-based tool — the Game and Interactive Software Scholarship Tool (GISST) — to play through the game, record our performance and bookmark specific instants, store these bookmarks in a database, and hyperlink the textual content to specific moments in that performance. A reader has only to load such a bookmark to witness the moment of the game we need for our argument; they may then continue to play or simply click the next bookmark. The searchable database of performances and game citations itself becomes fuel for future arguments and counterarguments, as scholars can start playing from any of these bookmarks and take different actions (or, indeed, can apply the same series of actions to a different version of the game): What if we had gone left instead of right?  How does this scene appear differently in the first alpha version versus the final released version?</p>
<p>Games and other interactive systems have dual lives: on the one hand, they are processes (often computer programs) which can generate sequences of emergent phenomena bounded only by combinatorics, with each additional choice or instant of time branching possible worlds by increasing exponents; on the other hand, every experience of a game is not a branching tree, but a linearized sequence of events and audiovisual outputs. Considering the game program as a text is not necessarily more or less correct than considering a specific play of a game as a text, and GISST illuminates the difference between game program and game performance which we explore later in this work.</p>
<p>As Stephen Ramsay and other digital humanists argue, computational tools can themselves be arguments for and about new humanistic expression <sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup> sentiment is echoed in convincing terms in the aforementioned polemic on the state of digital humanities <sup id="fnref1:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>. The authors assert that  “the next generation of digital experimenters could contribute to humanities theory by forging tools that quite literally embody humanities-centered views regarding the world”   <sup id="fnref2:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>. In this sense, the tool presented below is the embodiment of a potential future for the study of game history.</p>
<p>In creating such a tool, we aim to foreground issues of reference as regards the notoriously unstable nature of computer games and software.<sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>  These objects, in their requirements for future retrieval and use in historical arguments, call for the ideation of what Bethany Nowviskie refers to as a  “speculative collections”   <sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup>. These collections are those that do not yet exist, but will be required for future digital scholarship. The requirements of both game citation and the tool illustrated below invite the creation of a speculative future collection for managed and retrievable historical software data. Speculative collections call for the telegraphing of future scholarly use, and the creation of criteria for the descriptive, curative, and managerial requirements that would likely result.</p>
<p>The future of scholarship in game history is then not only based on the stabilization of records for future retrieval, but also in the ways that that retrieval is enacted in practice, and available for further exploration and exploitation by critical computational methods and tools.</p>
<p>We begin this article by identifying key problems in game and software citation. We explore these problems by applying theories of reduction and intertextuality. This leads us to new approaches for reference and from there to the design of GISST, the tool supporting the new kind of games scholarship illustrated above. Finally, we present a qualitative evaluation of GISST by other scholars.</p>
<h2 id="2-citation">2 Citation</h2>
<p>Citation is a foundational act in modern scholarship. Regardless of the discipline, any scholarly argument is based on, or a reaction to, previous work and/or citable primary sources. Different fields use citation practice in different ways, but the major functions remain consistent <sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup>.</p>
<p>Citation operates on two fundamental levels. Within a text, it legitimates the knowledge claims made by an author, and provides support for their arguments and findings. It also, through the connections a citation makes to related works, ties an author to the social organization of the discipline(s) to which they are contributing. In fact, any discipline is essentially constituted by these networks of citations, the collections of links that form being dense enough to support further claims and disclaimers, rebuttals and denials.</p>
<p>Studies of citation occupy the thoughts of numerous fields, from the quantitative analysis of bibliometrics, with its h-indexes and impact factors, to the applied socio-linguistic study of discourse analysis. The dual roles of citation practice, both in the form and content of knowledge links and as a base for social practices within disciplines, are certainly ripe enough for a pluck into the basket of game historical studies. Before diving into the morass of game citation, and even more specifically, game data citation, the rest of this section will set up some background definitions and support structures from citation-adjacent fields. These will then be leveraged into a fuller discussion of game citation and the citation tool as a speculation and intervention into future practice.</p>
<h2 id="21-citation-in-use">2.1 Citation in Use</h2>
<p>As discussed, citation functions both within a text, as a marker to other sources, and without, as a tie between an author and a discipline. The citations found inside texts follow the prescriptions of the common practices within a field of study. Common guides for the humanities include the Modern Language Association (MLA), American Library Association (ALA), and the University of Chicago Manual of Style. Most engineering disciplines, including Computer Science, follow similarly organized research guides. In CS’s case, a majority of the works are organized around the conference guidelines provided by the Association for Computing Machinery (ACM) or the Institute of Electrical and Electronics Engineers (IEEE). These guides are the products of the study of bibliography, with its most prominent scholarly effect being the enumerative bibliographies — the Works Cited lists — found after the conclusions of monographs or conference publications. The constitution of bibliography entries is the result of practices in descriptive bibliography, a subset of analytical bibliography.</p>
<p>While bibliographic practice in the age of the Internet is in some corners lamented as both a lost art and potentially unnecessary <sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup>, we believe that coherent, consistent and standardized bibliographic description is still essential.<sup id="fnref:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup>  The practice of organized analysis of the works in a field can help to reveal new research directions and provide a solid base for future claims. Game historical work needs more time and effort devoted to these foundational, field-constitutive activities.</p>
<p>Returning to in-text bibliographic citation practices, we find that an explicit function of citation is the legitimation of claims.<sup id="fnref:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup>  Authors need to legitimate their claims, both by crediting original sources and supporting dialogue with other scholars with whom they may agree or disagree. Citation also works as a means to pay an intellectual debt, as any addition to knowledge, being based on previous efforts, should acknowledge others’ contributions <sup id="fnref:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup>.</p>
<p>In history specifically, the formation of modern historical discourse is founded on the scientific practice of accurate historical sourcing. Commonly attributed to Leopold Von Ranke and his continental predecessors, the development of footnote and endnote showed that the author had  “done an acceptable amount of work, enough to lie within the tolerances of the field”   <sup id="fnref:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup>. Citation persuades others to pay attention to a scholar’s work and thoughts. It cannot  “explain the precise course”  taken by a historian, but can  “give the reader . . . enough hints to make it possible to work this out — in part. No other apparatus can give more information — or more assurance — than this”   <sup id="fnref1:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup>. Anthony Grafton, in reference to the preceding quotes, notes that the function of footnotes — the historian’s preferred mode of citation — is to give legitimacy to a claim and promote the authority of the claimant. Footnotes also provide means for enforcing the social space of historians through the inclusion or exclusion of particular works. In many cases, a notable omission provides for a deeper criticism than an antithetical reference, because at least the latter claim is being recognized and confronted instead of ignored.</p>
<h2 id="22-citation-as-discourse">2.2 Citation as Discourse</h2>
<p>Given that this article is devoted to the use and abuse of citation practices in game historical texts, we need to develop a suitable framing and terminology to discuss game citations. As alluded to above, the act of citation involves the coherent and consistent description of a source within a text. This is usually in the form of a bibliographic footnote or endnote, or as a list of entries (enumeration) following a text. Bibliography dictates that there is sufficient description to allow a future researcher to recover the described source. This creation of a knowledge link to another’s work, in light of bibliographic practice, is then a matter of practicality. One needs adequate description for future retrieval. What bibliographic notions do not cover is the use of a citation, a knowledge linkage, within the text itself. The correct form of a description for a source is not the same as how an author activates that source inside their text as a part of their argument. We then have to split the act of citation in two. The first part is simply the description and positioning of a citation inside a text, and the other is an analysis of how that citation is used as a way to further the objectives of an author, and by extension that author’s discipline. Fortunately, the discussion of citation’s effect on both textual composition and the social formation of disciplines is addressed in the fields of bibliometrics and discourse studies. In the following paragraphs we will borrow a few key disciplinary concepts, then position them in relation to practices in game historical scholarship.</p>
<p>The intersection of one text within another is an instance of intertextuality. Although the term intertextuality is used in a number of fields, our definition of it here is drawn from discourse studies. One of the founders of the field, Norman Fairclough, describes intertextuality as  “the property texts have of being full of snatches of other texts, which may be explicitly demarcated or merged in, and which may assimilate, contradict, ironically echo, and so forth”   <sup id="fnref:20"><a href="#fn:20" class="footnote-ref" role="doc-noteref">20</a></sup>. Intertextuality, in Fairclough’s consideration, implicitly calls to account the production, distribution, and consumption of texts. For production, a key consideration is the historicity of texts, how they add to previous knowledge and expand a specific discursive chain of thought.<sup id="fnref:21"><a href="#fn:21" class="footnote-ref" role="doc-noteref">21</a></sup>  Distribution reflects on the network of texts and how they transform and flow into different types and fields.<sup id="fnref:22"><a href="#fn:22" class="footnote-ref" role="doc-noteref">22</a></sup>  Fairclough uses the example of political speeches transformed into news reports. With consumption,  “the intertextual perspective is helpful in stressing that it is not just  the text,  not indeed just the texts that intertextually constitute it, that shape interpretation, but also those other texts which interpreters variably bring to the interpretation process”   <sup id="fnref1:20"><a href="#fn:20" class="footnote-ref" role="doc-noteref">20</a></sup>. All three implications of Fairclough’s  “intertextual perspective”  have a significant bearing on the interpretation of citation and reference in academic works. That texts are engaged with a disciplinary train of thought, flow and re-form based on context, and intimately involve presuppositions about the other texts they do (and could) use, are all points of reflection for both the citation practice of games, and the implications for a better use of the tool proposed below; it being a means of a new type of intertextual link for game history.</p>
<p>Fairclough’s intertextuality is drawn from a more elaborate framework based in French discourse analysis that traces back to Michel Foucault’s  <em>Archaeology of Knowledge.</em>  Rather than retrace this history ourselves, we can lean on Fairclough’s two notions of intertextuality, manifest and constitutive. Manifest intertextuality is  “where specific other texts are overtly drawn upon within a text,”  forming a  “heterogeneous constitution of texts.”  To clarify, the quote ending the previous sentence is an example of manifestation, as is  “Fairclough’s intertextuality”  at the beginning of the paragraph. Both are specific, overt calls out to other texts, with the direct quote being a more emphatic kind of manifestation. Constitutive intertextuality (for Fairclough  “interdiscursivity” ) is effectively the means of intertextuality for a specific text. How the configuration of its references, allusions, and implications for other texts, both explicitly mentioned and implicitly demanded, align to form a specific type of discourse for a specific audience.<sup id="fnref:23"><a href="#fn:23" class="footnote-ref" role="doc-noteref">23</a></sup>  Bibliographic actions — like enumerative bibliography, footnotes, and in-line citations — are then all types of manifestation while the act of citation itself, as a social and knowledge linking activity, is a constitutive act.</p>
<p>Before extending the application of constitutive and manifest intertextuality to game historical discourse, two more key insights from Fairclough are useful. The first is the idea of a presupposition of a text. Sometimes presuppositions are just  “propositions that are given for, and taken for granted by, text producers.”  When engaged in the act of writing and assembling an argument, authors bring into their writing numerous pointers to other works — through manifestation — or ideas from other works that are assumed to be part of the general knowledge of an assumed reader. Or, crucially, part of the knowledge that one is assumed to take from an other text that is insinuated in the current one.  “In many cases of presupposition the  other text  is not an individual specified or identifiable other text, but a more nebulous  text  corresponding to general opinion (what people tend to say, accumulated textual experience)”   <sup id="fnref2:20"><a href="#fn:20" class="footnote-ref" role="doc-noteref">20</a></sup>. A presupposition, through manifest citation, of a particular other author or other work, brings with it a host of assumptions based on an expected experience on the part of the reader. As a result, presuppositions are difficult to correct if they go awry because both the reader and the author are under a similar delusion about the content and shape of a reference.</p>
<p>This gap between presuppositions, as intended by an author and interpreted by a reader, leads us to a second point about ambiguity and the constitutive surface of a text. In drawing together the heterogeneous network of texts that constitute a new one, there are times when certain parts may not fit as well as others.</p>
<blockquote>
<p>Texts vary a great deal in their degrees of heterogeneity . . . [they] also differ in the extent to which their heterogeneous elements are integrated, and so in the extent to which their heterogeneity is evident on the surface of the text. . . . Again, texts may or may not be reaccentuated; they may or may not be drawn into the prevailing key or tone (e.g., ironic or sentimental) of the surrounding text. Or again, the texts of others may or may not be merged into unattributed background assumptions of the text being presupposed. So a heterogeneous text may have an uneven and bumpy textual surface, or a relatively smooth one.<br>
<sup id="fnref3:20"><a href="#fn:20" class="footnote-ref" role="doc-noteref">20</a></sup> That we should pay attention to qualities of a textual surface — and the ways in which its imbricated texts do and do not comport with each other — motivates our framing for the discussion of the citation work below. If we are combining references in text to games, and other systems, based on non-discursive experiences, their constitutive intertextuality needs to be examined, along with its effects on the resulting historical discourse.<sup id="fnref:24"><a href="#fn:24" class="footnote-ref" role="doc-noteref">24</a></sup>  We may want the alignment to have a specific texture, but we also need to be aware that that texture, that surface, is something deserving of reflective consideration and thought. How do the citation of games and the juxtaposition of program and text affect the reader’s experience and comprehension of the argument? What does this do to the issues of presupposition and scholarly assumption? Below we discuss the current practice of game citation in light of both manifest intertextuality and presupposition.</p>
</blockquote>
<p>One last implication of the heterogeneous constitution of texts is that it results in what Fairclough refers to as an ambivalence of argumentative meaning.  “If the surface of a text may be multiply determined by the various other texts which go into its composition, then elements of that textual surface may not be clearly placed in relation to the text’s intertextual network, and their meaning may be ambivalent; different meanings may coexist, and it may not be possible to determine  the  meaning”   <sup id="fnref4:20"><a href="#fn:20" class="footnote-ref" role="doc-noteref">20</a></sup>. The ambiguity inherent to any textual argument results from the fact that we — as the authors — are removing other texts from their original context and constituting them into our own. As such, the onus is on us, the researchers, to both explain and refine the other texts in a way that supports our argument and that is interpretable to the reader. Poking below the surface of the text and retrieving a shared context requires significant effort, and in the case of games, might not currently be possible due to a lack of access <sup id="fnref:25"><a href="#fn:25" class="footnote-ref" role="doc-noteref">25</a></sup>  <sup id="fnref:26"><a href="#fn:26" class="footnote-ref" role="doc-noteref">26</a></sup>.</p>
<h2 id="3-bibliography-and-citation-in-game-studies">3 Bibliography and Citation in Game Studies</h2>
<p>In the previous section we outlined the function of citation as both an act of descriptive bibliography and as an intertextual mechanism in the creation of textual discourse. This section brings both of those concepts to bear on the current practices of game citation in game studies and game historical texts. While both fields are large, with a significant amount of publication activity, the extent of game citation is currently rather limited. Additionally, in game studies works there is a good deal of presupposition about the accumulated played experiences of both the reader and author. These assumptions are a major reason for the current lack of specific bibliographic guidelines. As Nathan Altice writes, in one of the only other analyses of game citation practice,</p>
<blockquote>
<p>Our familiarity with and access to videogames is taken for granted, since many of us are old enough to recall first-hand experience with the entire history of videogames — a claim that cannot be made by scholars of other media. There is an implicit assumption that we all know what a  <em>Super Mario Bros.</em>  cartridge looks like, so why bother with thorough descriptions?<br>
<sup id="fnref:27"><a href="#fn:27" class="footnote-ref" role="doc-noteref">27</a></sup>Altice 2015, 334]</p>
</blockquote>
<p>This  “implicit assumption”  of  <em>Super Mario Bros.</em>  is the result of the presupposition at work in game studies texts. It is not uncommon for game references to be scant or potentially non-existent. This is a problem because the assumption of contemporary, tacit experience with historical games cannot hold up past the current generation of scholars. Game citation practices, like those of appraisal and description, need to be addressed with a mind toward future historical scholarship and needs. The rest of this section will describe how citation functions in game studies works, and briefly point to further recommendations for their improvement. All of this lays groundwork for the introduction of the citation tool in the next section — as a tool-assisted intervention into both the issues of citation standardization and presupposition of game play experiences.</p>
<p>As already noted, computer game bibliography and citation practice needs consistent and thorough standards. Again, from Altice,</p>
<blockquote>
<p>To claim that videogame bibliography demands a closer allegiance to the practices [of enumerative, and analytical bibliography] assumes that a unified practice called videogame bibliography even exists. At their best, videogame citations adhere to the barest enumerative models. Even in those texts that most seriously grapple with electronic artifacts as objects that exhibit physical properties worthy of description, such as Kirschenbaum’s  <em>Mechanisms</em>  or Montfort and Bogost’s  <em>Racing the Beam,</em>  videogames are still afforded scant bibliographic information.<br>
<sup id="fnref1:27"><a href="#fn:27" class="footnote-ref" role="doc-noteref">27</a></sup> Altice’s claim of the lack of a videogame bibliography practice is not difficult to substantiate. As he states, many works that are intimately tied to the exploration of the material constraints and expressive potentials of technical artifacts do not share consistent bibliographic practices. Both works mentioned above, Matt Kirschenbaum’s  <em>Mechanisms</em>  — a treatise on the oft-overlooked ambiguities in the expression of digital data — and Ian Bogost and Nick Montfort’s  <em>Racing the Beam</em>  — a platform study into the inner workings of the Atari 2600 — come from the same publisher, are intimately involved with the technical distinctions of computer software, and do not share a consistent practice in their bibliographies <sup id="fnref:28"><a href="#fn:28" class="footnote-ref" role="doc-noteref">28</a></sup>  <sup id="fnref:29"><a href="#fn:29" class="footnote-ref" role="doc-noteref">29</a></sup>.</p>
</blockquote>
<p><em>Racing the Beam</em>  is a part of a larger series of works on specific platforms. Each book investigates the constraints that a particular platform imposes on the expressive potential of its software. Each book also takes a different position on the placement, organization and depth of its enumerative bibliography of games. Some volumes, like Jimmy Maher’s on the Commodore Amiga, eschew any explicit listing of the games referenced in the text <sup id="fnref:30"><a href="#fn:30" class="footnote-ref" role="doc-noteref">30</a></sup>. Contrarily, works like Altice’s own  <em>I AM ERROR</em> , on the Nintendo Entertainment System, adopt meticulous, platform- and media format-specific reference schemes.</p>
<p>Now, given that there is a not a standard set of bibliographic and citation practices for software, and that most major sources of such practices — like the MLA, University of Chicago, and surprisingly even the ACM — lack any guidelines for software bibliography, it is perhaps not unexpected that academic book publishers and authors do not maintain consistent practices.<sup id="fnref:31"><a href="#fn:31" class="footnote-ref" role="doc-noteref">31</a></sup>  The work of the Game Metadata and Citation Project (GAMECIP), which looked at hundreds of game studies citations across a variety of online and print sources, also validates Altice’s (and our) assumption about the lack of consistent practice. In fact, even within the same journal,  <em>Game Studies</em> , which does have an explicit bibliographic policy, there was still lax enforcement of descriptive citation practice.<sup id="fnref:32"><a href="#fn:32" class="footnote-ref" role="doc-noteref">32</a></sup></p>
<p>Altice also links bibliography and citation to descriptive concerns. He notes,</p>
<blockquote>
<p>As a Famicom scholar, I may possess the terminology to describe that platform’s media but meanwhile lack the platform-specific knowledge to properly cite a PlayStation 2 game. . . . Granting each [reference] its due description poses a sizable research challenge. One solution is to build up a body of platform-specific descriptions that others may use as a model . . . but such shared knowledge will take time and work.<br>
<sup id="fnref2:27"><a href="#fn:27" class="footnote-ref" role="doc-noteref">27</a></sup> Computer game bibliography is distinct from that for other media forms mainly in the complex of technical requirements needed to retrieve the object. GAMECIP’s platform and format vocabularies, outlined in prior work <sup id="fnref1:25"><a href="#fn:25" class="footnote-ref" role="doc-noteref">25</a></sup>, speak to Altice’s call in a limited way by attempting to codify and standardize some basic descriptive information for computer games. The larger issue, however, is that  “rich bibliographic records require a baseline technical understanding of the objects they describe”   <sup id="fnref3:27"><a href="#fn:27" class="footnote-ref" role="doc-noteref">27</a></sup>.</p>
</blockquote>
<p>For game scholars concerned with the technical underpinnings and object materiality of software, each new platform, format, and data configuration incurs a significant descriptive cost. For items in Altice’s enumerative bibliography, each specific game is listed according, in part, to the configuration of components inside a Nintendo Entertainment System Game Pak, and in the case of emulated sources, the header information of a particular game data file. Clearly, for his arguments to validate, this level of depth is necessary, and it would benefit future scholarship if others could take advantage of his classification schemes or even extend them into their own specific sub-domain of software.</p>
<p>Altice and others in the platform studies community are more concerned with the material and technical conditions of games than other historical scholars. What works for platform studies might be overkill for other subdisciplines. However, at the very least, the technical information provided in a bibliography should be correct, and involve a level of detail specific enough to allow an unacquainted reader a fair chance to recover the source.<sup id="fnref:33"><a href="#fn:33" class="footnote-ref" role="doc-noteref">33</a></sup>  The lack of consistency in the description of computer game sources can damage the legitimacy of game historical arguments. As mentioned in the last section, one key way that a citation can fail is through a mistaken presupposition about the source it is referencing. This misalignment between the author’s expectation — or recollection — of a game play experience and that of a future reader’s is only exacerbated by incomplete and inconsistent citation practice.</p>
<h2 id="31-presupposition-of-doom">3.1 Presupposition of DOOM!</h2>
<p>One significant difficulty in game citation is that games are not as recoverable as other media forms. Many institutions do not have software collections, and those that do struggle against the technical and material constraints of hardware maintenance and access. When these limited access scenarios collide with a lack of rigor in citation practice, the result is that many outputs of game scholarship rely on only the barest descriptions for games. They are used more as pointers to the concept of a particular game, as presupposition, than to an emphatic, playable instance of one.</p>
<p>To take a particularly salient example, let us return to Dan Pinchbeck’s book  <em>DOOM: SCARYDARKFAST.</em>  This book — a good example of design-focused phenomenological game study — relies, almost exclusively, on presupposition of game citations. The work contains manifest citations, mostly through in-line references, to 130 other computer games. Most are used in passing to articulate how a particular structural, thematic, or kinesthetic element from each game relates to those of  <em>Doom</em> . The in-line references are of the form (Title, Year), leaving the reader to fill in the blanks based on their assumed knowledge of each title. Furthermore, given the breadth of games mentioned, it is likely that nearly all readers have not played, or at least not recently played, many of them. The references hang on a presupposition of past experiences with the titles, and hopefully they still resonate in ways commensurate with his arguments. The references are, as we mentioned above, presupposed shorthand for the shared played experience of both author and reader.</p>
<p>To illustrate how this form of manifest, presupposed citation functions, take this set of paragraphs describing the progress of the first-person perspective from Pinchbeck’s book:</p>
<blockquote>
</blockquote>
<p>We need to consider the context into which  <em>DOOM</em>  arrived. The very first FPS game was  <em>Maze War</em> , created by Steve Colley, Howard Palmer, and Greg Thompson (and other contributors) at the NASA Ames Research Center. Colley estimates that the first version was built during 1973, as an extension of the earlier game  <em>Maze</em> , which offered a first-person exploration of a basic wireframe environment. At some point during ’73 or ’74, networked capability was added, enabling multiplayer FPS play. The genre was born out of networked deathmatching. After Thompson moved to MIT, he continued to develop  <em>Maze War</em> , adding a server offering personalized games, increasing the number of players to eight, and adding simple bots to the mix. Twenty years before  <em>DOOM</em> , all of the prototypical features of the FPS were in place: a 3D real-time environment, simple ludic activity (look, move, shoot, take damage), and a basic set of goals and win/lose conditions — all this and multiplayer networked combat.</p>
<p>Around the same time, Jim Bowery developed  <em>Spasim</em>  (1974), which he has claimed to be the very first 3D networked multiplayer game.  <em>Spasim</em>  pitted up to thirty-two players (eight players in four planetary systems) against one another over a network, with each taking control of a spaceship, viewed to other players as a wireframe. A second version expanded the gameplay from simple combat to include resource management and more strategic elements. Whether or not Bowery’s argument that  <em>Spasim</em>  precurses  <em>Maze War</em>  and represents the first FPS holds water, its importance as a game is undiminished — even if for no other reason than because  <em>Spasim</em>  is a clear spiritual ancestor of  <em>Elite</em>  (Braben and Bell 1984) and its many derivatives. It perhaps even prototypes a game concept that would later spin out into combat-oriented real-time strategy (RTS) or even massively multiplayer online (MMO) gaming.</p>
<p>What certainly differentiates  <em>Spasim</em>  from  <em>Maze War</em>  is the perspective. Like other early first-person games, such as  <em>BattleZone</em>  (Atari 1980) and id’s  <em>Hovertank 3D</em>  (1991), the game is essentially vehicular, with no representation of the avatar onscreen other than a crosshair. It is interesting that, aside from occasional titles such as  <em>Descent</em>  (Parallax 1995) and  <em>Forsaken</em>  (Probe Entertainment 1998), the genre very swiftly settled down into the avatar-based perspective, abandoning vehicular combat more or less completely. It’s also interesting that contemporary shooters often opt for a shift to third-person when including vehicles, such as with  <em>Halo: Combat Evolved</em>  (Bungie 2002) or  <em>Rage</em>  (id Software 2011).  <em>Half-Life 2</em> ’s (Valve Software 2004) first-person car sequences are actually quite unusual.</p>
<p><sup id="fnref1:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup> These three paragraphs reference twelve games spanning a period from 1974 to 2011.  <em>Doom</em>  does not receive a full in-line citation since it is the topic of the book, and is addressed with in-line references in a previous section. Ignoring the general argument and focusing only on the citations and their relationship to the assertions being made on their behalf, we already encounter some significant issues.</p>
<p>Firstly, the citations are not particularly specific.  <em>Descent</em> , for example, was released in six different versions for three different platforms in three different localities in 1995 alone.<sup id="fnref:34"><a href="#fn:34" class="footnote-ref" role="doc-noteref">34</a></sup>  The description  “(Descent, 1995)”  then does not provide enough information for a reasonable assumption about the particular version Pinchbeck played (or presupposed). Second, the citations presuppose a significant amount of knowledge on the part of the reader. When Pinchbeck remarks,  “ <em>Spasim</em>  is a clear spiritual ancestor of  <em>Elite</em>  . . . and its many derivatives,”  we (the reader) are required to understand — through presupposition — that  <em>Spasim</em> , a first-person, cockpit oriented space exploration game is echoed in  <em>Elite</em> , a similarly-perspectived first-person space simulation game. Clearly, that assertion requires a familiarity with both games, and by extension knowledge of other  <em>Elite</em>  derivatives. Finally, even though a game might be recoverable through the sparse citation provided, much of the discussion still presupposes memories of experiences of play, both in Pinchbeck and in his readers. By referencing the vehicular segment of  <em>Half-Life 2</em> , Pinchbeck is implicitly requiring a future researcher, should they want to experience that sequence, to spend many hours of game time reaching and evaluating it. There is nothing inherently wrong with this, but we must highlight the extent of the assumptions being made of the reader. Either you already have contemporary experience of  <em>Half- Life 2</em> , and incidentally remember this game play sequence, or you are relying on Pinchbeck’s memory of his contemporary play. Both positions presuppose a temporally-situated accumulation of played experiences that aligns with the year of this work’s publication. Future researchers, removed from a contemporary, played understanding of the game, must assume that Pinchbeck is not committing any of the intertextual no-nos — like misinterpretation or incorrect presupposition — listed in the previous section. Otherwise, they will need to recover  <em>Half-Life 2</em>  for themselves, and assume that their version contains the vehicular sequence in question and that it is reachable through play.</p>
<p>While it may seem that we are being a bit drastic in this example, we cannot take for granted that our own presuppositions about  <em>Half-Life 2</em>  or any other game discussed in the quote above (or, for that matter, in this text) will align with the presuppositions of future scholars.</p>
<p>Pinchbeck’s references are intended to evoke a general idea of a specific title, relying primarily on the presupposition of reader knowledge. The referenced games in these cases stand in metonym for their specific constitutive function in the text.  <em>Halo</em> ,  <em>Rage</em> , and  <em>Half-Life 2</em>  for their comparative vehicular segments;  <em>Spasim</em> ,  <em>Elite</em> ,  <em>Descent</em> , and  <em>Forsaken</em>  for their 360-degree cockpit viewpoint; and  <em>HoverTank 3D</em> ,  <em>BattleZone</em> , Maze, and  <em>Maze War</em>  for their advances to first person representation. Concrete, retrievable instances of these games are secondary to the structural or thematic conceptualizations of them as presupposed into Pinchbeck’s argument.</p>
<p>In contrast, recalling Altice’s more extensive, object-based citations, we see that many of his claims are rooted in the minutiae of a single platform and its technical constraints. For Altice, his argument is dependent on the specifics, on the material differences between games rather than the higher level concepts they can evoke. He commonly uses emulated versions of games to illustrate points about Nintendo Entertainment System rendering techniques. Because rendering functions differ between the many versions of, say,  <em>Super Mario Bros.</em> , Altice’s citation of a specific version of the game’s data is important: his analysis would not be possible — or legitimate — without it.</p>
<p>The lesson is not that anything Pinchbeck is saying is particularly incorrect, but that the onus for clarification is heavily weighted toward the reader, and in particular, a presupposition about the reader’s accumulated knowledge of games. Pinchbeck’s work is intended for a game savvy audience, and is certainly not attempting to be a rigorous, formal history of  <em>Doom</em> .<sup id="fnref:35"><a href="#fn:35" class="footnote-ref" role="doc-noteref">35</a></sup>  But the type of underspecified, game-as-shorthand reference structure is endemic to a significant swath of game studies. It will also make these types of work less relevant the farther they are displaced from the contemporary titles to which they refer. Again, in clarification with Altice,  “Most contemporary game scholars are old enough to remember most of the entire historical arc of computer games, so further clarification for them, and audiences like them are not currently required.”  Those in the future, unversed in the early history of computer games, will need to do a significant amount of work to recover all of the implicit game history embedded within Pinchbeck’s references.</p>
<p>Another note is that Pinchbeck’s citations are more the norm in current game citation practice. The GAMECIP study of citation practice analyzed citations in over 300 publications relating to computer games. Of those, 102 different styles of citation were found, and of those only 31 included any information about game platforms. A majority simply focused on title, developer and year of publication. The main problem with this lax citation practice is that without at least a foundational set of descriptive elements tied to some expression of technical constraints and requirements, locating and replaying games referenced in scholarly works might be very difficult in the far future.<sup id="fnref:36"><a href="#fn:36" class="footnote-ref" role="doc-noteref">36</a></sup>  Altice’s end of the spectrum, with its acknowledgment of computer game materiality grounding out into the literal byte order of a file header, is more historically secure in theory but requires a level of technical understanding that might turn off scholars with less techno-materialist concerns.</p>
<p>In the end, a probable solution is to provide a minimally viable set of descriptive bibliographic fields based, again, on assumptions of reasonable compatibility and retrieval. These minimum specifications and recommendations for citation practice can be found in forthcoming work from the GAMECIP project. The main thrust, however, is that for the legitimation of any argument made about or through a game, there is a requisite depth of citation that aligns with the claim. From the above, the depth of Pinchbeck’s arguments dealt with apparent surface characteristics of games — characteristics that would hopefully be apparent to anyone playing one of the games cited. In the case of platform studies arguments, the claims are dependent on citation at a different depth, one close to the actual material existence of the program.</p>
<p>Hopefully, this section illuminated some of the problems with current manifest intertextuality in games, most specifically that, due to the current limitations of textual description, the field of game studies is dependent on a presupposition of played knowledge that is not tied to any specific material instances of games. The next section looks at this problem from the perspective of constitutive intertextual relations and provides a basis for our partial solution in the form of a citation tool for executable reference. This constitutive work is the result of confronting the current limitations of citations as they have been described so far. Primarily, when even the citation of specific, material data is not enough or of a kind with the expression of new historical claims.</p>
<h2 id="4-reduction-and-intertextual-expression">4 Reduction and Intertextual Expression</h2>
<p>Intertextuality is a fickle phenomenon. As noted by Fairclough above, when making one text manifest within another, work needs to be done to mold the “other text” in a form commensurate with the discourse surrounding it. Otherwise the textual surface is disturbed, and the flow of thought for the reader becomes more difficult to reconcile and interpret. (Of course in some instances, this might be desired as a way to remark on the disjunction between different textual forms and different ways of reading.) Fairclough looks at newspapers, medical interviews, and other forms of discourse dissimilar from the academic text within which he is operating. He focuses on the ways in which each discourse’s intertextuality contributes to its existence as a distinct genre, a distinct type of expression. This notion of constitutive intertextuality, the ways in which different discourses make use of and interact with other texts, is a fundamental aspect of discourse analysis. The constitutive act of bringing together other texts through manifest actions like citation, as noted by Ken Hyland in his study of academic citation practice,</p>
<blockquote>
<p>links text users to a network of prior texts depending on their group membership, and provides a system of coding options for making meanings. Because they help to instantiate or construe the meaning potential of a disciplinary culture, the conventions developed in this way foreclose certain options and make some predictions about meanings possible.<br>
<sup id="fnref:37"><a href="#fn:37" class="footnote-ref" role="doc-noteref">37</a></sup></p>
</blockquote>
<p>The organization of disciplines and regions of thought and inquiry, both in the humanities and the sciences, are then dictated through the intertextual relations of publications. These publications organize into networks that then enforce and negotiate the boundaries of disciplines, and the specific intertextual discourse required for group allegiance. We argue that the intertextual surfaces of these genres of discipline make use of certain conventions that can preclude certain types of meaning and the expression of certain types of thoughts.</p>
<p>By relying on standard conventions of manifest intertextuality, and therefore prescribing limitations on the expression of academic claims, we are preventing certain discussions from taking place. In the interest of this article, we are most concerned with the explanation and historical positioning of computer games as systems and technical objects. We remarked in the previous section that references stand in metonym for more complex thematic components and system interactions. In a sense, the discussion was really about the limitations of current textual discourse about games — discourse that relies on the narrativization of game play or the accumulated knowledge of the reader as player. That game academics use text as the major form of expression is understandable. Michael Lynch, when discussing scientists’ use of text over visuals, notes:</p>
<blockquote>
<p>The fact that writing is the dominant medium of academic discourse is not incidental; while pictorial subject matter is alien to written discourse, and requires a reduction to make it amenable to analysis, written subject matter can be iterated without any gap with the textual surface that analyzes it.<br>
<sup id="fnref:38"><a href="#fn:38" class="footnote-ref" role="doc-noteref">38</a></sup></p>
</blockquote>
<p>Games are even more removed from the textual surface than the visualizations Lynch is investigating. Their insertion into textual discourse filters through many different levels of  “reduction.”  Lynch’s argument focuses on the reduction of the worked scientific reality of the life sciences to the written page.  “Scientific research teams are described as agencies of mediation between an uncertain and chaotic research domain and the schematic and simplified products of research that appear in publications”   <sup id="fnref1:38"><a href="#fn:38" class="footnote-ref" role="doc-noteref">38</a></sup>. Researchers select and distill the appropriate data and reduce it to visualizations and textual description to align with the constraints of printed publications. This reduction of the chaos of a research project to the streamlined and validated prose of research publications is of a kind with the work of some game historical scholars in their attempts to better mind the gap between the played expression of games and their appearance and function in text-based academic arguments. Most of the time, game studies uses presupposition as a form of reduction, a way to fit the complex system interactions inherent to the experience of game play into readable discourse. This approach, however, largely limits the field of discussion for these games to their existence as  “objects played by a researcher in the past,”  preempting other means of using games in argument.</p>
<p>The notion of reduction is important to the overall discussion of intertextual surfaces and their effects on comprehension and expression. Reduction functions on a spectrum aligning with the goals of a particular discourse. The reduction from computer game to in-line textual citation is the most extreme form. Many others make use of, in progression: still images, sequences (or juxtaposition) of images, video, interactive visualization, and, in limited cases, emulation. In monographs, and examples like Pinchbeck’s and Altice’s above, only still and sequenced images are available. Pinchbeck narrates key areas of  <em>Doom</em>  with comparative juxtapositions of different game versions, and single images of key aesthetic and level design features. Altice makes extensive use of emulator tools for the visual display of internal memory states, or again, like Pinchbeck, comparative juxtapositions of key sequences or different passes of a rendering function.</p>
<p>Outlining the full extent of image usage in game studies monographs is well outside our scope, but the important consideration is the jump in textual mediation that occurs in the transition from collections of images to video, interactive visualization, or emulation. The textual surface described for the majority of this article is one of physical print and the constraints of its intertextuality. The newer forms of reduction are not mentioned by Lynch because they still remain unleveraged in the sciences — there are very few online publications in any field that leverage digital documents’ new textuality. Digital humanists cry out for more active, digital intertextual presentation, but their codified expression is only standardized in a handful of online publications.<sup id="fnref:39"><a href="#fn:39" class="footnote-ref" role="doc-noteref">39</a></sup>  Linking back, the expression of academic discussions of games is then dependent on the forms of manifest intertextuality that are available and commensurate with the dominant constitutive discourses that currently exist. When people want to engage with games in ways that are not commensurate with textual description they make use of less encumbering reductions. In our case, when trying to either explain embodied system interactions or complex dynamic processes, it is helpful to move beyond textual discourse as the only tool in the tool chest.</p>
<h2 id="5-types-and-examples-of-reduction">5 Types and Examples of Reduction</h2>
<p>Taking a step back, there are two key considerations at work in our discussion. The first is the intertextual surface of discourse and how it makes use of manifest actions, like citation, quotation, and images, in the constitution of a text. The flow of an argument is aided by the integration of “other” texts such that the discursive flow is smooth.<sup id="fnref:40"><a href="#fn:40" class="footnote-ref" role="doc-noteref">40</a></sup>  Whenever one is reading through a discussion and needs to refer to a figure, table, or other interstitial manifestation, there is then, borrowing from Lynch, a gap in the textual flow and thus in the discursive progression. An aim for any apparatus that allows for new types of manifestation must be to consider how that manifestation affects the constitution of a text, and the ways that manifestations can augment or potentially further distort a discursive surface. This surface is also, with advances in on-line technology and distribution methods, not only just a physical sheet of flattened, dead wood, but the transmediatic — interactive — surface of the computer screen and networked document. The medium of expression, in the case of computer games and systems, is now of the same stuff of the medium being described and discussed. There is potential for a better and more forceful alignment of textual surface with digital system expression.</p>
<p>The second key consideration is how reductions assist intertextual integration to enable new forms of argument. We are not the first to venture down this intermedial path, and by illuminating some further examples we can highlight the new types of expression that we hope to enable with the to-be-described tool. This section is mainly devoted to an elaboration of the second point about methods of reduction in light of the first’s concern for intertextual alignment and comprehension. The following discussion includes a collection of related and motivating work.</p>
<h2 id="51-video">5.1 Video</h2>
<p>Recall that the methods of reduction not discussed by Lynch are embedded video, interactive visualization, and emulation. Embedded is key since this allows us to present them as manifest intertextual objects (and later use some of the discourse analytical apparatus to discuss their effects). Reduction is a reduction of the embodied act of play to a form amenable to the constraints of the particular intertextual surface being created. Video reduction is fast becoming one of the major means for the dissemination of knowledge about how games are played, and the surface characteristics of their presentation to the player. As a phenomenon, this is beneficial to the progress of game historical study since at the very least there will likely be some video remnants of game play available to future preservation efforts.<sup id="fnref:41"><a href="#fn:41" class="footnote-ref" role="doc-noteref">41</a></sup>  The availability of video has also prompted some academics to begin experimenting with embeddable video as a means for discussion. For example, Doug Wilson, in an extended discussion of the game  <em>Spelunky</em>  for the Polygon website, makes extensive use of embedded YouTube videos to support a discussion and walkthrough of one of the game’s most difficult achievements, a no-death solo eggplant run.<sup id="fnref:42"><a href="#fn:42" class="footnote-ref" role="doc-noteref">42</a></sup>  By interweaving textual description with multiple embedded videos keyed to specifically salient moments, Wilson telegraphs a new type of intertextual surface where narrativized gameplay description is aligned with video supports. The technical mastery at work is made more apparent and visceral through the accompanying videos.</p>
<h2 id="52-visualization">5.2 Visualization</h2>
<p>Interactive visualizations as embedded arguments, the next step up the reduction ladder, are not a significant practice in academia. Certainly, visualizations — in the form of static images embedded in text — are very common in physical and social sciences, and in humanistic analysis of textual corpora. Woolgar and Lynch preside over two volumes dedicated to representation in scientific practice that are mostly focused on the constitutive power of manifest visualizations in scientific texts. The fact that the volumes are separated by 30 years indicates the continuing influence of visualization on scientific work. Analysis of the effect of visualizations on humanistic practice is probably most expressed in the attention to algorithmic criticism in the works of Stephen Ramsay — for corpora analysis — and David Staley — in historical visualization — among others <sup id="fnref1:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>  <sup id="fnref:43"><a href="#fn:43" class="footnote-ref" role="doc-noteref">43</a></sup>. However, the use of non-static, interactive visualization of dynamic systems is absent from the intertextual presentation of findings in most scholarly fields. The groundwork is actually being laid more by those interested in the pedagogical application of visualizations.</p>
<p>Bret Victor and his collaborators are at the forefront of so-called explorable explanations, juxtapositions of online text and embedded interactive visualizations designed to reveal the functionality of systems <sup id="fnref:44"><a href="#fn:44" class="footnote-ref" role="doc-noteref">44</a></sup>.<sup id="fnref:45"><a href="#fn:45" class="footnote-ref" role="doc-noteref">45</a></sup>  Some examples include a long explanation of the basic dynamics of simple animated pathing, and Vi Hart and Nicky Case’s interactive model of Thomas Schelling’s group segregation theories <sup id="fnref:46"><a href="#fn:46" class="footnote-ref" role="doc-noteref">46</a></sup>. Each example works to mollify the gap between the textual presentation inherent to the browser window and the machinations — and interactive features — of the supporting visualizations. Victor’s work espouses a pedagogical philosophy of system comprehension through manipulation and tacit experience. Readers are invited to read the expository prose, and then play with the interactive models of the phenomena on the page. The hope is that through tacit manipulation and play a deeper understanding of the underlying system will develop.</p>
<p>Victor refers to his online visualizations as reactive documents that allow the reader to test out the various models presented and gain insight through those interactions. The goal is to develop an active reader, someone who uses  “the author’s argument as a springboard for critical thought and deep understanding”   <sup id="fnref:47"><a href="#fn:47" class="footnote-ref" role="doc-noteref">47</a></sup>. Active reading owes much to the foundational pedagogical insights of Seymour Papert. Papert devoted his career to the development of computational tools to aid in mathematical and algorithmic thinking <sup id="fnref:48"><a href="#fn:48" class="footnote-ref" role="doc-noteref">48</a></sup>. He also believed that tacit experience with reactive systems would support better modeling capabilities in confronting new problems and challenges. He traced this potential to a youthful fascination with gears that implicitly enforced a tacit understanding of complex differential systems. His gears functioned as a model that enabled a better understanding of math  “than anything I was taught in elementary school”   <sup id="fnref1:48"><a href="#fn:48" class="footnote-ref" role="doc-noteref">48</a></sup>. The ability to present information in different and interactive ways led to a new model for the exploration of further knowledge. Papert linked this notion with the educational theories of Jean Piaget that espoused the assimilation of concepts into a learner’s world view. Papert’s gears functioned as an affective model, a mapping (assimilation) between their relational dynamics and other mathematical concepts. Victor’s work is then an attempt to embed these “affective models” as interactive visualizations into online documents.</p>
<p>The design and application of affective models that encourage comprehension of technical concepts is important for our larger argument about the potential for new forms of intertextuality to support new argumentation. Victor presents a prototypical means of doing more with online texts, and trying to engage the reader with the systemic processes under discussion. Victor’s reactive documents are a new discursive surface, one populated with interactive features aimed at creating a new type of active reader. They are also the result of a reduction from larger, complex system dynamics to concentrated, pedagogical visualizations designed to support textual arguments. The reduction, however, is much richer than an image or video, since it can support the creation of a tacit, embodied argument. Instead of referencing an image or video of a system processes, a smaller part of the system can be introduced into the discourse describing it — or, better yet, use the interactive surface as an argument in of itself for a particular point of view or affective process.</p>
<h2 id="53-emulation">5.3 Emulation</h2>
<p>Before discussing emulation as a form of reduction — in line with the progression outlined above — we need to clarify some basic technical distinctions and provide some related examples. This is necessary because the use of emulated systems as a form of argumentation about software history — or really any topic for that matter — has not before, to our knowledge, been explored or theorized.</p>
<p>Emulation, as a computational process, is the use of one system in reproducing the functionality and output of another.<sup id="fnref:49"><a href="#fn:49" class="footnote-ref" role="doc-noteref">49</a></sup>  Emulators, the programs responsible for emulation, are used in many corners of the software industry to allow for testing of applications on a variety of devices. For instance, most mobile phone applications are not developed on mobile phones. They are programmed in emulation on laptops and desktops more conducive to long bouts of typing and heartache. As noted by Nathan Altice, emulation has a long history tracing back to the historical origins of software development <sup id="fnref4:27"><a href="#fn:27" class="footnote-ref" role="doc-noteref">27</a></sup>. In the 1960s IBM developed the first sets of commercial emulators to allow software written for one mainframe model to be compatible with another.</p>
<p>More recently, in the 1990s, enthusiast game communities began to create emulators of popular game systems, like the Nintendo Entertainment System. Targeting then-current operating systems like Windows 95, these emulators allowed for the (re)play of older titles that might no longer be available for purchase, were released in foreign territories, or might otherwise be difficult to acquire. The games in this usage were data dumps extracted from the physical cartridges, and other forms of magnetic and optical media. For cartridge systems, these files are known as ROMs since they are copies of a cartridge’s read-only memory. A more general term for data extract from a media format is a data image, more commonly shortened to image.<sup id="fnref:50"><a href="#fn:50" class="footnote-ref" role="doc-noteref">50</a></sup>  Emulators operate on data formatted for a specific system, and over the last 20 years, emulation development and the extraction of legacy data from physical formats has flourished. Emulators now exist for thousands of different computational platforms, and are a ripe source for the exploration of software history. That is, assuming one ignores some significant legal issues.<sup id="fnref:51"><a href="#fn:51" class="footnote-ref" role="doc-noteref">51</a></sup></p>
<p>Important for our discussion of manifest intertextual presentation are recent developments in web browser technology (and speed improvements in general) that now make it feasible to run emulators inside online documents. The potential for embedded emulation has not yet been exploited or thoroughly explored. However, because most web browsers now run highly optimized JavaScript compilers, in-browser emulation is a growing phenomenon. The most prominent example is the JavaScript Multiple Arcade Machine Emulator (MAME) project. MAME began as a system for emulating arcade machines. Complementarily, the Multiple Emulator Super System (MESS), using a fork of MAME’s code base, provided support for most personal computers. MAME was open sourced in 2016 and MAME and MESS are now merged. The combined infrastructure supports thousands of different arcade machines and personal computers released over the last 40 years.<sup id="fnref:52"><a href="#fn:52" class="footnote-ref" role="doc-noteref">52</a></sup>  Initially a large C++ project, the Internet Archive, along with a collection of motivated developers, ported key components of MESS — before its integration with MAME — to JavaScript to create the Internet Arcade, a playable archive of the Internet Archive’s imaged software collection. After the open-source combination, JavaScript became one of a number of compilation targets for the entire MAME-MESS code base.</p>
<p>Similarly, many other emulators began organizing compilation to JavaScript. This included the emulators DOSBox, for legacy x86 MS-DOS machines; FCEUX, for the Nintendo Entertainment System; and Snes9x, for the Super Nintendo Entertainment System.<sup id="fnref:53"><a href="#fn:53" class="footnote-ref" role="doc-noteref">53</a></sup></p>
<p>Now that emulation is available in browsers, it is possible to place both a running program and the text describing or commenting on it onto the same intertextual surface. In the progression of reductions from images to interactive visualization, there was always a clear notion of how each step still represents a deficient copy of some object or system outside the text. For images and video, as mentioned by Lynch above, researchers put in a significant amount of effort to both make their samples more photogenic and thus more interpretable when presented on a textual surface. In dynamic visualizations, there is an implicit understanding that we are being presented with part of a system that has been distilled for comprehension and reader engagement. The very act of visualization is to provide a specific perspective (of many) on the data or system under discussion. With emulation, there does not appear to be a similar process at work. While the emulation is a program designed to conform to the constraints of a digital document, it is not a distillation of a system but a full version of the system itself. This challenges the basic premise of intertextuality presented above, that the texts made manifest in and reduced to a specific discursive surface are under the basic control of the author.<sup id="fnref:54"><a href="#fn:54" class="footnote-ref" role="doc-noteref">54</a></sup>  While some manipulation or presupposition — in cases where the author is misremembering or functioning with a divergent set of assumptions in regards to the reader — is always a potential issue, that the manifest references might have a mind or operation of their own was never imagined. In bringing emulation into the text, we therefore encounter a new type of intertextual interaction, and with it a different model of reduction — a model that requires significantly more effort in the legitimation of claims and the interpretive exercise. Presupposition of played experience cannot exert the same power because the system — the same system — is available to both author and reader.</p>
<p>The imbrication of emulation into argumentative texts has only been lightly attempted in the past. Nick Montfort wrote an article for the  <em>electronic book review</em>  that embedded a Java plugin-based emulation of the Infocom game Deadline <sup id="fnref:55"><a href="#fn:55" class="footnote-ref" role="doc-noteref">55</a></sup>.<sup id="fnref:56"><a href="#fn:56" class="footnote-ref" role="doc-noteref">56</a></sup>  However, his argument does not particularly integrate the emulation so much as position it in the article to simply show such a move is possible. Others have used online emulation for deeper systemic analysis. One notable example is Ben Fry’s early online visualization of the internal memory state of the Nintendo Entertainment System. The  “deconstructulator”  is a Java-based in-browser emulator based on a modified version of the NESCafe emulator <sup id="fnref:57"><a href="#fn:57" class="footnote-ref" role="doc-noteref">57</a></sup>.<sup id="fnref:58"><a href="#fn:58" class="footnote-ref" role="doc-noteref">58</a></sup>  The visualization presents three different windows, a rendering of the full sprite memory of  <em>Super Mario Bros.</em> , a playable emulation of that game, and the active memory contents of the NES’s PPU (Picture Processing Unit), a component that manages sprite rendering on positioning on screen. As the player plays  <em>Super Mario Bros.</em> , the contents of the sprite map highlight the currently active sprites in use, and the PPU map shows the current state of each 8x8 tile in the PPU. By moving Mario around, the player can see how the different animations and changes to the game’s background, enemies and platforms modify the NES’s system memory. Fry designed the piece for his  “Visually Deconstructing Code”  series, a set of small projects aimed at unearthing some of the hidden processes at work in NES code.</p>
<p>Other examples of emulation as a revelatory mechanism exist within the communities devoted to forms of what James Newman refers to as  “superplay”   <sup id="fnref:59"><a href="#fn:59" class="footnote-ref" role="doc-noteref">59</a></sup>. Activities like speed-running (both human and tool-assisted), glitch-hunting, sequence-breaking, and other forms of  “performative mastery”  of games benefit from research conducted with emulation. Many community emulators support tools for memory analysis and even scripting languages for the live manipulation of a running game. This allows player-performers interested in, for example, shaving that last second off of a run or getting past a boss without attacking, to dig beneath the representational surface of the game and mine its system for potential solutions. Some online streamers, like Clyde Mandelin, write custom emulator modifications that allow for live streaming of both their gameplay and aggregated statistics or interactive visualizations of the underlying system.<sup id="fnref:60"><a href="#fn:60" class="footnote-ref" role="doc-noteref">60</a></sup>  As we will discuss below, it is becoming clear that a community of practice is developing around the expressive potential of emulators. It’s also a sign that the products of community historical efforts are becoming more aligned with digital humanist insights about technical collaboration between academia and amateurs.<sup id="fnref:61"><a href="#fn:61" class="footnote-ref" role="doc-noteref">61</a></sup></p>
<p>However, it is also necessary to note that, in placing a running program into an online text, its reduction to that surface may suggest a raft of potentially misleading assertions about the historical play experience. As outlined extensively in the work of preservation-minded historical researchers, emulation, in its erasure of the original executable context, denies the experience of the original hardware <sup id="fnref:62"><a href="#fn:62" class="footnote-ref" role="doc-noteref">62</a></sup>  <sup id="fnref:63"><a href="#fn:63" class="footnote-ref" role="doc-noteref">63</a></sup>  <sup id="fnref1:59"><a href="#fn:59" class="footnote-ref" role="doc-noteref">59</a></sup>  <sup id="fnref1:29"><a href="#fn:29" class="footnote-ref" role="doc-noteref">29</a></sup>  <sup id="fnref2:25"><a href="#fn:25" class="footnote-ref" role="doc-noteref">25</a></sup>. The modern web browser, as a displayed surface, is very different from an Atari-era CRT, and most modern machines do not have a way to interface with original peripherals. Additionally, many emulators try to make the played experience smoother by modifying speed for the sake of accuracy.<sup id="fnref:64"><a href="#fn:64" class="footnote-ref" role="doc-noteref">64</a></sup></p>
<p>They also remove old constraints on the swapping of disks to load parts of the program in piecemeal and internal memory limitations. However, not all aspects of emulation are a historical loss, since the position of the emulation as running inside a host process allows for the introspection and revelation mentioned in the above examples. The ability that many emulators provide to save and load memory states is also, as we will see, a boon to players and researchers hoping to encounter difficult, confusing or novel locations inside games.</p>
<p>A key note about the reductions above is their ability to bring something from “out there in the world” into the text. Usually those studying academic discourse, or the social construction of academic arguments, focus on how that external evidence is transformed into a manifest object in the text. For scientific work, we have mentioned both discourse analysis and science and technology studies as fields that theorize on the reduction and distortion of tacit laboratory knowledge into written discourse.<sup id="fnref:65"><a href="#fn:65" class="footnote-ref" role="doc-noteref">65</a></sup>  In the humanities, citation and reduction are less epistemologically fraught, since the aim of humanistic discourse is not generally to re-organize some empirical object or finding into a textual form suitable for publication.</p>
<p>Within history, the use of manifest intertextuality is critically important to the sustenance of the field, but the act of manifestation does not usually imply a reduction of a finding out there; the out there of historical sources being mainly other texts. Rarely is the historical object, if there is one, reduced to a form commensurate with the textual surface. In fact, much historical work into objects specifically addresses this issue, a good example being John Law’s work in aircraft design that explicitly constructs different historical strands of documentation to reveal the fractal nature of the object in question. In his case, he looks at the construction of the British TSR2 strike and reconnaissance aircraft, and how it exists as an object of engineering, marketing, and an embodiment of the projection of hegemonic force. The aircraft is viewed along different evidentiary axes to support a conclusion about how objects exist in myriad ways depending on how they are documented and narrativized. This again ties back to ideas from discourse analysis, mainly in how the constitutive intertextuality at work in the history of science and technology defines the objects of analysis; a summary from Steve Woolgar:</p>
<blockquote>
<p>Surely, it is often said, it is absurd to say that we cannot distinguish between a thing and what is said about that thing. But the constitutive view does not prohibit such distinctions. It offers us a way of seeing these distinctions as actively created achievements rather than as pre-given features of our world. In particular, the distinction between talk and objects-of-talk is seen from the constitutive perspective as the upshot, rather than the condition, of discursive work.<br>
<sup id="fnref:66"><a href="#fn:66" class="footnote-ref" role="doc-noteref">66</a></sup></p>
</blockquote>
<p>The fundamental takeaway is that previously, describing any technical system as a historical object necessitated various forms of reduction and other intertextual strategies to remediate and insert it into a text. With embedded emulation, and to a lesser extent dynamic visualization, embedding the system itself must now be reconciled. When John Law describes his aircraft, he could not bring the aircraft into the text and let the reader hop into the cockpit. With computational systems increasingly the site of construction and reception for scholarly work, and with technical historical objects that are also computational systems, we can literally transcribe discussed objects into discourse and invite the reader to take the flight stick.</p>
<h2 id="6-back-to-citation-and-archives">6 Back to Citation and Archives</h2>
<p>In bringing a non-text-based object into textual discourse — like the reductions of image, video, visualization, and emulation above — there is a key link to archives and citation that has not been made explicit. In the case of online documents that incorporate various reductions, those texts are not singular objects but networked organizations contingent on access to the various sources of reduction. If one prints an image alongside text, the image is now part of the textual form, and is, from an archival standpoint, part of the same object. With online work, every page of information is an aggregate object. The basic markup for the page comes from one source, the styling of that page from another, and all the various images and other embedded entities from still others within that same domain or from some other (hopefully trusted) source. When things are embedded — as images, videos, visualizations, and emulations are — they necessitate and depend on the existence and maintenance of stable links to recover their data.</p>
<p>The maintenance of these links is a significant issue for the stability of knowledge online. Whenever a link leaves its local namespace (assuming that internal network links are maintained, which is not always a given) it relies on the existence, capabilities, and restrictions of a remote hosting repository. For videos, most content links resolve thanks to the embedded link architectures of mass scale video sites like YouTube or Vimeo. This allows the embedder to neither have to maintain their own video server nor provide the bandwidth necessary for playback. It also removes responsibility for intellectual property management and ties access to embedded content to the whims of the content provider. In the  <em>Spelunky</em>  example above, Wilson toyed with the constraints of YouTube’s embedded video player to reveal specific salient content inside the game. That action was only made possible by the affordances of YouTube, the repository hosting the content. In the case of historically stable online academic discourse, it should be apparent that any new ability to share or link to digital data incurs a commensurate necessity for a functional and stable repository. The current solutions for video leave a lot to be desired given that they are bound to the corporate imperatives of actors not emphatically concerned with preservation or link stability.<sup id="fnref:67"><a href="#fn:67" class="footnote-ref" role="doc-noteref">67</a></sup></p>
<p>The tool below is an attempt to organize a prototypical archive for embedded emulated content, and to try and reconcile some of the manifest intertextuality present in game historical work to a more stable set of practices regarding citation and linking. In the case of embedded emulation, the data has similar issues to that of video. Namely, that the IP rights for the distribution of streaming copyrighted gameplay data need to be correctly managed, and that the embedded content be presented in such a way to make it useful for inclusion into texts. The consideration for future scholarly use of emulated content is a way to dictate what a speculative collection of such works would look like at a larger, institutional scale. Additionally, the consistent citation of this content, as an initial condition of the system’s functionality, should be a concern for any future work in the creation of links to new forms of digital expression and reduction.</p>
<h2 id="7-a-tool-for-descriptive-and-manifest-citation-of-games">7 A Tool for Descriptive and Manifest Citation of Games</h2>
<p>This section outlines the design and functionality of the citation system in the Game and Interactive Software Scholarship Toolkit (GISST). GISST is a suite of tools aimed at helping with common game studies and game historical tasks, and includes a system for the management of manifest citation of both game emulation and game bibliographic references. The citation component of GISST — described below as the citation tool — is designed to partially address numerous issues presented above:</p>
<p>The need for more consistent bibliographic citation information for computer games.   An example use case for the placement and manipulation of various reductions of computer games into online text (in this case, images, videos and live emulation).  The need for a managed archive of the reductions used in (2).</p>
<p>The citation tool functions on three classes of objects — games, performances, and game system states — and resolves issues 1-3 for each. Game objects are collections of data about a game. This includes both basic descriptive metadata (required for correct bibliographic entries) and links to executable data needed to run the game in browser emulation. Performance objects are records of games as played or viewed by a player or group. These records also combine viewable performance data with descriptive metadata. By viewable performance data we mean two possible things. One is a collection of image frames — GIFs or video — representing some situated act of play. The other is replay data — input stream recordings for emulators or replay files for a specific game engine.<sup id="fnref:68"><a href="#fn:68" class="footnote-ref" role="doc-noteref">68</a></sup>   Game system states are snapshots of a game’s run-time memory, either saved by the emulator as a separate file or extracted directly from a system’s memory.</p>
<p>The tool manages game, performance, and game state records in a database, and allows for the embedding of any of these in a hypertext (assuming executable or viewable performance data is available). The rest of this section briefly accounts for the inclusion of performance in our citation apparatus and then lays out the functionality and potential future work for the tool.</p>
<h2 id="71-game-v-performance">7.1 Game v Performance</h2>
<p>The discussion above has mostly dealt with the citation of game objects as a means of presupposing their content and the contours of their gameplay. However, game performances as events are also commonly referenced in scholarly works. Performances result from two activities: games-as-performance, in the case of games tied to explicit geo-temporal contexts (alternate reality games (ARGs), installations, etc.) and gameplay performance. Gameplay performance is the play of a game that is not explicitly tied to a geo-temporal context, but that gains relevance from being situated in one. An example is a particular match at a fighting game tournament, where the event itself highlights game play as performance. The game in this case is not the operative site of performative relevance: its play at the tournament is. If the same match occurred in practice in a dorm room, it would not have the same significance.</p>
<p>Game performances as significant historical sources are well discussed in the literature. Clara Fernández-Vara, in her Introduction to Game Analysis, notes that,  “we may want to analyze a game that is an event, a be-there-or-be-square type of thing, a performance”   <sup id="fnref1:62"><a href="#fn:62" class="footnote-ref" role="doc-noteref">62</a></sup>. She describes the need for secondary sources — paratexts like video or firsthand description — in helping to reconstruct and corroborate information about a performance. This sentiment is echoed in Henry Lowood’s work on the reconstruction of events that take place in massively multiplayer online games (MMOGs) <sup id="fnref1:63"><a href="#fn:63" class="footnote-ref" role="doc-noteref">63</a></sup>. In this case, the study of virtual world game play is more akin to anthropological work. The game itself, while it could be recovered and run through emulation in the future, is devoid of the community that created meaning through the performative space and affordances the world provided. Lowood remarks on the fallacy of an ideal perfect capture of every event and input supplied to the virtual world.</p>
<p>Even if we save every bit of a virtual world — its software and the data associated with it and stored on its servers, along with a replay of every moment as seen by players — it may still be the case that we have completely lost its history. The essential problem with this approach is that it leaves out the identification and preservation of historical documentation, and these sources are rarely to be found in the data inside game and virtual worlds or on the servers that support them <sup id="fnref2:63"><a href="#fn:63" class="footnote-ref" role="doc-noteref">63</a></sup>.</p>
<p>Even with access to game replay files, or reproductions through emulation, evidence of a game performance must also be paired with secondary information to substantiate and analyze it. Our inclusion of performance citations in the tool is to enable a link between the game object’s data and description, and further contextualizing performances. Additionally, the ability to embed emulation in line with historical performance video and description adds further potential for somatic contextualization of game play. By bringing the emulated system to the reader, they can gain a sense of what Steve Swink refers to as  “game feel,”  an embodied understanding of the game system as felt through the act of play <sup id="fnref:69"><a href="#fn:69" class="footnote-ref" role="doc-noteref">69</a></sup>.<sup id="fnref:70"><a href="#fn:70" class="footnote-ref" role="doc-noteref">70</a></sup>  Pairing this embodied understanding of a game play system with performances adds another level of intuitive understanding to historical game play acts.</p>
<h2 id="72-citation-tool">7.2 Citation Tool</h2>
<p>The citation tool has two primary components:</p>
<p>A command line interface (CLI) responsible for the ingestion of game and performance data, the generation of citations, and the management of the citation database.   A web application (the app) that enables the live emulation of ingested game data, the live recording of game play performances, and the live recording of computational game states.</p>
<p>For the rest of this section we will use CLI to refer to the first component, and “app” to refer to the second. Their functionality is significantly interrelated, for example the CLI launches the app and the app’s backend server calls the CLI for certain processing tasks. The next two sections provide a very brief technical overview of both components; we refer the reader to our prior work for a more detailed account <sup id="fnref:71"><a href="#fn:71" class="footnote-ref" role="doc-noteref">71</a></sup>. Figure 1 illustrates the relationships between the various components.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000501/resources/images/figure01.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000501/resources/images/figure01_hufcd7c9c212df5937c109edf486dab707_53968_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000501/resources/images/figure01_hufcd7c9c212df5937c109edf486dab707_53968_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000501/resources/images/figure01.png 756w" 
     class="landscape"
     ><figcaption>
        <p>GISST components and pipeline.
        </p>
    </figcaption>
</figure>
<p>In the GISST pipeline, input resources (1) are fed to the CLI (2), which extracts their information (3) into an extraction table (for URLs) or the citation database (for performance and game data). The Web Application reads from the citation database (4) and the Indexer uses CiteState.js to create further citable resources (5). CiteState.js (the software powering the augmented  <em>Doom</em>  walkthrough from our introduction) can then use those resources’ permanent URLs (6) and its own citation function (7) to embed an executable program into a target HTML element (8).</p>
<p>Extraction begins with either local filenames or Universal Resource Identifiers (URIs). For games, the extraction files are either game data files or directories containing game data. Providing a URI to the game extraction command assumes that the linked resource hosts information about a particular game; currently, extraction supports game reference URIs for either MobyGames or Wikipedia. Performance extraction only accepts URIs from YouTube presently, but the design is modular and could easily support, for example, archives of game input sequences.<sup id="fnref:72"><a href="#fn:72" class="footnote-ref" role="doc-noteref">72</a></sup></p>
<p>Extraction obtains rough, noisy data from the given source or sources, with the assumption that a researcher will clean up the imported metadata. The extraction step is necessary to construct a stable citation because the information provided by a potential resource may exceed the constraints of the database&rsquo;s descriptive metadata scheme or require further disambiguation. As an example, the Wikipedia page for  <em>Super Mario Bros.</em> , originally released for the Nintendo Entertainment System, combines all information about the title, in all of its different versions and releases, onto a single page. Our Wikipedia extractor presents all of this information to the user and allows them to choose the particular version of  <em>Super Mario Bros.</em>  they wish to later cite.</p>
<p>GISST currently allows for extraction from a variety of sources and file types, as shown in Table 1 below:<sup id="fnref:73"><a href="#fn:73" class="footnote-ref" role="doc-noteref">73</a></sup><br>
Table 1: GISST Supported Resources    Citation Type  Supported File Types  Supported URI Sources      Game <br>
.NES ROM Format</p>
<p>.SMC ROM Format</p>
<p>Any directory containing a DOS compiled executable</p>
<p>.z64 ROM Format (partial)</p>
<p>MobyGames</p>
<p>Wikipedia<br>
Performance <br>
FM2 Replay Format</p>
<p>Generic Video Files<br>
YouTube   <br>
Any source data that is extracted (currently game files and videos) is linked to a dependent citation entry. This allows for the recovery of source data in the web application interface through either emulation or video playback.</p>
<h2 id="73-web-application">7.3 Web Application</h2>
<p>The GISST app is a standard browser-based web application, with a JavaScript/HTML/CSS front-end designed for use with the Chrome web-browser, and a backend interface that is linked to the same database and ingestion commands as the CLI. This is marked as steps 4 and 5 in Figure 1 above. The app allows for the play, recording and citation of game states through CiteState.js (a suite of automated JavaScript ports of emulators).</p>
<p>The app’s interface supports four basic views of citation data:</p>
<p>A basic listing page for the game and performance citations in the database.   A full text search page that includes all citation records and game save state descriptions.   A citation listing page that provides active links to previous save states and, for performances, the ability to create quick GIF animations based on a performance video.   An indexer page that allows for examination of an emulated game, and the creation of game save states and video recordings.</p>
<p>Entries 1-3 are mainly presented as data tables supplemented with item 3’s “active links.” The indexer, however, is the primary user interface for game scholars interested in creating new performance data. The indexer integrates live game play with game performance recording, retrieval, and reference tools. The game moments captured in our introductory  <em>Doom</em>  example were all acquired and stored via this indexer interface.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000501/resources/images/figure02.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000501/resources/images/figure02_hu201637389163fcc3faa379084431a97b_617809_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000501/resources/images/figure02_hu201637389163fcc3faa379084431a97b_617809_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000501/resources/images/figure02.png 945w" 
     class="landscape"
     ><figcaption>
        <p>Indexer User Interface. The emulation window is on the left, with a recording of that window displayed on the right.
        </p>
    </figcaption>
</figure>
<p>Proceeding to the app’s user interface (displayed in Figure 2), we will briefly describe each button and component. The buttons under the main emulation window provide for most of the data recording features. The user can load the emulation, save a state, load the most recently saved state, control video recording, and mute audio. When a state is saved, it is logged in the Available States tab along with a screen shot and generated descriptive information. Clicking on any available state will cause the emulator to immediately load that state into the main window. After a state is selected, the State tab in the left side bar lets the user change its descriptive metadata. All changes logged in the side bar are propagated to the server and will show up in search queries. Video recording functions in a similar way. Any time a start-stop sequence is completed, the beginning and ending state of the emulation will be saved along with the video.</p>
<p>Each recorded performance appears in the Available Performances tab. Clicking on a performance updates the sidebar’s Performance tab, allowing for the review of a recorded video and editing of its generated metadata.</p>
<p>Any performance or state saved in the analysis tool will appear in the main citation listing page, and on individual pages for each respective game and performance. The state links on each individual page operate as active links, in that they will load up the indexer page with the correct state preloaded into the emulator. This makes each active link a link into a running emulation as a specific point. We take up the pedagogical and analytical implications of this in our expert evaluation review (in the next major section of this article).</p>
<p>The indexer, in creating and storing the saved states and performance recordings, provides the source material for future links created by the CiteState.js module. The CiteState.js interface allows for a simple description of a target page element and an id from the citation database. CiteState.js then automatically handles the loading of a game, performance, or state, and places it into an HTML element that can be aligned by the user through CSS styling or other means of element positioning (this is steps 6-8 in Figure 1 above). This completes the chain from source ingestion through shared linking of a game emulation in a web browser.</p>
<h2 id="74-future-work">7.4 Future Work</h2>
<p>The citation tool opens up numerous opportunities for the dissemination and standardization of game historical sources. For bibliographic record purposes, all the information in a specific citation store could be exported into forms compatible with common citation database formats, like BibTeX, or linked with citation systems, like Zotero.<sup id="fnref:74"><a href="#fn:74" class="footnote-ref" role="doc-noteref">74</a></sup>  These citations could also automatically include information about a compatible emulator and the file-specific data required by more technical scholars, like the platform and software studies researchers mentioned above.</p>
<p>In the realm of reductions, since the emulation is a full computing system running in a web page, its memory and operations are totally available to introspection via other concurrent JavaScript processes. We are already working on including memory manipulation functionality in the CiteState.js interface, which would provide dynamic visualization of a complete program to occur coincidentally in the browser surface.</p>
<p>Lastly, since each of the citation types, games, performances, and game states also require a linkage between the citation and some form of born-digital data, new forms of storage and retrieval will be necessary. There is some work on storing and loading emulated systems or sharing the results of emulation produced on cloud-based servers, but still no general solutions for the storage and retrieval of executable software, nor support for citation as envisaged in the functionality of the tool above.</p>
<p>The next section — an expert evaluation of GISST’s citation component by practicing game studies scholars and library professionals — also presents some significant ideas for future work.</p>
<h2 id="8-evaluation">8 Evaluation</h2>
<p>As described, the citation component of GISST is an argument for more rigorous citation of computer games, and for the augmentation of their expression in game studies discourse. We believe the tool can ease the citation burden for game scholars and allow them to create new types of arguments and expressions about games. To corroborate this belief, we conducted a speculative expert evaluation of the tool, inviting comment from a group of professionals engaged with game study and preservation. The goal of the study was to ascertain if the intentions of the tool were clear, if our thoughts above aligned with those of practicing scholars, and to invite constructive commentary. This section outlines the evaluation and its responses, and how those responses aligned with our goals and ignited ideas for future work and collaboration.</p>
<p>The evaluation consisted of a set of 11 questions to be answered based on a 5-minute introduction to the CLI and web app components of the tool. We sent the evaluation to a select group of practitioners consisting of game designers, game studies scholars, and librarians. These groups align with those we hope will benefit most from the citation tool. All those chosen were already aware of GISST, and the video served as a reminder of functionality that had at some point been demonstrated to them in person. Responses were collected from seven people through an online form. Respondents included: Henry Lowood, curator of the History of Science and Technology and Film and Media Collections at Stanford University; Chaim Gingold, a game designer and historical researcher; Nathan Altice, a professor and game historian at the University of California, Santa Cruz; James Newman, a professor and game historian at the University of BathSpa; Glynn Edwards, head of technical services in Special Collections at Stanford University Libraries; Shane Denson, a professor of Art History at Stanford; Douglas Wilson, a game designer and lecturer at RMIT University; and a professor who wished to remain anonymous.<sup id="fnref:75"><a href="#fn:75" class="footnote-ref" role="doc-noteref">75</a></sup>  The remainder of this section will describe the responses to the tool, followed by recommended improvements.</p>
<h2 id="81-discussion">8.1 Discussion</h2>
<p>Overall, the responses were overwhelmingly positive, with one game studies scholar stating that the availability of the tools could be huge for the field. The respondents hailed from an overlapping set of backgrounds, but the responses aligned along two basic paths. The first was how the tool could affect game studies and game historical practices in citation, and what the tool could contribute, through state citation and retrieval, to students and game studies scholars. The second turned toward more of the potential for preservation that the tool presents in its management of game citation and game states.</p>
<p>The potential influences noted for game studies practice included (1) the formalization of game citation practices, (2) the removal of obstacles to game access, (3) the automation of game history tasks currently taken on in an ad-hoc manner, and (4) the presentation of deeper, and more comprehensive, historical analysis. Multiple respondents noted the tool’s implicit call for a more  “formal and robust”  citation practice for games, with Henry Lowood stating that the tool provided a first take on a  “citation framework where there was none.”  The tool functioned as a way to call attention to the potential of better citation practices. James Newman explained,  “a contribution of the tool will surely be to heighten discussion of citation and [its] limits and variations in current practices.”  This therefore aligns with our arguments for more consistent citation practice in the game studies section above.</p>
<p>Altice, Newman and Shane Denson highlighted the tools’ ability to provide an easier route to specific game locations and gameplay sequences. Altice and Denson specifically work on comparative analysis of game versions and emulators, so the potential for the tool to make parts of games more reachable was appreciated <sup id="fnref5:27"><a href="#fn:27" class="footnote-ref" role="doc-noteref">27</a></sup>  <sup id="fnref:76"><a href="#fn:76" class="footnote-ref" role="doc-noteref">76</a></sup>. This concern for access to game history also extended to the other scholars, who all remarked on the ability of the tool to make classroom lectures more engaging, and according to Altice provide for  “in-class play that isn’t contingent upon equipment or playing skill.”  He felt this allowed for a  “wider breadth of examples”  since lengthy equipment set up or hours spent trying to get to a particular spot in a game could be removed from the equation. Altice also felt that this approach could make exploring games like  “flipping to a relevant page in a book, which could make citation more prolific and illustrative.”  He also believed that playing a game’s citation would have a more powerful rhetorical effect than other forms of reduction, nicely pairing with our claims about rhetoric and game feel above. Chaim Gingold also felt that the tools provided a significant new way to share content with students. He imagined providing state citations to students in the future as one might today assign videos on YouTube or Twitch.</p>
<p>The tools as an automated solution for citation also struck a chord with the researchers who already use an assortment of ad-hoc solutions for emulation, and gameplay recordings and analysis. Newman already organizes a rather complex chain of tools for gameplay capture and analysis, and the tools provided a way to alleviate some of the burden in getting multiple programs and systems to work together. Gingold also noted that the tools could allow students to engage in the types of historical analysis that are only available to interdisciplinary scholars with programming and humanities backgrounds. Students could incorporate  “their interaction into their scholarship (like us!),”  and provide them a starting point for more detailed analysis of design and game play interactions.</p>
<p>The last major response area (in this first thread of responses, on game studies and game historical practices) was about the ability for the tools to provide a new level of analysis for game studies and game history. Newman noted that having access to a GISST-like system would remove the need to engage in extended descriptions of game play or game scenes. If a citation is also a playable instance, he could rely on the reader to play what he was talking about, and then focus his time on deeper analysis instead of front-loading arduous amounts of descriptive text to set up his points.</p>
<p>Newman suggested that being able to refer to a persistent recording or savestate would give him increased confidence in writing more detailed analyses of sequences of gameplay and would, hopefully, alleviate some of the need for description in favour of close commentary and annotation.</p>
<p>Denson concurred, stating that  “arguments can [now] be illustrated directly (through video, for example) and even mounted through hands-on engagement (gameplay) rather than merely discursive description.”</p>
<p>The second major thread in the responses highlighted the tools’ implicit effects on game preservation and the organization of game history. Some viewed the tools as an argument for more robust digital repositories able to handle and retrieve executable content. One noted that the tools displayed the power of centralizing documentation about games and how the coordination of tools could foster new expressions through the linkages of different technologies. In this case, the concordance of emulation, documented citation, and gameplay videos invited a discussion of the need for coherent underlying infrastructure to support preservation of those outputs. Lowood agreed, saying that the tools put  “issues around documentation, archiving and gameplay preservation front and center.”  Glynn Edwards also focused on the needs to create consistent metadata schemes for the emulated save states and companion documentation. There was a general agreement among the preservation professionals that the tools’ existence, in and of themselves, functioned as an argument for better preservation practice, and they were excited to begin working towards solutions.</p>
<p>Another small preservation note was that of the tools’ ability to allow for quick validation of game files, and game data integrity. By ingesting an executable into the system, one can easily check if it is compatible with a specific emulator, and if it is actually the file it claims to be. Lowood also noted that the tool could push repositories to negotiate better IP rights access to executable software, or, at the very least, further reveal the need for that work to be figured out (see <a href="#fn25">footnote 25</a>).</p>
<h2 id="82-improvements-and-future-work">8.2 Improvements and Future Work</h2>
<p>Given the positive response to the work, most of the critical discussion of the tools pointed solely to means of immediate improvement and new features. In the main, many respondents wanted the tools to continue development of better UI and user accessibility features. Right now, most of the ingestion apparatus occurs on the command line, and Altice correctly felt that  “freeing the tool from dependence on the CLI”  would be necessary since  “this would be a non-starter for many (most?) scholars without a technical background.”  Many also pointed out that while the technology had obvious potential, as noted in the last section, it needed a set of coherent examples and illustrative case studies. The tools represented more of a starting point for new discussions in game studies and game preservation but were not really yet a solution (though with work they could be). This appears to show that the tools are a ripe ground for future work, as just making them more accessible and easier to use excited many of the respondents. One even requested a basic tutorial for the current alpha prototypes since they felt they required hands-on access to fully appreciate the prototypes’ potential use cases.</p>
<p>Another general request was for the inclusion of more emulators than the four currently available to allow both for comparison of the emulators themselves, and to help with further preservation questions in the description of the configured environments needed to support game data. Many noted that the tools, in both their analytic and preservation potential, pointed to uses outside of games, and would be a boon for software studies and general software preservation. This was edifying, since another thread of our overall thesis is that most advances for game software history are also significantly applicable to broader classes of software.</p>
<p>Future work dovetailed with the requests for more system support and usability. Many wanted to see what a larger, shared repository of game state citation could afford, either in a classroom setting or for executable collections in libraries and archives. There was also a call for more explanation of the citation description formats, and perhaps even integration with current scholarly citation tool sets like Zotero. Multiple respondents also mentioned the potential for annotation tools to add voice overs to videos, and record and remark on game play input traces. Gingold specifically was excited about the ability to introspect on the running emulations, and visualize their system dynamics and memory states, similar to our own thoughts on future work above.</p>
<p>In closing, the responses to the tool essentially agree with the arguments presented above. Respondents believed that embedded emulations represent a new form of expression for game history, and that tools themselves function as an argument for further work on technical system visualization, documentation management, game citation, and preservation. We believe that this suitably validates the tools, the methodologies they support, and theories behind them in ways that not only legitimate them as contributions to multiple fields, but as a starting point for more significant future work, and perhaps even future research areas.</p>
<h2 id="9-conclusion">9 Conclusion</h2>
<p>At the beginning of this article, we argued that game and game performance citation are underdeveloped, immature aspects of scholarly practice. This proceeded through a more in-depth discussion of the purpose and functionality of citation in both scholarly discourses in general, and towards the ways in which games are a new and special case. As a result, the system we described to manage and create playable manifest citations required not just novel engineering effort but a theoretical consideration of the types of objects within and around games that could be leveraged in arguments. The initial prototypes of the GISST system actually preceded and evolved in dialogue with our new conceptualization of game citation.<sup id="fnref:77"><a href="#fn:77" class="footnote-ref" role="doc-noteref">77</a></sup>  In attempting to find ways to cite games, we incidentally were forced to create the technical means for those citations, and to figure out how those citations could be made available and useful for argumentation. This then contributed new means of historical expression: we realized that the results of the citation system creation represented a new general class of activity in the design of systems to support game scholarship. The citation system is a central component of GISST because we believe that citation work is a necessary precondition for a whole range of possible tools for game history, game studies, and software studies works.</p>
<p>The citation work elaborated on a means for the manifest citation of new objects into scholarly discourse, but it also supported those objects’ creation and storage. As a result, this opened up those objects (performance videos, executable game play, and indexed game states) to further analysis. In addition to functioning as a form of reduction in supporting textual discourse, the objects are also now organized and manipulable by any potential future extensions of GISST’s toolset. As mentioned in Future Work above, this could mean input analysis and replay of game play — a means to further look at instances of superplay like speed running or glitch-hunting — and introspection on the game’s system state and run-time memory. In tracking the needs for a citable base of games, performance, and states, we have opened up a whole new set of resources and opportunities for exploration and expression through scholarship. This provides yet another example of how the stabilization of historical resources can lend itself to new uses and articulations.</p>
<p>GISST resolves many key issues in game citation, but more importantly it comprises a compelling argument in the ongoing development of norms and standards in game scholarship. It encodes in its database schema and user interface a view of what it means to talk about a game or game performance in a recoverable, useful way which is not prone to presupposition. It is an instance of the rhetorical approach promoted by Ramsay and by Burdick et al: we  “contribute to humanities theory by forging tools that quite literally embody humanities-centered views regarding the world”   <sup id="fnref3:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>.</p>
<h2 id="acknowledgements">Acknowledgements</h2>
<p>The authors wish to thank Brandon Butler, Director of Information Policy at the University of Virginia Library, for help in the clarification of GISST’s potential legal context in endnote 23. This work was supported, in part, by Institute for Museum and Library Services grant LG-06-13-0205-13.</p>
<h2 id="appendix">Appendix</h2>
<p>Figures below illustrate the interactive saved states.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000501/resources/images/01_wolf3d_start.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000501/resources/images/01_wolf3d_start_hu9f73dda9bb1ec4173aa2334cf4af420f_88732_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000501/resources/images/01_wolf3d_start_hu9f73dda9bb1ec4173aa2334cf4af420f_88732_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000501/resources/images/01_wolf3d_start_hu9f73dda9bb1ec4173aa2334cf4af420f_88732_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000501/resources/images/01_wolf3d_start.png 1286w" 
     class="landscape"
     ><figcaption>
        <p>wolf3d_start. The initial player view upon loading the first level of Wolfenstein 3D the player&rsquo;s weapon, a door and a dead body.
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000501/resources/images/02_first_barrel.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000501/resources/images/02_first_barrel_hub86a6a52c5c3aceb06243750e92c1e34_246582_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000501/resources/images/02_first_barrel_hub86a6a52c5c3aceb06243750e92c1e34_246582_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000501/resources/images/02_first_barrel_hub86a6a52c5c3aceb06243750e92c1e34_246582_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000501/resources/images/02_first_barrel.png 1280w" 
     class="landscape"
     ><figcaption>
        <p>first_barrel. One of the first interactive explosive barrels in Doom, waiting to be shot by the player in Episode 1 Map 1 (E1M1).
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000501/resources/images/03_window_goo_lake.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000501/resources/images/03_window_goo_lake_huf015515d529a4f98e6980f087be76a1e_215344_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000501/resources/images/03_window_goo_lake_huf015515d529a4f98e6980f087be76a1e_215344_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000501/resources/images/03_window_goo_lake_huf015515d529a4f98e6980f087be76a1e_215344_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000501/resources/images/03_window_goo_lake.png 1286w" 
     class="landscape"
     ><figcaption>
        <p>window_goo_lake. A view out of the first window in Doom, showing the tantalizing prospect of a hidden armor upgrade in a lake of toxic goo.
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000501/resources/images/04_armor_plinth.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000501/resources/images/04_armor_plinth_hufc50c9c6f989267f9f1d61f81bd2f4ee_223047_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000501/resources/images/04_armor_plinth_hufc50c9c6f989267f9f1d61f81bd2f4ee_223047_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000501/resources/images/04_armor_plinth_hufc50c9c6f989267f9f1d61f81bd2f4ee_223047_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000501/resources/images/04_armor_plinth.png 1284w" 
     class="landscape"
     ><figcaption>
        <p>armor_plinth. A prominent armor upgrade resting on a plinth
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000501/resources/images/05_nightmare_armor_plinth.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000501/resources/images/05_nightmare_armor_plinth_hu8724cc2933e38b81cf22bf5f8eb5e8f5_206916_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000501/resources/images/05_nightmare_armor_plinth_hu8724cc2933e38b81cf22bf5f8eb5e8f5_206916_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000501/resources/images/05_nightmare_armor_plinth_hu8724cc2933e38b81cf22bf5f8eb5e8f5_206916_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000501/resources/images/05_nightmare_armor_plinth.png 1282w" 
     class="landscape"
     ><figcaption>
        <p>nightmare_armor_plinth. The armor plinth is guarded by shotgun wielding baddies on the Nightmare difficulty level.
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000501/resources/images/06_health_bonus_armor_room.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000501/resources/images/06_health_bonus_armor_room_hu29f85e064286ebc68a46de0069801ccd_191438_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000501/resources/images/06_health_bonus_armor_room_hu29f85e064286ebc68a46de0069801ccd_191438_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000501/resources/images/06_health_bonus_armor_room_hu29f85e064286ebc68a46de0069801ccd_191438_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000501/resources/images/06_health_bonus_armor_room.png 1282w" 
     class="landscape"
     ><figcaption>
        <p>health_bonus_armor_room. Small health items located next to the initial armor plinth.
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000501/resources/images/07_zig_zag_room.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000501/resources/images/07_zig_zag_room_hua4b203ddc4514ee977ffc57aceb9e876_213499_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000501/resources/images/07_zig_zag_room_hua4b203ddc4514ee977ffc57aceb9e876_213499_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000501/resources/images/07_zig_zag_room_hua4b203ddc4514ee977ffc57aceb9e876_213499_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000501/resources/images/07_zig_zag_room.png 1290w" 
     class="landscape"
     ><figcaption>
        <p>zig_zag_room. A zig zag platform room in E1M1 featuring a fireball shooting Imp demon.
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000501/resources/images/08_wall_secret.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000501/resources/images/08_wall_secret_hu2dddfbc1af48681fed05a8c0963718c4_217467_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000501/resources/images/08_wall_secret_hu2dddfbc1af48681fed05a8c0963718c4_217467_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000501/resources/images/08_wall_secret_hu2dddfbc1af48681fed05a8c0963718c4_217467_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000501/resources/images/08_wall_secret.png 1282w" 
     class="landscape"
     ><figcaption>
        <p>wall_secret. The first hidden wall secret in Doom, located in the zig zag room and opened with a door opening command (generally space bar).
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000501/resources/images/09_lowered_imp_pillar.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000501/resources/images/09_lowered_imp_pillar_huc151705af14c0b5e0bfcefc14bbe38cf_229719_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000501/resources/images/09_lowered_imp_pillar_huc151705af14c0b5e0bfcefc14bbe38cf_229719_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000501/resources/images/09_lowered_imp_pillar_huc151705af14c0b5e0bfcefc14bbe38cf_229719_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000501/resources/images/09_lowered_imp_pillar.png 1286w" 
     class="landscape"
     ><figcaption>
        <p>lowered_imp_pillar. The secret platform in the zig zag room that is lowered after exploring the next room. This grants access to the secret shotgun room.
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000501/resources/images/10_shotgun_room.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000501/resources/images/10_shotgun_room_hue1aa3934ab92d0f66099902b16efbfd2_218345_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000501/resources/images/10_shotgun_room_hue1aa3934ab92d0f66099902b16efbfd2_218345_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000501/resources/images/10_shotgun_room_hue1aa3934ab92d0f66099902b16efbfd2_218345_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000501/resources/images/10_shotgun_room.png 1280w" 
     class="landscape"
     ><figcaption>
        <p>shotgun_room. The hidden shotgun room adjacent to the zig zag room.
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000501/resources/images/11_elevator_secret.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000501/resources/images/11_elevator_secret_hu1ae059c8c7c45d6e0bcece87b0232fa2_76099_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000501/resources/images/11_elevator_secret_hu1ae059c8c7c45d6e0bcece87b0232fa2_76099_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000501/resources/images/11_elevator_secret_hu1ae059c8c7c45d6e0bcece87b0232fa2_76099_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000501/resources/images/11_elevator_secret.png 1278w" 
     class="landscape"
     ><figcaption>
        <p>elevator_secret. The hidden elevator in the corner of the shotgun room lowering.
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000501/resources/images/12_secret_corridor_elevator_top.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000501/resources/images/12_secret_corridor_elevator_top_hu3d5d6258f1c90a2496176b928b8d9a73_206633_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000501/resources/images/12_secret_corridor_elevator_top_hu3d5d6258f1c90a2496176b928b8d9a73_206633_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000501/resources/images/12_secret_corridor_elevator_top_hu3d5d6258f1c90a2496176b928b8d9a73_206633_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000501/resources/images/12_secret_corridor_elevator_top.png 1280w" 
     class="landscape"
     ><figcaption>
        <p>secret_corridor_elevator_top. The hidden path at the top of the hidden elevator in the shotgun room.
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000501/resources/images/13_secret_window.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000501/resources/images/13_secret_window_hu1d044041fe640bab8242623405d21528_212396_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000501/resources/images/13_secret_window_hu1d044041fe640bab8242623405d21528_212396_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000501/resources/images/13_secret_window_hu1d044041fe640bab8242623405d21528_212396_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000501/resources/images/13_secret_window.png 1284w" 
     class="landscape"
     ><figcaption>
        <p>secret_window. A hidden window at the end of the hidden path in Figure 14.
        </p>
    </figcaption>
</figure>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Borgman, C.L., 2007.  <em>Scholarship in the digital age: information, infrastructure, and the Internet</em> . MIT Press, Cambridge, Mass.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Owens, T. and Padilla T., 2020.  “Digital sources and digital archives: Historical evidence in the digital age” .  <em>The International Journal of Digital Humanities</em> .&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Siemens, R.G., Moorman, D. (Eds.), 2006.  <em>Mind technologies: humanities computing and the Canadian academic community</em> . University of Calgary Press, Calgary.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Svensson, P., Goldberg, D.T. (Eds.), 2015.  <em>Between humanities and the digital</em> . The MIT Press, Cambridge, Massachusetts.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>Warwick, C., Terras, M.M., Nyhan, J. (Eds.), 2012.  <em>Digital humanities in practice</em> . Facet Publishing in association with UCL Centre for Digital Humanities, London.&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>Resurrection could also apply to the physical reconstruction (or acquisition) of legacy hardware. However, this article is only able to comment on the data resurrection through emulation — for reasons that will become clear below.&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>Burdick, A., Drucker, J., Lunenfeld, P., Presner, T., Schnapp, J., 2016.  <em>Digital_Humanities</em> . The MIT Press.&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref3:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>That  “less authoritative records are taking their place”  is actually more a symptom of not creating systems and  “authoritative”  sources that can better deal with new types of records, as we attempt to show through the citation system below and have explored in our prior work on the appraisal of new forms of game development documentation and new descriptive apparatus for game software objects. See <a href="#kaltman2014">Kaltman et la. (2014)</a>, <a href="#kaltman2016">Kaltman et al. (2016)</a>.&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>We make extensive use of the computer game  <em>Doom</em> , more specifically the  <em>Doom</em>  Version 1.9 Shareware, because it is freely distributable, a significant part of game history, and very well documented compared to other historical computer games.&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>Pinchbeck, D., 2013.  <em>Doom: scarydarkfast</em> . University of Michigan Press, Ann Arbor.&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>Ramsay, S., 2011.  <em>Reading machines: toward an algorithmic criticism</em> , Topics in the digital humanities. University of Illinois Press, Urbana.&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p>See <a href="#mcdonough2010">McDonough (2010)</a>; <a href="#newman2012b">Newman (2012b)</a>.&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13">
<p>Nowviskie, B.P., 2016.  “speculative collections” . URL <a href="http://nowviskie.org/2016/speculative-collections/">http://nowviskie.org/2016/speculative-collections/</a> (accessed 11.18.16).&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14">
<p>Sula, C.A., Miller, M., 2014.  “Citations, contexts, and humanistic discourse: Toward automatic extraction and classification” .  <em>Literary and Linguistic Computing</em>  29, 452–464. <a href="https://doi.org/10.1093/llc/fqu019">https://doi.org/10.1093/llc/fqu019</a>&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:15">
<p>Parks, T., 2014.  “References, Please”  [WWW Document]. The New York Review of Books. URL <a href="http://www.nybooks.com/daily/2014/09/13/references-please/">http://www.nybooks.com/daily/2014/09/13/references-please/</a> (accessed 4.5.17).&#160;<a href="#fnref:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:16">
<p>Additionally, the practices of analytical bibliography — probably most impressively displayed in Frank Manchel’s magnum opus,  _Film Study: An Analytical Bibliography _ — can help to reconcile the production and history of academic disciplines <sup id="fnref:78"><a href="#fn:78" class="footnote-ref" role="doc-noteref">78</a></sup>. Manchel’s four-volume, 2500 page work enumerates and describes the entire breadth of English language film and film studies produced between 1965 and 1990. Interestingly, in line with the current project of scholarly support through tools, Manchel’s work is emphatically indebted to SCRIPT/VS word processing system and the IBM 6670 Laser Printer for help in maintaining and organizing the necessary subject and author indexes required for his work <sup id="fnref1:78"><a href="#fn:78" class="footnote-ref" role="doc-noteref">78</a></sup>.&#160;<a href="#fnref:16" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:17">
<p>In the most recent MLA Handbook, the editors write that citation practice involves,  “demonstrating the thoroughness of the writer’s research, giving credit to the original sources, and ensuring that readers can find the [sources] . . . to draw their own conclusions about the writer’s argument”   <sup id="fnref:79"><a href="#fn:79" class="footnote-ref" role="doc-noteref">79</a></sup>. Additionally, authors must provide a  “comprehensible, verifiable means of referring to one another’s work . . . to give credit to the precursors whose ideas they borrow, build on or contradict and allow future researchers interested in the history of the conversation to trace it back to the beginning”   <sup id="fnref1:79"><a href="#fn:79" class="footnote-ref" role="doc-noteref">79</a></sup>.&#160;<a href="#fnref:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:18">
<p>Eco, U., 2015.  <em>How to Write a Thesis</em> . MIT Press, Cambridge, MA; London.&#160;<a href="#fnref:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:19">
<p>Grafton, A., 1997.  <em>The footnote: a curious history</em> , Revised edition. ed. Harvard University Press, Cambridge, Mass.&#160;<a href="#fnref:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:20">
<p>Fairclough, N., 1992.  <em>Discourse and social change</em> . Polity Press, Cambridge, Mass.&#160;<a href="#fnref:20" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:20" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:20" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref3:20" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref4:20" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:21">
<p>“Discursive chain of thought”  is basically the organization of knowledge in a specific discipline. All practitioners are contributing to the historical accumulation that furthers the course of their discipline and the history of its claims.&#160;<a href="#fnref:21" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:22">
<p>There are parallels here with Latour and the translation of concepts in networks, as well as the use of inscription in the creation of research works. We will briefly bring in some of the history of science and technology perspective on text formation, with its focus on practice, below.&#160;<a href="#fnref:22" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:23">
<p>We are careful to note that the assumption of an audience here is not only referring to the actions taken by an author, with an audience in mind, to clarify and align their text with others’ expectations, but also to the implicit knowledge that a potential reader will bring to a text given that it is in a specific discursive form. Fairclough goes into more depth on discursive types in <a href="#fairclough1991">Fairclough (1992)</a>, specifically in chapters 2 and 3.&#160;<a href="#fnref:23" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:24">
<p>The organization of citations in a text (and, in our case, the organization of text and running executable programs) does also call out to various traditions relating to visual design and the juxtaposition of text and image, specifically in art criticism see <a href="#berger1973">Berger (1973)</a>.&#160;<a href="#fnref:24" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:25">
<p>Kaltman, E., Wardrip–Fruin, N., Lowood, H., Caldwell, C., 2015.  “Methods and Recommendations for Archival Records of Game Development: The Case of Academic Games” .  <em>Proceedings of the 10th International Conference on the Foundations of Digital Games</em> .&#160;<a href="#fnref:25" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:25" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:25" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:26">
<p>McDonough, J.P., 2010.  “Preserving virtual worlds: Final Report” . Graduate School of Library and Information Science, University of Illinois at Urbana-Champaign.&#160;<a href="#fnref:26" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:27">
<p>Altice, N., 2015.  <em>I am error: the Nintendo family computer/entertainment system platform</em> , Platform studies. The MIT Press, Cambridge, Massachusetts.&#160;<a href="#fnref:27" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:27" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:27" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref3:27" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref4:27" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref5:27" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref6:27" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:28">
<p>Kirschenbaum, M.G., 2008.  <em>Mechanisms: New Media and the Forensic Imagination</em> . The MIT Press, Cambridge, Mass.; London.&#160;<a href="#fnref:28" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:29">
<p>Bogost, I. and Montfort, N., 2009.  <em>Racing the Beam: The Atari Video Computer System</em> . MIT Press, Cambrige, Mass.&#160;<a href="#fnref:29" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:29" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:30">
<p>Maher, J., 2012.  <em>The future was here: the Commodore Amiga</em> , Platform studies. MIT Press, Cambridge, Mass.&#160;<a href="#fnref:30" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:31">
<p>The ACM is currently investigating ways to embed software references into publications through an  “artifact review”  badging process. See: <a href="https://www.acm.org/publications/policies/artifact-review-badging">https://www.acm.org/publications/policies/artifact-review-badging</a>&#160;<a href="#fnref:31" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:32">
<p><a href="http://gamestudies.org/1802/submission_guidelines#GSCitation">http://gamestudies.org/1802/submission_guidelines#GSCitation</a>&#160;<a href="#fnref:32" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:33">
<p>This is in line with the recommendation for  “reasonable compatibility”  in <a href="#kaltman2015">Kaltman et al. (2015)</a>. We argue that a game resource in a collection catalog should provide granular enough information to give a researcher a reasonable guess at the technical apparatus required for the resource’s recovery.&#160;<a href="#fnref:33" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:34">
<p>According to MobyGames, <a href="https://web.archive.org/web/20160421081803/http://www.mobygames.com/game/descent/release-info">https://web.archive.org/web/20160421081803/http://www.mobygames.com/game/descent/release-info</a>, Descent has 15 different releases, 6 of which occurred in 1995 in the United States, Japan, and Germany for DOS, Mac, and PC-98.&#160;<a href="#fnref:34" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:35">
<p>Relevantly, Pinchbeck was involved with the KEEP project, an early attempt to provide an emulation framework for the recovery of older games <sup id="fnref:80"><a href="#fn:80" class="footnote-ref" role="doc-noteref">80</a></sup>P. A specific purpose of KEEP was to insure that people could play old games to explicitly understand their affordances through play and reference their historical context. As such, we wish to reiterate that Pinchbeck’s work is used as an example of a specific type of game historical discourse and not a criticism of the work itself.&#160;<a href="#fnref:35" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:36">
<p>For more information on recommendations for citation guidelines as a result of the GAMECIP work, please refer to our citation recommendations: Kaltman, Eric, Stacey Mason, and Noah Wardrip-Fruin.  “The Game I Mean: Game Reference, Citation and Authoritative Access”   <sup id="fnref:81"><a href="#fn:81" class="footnote-ref" role="doc-noteref">81</a></sup>. Additionally, recent citation recommendations and a discussion of how citation of games is always a political and discipline-specific act can be found in  “How to Reference a Digital Game”   <sup id="fnref:82"><a href="#fn:82" class="footnote-ref" role="doc-noteref">82</a></sup>.&#160;<a href="#fnref:36" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:37">
<p>Hyland, K., 2000.  <em>Disciplinary discourses: social interactions in academic writing, Applied linguistics and language study</em> . Longman, Harlow ; New York.&#160;<a href="#fnref:37" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:38">
<p>Lynch, M., 1988.  “The externalized retina: Selection and mathematization in the visual documentation of objects in the life sciences” .  <em>Human studies</em>  11, 201–234.&#160;<a href="#fnref:38" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:38" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:39">
<p>Prominent examples are  <em>Kairos</em>  (<a href="http://kairos.technorhetoric.net/">http://kairos.technorhetoric.net/</a>),  <em>Scalar</em>  (<a href="http://scalar.usc.edu">http://scalar.usc.edu</a>), and  <em>Vectors</em>  (<a href="http://vectors.usc.edu">http://vectors.usc.edu</a>).&#160;<a href="#fnref:39" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:40">
<p>Unless, as indicated above, the interruption of the flow serves a discursive function — the disjunction of meanings being relevant to some point or elaboration. Sometimes contrasts in discursive presentation inform limitations of either one.&#160;<a href="#fnref:40" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:41">
<p><a href="#newman2012a">Newman (2012a)</a> actually calls for game preservation policies to prioritize videos of gameplay on the assumption that executable access is a less likely future scenario.&#160;<a href="#fnref:41" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:42">
<p>As described in <a href="#wilson2013">Wilson (2013)</a>. The solo eggplant run is a secret completion achievement for the computer game  <em>Spelunky</em> ; a rogue-like dungeon exploration game modeled on Indiana Jones and “explore the tomb”-type motifs. The eggplant run involves carrying a useless item from the beginning of the game through completion without losing it or losing one’s life. It was so difficult that it took years before its first completion by Bananasauras Rex in 2013.&#160;<a href="#fnref:42" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:43">
<p>Staley, D.J., 2015.  <em>Computers, visualization, and history: how new technology will transform our understanding of the past</em> , Second Edition. ed, History, the humanities, and the new technology. Routledge, Abingdon.&#160;<a href="#fnref:43" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:44">
<p>Victor, B., 2011.  “The Ladder of Abstraction”  [WWW Document]. URL <a href="http://worrydream.com/#!2/LadderOfAbstraction">http://worrydream.com/#!2/LadderOfAbstraction</a> (accessed 12.3.14).&#160;<a href="#fnref:44" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:45">
<p>However, being at the forefront of something is not the same as inventing it. And Victor’s work is heavily inspired by Alan Kay’s in active essays and Ted Nelson’s educational musings in  “No More Teacher’s Dirty Looks”  from <a href="#nelson1974">Nelson (1974)</a>. For more information on the genealogy of the term see <a href="#yamamiya2009">Yamamiya (2009)</a>.&#160;<a href="#fnref:45" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:46">
<p>Hart, V., Case, N., 2016.  “Parable of the Polygons”  [WWW Document]. Parable of the Polygons. URL <a href="http://ncase.me/polygons">http://ncase.me/polygons</a> (accessed 4.5.17).&#160;<a href="#fnref:46" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:47">
<p>Victor, B., 2011.  “Explorable Explanations”  [WWW Document]. URL <a href="http://worrydream.com/#!/ExplorableExplanations">http://worrydream.com/#!/ExplorableExplanations</a> (accessed 4.5.17).&#160;<a href="#fnref:47" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:48">
<p>Papert, S., 1980.  <em>Mindstorms: Children, computers, and powerful ideas</em> . Basic Books, Inc.&#160;<a href="#fnref:48" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:48" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:49">
<p>More technically, it is  “a technique for implementing a virtual machine on a host computer whose instruction set is different from the host computer’s”   <sup id="fnref:83"><a href="#fn:83" class="footnote-ref" role="doc-noteref">83</a></sup>. Although the line between virtualization and emulation does get a bit murky by this definition — the computer I’m typing this on, a MacBook Pro running Apple Mac OS X 10.11, shares the same basic instruction set with a Sony PlayStation 4 — the encapsulation of one system within another is the basic function of emulation.&#160;<a href="#fnref:49" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:50">
<p>The use of image in referring to an extracted data set is drawn from operations in mathematics. When you use a function to operate on a set of numbers, you are mapping the input values with potential outputs. In the case of y = x + 1, y is a function of x (f(x)) with x standing for a range of input values, and y for the output values of the function. The input values in this case “map” through the function to a specific set of output values. This resultant map, which is just the set of inputs each incremented by 1, is an image of those inputs in a new domain. Similarly, the data extracted from a physical medium is not the same data (nothing changes from the physical medium’s point of view) but a mapping of the data stored on that medium to an equivalent set of data now migrated from the media to another machine. Thus, imaging is very directly the process of mapping the data stored on a specific physical media to an identical configuration on a separate machine with its own storage. This distinction between types of data, and the means by which they are migrated, mapped and translated between machines will become relevant below for issues of reduction.&#160;<a href="#fnref:50" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:51">
<p>Emulation, specifically in its copying and use of potentially copyrighted data, is in unclear legal territory. The current GISST system would therefore need to be modified to adapt a BYOD (bring your own data) approach where users provide their own legal game data for citable manipulation. Another alternative would be for GISST to function as a research service, like a more conventional research database, within which executable citations would function for institutions paying for the service. Counterintuitively, providing GISST as a service that draws from data stored on third-party sites elsewhere on the Internet, like ROM sharing sites, may help limit legal risk for institutions. A GISST service tailored to enable only legitimate fair uses would be less exposed to liability than sites that support a broad array of more legally dubious uses. GISST could rely on third-party sites for content without necessarily taking on the risk associated with making the content available for open-ended reuse. For more extensive legal information on this topic, check out the aforementioned <a href="#mcdonough2010">McDonough (2010)</a> and <a href="#rosenthal2015">Rosenthal (2015)</a>.&#160;<a href="#fnref:51" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:52">
<p>The MAME project is known for its attention to detail and support for esoteric systems. For example, one addition in 2016 was a Sonic the Hedgehog popcorn vending machine with embedded display, SegaSonic Popcorn Shop, that was marketed only in 1993 in certain Japanese cities.&#160;<a href="#fnref:52" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:53">
<p>Many other emulators now have JavaScript versions, the three listed above are the three we use in the citation tool and so are explicitly mentioned. For a full listing of emulators (including JavaScript) check <a href="https://en.wikipedia.org/w/index.php?title=List_of_video_game_emulators">https://en.wikipedia.org/w/index.php?title=List_of_video_game_emulators</a> which keeps a running list of projects and compatibility.&#160;<a href="#fnref:53" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:54">
<p>The inclusion of full emulated systems in a document might be closer to Ted Nelson’s notion of transclusion than to traditional, print-based models of intertextuality. In Nelson’s proposed (and, with Autodesk’s involvement, partially implemented) Xanadu system, a precursor of the World Wide Web, portions of documents could be manifestly represented inside other documents by transcluding a portion of a full version of the document. Those reading a Xanadu document could reach a full version of any transcluded document, or change the portion viewed through the transclusion window, or pull it up alongside the transcluding document, as easily as we now traverse links on the Web — making the transcluded document somewhat less under the control of the transcluding. For more, see <a href="#nelson1993">Nelson (1993)</a>.&#160;<a href="#fnref:54" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:55">
<p>Montfort, N., 2000.  “Cybertext Killed the Hypertext Star | Electronic Book Review”  [WWW Document]. URL <a href="http://www.electronicbookreview.com/thread/electropoetics/cyberdebates">http://www.electronicbookreview.com/thread/electropoetics/cyberdebates</a> (accessed 2.26.17).&#160;<a href="#fnref:55" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:56">
<p>The Java-based plugins are now flagged as security threats, and even forcing the browser to ignore those warnings did not result in the emulations executing correctly.&#160;<a href="#fnref:56" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:57">
<p>Fry, B.  “deconstructulator | ben fry”  [WWW Document], 2003. URL <a href="https://benfry.com/deconstructulator/">https://benfry.com/deconstructulator/</a> (accessed 3.23.19).&#160;<a href="#fnref:57" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:58">
<p>On a preservationist note, while the  “deconstructulator”  (<a href="https://www.benfry.com/deconstructulator">https://www.benfry.com/deconstructulator</a>) is still available, it requires a Java plugin to function. Due to security concerns over the last decade, Java support has been dropped or disabled in many browsers. There is no longer any mention of the NESCafe emulator on its creator’s website, and its most recent update (July 22, 2008 according to <a href="https://www.zophar.net/java/nes/nescafe.html">https://www.zophar.net/java/nes/nescafe.html</a>) is over 10 years old as of this writing in 2019.&#160;<a href="#fnref:58" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:59">
<p>Newman, J., 2012.  <em>Best before: Videogames, supersession and obsolescence</em> . Routledge.&#160;<a href="#fnref:59" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:59" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:60">
<p>See <a href="https://www.youtube.com/user/ClydeMandelin">https://www.youtube.com/user/ClydeMandelin</a>, specifically  “Poemato CX&rsquo;s Twitch Stream Magic”   <a href="https://www.youtube.com/watch?v=rX87i71IC7g">https://www.youtube.com/watch?v=rX87i71IC7g</a>&#160;<a href="#fnref:60" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:61">
<p>See <a href="#rinehart2014">Rinehart (2014)</a> for a more detailed discussion of the benefits and perils of incidental community archival practices.&#160;<a href="#fnref:61" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:62">
<p>Fernández-Vara, C., 2014.  <em>Introduction to Game Analysis</em> , 1 edition. ed. Routledge, New York.&#160;<a href="#fnref:62" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:62" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:63">
<p>Lowood, H., 2011.  “Perfect Capture: Three Takes on Replay, Machinima and the History of Virtual Worlds” .  <em>Journal of Visual Culture</em>  10, 113–124. <a href="https://doi.org/10.1177/1470412910391578">https://doi.org/10.1177/1470412910391578</a>&#160;<a href="#fnref:63" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:63" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:63" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:64">
<p>Some emulators running in constrained environments, like JavaScript emulators in web browsers, need to cut corners to get processing up to an acceptable speed. Other emulators running in native execution contexts, like Microsoft Windows applications, sometimes intentionally slow down processing in order to match the timings of older machines.&#160;<a href="#fnref:64" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:65">
<p>See <a href="#woolgar1986">Woolgar (1986)</a> for a discussion of the different philosophical roots behind discourse analysis (Continental Philosophy) and STS (Anglo-American Analytic Philosophy).&#160;<a href="#fnref:65" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:66">
<p>Woolgar, S., 1986.  “On the alleged distinction between discourse and praxis” .  <em>Social Studies of Science</em>  309–317.&#160;<a href="#fnref:66" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:67">
<p>This issue must be addressed as we explore the future of academic publishing. An intriguing example is Alexandra Juhasz&rsquo;s  <em>Learning from YouTube</em> , an online book which includes and links to many YouTube videos, published by The MIT Press in partnership with the Alliance for Networking Visual Culture <sup id="fnref:84"><a href="#fn:84" class="footnote-ref" role="doc-noteref">84</a></sup>. Given YouTube’s (and the wider corporate web’s) lack of concern for preservation and stability, one of the first pages readers are likely to encounter ( “HOW TO USE THIS VIDEO-BOOK” ) suggests that they can help the project by reporting broken links. It’s difficult to imagine such a request in a book built on research in a traditional archive. (What form would it even take?)&#160;<a href="#fnref:67" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:68">
<p>The term game engine is commonly used for software platforms that support particular types of game play experiences. Some are relatively specific, such as the id Tech engines primarily used for first-person shooter games. Some are more general, such as the Unity engine, which is used for many types of graphical games — but which would be ill-suited to text adventure or interactive fiction games, for example. However, more broadly, a game’s engine can describe the game’s processes as separated from its data. This is why Nathan Altice can describe community-produced data that targets the software and hardware processes of 1980s Nintendo Entertainment System games as  “spawning fresh games from obsolete engines”   <sup id="fnref6:27"><a href="#fn:27" class="footnote-ref" role="doc-noteref">27</a></sup> (Emulators are discussed in <a href="#section05.3">section 5.3</a>). If this sounds like a slippery concept, that’s because it is. As Eike Falk Anderson, Steffen Engel, Peter Comninos, and Leigh McLoughlin put it,  “[T]here is disagreement about exactly what a game engine is, with sometimes fundamental differences between definitions”   <sup id="fnref:85"><a href="#fn:85" class="footnote-ref" role="doc-noteref">85</a></sup>. Nevertheless, it remains a useful concept in practice.For any game that displays to a screen, players may attempt to capture game play performances using screen recordings, resulting in a series of image frames. In addition, some games support the creation of “replay files” — which cause the performance to be re-enacted in the game/engine software, reducing file size and making it possible to, for example, change the perspective from which the performance is viewed. Some emulators support interaction through input streams — which cause a scripted set of controller events to be executed at particular times. If the input stream is a recording of the controller events of a particular performance, and the game begins from the same computational state, the result will be a reproduction of the performance in the game/engine.&#160;<a href="#fnref:68" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:69">
<p>Swink, S., 2009.  <em>Game Feel: A Game Designer’s Guide to Virtual Sensation</em> . Morgan Kaufmann, Burlington, MA.&#160;<a href="#fnref:69" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:70">
<p>Swink’s game feel is focused exclusively on continuous input games, like platformers or action titles. Doug Wilson has argued that game feel should extend to other types of interactions with computational feedback systems, from menu systems to mouse interaction in strategy games <sup id="fnref:86"><a href="#fn:86" class="footnote-ref" role="doc-noteref">86</a></sup>. We take the latter, more liberal view of game feel in the context of providing an emulated system in argument for the significance of a performative act or as a means of elaborating on a deeper understanding of embodied play experiences.&#160;<a href="#fnref:70" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:71">
<p>Kaltman, E., Osborn, J., Wardrip-Fruin, N., Mateas, M., 2017.  “Getting the GISST: A Toolkit for the Creation, Analysis and Reference of Game Studies Resources” , in:  <em>Proceedings of the 12th International Conference on the Foundations of Digital Games</em> . Presented at the Foundations of Digital Games, Hyannis, MA.&#160;<a href="#fnref:71" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:72">
<p><a href="http://tasvideos.org/Movies.html">http://tasvideos.org/Movies.html</a> contains a listing of different platforms along with downloads for their various movie formats, which are usually files of tabulated input sequences.&#160;<a href="#fnref:72" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:73">
<p>The table does not include game states because the CLI does not ingest arbitrary state data. This is mainly due to the fact that emulated state data is specific to both a game and the emulator supporting it and effectively useless without those dependencies. Additionally, in the case of some of GISST’s supported emulators, like DOSBox, there are no independent save state formats, just data derived from the live emulation during run-time. That said, GISST-saved states should in principle be portable across browsers running the GISST app.&#160;<a href="#fnref:73" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:74">
<p><a href="http://www.zotero.org">http://www.zotero.org</a>&#160;<a href="#fnref:74" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:75">
<p>Reviewers commented on the gender imbalance (6:2 male/female) among the evaluation set. Initial requests for commentary were sent to a slightly more diverse set (9:5) of evaluators, however limits due to publishing deadlines prevented further pursuit of more gender parity. Continued evaluative work on the GISST tool set will be sure to more suitably address this disparity.&#160;<a href="#fnref:75" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:76">
<p>Denson, S., 2017.  “Visualizing Digital Seriality or: All Your Mods Are Belong to Us!”  22.1.&#160;<a href="#fnref:76" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:77">
<p>Note that this is conceptualization of GISST as a system and not conceptualization in the ontological sense.&#160;<a href="#fnref:77" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:78">
<p>Manchel, F., 1990.  <em>Film study: an analytical bibliography</em> . Fairleigh Dickinson University Press ; Associated University Presses, Rutherford : London.&#160;<a href="#fnref:78" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:78" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:79">
<p>Modern Language Association of America (Ed.), 2016.  <em>MLA handbook</em> , Eighth edition. ed. The Modern Language Association of America, New York.&#160;<a href="#fnref:79" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:79" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:80">
<p>Pinchbeck, D., Anderson, D., Delve, J., Alemu, G., Ciuffreda, A., Lange, A., 2009.  “Emulation as a strategy for the preservation of games: the KEEP project” , in:  <em>DiGRA 2009-Breaking New Ground: Innovation in Games, Play, Practice and Theory</em> .&#160;<a href="#fnref:80" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:81">
<p>Kaltman, E., Mason, S., Wardrip-Fruin, N., n.d.  “The Game I Mean: Game Reference, Citation, and Authoritative Access” . In submission.&#160;<a href="#fnref:81" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:82">
<p>Gualeni, S., Fassone, R., Linderoth, J., 2019.  “How to Reference a Digital Game” , in:  <em>Proceedings of the 2019 DiGRA International Conference</em> . Presented at DiGRA, Kyoto, Japan.&#160;<a href="#fnref:82" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:83">
<p>Rosenthal, D.S., 2015.  <em>Emulation &amp; Virtualization as Preservation Strategies</em> .&#160;<a href="#fnref:83" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:84">
<p>Juhasz, A., 2011.  “Learning From YouTube: YOUTUBE IS &hellip;”  [WWW Document]. URL <a href="http://vectors.usc.edu/projects/learningfromyoutube/">http://vectors.usc.edu/projects/learningfromyoutube/</a> (accessed 4.7.19).&#160;<a href="#fnref:84" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:85">
<p>Anderson, E., Engel, S., Comninos, P., and McLoughlin, L.  “The Case for Research in Game Engine Architecture” . In  <em>Proceedings of the 2008 Conference on Future Play: Research, Play, Share,</em>  228–231. Future Play ’08. New York, NY, USA: Association for Computing Machinery, 2008. <a href="https://doi.org/10.1145/1496984.1497031">https://doi.org/10.1145/1496984.1497031</a>.&#160;<a href="#fnref:85" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:86">
<p>Wilson, D., 2017.  “A Tale of Two Jousts: Multimedia, Game Feel, and Imagination” . URL <a href="https://www.youtube.com/watch?v=hpdcek4hLA8">https://www.youtube.com/watch?v=hpdcek4hLA8</a>&#160;<a href="#fnref:86" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry><entry><title type="html">Healing the Gap: Digital Humanities Methods for the Virtual Reunification of Split Media and Paper Collections</title><link href="https://rlskoeser.github.io/dhqwords/vol/15/1/000509/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://rlskoeser.github.io/dhqwords/vol/15/1/000509/</id><author><name>Stephanie Sapienza</name></author><author><name>Eric Hoyt</name></author><author><name>Matt St. John</name></author><author><name>Ed Summers</name></author><author><name>JJ Bersch</name></author><published>2021-03-05T00:00:00+00:00</published><updated>2021-03-05T00:00:00+00:00</updated><content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>It is difficult to talk about audiovisual archives as a field of practice, since audiovisual collections are never typical, existing in institutional repositories so varied as to make it impossible to talk about them as a monolithic whole. Audiovisual media collections are found within museums, libraries, historical societies, private collections; within media production units; and within traditional archives (only a small percentage of which are specifically dedicated to audiovisual collections). This wide array of communities and institutions has developed unique principles and standards, each individual community or institution borrowing the professional descriptive approaches needed for their particular circumstance, with no unifying methodology.</p>
<p>These splintered approaches have led to a particular set of challenges with a/v collections, which are woefully underdescribed and often segregated from contextual resources. This separation from contextual resources prevents understanding of the conditions that produced a/v materials, often limiting the types of research questions that can be answered by the a/v media alone. An additional challenge is what scholar Sarah Florini has called a &ldquo;temporal commitment barrier,&rdquo; meaning that users of a/v collections can easily become overwhelmed by the sheer listening hours needed to research time-based collections <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>  <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. The unfortunate result of the above three challenges is that a/v resources are undervalued and underutilized as primary source materials for scholarship, which in turn has meant that they also receive less attention in the sphere of digital humanities. The field of digital humanities is poised to innovate and meet these challenges by employing methods such as linked data and virtual reunification to virtually connect audiovisual collections to related contextual resources. DH can also exploit its ability to experiment and push boundaries by encouraging the use of new standards such as WebVTT and new approaches like minimal computing, to enable distant reading and increase searchability via synced transcripts.</p>
<p>This paper will describe original research at the Maryland Institute for Technology in the Humanities (MITH), in conjunction with the University of Wisconsin-Madison and the Wisconsin Historical Society, on a project entitled <a href="https://mith.umd.edu/research/unlocking-the-airwaves/"> <em>Unlocking the Airwaves: Revitalizing an Early Public Radio Collection</em> </a>. The project aims to provide a model for innovation within digital humanities by virtually reunifying two geographically segregated collections of the National Association of Educational Broadcasters (NAEB), which are currently split between the University of Maryland (audio files) and the Wisconsin Historical Society (paper collections).The NAEB collections provide an in-depth look at the messages being broadcast to the general public through the rubric of &rsquo;educational radio,&rsquo; which predated (and heavily informed) what we now know as public radio, between 1950-1970. By coordinating the expertise of archivists, humanities researchers, and digital humanists, the creation of this new online resource for humanities research will deliver enhanced access to important, mostly hidden, archival audiovisual materials.</p>
<h2 id="research-questions">Research Questions</h2>
<p>At the heart of this project lies a question that we have identified as being central to many ongoing conversations surrounding the role of the audiovisual in digital humanities:  <em>how can we utilize digital humanities methods to encourage the use of audiovisual collections as primary historical records, both in and outside of the academy</em> ? To move towards an answer to that question, it was necessary to unpack why A/V collections are underutilized in the first place. We identified three core reasons: A/V collections are a) underdescribed as a whole, trending towards skeletal descriptions at the item level, b) are often segregated from related contextual resources, and c) present an additional challenge in the form of a temporal commitment barrier. The below sections of this paper will walk through each of these three reasons more in depth, before returning to our central research question.</p>
<h2 id="why-are-audiovisual-collections-underdescribed">Why are audiovisual collections underdescribed?</h2>
<p>The term underdescribed is used here as an overarching descriptor for the whole of audiovisual materials across the globe, which as a general rule, do not enjoy a level of descriptive attention anywhere close to their textual counterparts. A/V collections exist within and have borrowed strategies from libraries and museums <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>. However, this paper will limit discussions to scenarios when A/V collections exist in an archival context — a situation that can be particularly complex due to the ways that audiovisual materials can challenge archival conceptions of uniqueness and authorship. The provenance of an archival audiovisual collection can fall into one of these categories:</p>
<p>It is part of a traditional, primary textual or paper-based archival collection;</p>
<ul>
<li>Ex: Mixed paper/audiovisual collections at the <a href="https://www.aaa.si.edu">Smithsonian Archives of American Art</a>; the <a href="https://bfca.sitehost.iu.edu/home/">Black Film Center/Archive</a> at Indiana University, Bloomington;</li>
</ul>
<p>It has been constructed entirely separately from a textual archive, but has a shared provenance, or a substantial number of shared characteristics or authorities;</p>
<ul>
<li>Ex: Several distinct collections related to the Jam Handy Organization (JHO), a producer of sponsored films, exist at the Detroit Public Library (<a href="https://www.worldcat.org/title/jam-handy-organization-records-1894-1984/oclc/547043600">paper</a>), Stanford Archive of Recorded Sound (<a href="https://oac.cdlib.org/findaid/ark:/13030/kt4f59s18n/">audio</a>), the University of Michigan (paper), and the Prelinger Archives (<a href="https://archive.org/details/prelinger">paper and film</a>)</li>
</ul>
<p>It was once part of a whole collection which was subsequently split up;</p>
<ul>
<li>Ex: the National Association of Educational Broadcasters (NAEB) collections at University of Maryland Libraries (<a href="https://archives.lib.umd.edu/repositories/2/resources/1666">audio</a>) and Wisconsin Historical Society (<a href="http://digicoll.library.wisc.edu/cgi/f/findaid/findaid-idx?c=wiarchives;view=reslist;subview=standard;didno=uw-whs-us0076af">paper</a>).</li>
</ul>
<p>Some collections do inevitably get described at the collection level as per traditional archival principles, particularly in category 1, where they are often (but not always) folded into the container list of a finding aid as a separate series. But many stewards of audiovisual collections tend to focus on item level description for various reasons. They sometimes do this to maintain bibliographic control, in circumstances like where the collection comprises primarily completed works, or in instances when &rsquo;the work&rsquo; is clear and has available bibliographic information. This approach is employed for the majority of a/v holdings at the <a href="https://www.cinema.ucla.edu">UCLA Film &amp; Television Archive</a>, which includes records for their materials in the MARC-based <a href="https://www.library.ucla.edu">UCLA Library Catalog</a>  <sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>. More often, however, item-level inventories are created to prepare for a preservation effort, frequently in the form of inventories concentrating on condition assessment. Many of the materials in these collections are rapidly decaying, or their physical format is in a state of obsolescence such that they cannot even be played back without some level of repair or preservation. Thus, for many such collections, there is less description even possible without reformatting or digitization. This is the case with the mass surveys undertaken by Indiana University-Bloomington and the Smithsonian, both of which undertook initial inventories, simple counts that took years just to get to the point of funding a preservation effort, which needed to happen before the inventory records could become part of any collection- or item-level descriptive process <sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>  <sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>. In these cases, the requirements for preservation often necessitate descriptive choices which cannot always be reconciled with collection-level processing.</p>
<p>Michael Heaney has previously described a typology for collection description, which now exists as the basis for Dublin Core&rsquo;s Collection Description Type (CDType) Vocabulary <sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>. Using this vocabulary, the collection level approach most used by traditional archivists would be classified as a unitary finding aid (contains information solely or primarily about the collection as a whole), and the item-level approach would be classified as an analytic finding aid (contains solely information about individual items and their content, much like a library catalog). Andrea Leigh&rsquo;s 2006 piece &ldquo;Context! Context! Context! Describing Moving Images at the Collection Level&rdquo; in  <em>The Moving Image</em>  goes into great detail describing how these different approaches apply to A/V collections, using the UCLA Film &amp; TV Archive&rsquo;s various collection categories as a case study <sup id="fnref1:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>. UCLA began shifting its approach to accommodate collection-based description when it began to receive large collections of home movies, commercials, promotional or educational materials which were sometimes (but not always) related to other collections, or were too vast to describe with the amount of bibliographic control given to the core film collections <sup id="fnref2:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>.</p>
<p>When thinking about the very salient points that archivists make to defend the use of each of these different approaches, a useful exercise is to review an online discussion in 2013-14 that occurred between Megan McShea of the Smithsonian Archives of American Art and AVP&rsquo;s Josh Ranger. Ranger originally published a blog post entitled &lsquo;Does the Creation of EAD Finding Aids Inhibit Archival Activities?&rsquo; which posited that using Encoded Archival Description (EAD) in the most traditional sense is problematic for A/V collections, since EAD often has issues with discoverability through Internet search engines, and partially because the lack of item-level information does nothing to prepare/plan for preservation efforts <sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>. McShea replied to this post with a longer retort on the values of EAD for certain situations, addressing a significant number of Ranger&rsquo;s points as valid, but arguing that in many cases, finding aids become the easiest way for archivists with mixed collections to deal with institutional realities. And that, if proper measures are taken (as with the practices she&rsquo;d established at the Smithsonian), archivists can effectively combine item-level and collection-level processing <sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup>  <sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup>. This approach is in line with category 1 above. Ranger replied that every one of McShea&rsquo;s points was also valid, but then delivered a set of final points about EAD. He posited that &ldquo;item level processing is really the only way to tell what&rsquo;s what and find the right pieces to preserve or transfer for access, and that EAD does not achieve this level of need. In my view the reliance on EAD has resulted in it becoming an endpoint or cul de sac, not a pivot point&rdquo; <sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>.</p>
<p>In many ways, these two outlooks are equally valid, as both integrate concerns over utilizing More Product, Less Process (MPLP) techniques to maximize descriptive efficiency, and also reflect the nature of each person&rsquo;s position and institutional mandate. Ranger frequently worked on consulting projects with archives who often needed a complete item-level inventory to prepare for a preservation project with as much metadata as possible, whereas McShea works at a research institution with a substantial amount of both textual and audiovisual collections, whose goal is to make materials available for research. In either case, institutional needs determine the level of description, both of which lead to collections being &lsquo;underdescribed&rsquo; in different ways. With an item-level approach, descriptive data about the collection can remain inaccessible to researchers for longer; whereas with a collection-level approach, the audiovisual resources can either become subsumed by their textual counterparts, or segregated from them entirely.</p>
<h2 id="why-are-audiovisual-collections-segregated-from-related-contextual-resources">Why are audiovisual collections segregated from related contextual resources?</h2>
<p>Over the past two decades, the archival field has taken great strides in the area of capturing contextual associations as part of the description and arrangement continuum. Heany&rsquo;s typology of a &ldquo;unitary finding aid&rdquo; was written in 2000 <sup id="fnref1:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>. Joseph Deodato wrote an evocative piece in 2006 calling for archivists to employ postmodern strategies in description and arrangement as part of a call for archivists to be &lsquo;responsible mediators&rsquo; of history, recalling and building upon the earlier work of Terry Cook, Wendy Duff, and Verne Harris in evoking the role of the archivist in constructing meaning <sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>  <sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup>  <sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup>. What Deodato called a &lsquo;&ldquo;creation continuum&rdquo; is &ldquo;but one aspect of a complex provenance that also includes the context in which the records were created, the functions they were intended to document, and the record-keeping systems used to maintain and provide access to them&rdquo; <sup id="fnref1:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>. Since that time, standards have emerged such as Describing Archives, a Content Standard (DACS), which is &ldquo;consciously designed to be used in conjunction with other content standards to meet local institutional needs for describing collection materials,&rdquo; <sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup> and includes a newly revised Part II section devoted to the creation of context through archival authority records, or records about the people, corporate bodies, and families associated with archival collections <sup id="fnref1:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup>. The emerging standard Encoded Archival Context — Corporate bodies, Persons, and Families (EAC-CPF) was designed to standardize the encoding of such archival authorities <sup id="fnref3:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>.</p>
<p>What Heaney invoked in his discussion of a unitary finding aid was similar to other movements in the field to address ideas of context and distributed collections, going as far back as documentation strategy in the late 1980s <sup id="fnref:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup>, Carole Palmer&rsquo;s notion of &lsquo;Thematic Research Collections&rsquo; in 2004 <sup id="fnref:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup>, and virtual reunification, which has appeared in the literature as early as 2004 and evolved into its current form over the past five years or so due to the work of Ricardo Punzalan. As one of the preeminent scholars continuing to define and contextualize the forms and trajectory of distributed collections and the &lsquo;Archival Diaspora&rsquo; that results from dispersed materials, Punzalan has continuously championed and focused on virtual reunification as an emerging and flexible strategy which exhibits some of the more useful qualities of several other methodologies and theories. In both his 2013 dissertation <sup id="fnref:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup> and subsequent pieces in  <em>Library Quarterly</em>  and  <em>American Archivist</em>   <sup id="fnref:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup>  <sup id="fnref:20"><a href="#fn:20" class="footnote-ref" role="doc-noteref">20</a></sup>, he tracks and analyzes different projects which have employed the strategy, while drawing parallels and distinctions between it and other approaches. At the heart of his analysis is an acknowledgement that all virtual reunification projects require an &lsquo;unprecedented&rsquo; amount of inter-institutional collaboration a broad swath of technical expertise, and doing more than simply reuniting geographically separated collections, but producing an entirely different digital entity.</p>
<p>Acknowledging that A/V collections are also part of the &lsquo;Archival Diaspora&rsquo;, it follows that they are often constructed in an entirely different sphere from the paper materials of their various creators (producers, artists, studios, etc.). Compounding this are the issues described in the above section, where long-standing descriptive standards for processing paper collections simply diverge from many of the needs of an audiovisual collection. The facts surrounding this phenomenon can widely vary, but they lead to a number of different scenarios where you end up with A/V collections which have little to no contextual information available to their stewards, although that context exists elsewhere in the information sphere. With the exception of certain categories of orphaned works (home movies, educational, sponsored, experimental, and amateur films), most of the time there is some extant documentation of a media object&rsquo;s creation and dissemination — transcripts, production and field recording notes, press kits, photos, correspondence, provenance and copyright materials. That documentation can exist in different divisions of the same institution, either at the same or a different geographic location, or at an entirely different institution altogether.</p>
<p>Within that context, archival collections are often &ldquo;mixed,&rdquo; meaning that at some point in their chain of custody (at or before the point of accession) they include both media materials as well as the related textual or photographic materials that are part of a shared provenance. It is a very common scenario when these mixed collections are accepted into archival repositories, accessioned, and then broken apart and processed using very different and separate techniques, guidelines, and description schemas. The Academy of Motion Picture Arts &amp; Sciences (AMPAS) contains several divisions comprising one of the world&rsquo;s largest repositories devoted to film history, including the Academy Film Archive, the Margaret Herrick Library, the Academy Oral History Projects, and the Science &amp; Technology Council. AMPAS has been receiving donations and gifts in the form of these &ldquo;mixed&rdquo; collections for years, from producers, artists, private collectors, and studios. Depending on the division receiving the accession, it was standard practice for years to split them, a practice that they&rsquo;ve actively been shifting away from over the past 5-7 years by integrating an enterprise collections management software product called Adlib, which grants them the capability to unite and ontologically link film and paper collections sharing provenance or shared authorities across different divisions. These shifts, and the earlier pivots taken by UCLA, demonstrate that these two repositories, as two of the world&rsquo;s largest collections of our audiovisual history, have had to adapt to changing realities in the field and within their institutions. Both have the resources for some form of solution when they receive and handle &lsquo;mixed&rsquo; collections comprising both A/V and paper resources. It also stands to reason that the types of collections that each of these repositories receive most often (with notable exceptions) are more cleanly identifiable by a shared provenance, shared authorities, and some level of bibliographic control.</p>
<p>But quite often, either in situations where the institution doesn&rsquo;t have the kind of resources needed to direct attention to this kind of work, or when the collections fall under the category of so-called &lsquo;orphaned&rsquo; works, the two collections never get near each other again – physically or ontologically. It is these circumstances which lead to culturally rich collections being siloed, underdescribed, under-contextualized, and thus largely ignored by the larger sphere of possible users. For the category of orphaned works, the value of the materials for researchers frequently remains unrealized, as the archival practices outlined above tend to separate them from the contextual associations that clarify their historical and social importance. Scholars such as Devin Orgeron, Marsha Gordon, and Dan Streible have addressed this problem, noting in  <em>Learning with the Lights Off: Education Film in the United States</em>  that nontheatrical films have experienced limited availability in archives, which has in turn caused a lost sense of their historical significance <sup id="fnref:21"><a href="#fn:21" class="footnote-ref" role="doc-noteref">21</a></sup>. In recent years, sustained archival and scholarly interest has led to a number of projects attempting to reinstate this historical value by providing context for orphaned media objects. For example, the Canadian Educational, Sponsored, and Industrial Film (CESIF) project, led by Charles Acland and Louis Pelletier, in part contextualizes orphaned films through a set of circulating institutions, showing the range of nontheatrical materials that were produced in Canada <sup id="fnref:22"><a href="#fn:22" class="footnote-ref" role="doc-noteref">22</a></sup>. Other projects have invited user contributions in these efforts. Mark Williams&rsquo;s Media Ecology Project at Dartmouth College allows users to add information, including metadata, to create contextual connections across archival collections, intended to &ldquo;facilitate a dynamic context of research that develops in relation to its use over time by a wide range of users&rdquo; <sup id="fnref:23"><a href="#fn:23" class="footnote-ref" role="doc-noteref">23</a></sup>. This approach has been applied to orphaned collections in the Media Ecology Project&rsquo;s Historical News Media Study.</p>
<p>For split collections, contextual information that can highlight the significance of the audiovisual materials already exists, so the obstacle becomes connecting the media objects and their contextual materials. This challenge provides an opportunity for enterprising archivists who are able to approach a more postmodern approach to description, or for digital humanists who have the ability to help archivists innovate in these areas, to think about virtually unifying the two collections. The collections of the National Association of Educational Broadcasters (NAEB), which are at the heart of the case study below, are a prime example of this lost potential for discoverability.</p>
<h2 id="what-is-a-temporal-commitment-barrier-and-how-does-it-pose-challenges-for-the-use-of-audiovisual-collections-as-primary-resources">What is a &rsquo;temporal commitment barrier,&rsquo; and how does it pose challenges for the use of audiovisual collections as primary resources?</h2>
<p>In a <a href="https://mith.umd.edu/dialogues/dd-fall-2017-sarah-florini/">2017 talk at MITH</a>, scholar Sarah Florini discussed how black social media fandom employed strategies to mitigate the affordances of particular platforms, including podcasts, invoking the term  “temporal commitment barrier.”   <sup id="fnref1:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> Lifting that out and employing it here, it becomes a useful term to invoke the notion of how users of A/V collections can easily become overwhelmed by the sheer listening hours needed to research time-based collections. Obviously, this barrier increases in size exponentially with a large corpus of materials and many thousands of hours of content. A solution to overcome this barrier, the use of time-stamped, synchronized transcripts, has been increasingly adopted by the audiovisual archiving field. An early adopter was the WGBH Media Library and Archives, who employed it for their project  <em>Vietnam: A Television History</em>  as early as 2006. Use of this approach has been fueled in part by access to automated speech-to-text tools, which has lowered the barrier of entry and made it so that human labor was not needed to transcribe entire time-based works. Oral histories are most often embracing the technology, as evidenced by the popularity of the open source software OHMS (Oral History Metadata Synchronizer) out of the University of Kentucky <sup id="fnref:24"><a href="#fn:24" class="footnote-ref" role="doc-noteref">24</a></sup>. OHMS employs both transcripts and indexing functionality to gain access to subparts of a time-based asset. The consulting company AVP (formerly AVPreserve) integrates OHMS with its popular platform Aviary, which AVP licenses as a robust front-end interface. Aviary can be integrated with a variety of platforms, providing a functional, usable display next to descriptive context. The AMPAS Oral History Projects department uses OHMS to index and display synced transcripts for its collection of filmmaker interviews.</p>
<p>Scholar Tanya Clement has discussed the use of transcripts as a means to link a single audio or video event together with transcripts as a sort of scholarly primitive, invoking John Unsworth&rsquo;s use of the term <sup id="fnref:25"><a href="#fn:25" class="footnote-ref" role="doc-noteref">25</a></sup>, while also acknowledging the limitations and limited vision of such approaches, stating  “It is also our inability to conceive of and to express what we want to do with sound — what Jerome McGann (2001) calls  imagining what you don&rsquo;t know  — that precludes us from leveraging existing computational resources and profoundly inhibits DH technical and theoretical development in sound studies”   <sup id="fnref1:25"><a href="#fn:25" class="footnote-ref" role="doc-noteref">25</a></sup>. The use of transcripts to &lsquo;read the text&rsquo; of time-based media objects may indeed be a form of scholarly primitive, but it&rsquo;s still only just gaining traction in the field, with many adopters still struggling to implement synced transcripts in a usable fashion. It was only this year when the OHMS software was even able to introduce the ability to import pre-existing timestamps for transcripts — previously the tool required its users to manually create the sync points through a very clunky process of hearing beeps and marking the text you hear. The Pop Up Archive, a nonprofit which for several years received copious financial support from various government and private funders, provided speech to text services as well as integrations and features to correct, reuse, and embed those transcripts in other applications. The service fulfilled a need which was acknowledged and lauded by the funding community, before it closed down services in 2017 after being bought out by Apple. Since then, former users of the service have splintered off and utilized different methodologies to achieve this same need. Some of them go towards open source solutions, some develop their own approaches. One of the transcript file formats that Pop Up Archive provided included WebVTT. Since 2012 WebVTT has been supported in all major Web browsers. The standardization of WebVTT was not without controversy, due to its decision to forgo the use of the XML based transcription standard TTML. However, browser support for WebVTT and the Web Audio API <sup id="fnref:26"><a href="#fn:26" class="footnote-ref" role="doc-noteref">26</a></sup> make it a logical choice for audio transcription projects because it greatly simplifies the synchronization of audio and transcripts in the browser.</p>
<h2 id="increasing-the-use-of-audiovisual-collections-for-scholarly-study">Increasing the use of audiovisual collections for scholarly study</h2>
<p>Scholars have addressed the enormous challenges of arrangement and description for the vast amount of audiovisual material now produced every day, such as Virginia Kuhn&rsquo;s discussion of this problem in her argument for a mixed methods approach to analyzing filmic media <sup id="fnref:27"><a href="#fn:27" class="footnote-ref" role="doc-noteref">27</a></sup>. But the issues outlined above have contributed to a situation in which archival A/V collections similarly face problems of underdescription and temporal barriers, leading them to become neglected materials despite their availability to researchers. Many collections lack the detailed metadata and clear contextual explanation that would make them more accessible, so a/v materials often remain underexplored by scholars and educators. Digital humanists have an opportunity to promote the use of a/v materials by restoring the relationships that were severed by split collections, highlighting their value as primary historical records alongside contextual paper materials. As one example of a project with this goal,  <em>Unlocking the Airwaves</em>  is presented below. The section starts off with some background into both the NAEB collections and the project itself, and then lays out the DH techniques that were deployed through a description of the application&rsquo;s design, object model, descriptive standards, and workflows.</p>
<h2 id="case-study-the-unlocking-the-airwaves-project">Case Study: The Unlocking the Airwaves project</h2>
<p><em>Unlocking the Airwaves</em>  was in development and fundraising between 2013-2018 prior to being funded by the National Endowment for the Humanities in April 2018. As of June 2020, the project team has launched a beta version of the website, and has made substantial progress towards a final launch in Spring 2021. Although we still have five months to complete the final stages of the project, including the publication of exhibits and teaching tools, we have leveraged a range of digital strategies to bring together the NAEB&rsquo;s split paper and audio collections. Throughout this project, we have successfully combined existing tools and frameworks, such as a minimal computing application design, linked data, and synced transcripts, to make the collections accessible to users while minimizing cost and long-term maintenance. Although every split collection poses different problems for potential reunification efforts,  <em>Unlocking the Airwaves</em>  provides one method for digital humanists to reunite collections, creating new access points to A/V materials and positioning them within their historical context.<sup id="fnref:28"><a href="#fn:28" class="footnote-ref" role="doc-noteref">28</a></sup></p>
<h2 id="the-naeb-collections">The NAEB Collections</h2>
<p>Before the ubiquity of National Public Radio (NPR) and the Public Broadcasting Service (PBS), the National Association of Educational Broadcasters (NAEB) was the primary institution responsible for promoting and distributing public broadcasting content in the United States. The NAEB was initially established as the Association of College and University Broadcasting Stations in 1925. Member stations attempted to share programming resources in various ways until the organization created a distribution network in 1949, which was run from the NAEB&rsquo;s first national headquarters at the University of Illinois in Champaign-Urbana beginning in 1951. The organization grew steadily during the 1950s and 1960s, achieving a key moment in its history when the NAEB&rsquo;s director of radio distribution Jerrold Sandler successfully lobbied to insert language supporting public and educational radio into the Public Broadcasting Act of 1967. While this led to the creation of NPR, it also caused the demise of the NAEB in 1981 and preceded the separation of its historical records. Thousands of audio recordings were transferred to NPR in Washington DC and later added to the University of Maryland Libraries&rsquo; National Public Broadcasting Archives, whereas the paper records were mostly archived in the Wisconsin State Historical Society (now Wisconsin Historical Society, WHS), deposited by longtime NAEB President William G. Harley. This separation of the NAEB&rsquo;s audio and paper collections complicates a full understanding of broadcasting&rsquo;s history in the United States.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000509/resources/images/figure01.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000509/resources/images/figure01_hu8b831983e752803308bc0a26e3994be4_298535_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/15/1/000509/resources/images/figure01_hu8b831983e752803308bc0a26e3994be4_298535_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/15/1/000509/resources/images/figure01_hu8b831983e752803308bc0a26e3994be4_298535_1200x0_resize_q75_box.jpg 1200w,/dhqwords/vol/15/1/000509/resources/images/figure01_hu8b831983e752803308bc0a26e3994be4_298535_1500x0_resize_q75_box.jpg 1500w,/dhqwords/vol/15/1/000509/resources/images/figure01_hu8b831983e752803308bc0a26e3994be4_298535_1800x0_resize_q75_box.jpg 1800w,/dhqwords/vol/15/1/000509/resources/images/figure01.jpg 1941w" 
     class="portrait"
     ><figcaption>
        <p><em>NAEB Newsletter</em> , archived in the collections of the Wisconsin Historical Society.
        </p>
    </figcaption>
</figure>
<p>Unfortunately, although researchers at the WHS could  <em>see</em>  these aspects of the NAEB, they could not  <em>hear</em>  them. The split between the paper and audio materials in the collection prevented the discovery of connections between the everyday practices of the NAEB and the media it presented to the public. Considering the two sides of the archival collection together can contextualize the philosophy, objectives, and practices of the organization that broadcast the radio programs. In the daily correspondence of the NAEB&rsquo;s staff and key members and publications like the  <em>NAEB Newsletter</em>  (see <a href="#figure01">Figure 1</a>), the paper material covers everything from aesthetic norms to political aims, and illustrates how the values and goals of the organization influenced the types of programs it broadcasted and the eventual development of the public radio that we know today. The importance of the NAEB cannot be fully understood without restoring the connections between the audio and the paper materials, and the NAEB collection is just one of many collections that could benefit from this reunification of split materials.</p>
<p>The availability of A/V materials, like the NAEB radio programs, can allow scholars and educators to answer significant research questions that might not be possible with only a collection&rsquo;s paper materials. In the case of the NAEB, the content of thousands of episodes reveals distinct perspectives on key social and political topics from the period. For example,  <em>People Under Communism</em>  focuses on various aspects of life in Soviet Russia, including music and literature, and  <em>Seeds of Discontent</em>  examines the disadvantages facing various &ldquo;discontented forces&rdquo; in the United States through interviews with groups ranging from black artists in the entertainment industry to public school teachers. Other programs like  <em>American Adventure</em>  and  <em>The Jeffersonian Heritage</em>  seek both to educate and entertain with dramatized figures and moments from American history. Even when the paper materials might include summaries or outlines of a program&rsquo;s individual episodes, accessing the audio itself affords the study of aesthetic questions beyond content alone, like music, performance, audio effects, and timing. The topics generated by the audio programs can fruitfully intersect with the paper materials, like the correspondence and reports from the NAEB that reveal the program committee&rsquo;s aesthetic priorities and tastes. The committee&rsquo;s discussions cover everything from the quality of acting to the adequacy of episode introductions in program submissions, providing context for the types of radio that the NAEB preferred, as well as the responses to programs that did not fit the members&rsquo; interests.</p>
<h2 id="background--project-goals">Background &amp; Project Goals</h2>
<p>At the onset of the project, the radio programs were already digitized and transcribed through the University of Maryland Libraries&rsquo; prior collaborations with the American Archive of Public Broadcasting (AAPB) and Pop Up Archive. The paper collections had been described in an online finding aid, but not yet digitized. As of June 2020, we&rsquo;ve achieved four of the project&rsquo;s five major milestones and are quickly approaching the completion of the fifth:</p>
<p>Digitize an identified subsection of the NAEB paper collection in-house at the Wisconsin Historical Society;  Create new metadata about the digitized material, including a set of archival authority records about early educational and public broadcasting;  Design the backend structure of the application utilizing a combination of select linked data and minimal computing methodologies;  Design and test a user interface informed by gathered user stories;  Integrate exhibits and teaching guides in the application, as well as curated access points both in and outside of the application.</p>
<h2 id="system-design-and-object-model">System Design and Object Model</h2>
<p><em>Unlocking the Airwaves</em>  employs a minimal computing application design, which combines static site generation, client-side web framework and indexing technologies to minimize server side dependencies, and greatly reduce the cost of deployment and long-term maintenance. As the Internet has become more central to the practice of humanities research and cultural heritage, the tools to publish websites have become increasingly complex. This is especially true for websites meant to serve as digital collections or archives. While many libraries, archives, and museums now recognize the value of having their collections accessible online — and while many humanities scholars build personal or thematic digital research collections as part of their scholarship — the tools and infrastructure to achieve this can strain the capabilities and resources of even privileged institutions. One of the primary goals of minimal computing is to reduce the dependence on costly web and Internet service infrastructures.</p>
<p>&ldquo;Static site generators&rdquo; are pieces of software for generating static content that can then be copied to and served up by web servers. This web content is called &ldquo;static&rdquo; because of its representation as simple files on disk (HTML, CSS, JavaScript, images, video) that do not require content management software (CMS) to access, and can simply be viewed as-is in a web browser. Static websites are useful for sustainability since they require very little in terms of maintenance and monitoring. But this sustainability is achieved by pushing some of the complexity of a dynamic website into the static website&rsquo;s build process. Fortunately, this build process happens just once when the site is deployed, rather than every time a resource is fetched by a user. Although tools such as Columbia University&rsquo;s Wax and University of Idaho&rsquo;s Collection Builder have used static site generators to provide a generalized framework for building digital exhibitions and collections in an easily deployable, portable and low-maintenance web application, none of them have attempted to do so while simultaneously reunifying distributed collections. Combining minimal computing and static site generation with the aforementioned methodologies of virtual reunification, linked data, and synced transcripts allows us to examine facets of digital collection building and maintenance that have been explored with &ldquo;standard&rdquo; online platforms and software, but which have not been investigated in these more complex frameworks.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000509/resources/images/figure02.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000509/resources/images/figure02_hua3f042534bb49069379f175b3713cdb9_115034_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/15/1/000509/resources/images/figure02_hua3f042534bb49069379f175b3713cdb9_115034_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/15/1/000509/resources/images/figure02.jpg 852w" 
     class="portrait"
     ><figcaption>
        <p><em>Unlocking the Airwaves</em> System and Process Flow Diagram.
        </p>
    </figcaption>
</figure>
<p>The above system diagram displays all the various components of the  <em>Unlocking the Airwaves</em>  application (<a href="#figure02">Figure 2</a>). The application website is being generated as a static site using Gatsby which is a static website generator written in the NodeJS development environment. Even though the website is deployed as a set of static files to MITH&rsquo;s webserver, it relies on several other services during its   <em>build</em>   and   <em>runtime</em> . The   <em>build</em>   is a process that happens once when the site is deployed, and the   <em>runtime</em>   is the process run by a browser when a user accesses the site. These services include:</p>
<ul>
<li><a href="https://github.com">GitHub</a>: platform where the source code for the website is stored and versioned ( <em>build</em> );</li>
<li><a href="https://airtable.com">Airtable</a>: a cloud-based database containing audio and document metadata ( <em>build</em> );</li>
<li><a href="https://archive.org">Internet Archive</a>: hosts the digitized document scans which are then made available through their IIIF Service [https://iiif.archivelab.org/iiif/documentation] ( <em>build</em>   +   <em>runtime</em> );</li>
<li><a href="https://aws.amazon.com/s3/">Amazon S3</a>: storage of MP3 audio files and WebVTT audio transcripts ( <em>runtime</em> );</li>
<li><a href="https://www.netlify.com">Netlify</a>: content publishing service for exhibit data ( <em>build</em> );</li>
<li><a href="http://elasticlunr.com">ElasticLunr</a>: client-side search powers navigation and discoverability of the site&rsquo;s content. ( <em>runtime</em> )</li>
</ul>
<p>The audio files are redundantly stored on the servers of the American Archive of Public Broadcasting (AAPB) and on hard drives at UMD Libraries, with streaming copies hosted in an Amazon Web Services S3 bucket. Machine-generated audio transcripts are currently being stored as WebVTT files, also in an S3 bucket, which feeds into the application and merges the audio and transcripts for display in  <em>a</em>  WebVTT player designed by MITH Developer Ed Summers. This display creates one way to work around the &rsquo;temporal commitment barrier&rsquo; that often accompanies audiovisual collections, as users will be able to search for terms that appear in the episode transcripts or scroll through and navigate to certain parts of episodes based on their interests, without having to listen to them in their entirety.</p>
<p>The digitized images of the paper collections are being redundantly stored on a 34TB RAID at the University of Wisconsin-Madison&rsquo;s CommArts Instructional Media Center, as well as on Internet Archive (IA) servers. Co-PI Eric Hoyt has been utilizing the Internet Archive for materials he and his team have produced for the Media History Digital Library, which creates automatic access derivatives including OCR files. Since the Internet Archive offers integration with the International Image Interoperability Framework (IIIF), the project team made the decision to use the IA as part of our digitization workflow, not only for redundant storage/access points and creating derivatives for all scanned materials, but also so we could utilize the Mirador image viewer. Mirador supports the IIIF standard, and the new version is written in the JavaScript library React, which dovetails nicely with Gatsby. This approach has saved us significant time and effort, as we can capitalize on linked data technology to display the documents, as opposed to hosting them on our own servers.</p>
<p>To track and consolidate metadata for the paper and audio materials, we opted to utilize the cloud-based relational database software tool Airtable for distributed data curation. This Airtable database has the added benefit of functioning as a project management tool for the digitization process, and as a means for publishing the digitized materials to the Internet Archive. Summers also developed a custom uploader program that utilizes Airtable&rsquo;s Application Programming Interface (API) to automatically take a folder of individual digitized images from a given folder, bundle them into one package, upload them to the NAEB collection on the Internet Archive, and map the metadata from Airtable into the Internet Archive&rsquo;s metadata schema. More about our choices for descriptive workflows and standards is included in the section below.</p>
<p>The exhibits and teaching tools are created by a distributed team of curators, who enter text, links, images, and captions into a graphical user interface using Netlify CMS. Netlify watches the airwaves GitHub repository, automatically builds a distinct staging site used for development and testing, then generates the exhibit pages in the application. As a plugin, Netlify is also responsive to our content model, which means that curators are able to directly attach links to people, organizations, or programs directly from its user interface.</p>
<p>The CSS files comprising the front end design graphics are being generated by MITH Designer Kirsten Keister using Dreamweaver, then published to the Airwaves application in GitHub. Tweaks and modifications to text on static pages can be modified directly in GitHub, but design changes must be separately committed by Keister. Select screen captures from the beta version of the application website can be viewed below (see <a href="#figure02">Figure 2</a>  <a href="#figure03">Figure 3</a>  <a href="#figure04">Figure 4</a>  <a href="#figure05">Figure 5</a>) and also through a screencast with an example of a recording [<a href="https://vimeo.com/437614686">https://vimeo.com/437614686</a>].</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000509/resources/images/figure03.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000509/resources/images/figure03_huf1dac50e3c0142f5af457b1fbe70a5f8_661909_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000509/resources/images/figure03_huf1dac50e3c0142f5af457b1fbe70a5f8_661909_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000509/resources/images/figure03.png 1121w" 
     class="portrait"
     ><figcaption>
        <p><em>Unlocking the Airwaves</em> Home Page.
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000509/resources/images/figure04.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000509/resources/images/figure04_hu5742eceae87946586ddf4f39e04e0786_186000_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000509/resources/images/figure04_hu5742eceae87946586ddf4f39e04e0786_186000_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000509/resources/images/figure04.png 1128w" 
     class="portrait"
     ><figcaption>
        <p><em>Unlocking the Airwaves</em> program landing page, with embedded transcript viewer/audio player. PBCore metadata is displayed on the left. Clicking on individual subjects, genres, or people leads users to a canned search on the Search page.
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000509/resources/images/figure05.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000509/resources/images/figure05_huce01f964e9e98b26e9f4189e0b49c7cf_226018_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000509/resources/images/figure05_huce01f964e9e98b26e9f4189e0b49c7cf_226018_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000509/resources/images/figure05.png 1133w" 
     class="portrait"
     ><figcaption>
        <p><em>Unlocking the Airwaves</em> Search page, displaying a canned search for subject=Air Pollution. Facets can then be used to filter results.
        </p>
    </figcaption>
</figure>
<h2 id="descriptive-standards-and-workflows">Descriptive Standards and Workflows</h2>
<p>The challenge inherent with possible data models for this application is that we want to enhance discoverability between these two major collections, while retaining the semantic and descriptive properties of each. Utilizing virtual reunification and linked data, our approach allows us to meet the challenges presented by the Archival Diaspora, while respecting institutional mandates and priorities. Below (see <a href="#figure06">Figure 6</a>) is the object model, which recalls key elements and components of the project data such as radio series, radio episodes, archival series, archival folders, and authorities (Person, Organization, Topic).</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000509/resources/images/figure06.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000509/resources/images/figure06_hu4d83c7e53253c9c40a88744ca4e17bee_125049_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000509/resources/images/figure06_hu4d83c7e53253c9c40a88744ca4e17bee_125049_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000509/resources/images/figure06.png 1106w" 
     class="landscape"
     ><figcaption>
        <p><em>Unlocking the Airwaves</em> Object Model.
        </p>
    </figcaption>
</figure>
<p>Both the NAEB paper materials (at WHS) and the NAEB audio materials (at UMD) had already been cataloged and described according to established professional standards. The finding aid for the paper materials at WHS were encoded with the Encoded Archival Description (EAD) standard (see <a href="#figure07">Figure 7</a>). The audio materials had been described at the item level using the PBCore standard, which is based on the Dublin Core metadata schema with a number of added elements useful for media, making it possible to extend basic descriptive records by specifying sources, taxonomies, and parts of a media object.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000509/resources/images/figure07.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000509/resources/images/figure07_hu7c76411adc27e569f2a4492e1a0fd1fb_125786_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000509/resources/images/figure07_hu7c76411adc27e569f2a4492e1a0fd1fb_125786_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000509/resources/images/figure07.png 1032w" 
     class="landscape"
     ><figcaption>
        <p>EAD-encoded finding aid for the National Association of Educational Broadcasters Records, 1925-1977. Available online at <a href="http://digital.library.wisc.edu/1711.dl/wiarchives.uw-whs-us0076af">http://digital.library.wisc.edu/1711.dl/wiarchives.uw-whs-us0076af</a>.
        </p>
    </figcaption>
</figure>
<p>Recognizing the aforementioned challenges with different institutional imperatives driving archival description choices, we wanted to identify and track descriptive data about digitized materials in a manner that didn&rsquo;t interrupt or fundamentally change each collection&rsquo;s canonical metadata. The choice of Airtable as an interlocutor of sorts helped us enable our virtual reunification strategy, serving as a centralized location where we could create these links within the database by harnessing Airtable&rsquo;s various features, while also exploiting its API to connect this connective tissue out to various endpoints at various stages in the workflow.</p>
<p>Folder level details already present in the EAD were imported into an initial table in the Airtable base, which was then used to a) identify and prioritize boxes and folders to digitize, b) monitor their digitization and description progress, and c) use the Dublin Core metadata schema to describe the paper materials at the folder level.</p>
<p>PBCore records for the audio materials were also imported into the same Airtable base as a separate table. This allowed us to connect the paper to the audio by linking key descriptive elements where they align across the Dublin Core and PBCore schemas: by Creator, Contributor, Subjects, Temporal and Spatial Coverage, and Dates. These linkages were achieved through separate tables for People, Corporate Bodies, Subjects, and Geographic Locations. Figures 8-12 below illustrate how all of these various tables were structured within the Airtable base.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000509/resources/images/figure08.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000509/resources/images/figure08_hua945e289c03bca3eca7662b7d5760126_156435_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000509/resources/images/figure08_hua945e289c03bca3eca7662b7d5760126_156435_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000509/resources/images/figure08_hua945e289c03bca3eca7662b7d5760126_156435_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000509/resources/images/figure08.png 1428w" 
     class="landscape"
     ><figcaption>
        <p>Airtable base tracking extant data (including box number, folder number and title, and archival series) pulled from the EAD finding aid for the NAEB paper collections at Wisconsin Historical Society.
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000509/resources/images/figure09.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000509/resources/images/figure09_hud35e7acfea3871530a882ff853cd2a0c_175762_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000509/resources/images/figure09_hud35e7acfea3871530a882ff853cd2a0c_175762_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000509/resources/images/figure09_hud35e7acfea3871530a882ff853cd2a0c_175762_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000509/resources/images/figure09.png 1425w" 
     class="landscape"
     ><figcaption>
        <p>Table tracking descriptive Dublin Core metadata at the folder level for the NAEB paper collections being digitized at the Wisconsin Historical Society. Dublin Core Elements, such as &ldquo;Type,&rdquo; are described in the table.
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000509/resources/images/figure10.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000509/resources/images/figure10_hu1217fe6e7fc66bc83f5acb60a39d8298_94217_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000509/resources/images/figure10_hu1217fe6e7fc66bc83f5acb60a39d8298_94217_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000509/resources/images/figure10_hu1217fe6e7fc66bc83f5acb60a39d8298_94217_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000509/resources/images/figure10.png 1320w" 
     class="landscape"
     ><figcaption>
        <p>Airtable base tracking item-level PBCore metadata about individual radio programs, grouped in this view by Series.
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000509/resources/images/figure11.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000509/resources/images/figure11_huafee9e8fc8c4544e7d14fdd4e2a62fa7_89459_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000509/resources/images/figure11_huafee9e8fc8c4544e7d14fdd4e2a62fa7_89459_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000509/resources/images/figure11_huafee9e8fc8c4544e7d14fdd4e2a62fa7_89459_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000509/resources/images/figure11.png 1425w" 
     class="landscape"
     ><figcaption>
        <p>Table tracking digitization progress by production assistants at the University of Wisconsin-Madison.
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000509/resources/images/figure12.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000509/resources/images/figure12_hu836a7c10163c0d54fb8d574e2cb5c3e1_95230_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000509/resources/images/figure12_hu836a7c10163c0d54fb8d574e2cb5c3e1_95230_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000509/resources/images/figure12_hu836a7c10163c0d54fb8d574e2cb5c3e1_95230_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000509/resources/images/figure12.png 1266w" 
     class="landscape"
     ><figcaption>
        <p>Airtable base tracking subject authorities as a linked table to item-level PBCore metadata. These authorities are also linked to the records in the same base tracking descriptive Dublin Core metadata about individual folders.
        </p>
    </figcaption>
</figure>
<p>In the final phases of the project, we are establishing workflows and documentation for porting the newly created metadata back out to select endpoints. This involves utilizing Airtable&rsquo;s API to generate JSON files, which we can then be transformed into different formats:</p>
<p>valid PBCore XML records to deliver to University of Maryland Libraries and to the AAPB in order to respect the descriptive formats and choices of the institutional stewards of the collections;  EAC-CPF records to submit to the Social Networks and Archival Context (SNAC) project in order to encourage wider adoption of standardized authority records about early educational radio by other institutions with complementary holdings (thus bridging the Archival Diaspora); and  mapping archival authorities to Wikidata, in order to further extend the use of the newly created authority records to a wider audience.</p>
<p>For the latter two, we are identifying a key subset of archival authorities (roughly 200) for People and Corporate Bodies which are either central/crucial to the NAEB&rsquo;s development, initiatives, or programs, or not currently represented in the broader world of published authorities, i.e. in the Library of Congress, SNAC, VIAF, or Wikidata. All of the above choices reflect a calculated assessment of established and emerging descriptive standards and practices in the archival field, while also harnessing digital humanities tools, methodologies, and workflows to deploy them in innovative ways.</p>
<h2 id="conclusions-and-possible-futures">Conclusions and Possible Futures</h2>
<p>Digital humanities projects like  <em>Unlocking the Airwaves</em>  not only make A/V materials more accessible to scholars, but they also position A/V within their specific contexts of production and distribution. Split collections make A/V materials less accessible and obscure the connections that can emerge from contextual resources, so virtual reunification projects offer the possibility of elevating A/V materials as research sources. In the case of the NAEB, the paper records tell the story of an organization as it developed an infrastructure for national educational radio, and the A/V materials include hundreds of programs covering a wide array of topics and approaches. The NAEB&rsquo;s programs, produced between 1952 and 1970, represent a period of astonishing growth, turmoil and social change in the United States. Bringing the NAEB&rsquo;s papers and audio together can reinstate the centrality of the radio programs in the story of the NAEB, while presenting them in the context of their circulation. Beyond its obvious relevance to scholars interested in the history of educational and public media there is also an important pedagogical added value to such an approach. In its present form, the online collection can be integrated into curricula in American History, Politics, Social Science, African American History and Culture, Art and Music. This approach thus not only enriches research possibilities, but also restores the status of A/V collections as primary sources worthy of attention and analysis.</p>
<p>Although we are still in the project&rsquo;s final stages, including the development of all exhibits and teaching tools, the team is already thinking through ways we can improve our interoperability and linked data approaches to exploit the possibilities of the semantic web. We have been thinking through transformation of project data into JSON-LD, developing workflows with OpenRefine, using Wikidata to display knowledge graphs within the application, and integrating our archival authorities with other related collections within SNAC. For these future phases, we welcome inquiries and ideas from potential collaborators working in similar ways with similar resources.</p>
<p>A major benefit of this work will be demonstrating new and innovative digital humanities approaches towards increasing the discoverability of audiovisual collections in ways that allow for better contextual description, and a flexible framework for connecting audiovisual collections to related archival collections. With the publication of this resource, we will have documented a tangible solution for this set of challenges.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Vinson, E.  “Reassessing A/V in the Archives: A Case Study in Two Parts.”  <em>The American Archivist</em> , 82:2 (2019): 421–439.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Florini, S.  “Oscillating Networked Publics: Contingent Uses of Black Digital Networks”  Digital Dialogues lecture series at MITH (2017): <a href="https://mith.umd.edu/dialogues/dd-fall-2017-sarah-florini/">https://mith.umd.edu/dialogues/dd-fall-2017-sarah-florini/</a>.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Becker, S.  “Family in a Can: Presenting and preserving home movies in museums”    <em>The Moving Image</em> , (1)2 (2001): 54-62.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Leigh, A.  “Context! Context! Context! Describing Moving Images at the Collection Level”    <em>The Moving Image</em> , 6:1 (2006): 33-65.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref3:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>Indiana University Bloomington Media Preservation Initiative Task Force.  “Meeting the Challenge of Media Preservation: Strategies and Solutions”  Indiana University (2011): <a href="http://hdl.handle.net/2022/14135">http://hdl.handle.net/2022/14135</a>.&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>Forsberg, W.  “Yes, We Scan: Building Media Conservation and Digitization at the National Museum of African American History and Culture”  Maryland Institute for Technology in the Humanities Digital Dialogue (November 7, 2017): <a href="https://mith.umd.edu/dialogues/dd-fall-2017-walter-forsberg/">https://mith.umd.edu/dialogues/dd-fall-2017-walter-forsberg/</a>.&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>Heany, M.  “An Analytical Model of Collections and their Catalogues”  Collection Description, 3:1 (2000): <a href="http://www.ukoln.ac.uk/metadata/rslp/model/amcc-v31.pdf">http://www.ukoln.ac.uk/metadata/rslp/model/amcc-v31.pdf</a>.&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>Ranger, J.  “Does the creation of EAD finding aids inhibit archival activities?”    [Blog Post]. 22 May 2013. Retrieved from: <a href="https://www.avpreserve.com/blog/does-the-creation-of-ead-finding-aids-inhibit-archival-activities/">https://www.avpreserve.com/blog/does-the-creation-of-ead-finding-aids-inhibit-archival-activities/</a>.&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>McShea, M.  “Megan McShea responds to &ldquo;Does the creation of EAD finding aids inhibit archival activities?&quot;”  [Blog Post]. 18 December 2013. Retrieved from <a href="http://archivesnext.com/?p=3617">http://archivesnext.com/?p=3617</a>.&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>McShea, M.  “Putting Archival Audiovisual Media into Context: An Archival Approach to Processing Mixed-Media Manuscript Collections”  Council on Library and Information Resources (2015): <a href="http://www.clir.org/wp-content/uploads/sites/6/McShea.pdf">www.clir.org/wp-content/uploads/sites/6/McShea.pdf</a>.&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>Ranger, J.  “Data is a simple machine”  [Blog Post]. 10 January 2014. Retrieved from <a href="https://www.avpreserve.com/blog/data-is-a-simple-machine/">https://www.avpreserve.com/blog/data-is-a-simple-machine/</a>.&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p>Deodato, J.  “Becoming Responsible Mediators: The Application of Postmodern Perspectives to Archival Arrangement &amp; Description”    <em>Progressive Librarian</em> , 27 (Summer 2006): 56.&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13">
<p>Cook, T.  “Fashionable Nonsense or Professional Rebirth: Postmodernism and the Practice of Archives”    <em>Archivaria,</em>  51 (Spring 2001): 14-35.&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14">
<p>Duff, W. M. and Harris, V.  “Stories and Names: Archival Description as Narrating Records and Constructing Meanings”    <em>Archival Science</em> , 2.3 (2002): 263-285.&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:15">
<p>DACS,  “Describing Archives: A Content Standard”  DACS (2019): <a href="https://saa-ts-dacs.github.io">https://saa-ts-dacs.github.io</a>.&#160;<a href="#fnref:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:16">
<p>Samuels, H.  “Who Controls the Past?”  <em>The American Archivist</em> , 49, no. 2 (1986): 109–124.&#160;<a href="#fnref:16" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:17">
<p>Palmer, C. L.  “Thematic Research Collections.” In S. Schreibman, R. Siemens and J. Unsworth (eds),  <em>A Companion to Digital Humanities</em> , Blackwell Publishing, Hoboken (2004), pp. 348-382&#160;<a href="#fnref:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:18">
<p>Punzalan, R.  “Virtual Reunification: Bits and Pieces Gathered Together to Represent the Whole”  Thesis, University of Michigan, Ann Arbor (2013): <a href="http://hdl.handle.net/2027.42/97878">http://hdl.handle.net/2027.42/97878</a>&#160;<a href="#fnref:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:19">
<p>Punzalan, R.  “Understanding Virtual Reunification”    <em>Library Quarterly: Information, Community, Policy</em> , 84:3 (2014): 294-323.&#160;<a href="#fnref:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:20">
<p>Punzalan, R.  “Archival Diasporas: A Framework for Understanding the Complexities and Challenges of Dispersed Photographic Collections”    <em>The American Archivist</em> , 77 (2014): 326-349.&#160;<a href="#fnref:20" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:21">
<p>Orgeron, D., Orgeron, M. and Streible, D.  <em>Learning with the Lights Off: Education Film in the United States</em> , Oxford University Press, New York (2012).&#160;<a href="#fnref:21" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:22">
<p>Acland, C.  “Low-Tech Digital.”  In C. R. Acland and E. Hoyt (eds),  <em>The Arclight Guidebook to Media History and the Digital Humanities</em> . Reframe Books, Sussex (2016), pp. 132-144.&#160;<a href="#fnref:22" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:23">
<p>Williams, M.  “Networking Moving Image History: Archives, Scholars, and the Media Ecology Project”  In C. R. Acland and E. Hoyt (eds),  <em>The Arclight Guidebook to Media History and the Digital Humanities</em> , Reframe Books, Sussex (2016) pp. 335–345.&#160;<a href="#fnref:23" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:24">
<p>Breaden, C., Holmes, C. and Kroh, A.  “Beyond Oral History: Using the Oral History Metadata Synchronizer to Enhance Access to Audiovisual Collections”    <em>Journal of Digital Media Management</em> , vol. 5, no. 2 (2016):133–150.&#160;<a href="#fnref:24" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:25">
<p>Clement, T. (2015).  “When Texts of Study are Audio Files.” In S. Schreibman, R. Siemens and J. Unsworth,  <em>A New Companion to Digital Humanities</em> , Wiley-Blackwell, Hoboken (2015), pp. 348-357.&#160;<a href="#fnref:25" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:25" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:26">
<p>Adenot, P. and Toy, R.  “Web Audio API” . World Wide Web Consortium. (2018) Retrieved from <a href="https://www.w3.org/TR/webaudio/">https://www.w3.org/TR/webaudio/</a>.&#160;<a href="#fnref:26" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:27">
<p>Kuhn, V.  “Images on the Move: Analytics for a Mixed Methods Approach.” In J. Sayers (ed.),  <em>The Routledge Companion to Media Studies and Digital Humanities</em> , Routledge, New York (2018), pp. 300-309.&#160;<a href="#fnref:27" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:28">
<p>To view a demonstration of the interface and functionality of the Unlocking the Airwaves beta web application, please reference the Vimeo video for  “The NAEB Radio Collection: Document Deep South and Mississippi Waterways” , by Stephanie Sapienza (<a href="https://vimeo.com/419551316#t=462s">https://vimeo.com/419551316#t=462s</a>).&#160;<a href="#fnref:28" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry><entry><title type="html">Hearing Change in the Chocolate City: Computational Methods for Listening to Gentrification</title><link href="https://rlskoeser.github.io/dhqwords/vol/15/1/000513/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://rlskoeser.github.io/dhqwords/vol/15/1/000513/</id><author><name>Alison Martin</name></author><published>2021-03-05T00:00:00+00:00</published><updated>2021-03-05T00:00:00+00:00</updated><content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>Gentrification is a sonic phenomenon. As cities across the globe reinvest in previously disenfranchised neighborhoods and invite middle-and upper-class residents to build communities in their own images, the sonic characters of these neighborhoods are shifting as well. Some neighborhoods become louder, newly formed nightlife hubs where incoming partygoers care little for residential sleep schedules. Others become quieter, as new residents enforce particular sonic expectations onto long-term residents. These shifts are, as sound is, enmeshed in complex histories of race, gender, class, sexuality, and ability, and documenting these sonic shifts has the potential to help us imagine and create more equitable soundscapes.</p>
<p>In 2018, I was thinking through the possibilities of using the digital humanities to listen to gentrification in the Shaw neighborhood of Washington D.C. My analysis of the sonic dimensions of gentrification in D.C. is couched in histories of policing black sound, from the black codes of the 19th century to the city&rsquo;s recent #DontMuteDC movement, which has fundamentally shifted the city&rsquo;s public conversation surrounding gentrification and black cultural displacement. The black codes, laws passed in the early 19th century, dictated in cruel detail what was legal or illegal in the life of black people, both freed and enslaved. They included several laws about the legality of black gatherings, which had to be approved and supervised by white men of authority (largely police), and imposed limits on the number of people that could gather and at what times <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. Over 100 years later, traces of the restrictive black codes are still present in D.C. and in cities across the country.</p>
<p>In April of 2019, the #DontMuteDC Movement originated at one particular intersection in D.C., at the corner of 7th Street and Florida Avenue Northwest. 7th and Florida is home to &ldquo;Central Communications,&rdquo; a MetroPCS cell phone store whose owner, Donald Campbell, plays go-go music from large storefront speakers during business hours. Go-go music is D.C.&rsquo;s local subgenre of funk, pioneered in the 1970s by bands such as Chuck Brown and the Soul Searchers, Trouble Funk, and The Young Senators. Characterized by energetic live performance, intense rhythmic grooves, and ample audience participation, go-go music has been the sound of working-class black D.C. for over 40 years. Go-go became the catalyst for the #dontmuteDC movement because of complaints leveled at the store by new residents who threatened to sue T-Mobile, MetroPCS&rsquo;s parent company, if the music was not turned off. Public outcry and swift organizing forced T-Mobile to allow the go-go music to continue, but the intersection remains the epicenter of conversations regarding gentrification and the silencing of black sonic life. Despite the positive outcome, this is what gentrification in Washington, D.C. sounds like: increased tensions surrounding the role of sound, music, and noise in &ldquo;public&rdquo; space.</p>
<p>In this article, I demonstrate the potential of the computational methods to hear gentrification., using a combination of ethnography, passive acoustic recording, and computational sound analysis. My focus here is the intersection of 7th and Florida, which is located in the rapidly gentrifying Shaw neighborhood of Washington D.C.. This recording and subsequent computational analysis is a part of my larger project of exploring how black people hear gentrification in D.C., and contributes to the black digital humanities by thinking through how digital tools can transform how we hear black life. Furthermore, while the digital humanities have turned toward the sonic in recent years, there is still much to be done to embrace aurality in the field. The project invites us to listen closely to a changing neighborhood, and emphasizes sound as an essential mode of knowledge production, all the while arguing that a sonic rendering of gentrifying space through the digital has the potential to move us toward more equitable soundscapes.</p>
<p>As I listen to 7th Street and Florida Avenue and the broader D.C. area, I hear continued attempts to silence of black music and sound through the threat of legal action, legislation, and the everyday criminalization of black people. In the same space, I also hear the flourishing of black sonic life in all its forms, from the quiet interior to raucous protest. In engaging sound in this way, we might imagine a soundscape in which any and all aural manifestations of blackness might be welcomed and not policed in the ways that they have been for centuries. Engaging with computational sonic methods leads us to a fuller understanding, articulation, and speculation of the auralities of black life, so that we might listen in and through and against gentrifying forces.</p>
<h2 id="digital-humanities-and-black-sound">Digital Humanities and Black Sound</h2>
<p>Although the digital humanities consideration of sound has been behind that of other fields, there are a number of scholars and thinkers whose projects treat sound as data within the realm of DH. Most influential for this project has been those that engage black digital humanities discourses as well as those that engage in large sonic datasets from a computational perspective. This combination of perspectives is critical because of big data&rsquo;s potential to harm marginalized communities, particularly black people. At the forefront of these conversations is the 2019 volume  <em>Digital Sound Studies,</em>  published by Mary Caton Lingold, Darren Mueller, and Whitney Trettian. Acknowledging early on that Black Studies has been on the vanguard of sound studies since its inception, Lingold et al. set the tone for what a sonic digital humanities can be, considering a variety of projects from Jennifer Stoever&rsquo;s foundational blog &ldquo;Sounding Out!&rdquo; to Myron Beasley&rsquo;s attention to how Zora Neale Hurston&rsquo;s voice might remix the archive. The volume argues that &ldquo;while digital media thus create a space of possibility for the study of sound, critical, interpretive labor fulfills this potential, not the technology itself,&rdquo; reinforcing the critical potential of sonic digital work when grounded in rigorous humanistic analysis. This reinforcement becomes ever more important as the #dontmuteDC movement forces the city and even the United States to have a stark conversation on the histories of policing black sound, and how technology might engage, unravel, and work against such legacies. Digital humanities and sound studies are sites in which to disrupt the violences of gentrification, and can do so through engaging with and against technological tools, from recording to machine learning.</p>
<p>In addition to those within digital humanities, ethnomusicologists have also long been engaging sound and the digital. For example, Regina Bradley&rsquo;s &ldquo;Outkasted Conversations&rdquo; provides a digital space dedicated to the study and archiving of southern hip-hop scholarship, featuring interviews, essays, and syllabi all dedicated to the study of Atlanta&rsquo;s pioneering rap group Outkast. Additionally, Kyra Gaunt&rsquo;s scholarship on the exploitation of black girls on YouTube and other social media platforms is offering a deep dive into the sonorities of online racial and gendered oppressions. Gaunt&rsquo;s work, drawing on data from YouTube (and therefore Google) makes audible the harm enacted onto black girls and teenagers online, even as their content builds profits for other entities <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. These projects incorporate sound into broader datasets that also include text-based media, arguing for sound&rsquo;s presence as an integral part of multifaceted humanistic projects.</p>
<p>Regarding the computational analysis of sound, Tanya Clement&rsquo;s work on machine learning and recorded sound collections has long been on the vanguard of sound work in the digital humanities. Her early use visualization tools such as ProseVis for the sonic analysis of Gertrude Stein&rsquo;s work has been key in developing questions of how to read sound and how to treat sound as data. Drawing on Johanna Drucker&rsquo;s &ldquo;data as capta,&rdquo; Clement advocated early on for reading data visualization like musical scores, where the visualization itself is dependent on the observer rather than existing on its own <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>. She also developed &ldquo;High-Performance Sound Technologies for Access and Scholarship&rdquo; (HiPSTAS), a program that leverages machine learning to generate metadata for large speech based aural datasets <sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>.</p>
<p>Digital humanities practices in oral history have also been key to this genealogy. On the forefront of this work has been Doug Boyd and the Oral History in the Digital Age Project, where authors and contributors have advocated for pushing understandings of oral history practice into digital humanities conversations, from tools to capture histories to using recordings as data. Sharon Webb et al. have developed a minimal computing project to mine oral history recordings for important sonic information that is typically overlooked in favor of text only transcripts. Utilizing the soundscapes of the recording themselves has the potential to help researchers work through issues of gender, silence, emotion, and other factors that are lost within the production of the transcript <sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>. These projects that consider how engaging in the work of signal processing and managing a large sonic dataset move us toward a broader potential for archival recordings, particularly those based in speech.</p>
<p>My intervention into this conversation in sonic digital humanities is to leave behind the text as an anchoring force, and to create work with large sonic datasets that respects the call and parameters of black digital humanities, described by Kim Gallon as &ldquo;the intersection between Black studies and digital humanities, transforming the concept into corporeal reality while lending language to the work of the black digerati in and outside of the academy&rdquo; <sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>. Gallon forces us to ask the question of how the complete premise of the digital humanities changes for a group of people who are in many ways, historically and contemporarily, not considered human. Engaging with my own dataset, then, is always a question of amplifying black life. In the case of listening to gentrification in Washington, D.C., I have had to consider how both the creation and analysis of a large sonic dataset might engage the aural to disrupt visual narratives of gentrification, which emphasize dramatic change, displacement, and erasure <sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>.</p>
<h2 id="methodology">Methodology</h2>
<p>I conducted the majority of the fieldwork for this project in Washington, D.C. between 2016 and 2018, employing three interwoven methodologies: interviews, participant observation, and passive acoustic recording. I engaged this combination of methodologies in order to arrive at a kind of &ldquo;thick data&rdquo; <sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>. Thick data combines Geertz&rsquo;s thick description with the big data that is often central to digital humanities conversations, aiming for a generative kind of movement between the macro and the micro, which was essential for this project. During fieldwork, I was interested in telling the stories of individuals, places, and songs through interviews and participant-observation, but also in listening to change over time and detecting broader patterns that only a vast number of soundscape recordings could provide. Approaching sound in &ldquo;multiple registers&rdquo; consistently allowed me to engage more meaningfully with the ethnographic and the computational aspects of the project.</p>
<p>I conducted dozens of interviews with a wide range of conversation partners: musicians, local neighborhood officials, church members, business owners, and residents. These interviews were typically semi structured conversations, drawing on a mix of my own questions related to sonic impressions of the neighborhood as well as stories that people were willing to share. Gaining access to interviews was often an exercise of positioning myself in layers, typically beginning with my &ldquo;official&rdquo; classification as a PhD student or Smithsonian intern working on a project focused on neighborhood change. Language was important here, as &ldquo;gentrification&rdquo; carries different connotations than &ldquo;neighborhood change&rdquo; in local discourses. Neighborhood change is a less racially charged term than gentrification, with the former indicating that the neighborhood has undergone some development and the latter insinuating that the development can be blamed on a particular group of (white) people. At a neighborhood association meeting, I once described my project to a black resident of Maryland as being about &ldquo;gentrification and sound,&rdquo; and he remarked that it was very interesting, &ldquo;because of that word that you just used,&rdquo; rather than repeating it himself. I moved between the two terms as the project unfolded, often times mirroring the vocabulary of my interlocuters.</p>
<p>Two other layers of identification were key in gaining access to interviews: my blackness as well as my being from Prince George&rsquo;s County, Maryland, located directly east of the city. Because of the exploitative legacies of fields such as ethnomusicology, sociology, and anthropology, communities of color often exercise a legitimate skepticism with outside researchers, no matter their racial identity <sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup>. During phone calls to solicit interviews, I noticed that I was code switching more than usual, drawing out my D.C. accent in order to be heard and coded as a black woman from the area, working against the violent legacy of my discipline. In this way I was both insider and outsider for the duration of the project, at home in my proximity, racial identity, and family ties to the city yet distant in my academic training. Even as I did fieldwork in places very familiar to me, I was always an outsider just by virtue of doing the fieldwork <sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup>.</p>
<p>I articulate these layers in order to complicate the notion of &ldquo;fieldwork at home,&rdquo; which has been inconsistently theorized in ethnomusicology and is so often conducted by people of color. As Mellonee Burnim has argued, shared racial or ethnic identity with a group of people does not automatically create a culture-bearer. Regarding her fieldwork on gospel music, she emphasizes that her racial identity is not the only reason why she was trusted in her various research sites, but also because she was a culture-bearer within the gospel music tradition, and was able to offer these churches her skills as a musician in return for conducting research <sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>. My blackness, then, did not always endear people to me. Fieldwork for me was at home but-not-home, where I was often familiar with the people and places around me but had arrived with a new set of expectations and questions that required me to become an outsider.</p>
<p>Participant observation for this project involved attending musical performances, neighborhood meetings, rallies, and protests. At these events, I was typically most interested in sound sources and tensions surrounding them. For example, at an Advisory Neighborhood Commission (ANC) meeting one evening, half the attendees left the event halfway through because their hot-button agenda item had been completed. They then proceeded to be so loud in the hallway that one of the commissioners running the meeting had to step out to tell them to be quiet, reminding them that there were still more items on the agenda besides theirs. These tensions, stemming from who is able to be heard and whose sounds or voice are deemed important, were central for the project. Furthermore, many of my observations were digital rather than in person; I listened to online radio, watched Facebook and Twitter videos, and watched live streams of D.C. City Council Hearings. In this way, my ethnography was necessarily digital because many of the city&rsquo;s conversations on gentrification are happening online, both privately and publicly. As a result, I was thinking across multiple forms and formats of sound.</p>
<p>Passive acoustic recording was the most unconventional method for the project, because it is most often used to listen to bioacoustic changes in environments coded as &ldquo;natural,&rdquo; such as forests or underwater to listen to marine life. However, this method of long-term passive acoustic recording has been established as an effective way of detecting patterns and changes in a large number of soundscapes <sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>. I was inspired by a colleague&rsquo;s work in the Ecuadorian cloud forest to transfer the methodology to the heart of Washington, D.C., applying a few modifications but utilizing the same recording equipment. For the duration of the project, I utilized two Wildlife Acoustics Song Meter SM4 recorders, described by the manufacturers as &ldquo;a compact, weatherproof, dual-channel acoustic recorder capable of long-term acoustic monitoring of birds, frogs, insects, and aquatic life.&rdquo; <sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup>   <sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup>.</p>
<p>From the beginning, this passive acoustic recording was carefully intertwined with ethnography, because I chose to install the recorders on two building rooftops in and around the intersection of 7th and Florida. Acoustically, rooftops offer the best vantage point for urban passive acoustic recording. In order to install the recorders, I first had to receive permission from the owners of those particular buildings (one a church and one a church office), both of whom agreed after an interview with me. This agreement also came with the promise of letting me inside and on the rooves once a month to download data and change the batteries for the recorders, which allowed me to get to know various office staffs and grounded this project in people, rather than soundscape data. The first rooftop was about five stories off the ground, and covered with solar panels. In March of 2018, I hung the first recorder, nestled in a wooden arm, off of the southwestern edge of the roof, so as to be directed towards the intersection of 7th street and Florida Avenue. I installed the second recorder in May over a third-floor balcony in the intersection. Both recorders captured one minute out of every five minutes all day, every day. That resulted in 288 recordings per day, per recorder. The final tally of over 100,000 recordings is nearly impossible to listen or analyze manually, and yet the possibilities for the dataset seem endless.</p>
<p>While the go-go music at Central Communications is certainly the defining element of 7th and Florida, there are a number of other features that provide a rich space for a cultural soundscape study. Given its proximity to the Historically Black College/University Howard University, the intersection is heavily trafficked by students, residents, and people who work in the nearby Howard University Hospital. The northern half of the intersection is home to a florist, beauty supply store, restaurant, and church offices. At the southwest corner of the intersection is a CVS pharmacy owned by Howard University, complete with a small parking lot that is always at least half full of cars. In addition to these businesses, there are Metro Bus stops at the southwest and northeast corners of the intersection, where people constantly mill about and wait for the bus. In addition to foot traffic, 7th and Florida is one of the busiest intersections in the city for vehicular traffic, also ranked in the top 10 dangerous intersections for drivers between 2015 and 2018 <sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup>. Where Florida Avenue continues west directly to the U Street Corridor and into a busy nightlife district, 7th Street southbound leads to the Chinatown/Gallery-Place neighborhood, comparable to a smaller version of New York City&rsquo;s Times Square. The result is a bustling junction that is never silent, if only for the crosswalk signal that beeps 24 hours a day.</p>
<p>Engaging 100,000 raw .wav files required a healthy amount of data cleaning, an essential and time-consuming phase of every digital humanities project. Data cleaning is generally understood as the manipulation of a raw dataset (sound, text, images, etc.) into a more categorized, segmented, and manageable whole. When it comes to projects based in humanistic inquiry, though, data cleaning in and of itself is contested. The language of cleaning data implies that &ldquo;things already have a rightful place, but they&rsquo;re not in it&rdquo; <sup id="fnref:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup>. The practice of putting data into categories is an imposition, and requires the questioning of the categories themselves. In my work, I treat listening as a speculative method, following Deborah Kapchan&rsquo;s call for scholars to &ldquo;release our hold on intellective knowledge (with its drives to categorize, objectify, and subjugate)&rdquo; <sup id="fnref:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup>. Similarly, I seek a mode of knowledge production through the aural that speculates without being ahistorical, and listens with the intent of amplification rather than categorization. This type of speculative engagement with sound led to a flexibility within both data cleaning and analysis, where I engaged in different levels of cleaning depending on the particular goal of the project in that moment. The disruption of visual narratives of gentrification, which emphasize displacement and measurable outcome, lies here in the slippage between the speculative and the categorizable. My intention in questioning the process of data cleaning is not to arrive at a perfect solution of how to categorize data, but instead to use both necessary processes of categorization and speculation to interrupt prevalent narratives of loss.</p>
<p>Tanya Clement describes computational sound analysis as a question of compromises balanced amidst an exercise in imagination <sup id="fnref1:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>. Sound is ephemeral, difficult to track, and contains variants that text-based data does not. However, in order to engage in large scale analysis, some level of categorization is indeed required for a sonic data set. In the case of the data from the recorders, I went through a trial and error process in order to clean the data, beginning with Kaleidoscope, the software suite built by Wildlife Acoustics in order to visualize and classify the data from their Song Meter recorders. The software features a number of different functions, including a spectrogram to visualize data, a clustering mechanism to group like recordings, and a noise analysis tool. I began working with the data month by month, as my data was already organized in this schema because I ascended the rooftops monthly to download data and change the batteries on the recorders.</p>
<p>In order to train the data set, I first ran a month&rsquo;s worth of data through Kaleidoscope&rsquo;s unsupervised classifier, which allows the algorithm to group the recordings into unnamed clusters based on similarities in frequency and timing. This classification is conducted through Fast Fourier Transform, an algorithm that decomposes complex frequencies. With these base classifiers in hand, I then went through manually and &ldquo;tagged&rdquo; clips within my own named clusters, which included the following: brakes, bus hiss, backup beeps, music, voices, and sirens. I developed this final set of categories through trial and error, and then went through to only tag the best examples of these sounds. I then re-ran the classifier with the same data to only include the clips that I tagged. This second .csv file then becomes the training set. Finally, I ran a new set of data under a supervised classification, using the second .csv file that I named as the training set. This then produces a results page that clusters the data based on my training set, and is fairly accurate within a set distribution.</p>
<p>I trained the dataset at least three different times, working towards what I considered the best result for my research questions and within the ethical limitations I set for myself. One of the primary dilemmas during this process was the inclusion of voices as a tagged component. Surveillance is a fact of blackness, and keeping this in mind, this project has required careful consideration about the legacy of surveillance that weighs on black people across the diaspora <sup id="fnref:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup>. I was therefore determined to ignore potential identifiers in the data, and purposely &ldquo;cleaned&rdquo; in such a way that would obscure conversation rather than identify it. In fact, the only reason I classified voices at all was just to be able to train the classifier to be able to listen to go-go music, which contains a great deal of spoken vocals and rapping from lead talkers.<sup id="fnref:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup>  After solidifying the training set, I was then able to run all nine months of data from both recorders through Kaleidoscope. After further cleaning the final spreadsheet to only include the relevant information, I uploaded this spreadsheet into Sequel Pro, a database management application used for working with Structured Query Language (SQL) databases. Working with SQL and Indiana University&rsquo;s supercomputing support, I was then able to write queries for the larger dataset, getting back to those crucial questions about gentrification, race, and sound.</p>
<h2 id="analysis">Analysis</h2>
<p>Drawing on this extended cleaning process as well as other methods of sonic representation, I offer a tripartite analysis of the soundscape data here, focused at different levels of scope to engage a variety of aural insights in a gentrifying intersection. I begin with close listenings, because even amid the possibilities of large scale machine learning, there is still room for and urgency within both traditional and computationally informed close listenings. By computationally informed close listening, I am referring to a kind of deep engagement with and interpretation of a sound source that stems from mediation types other than listening from a standard playback device. In this case, I facilitate close listenings via spectrograms, which are visual representations of sound that mark frequency and amplitude over an x axis of time. Utilizing the Google Chrome Music Lab&rsquo;s Spectrogram tool, I include here a short spectrogram film from the intersection, featuring the sounds from the intersection as well as my own annotations (<a href="#figure01">Figure 1</a>).</p>
<p>In this video, a spectrogram moves left to right in time, with sounds identified by color and 3-D intensification based on their frequency and loudness in relation to the sounds around them. In this short clip, I have annotated the sounds that are most recognizable in the intersection: go-go music, the hiss and beep of a Metro bus, and a siren. These sounds offer a striking visual representation of the collection of sounds in the intersection.</p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://player.vimeo.com/video/508242911" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="Go-Go Music" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div>
<p>In addition to clips such as these, I also engage in more traditional ethnographic descriptions drawn from my time as a participant-observer in the intersection. Take, for example, an ethnographic sketch of an August evening in 2018:</p>
<blockquote>
<p>As I sit down at the bus stop, a driver is honking intensely as they go through the intersection, practically laying on their horn. There is also a black woman&rsquo;s voice rapping N.E.R.D.&rsquo;s &ldquo;Lemon&rdquo; from the speaker in front of Central Communications. A black man, who is actually in the intersection, is rapping along, but with different words. They&rsquo;re both rapping over a go-go pocket beat, and cars are driving by. A heavy engine idles at the intersection, then accelerates after the light turns green. As vehicles drive past, the pocket beat remains. In the distance, there is a siren blaring for a few moments, though it never passes directly through the intersection. The man in the intersection that was rapping along with the woman on the speaker has begun to rap on his own, because the speakers are currently on the breakdown, and there are no voices on the pocket now. He raps, &ldquo;Lock It, Lock it in the Pocket,&rdquo; on repeat. His voice is momentarily drowned out by another heavy engine, but upon it leaving the intersection he seems to have moved on to rapping, &ldquo;get money, get money.&rdquo; More engines, more brakes, more horns. The song changes, seemingly beginning a new set altogether. Now the song is a go-go cover of Frankie Beverly and Maze&rsquo;s &ldquo;Before I Let Go.&rdquo; The man that was rapping is now saying &ldquo;Hey baby, hey baby,&rdquo; but it&rsquo;s unclear who he&rsquo;s addressing. A metrobus stops, with its customary hiss-and-beep combination. A woman&rsquo;s disembodied voice comes from the bus, announcing that &ldquo;the base fare is two dollars.&rdquo; She sounds like Siri. While trucks idle at the traffic light, the music is muffled.</p>
</blockquote>
<p>This sketch is drawn from my time sitting one of the bus stops in the intersection, watching the passersby, and occasionally speaking with people. On this particular day, all of the people that lingered in the intersection, including myself, were black, and we existed as and within the boundaries of a porous space that people moved through to get to their next destination. For many of us, the intersection serves as a destination in and of itself, a place to come and exist outside in public space.</p>
<p>How can the combination of a narrative close listening and a spectrogram film that approaches the granular help us to articulate the sonorities of gentrification? While many studies and narratives of gentrification emphasize displacement and loss, these sonic materials assert the enduring presence of black life in the intersection. I posit here that all of these sounds, from the bus stop to the siren to the go-go music, are &ldquo;black sounds.&rdquo; The bus network is an important facet of black working life and labor in D.C., and sirens are closely connected to the same notions of policing and criminality that frame core conceptions of blackness <sup id="fnref:20"><a href="#fn:20" class="footnote-ref" role="doc-noteref">20</a></sup>. So as these neighborhoods become visually unrecognizable, their soundscapes remain discernable as a space in which black people exist. As gentrification threatens to erase all memory of a black D.C., the sonic footprint of a space on a granular level serves as an important timestamp and a refusal to be overlooked.</p>
<p>These materials are an exercise in amalgamation, where all of the elements of the intersection are displayed and contemplated together. As I have conducted this analysis parallel to the rise of the #dontmuteDC movement, I understand this amalgamation to be a contribution to the amplification of black sonic life in that space. I argue that this close listening emphasizes the immersion of black sound in the intersection. The sounds are not neatly stacked or delineated individually but rather all incorporated together, as they are heard in the intersection. Spectrograms and ethnographic sketches support the assertions of the #dontmuteDC movement, namely that black music, in turn black people, are a foundational part of 7th and Florida, and cannot be silenced or removed because someone deems them too loud, too noisy, or too much of a nuisance.</p>
<p>The final advantage to these close examinations of sound in the intersection lies in their combined elements. The spectrogram offers a visual narrative of sound that an ethnographic sketch alone cannot support in part because of the hierarchy of the senses which presumes visual evidence to be more true than sonic or narrative. And yet, the unreliability of my own narrative allows for the inclusion of more detailed information. For example, my description of the man&rsquo;s rapping is important here: his familiarity with the space and demonstration of go-go music&rsquo;s intertextuality is emblematic of the complex layers of sound in the intersection. But, I choose not to include the audio of that recording because it serves as a potential identifier, and my emphasis on black digital humanities does not allow me to intentionally surveil or put a black person in harm&rsquo;s way. These two modes of listening, then, together move us closer toward a sonic interpretation of gentrifying space and a more equitable soundscape the rejects the violence of surveillance as evidence.</p>
<p>My next level of analysis zooms out to focus on particular days rather than minutes, represented by a 24 hour timeline built with Microsoft&rsquo;s Timeline Storyteller. Several days are important to the life of this intersection, from major holidays to the first day of each month, which is a popular day for many to go to Central Communications to pay their phone bills. Here I offer a 24-hour look at July 4th, 2018, which features commentary about the various sounds heard on that day, from sirens to fireworks, which are common within the neighborhood as well as on the National Mall (See <a href="#figure02">Figure 2</a>).</p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://player.vimeo.com/video/508243017" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="4th of July at Seventh and Florida" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div>
<p>In this timeline, hours of the day are represented by rectangular blocks that move clockwise beginning at 12 midnight. The beginning of the day, then, visually, is actually late in the night. These hour blocks are delineated in larger groups of three hours that mark time around a standard 24-hour day. In this representation, the only space continuously filled is that of the beeping crosswalk. While crosswalks are known to beep for accessibility purposes, this one in particular sounds throughout the day without prompting, inserting itself as the most consistent sound in the recording process. The consistent beeping of the intersection offers up a glimpse into a broader conversation about sound, gentrification, and accessibility, as the intersection serves a large population of disabled people. Go-go music, as per usual, begins at 10am when Central Communications opens, ending at 6pm which is earlier than its usual 8pm close but reasonable because of the holiday. Car horns are present throughout the day, as 7th and Florida is a busy vehicular and pedestrian intersection in the city. I offer &ldquo;music&rdquo; in the representation here distinct from &ldquo;go-go music&rdquo; and &ldquo;club bass&rdquo; to encompass the myriad of other types of musical expression that permeate the intersection, from people playing music on their phones as they pass through or the radio that spills out of cars that idle at red lights. For the remainder of the evening and into the night, sirens and fireworks crisscross each other in the timeline, giving the impression of a loud and busy night. Club bass is an interesting swatch in the timeline, present briefly in the late afternoon before asserting a more sustained presence from 12am-3am. Bass is more visible here than audible in actual recordings, because of the low frequencies spilling out of the club. Bass is felt, rather than heard, in most instances, and visual representation of such a consistent frequency is helpful here because many noise complaints (whether overtly racialized or not) begin and end with bass being too loud.</p>
<p>I chose the 4th of July as a site for analysis because of the frictions between federal and local life in Washington, D.C. The 4th of July is perhaps the city&rsquo;s most popular holiday, where tourists flock in to visit national museums by day and enjoy fireworks by night. However, amid national festivities lie a city that is characterized by its lack of statehood, a drastic wealth gap, and a vibrant local cultural life. This timeline provides sonic entry into these tensions, noting how the neighborhood shifts because of the holiday, but also maintains its own character and traditions, in this case the go-go music at Central Communications.</p>
<p>This circle timeline offers a grammar of audibility in the intersection through the unit of a day, a way of hearing the space that acknowledges patterns and routines but does not obscure or leave behind the sonic messiness of the day. Although gentrification is often considered primarily in longitudinal capacities, the unit of the everyday remains an incredibly useful metric of analysis. As de Certeau argued in &ldquo;Walking in the City,&rdquo; the city is made by those that walk every day, made not from looking down from the empire state building and mapping the urban as city blocks, but instead in the compilation of those steps <sup id="fnref:21"><a href="#fn:21" class="footnote-ref" role="doc-noteref">21</a></sup>. There is a hubris in the longitudinal that I use the day to avoid and to disrupt. As Amira Baraka wrote about tradition, this is the changing same <sup id="fnref:22"><a href="#fn:22" class="footnote-ref" role="doc-noteref">22</a></sup>. Much of the gentrification story is not about what changes, but what stays the same. The crosswalk beeps, the music plays, the sirens wail.</p>
<p>My final level of analysis, which is most dependent on that initial data cleaning process, scales back even further to think about gentrification across a series of months in the intersection. In this section, I consider data over the course of the months long period in which the recorders were installed. Gentrification is, on the surface, a problem of time. If I had wanted to hear gentrification over time in a purely longitudinal way, I would have needed to install the recorders in 2008 or 1998 instead of 2018. But this is a case, perhaps even a call, to lean in to the limitations of a dataset. What can nine months tell us about an intersection? What can this interval of time give us about a space? Does this period function only in anticipation of a longer, future dataset, or is there something to learn in listening to these months as incidents themselves?</p>
<p>I draw on sirens here as an example, which operate as both icon and index of a broader urban imaginary <sup id="fnref:23"><a href="#fn:23" class="footnote-ref" role="doc-noteref">23</a></sup>. Sirens, depending on their source, can signal crime, emergencies, crises, danger, impending violence, even terror <sup id="fnref:24"><a href="#fn:24" class="footnote-ref" role="doc-noteref">24</a></sup>. Both interviews and informal conversations led me to understand sirens as a powerful indicator of a neighborhoods crime status, with some of my conversation partners sharing that they heard less sirens as the neighborhood around 7th and Florida gentrified. Furthermore, this decrease in sirens was often interpreted as a decrease in criminal activity. I was therefore interested in tracking the presence of sirens across time at 7th and Florida, and offer here a brief representation of that pursuit (See <a href="#figure03">Figure 3</a>).</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000513/resources/images/figure03.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000513/resources/images/figure03_hu23a542d3dc9335f3fc954dfe670dd5be_339842_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000513/resources/images/figure03_hu23a542d3dc9335f3fc954dfe670dd5be_339842_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000513/resources/images/figure03_hu23a542d3dc9335f3fc954dfe670dd5be_339842_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000513/resources/images/figure03_hu23a542d3dc9335f3fc954dfe670dd5be_339842_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/1/000513/resources/images/figure03_hu23a542d3dc9335f3fc954dfe670dd5be_339842_1800x0_resize_box_3.png 1800w,/dhqwords/vol/15/1/000513/resources/images/figure03.png 2740w" 
     class="landscape"
     ><figcaption>
        <p>Siren Counts at Seventh and Florida
        </p>
    </figcaption>
</figure>
<p>This simple line chart shows sirens from July to October of 2018, and displays the frequency of detected sirens every day.<sup id="fnref:25"><a href="#fn:25" class="footnote-ref" role="doc-noteref">25</a></sup>  A number of insights arise from the chart. The peaks and valleys here are often indicative of the number of people active in the space on any given day. For example, most peaks, where the recorder detected 20+ sirens, were on weekends, typically Fridays and Saturdays. Days with the least number of sirens often corresponded with difficult weather patterns, from heavy rain to thunderstorms. Looking across the longer four month period, from July to October, there is a noticeable decrease in sirens as the season turns from summer to fall. This decrease is consistent with two narratives of a gentrifying city: The first is that crime levels peak during summer months because summers are hot, in terms of both heat and criminal activity. The second narrative is that crime decreases over time in gentrifying neighborhoods, as the crime rate in D.C. has attested to. My intervention lies in the disruption of these narratives, however true they may be. Gentrification is not a linear, gradual process of displacement. Gentrification is a series of jagged interruptions, community meetings, demolitions, protests, contracts, construction projects, and violence.</p>
<p>In carrying connotations of crime and loudness, sirens exist within a nexus of aural indicators of race, criminality, and urban life. Sirens are so closely molded to black urban life because they are indicative and representative of those forces that have deemed themselves in control of black life, particularly the police. The chart here details the pattern of sirens in the intersection, offering a visual experience of what it is to exist in a space in which blackness has been marked as existent and available. I read this pattern as a demonstration of the cycle of potential sonic traumas that come through the intersection every day, sometimes dozens of times a day: of policing, terror, emergency, and health crises. This chart disrupts a visual narrative of gentrification, in which neighborhoods become better and more beautiful looking because it repositions gentrification as a cyclical process of sonic violences, an ebb and flow of sirens.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Taken together, these points of analysis start a conversation about how we might engage in computational analysis and data visualization to bear witness to the sonorities of black life, in both stifling and amplification. Bearing witness alone, however, is not enough. It is the role of the black digital humanist to develop, engage, and experiment with tools and methodologies that are in the service of black life. Here I have utilized spectrogram films to emphasize the ubiquity of black sound, considered how we might imagine the sonic life of a day in a particular gentrifying space, and used long-term analysis to consider gentrifying space as a series of potential sonic traumas. The intention of this work is to arrive at an aurality that disrupts and outright rejects the everyday violences imposed onto black people.</p>
<p>This work has many applications and futures. In legislative arenas, lawmakers across the country decide acceptable decibel levels for city musicians and gatherings. Understanding the connections between sonic datasets and the racialization of sound could lead to more humanely written policies that do not foreground punitive measures for perceived sonic offenses. In addition to policy work, this work also calls for black DH to be recognized as a leader in the building of an anti-racist future. Following the lead of the Data for Back Lives Collective, I believe that research in big data should emphasize approaches to do no (additional) harm. Finally, this work needs to be of use for members of communities that are being silenced, be it through rigorous research that deconstructs the seemingly infallible connection between blackness, loudness, and criminality or through infrastructure and support for utilizing soundscape analysis to bolster the sonic health of a neighborhood.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Snethen, W.G.  <em>The Black Code of the District of Columbia.</em>  Published for the A&amp;F Anti-Slavery Society, New York (1848).&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Gaunt, K. D.  “The Disclosure, Disconnect, and Digital Sexploitation of Tween Girls&rsquo; Aspirational YouTube Videos.”  <em>Journal of Black Sexuality and Relationships</em>  5.1 (2018): 91-132.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Clement, T.  “Distant Listening or Playing Visualisations Pleasantly with the Eyes and Ears.”  <em>DigitalStudies/leChampNumérique</em>  3.2 (2013)&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Clement, T.  “word. spoken. Articulating the Voice for High Performance Sound Technologies for Access and Scholarship (HiPSTAS).” In M.C. Lingold, D. Mueller, and Trettien, W. (eds)  <em>Digital Sound Studies</em> . Duke University Press, Durham (2018).&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>Webb, S., Kiefer, C., Jackson, B., Baker, J. and Eldridge, A.  “Mining Oral History Collections Using Music Information Retrieval Methods.”     <em>Music Reference Services Quarterly</em> ,   <em>20</em> .3-4(2017):168-183.&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>Gallon, K.  “Making a Case for the Black Digital Humanities.” in M. K. Gold and L. F. Klein (eds)  <em>Debates in Digital Humanities</em>  (2016).&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>Parham, M.  “You Can&rsquo;t Flow Over This&quot;: Ursula Rucker&rsquo;s Acoustic Illusion.” In S. Mieszkowski, J. Smith, and M. de Valck (eds),  <em>Sonic Interventions</em>  (2007):87-100.&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>Wang, T.  “Big Data Needs Thick Data.”  <em>Ethnography Matters</em>  13 (2013).&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>McDougal, S.  <em>Research Methods in Africana Studies</em> . Peter Lang, New York (2014).&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>Wong, D.  “Moving: From Performance to Performative Ethnography and Back Again.” In G. Barz and T. Cooley (eds),  <em>Shadows in the Field: New Perspectives for Fieldwork in Ethnomusicology</em>  (2008): 76-89.&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>Burnim, M.  “Culture Bearer and Tradition Bearer: An Ethnomusicologist&rsquo;s Research on Gospel Music.”  <em>Ethnomusicology</em>  29.3 (1985): 432-447.&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p>Pijanowski, B.C., Villanueva-Rivera, L.J., Dumyahn, S.L., Farina, A., Krause, B.L., Napoletano, B.M., Gage, S.H. and Pieretti, N., 2011.  “Soundscape Ecology: The Science of Sound in the Landscape.”     <em>BioScience</em> , 61.3 (2011):203-216.&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13">
<p>In my initial curiosity about the potential of these recorders for listening to gentrification, I called Wildlife Acoustics and asked a general question what the recorders would be able to hear. The man I spoke to then asked me what I was trying to hear, and my response of &ldquo;gentrification&rdquo; audibly threw him off. He did admit, though, that they can pick up &ldquo;everything.&rdquo;&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14">
<p>Wildlife Acoustics.  “Song Meter SM4 Acoustic Recorder.”   <a href="https://www.wildlifeacoustics.com/products/song-meter-sm4">https://www.wildlifeacoustics.com/products/song-meter-sm4</a>.&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:15">
<p>Smith, M.  “Most Dangerous D.C. Intersections of 2018.”  <em>WTOP</em>  (2019).&#160;<a href="#fnref:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:16">
<p>Rawson, K., and Muñoz, T.  “Against Cleaning.”  <em>Curating Menus</em>  6 (2016).&#160;<a href="#fnref:16" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:17">
<p>Kapchan, D.  “The Splash of Icarus: Theorizing Sound Writing/Writing Sound Theory.” In D. Kapchan (ed)  <em>Theorizing Sound Writing</em> . Wesleyan University Press, Middletown (2017): 1-22.&#160;<a href="#fnref:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:18">
<p>Browne, S.  <em>Dark Matters: On the Surveillance of Blacknes</em> s. Duke University Press, Durham. (2015).&#160;<a href="#fnref:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:19">
<p>Lead talkers of go-go bands serve as bandleaders as well as the principle connection between band and audience. They talk to people in the front, give shout outs, call out neighborhoods, etc.&#160;<a href="#fnref:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:20">
<p>Summers, B. T.  <em>Black in Place: The Spatial Aesthetics of Race in a Post-Chocolate City</em> . University of North Carolina Press, Chapel Hill (2019).&#160;<a href="#fnref:20" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:21">
<p>Certeau, M.  <em>The Practice of Everyday Life</em> . University of California Press, Berkeley (1984).&#160;<a href="#fnref:21" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:22">
<p>Baraka, A.  <em>The LeRoi Jones/Amiri Baraka Reader.</em>  Thunder&rsquo;s Mouth Press, New York (1991).&#160;<a href="#fnref:22" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:23">
<p>Jones, Alisha Lola.  “Stereotypical Images in Film and Media.”  <em>Global Popular Music FOLK-151, Indiana University</em>  (2017).&#160;<a href="#fnref:23" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:24">
<p>Stone, R.  “&lsquo;Ebola in Town&rsquo;: Creating Musical Connections in Liberian Communities during the 2014 Crisis in West Africa.”  <em>Africa Today</em>  63.3(2017):79-97.&#160;<a href="#fnref:24" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:25">
<p>Because the recorders only recorded one minute out of every five, it&rsquo;s important to note that this is not every siren.&#160;<a href="#fnref:25" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry><entry><title type="html">Introduction: Special Issue on AudioVisual Data in DH</title><link href="https://rlskoeser.github.io/dhqwords/vol/15/1/000541/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://rlskoeser.github.io/dhqwords/vol/15/1/000541/</id><author><name>Taylor Arnold</name></author><author><name>Stefania Scagliola</name></author><author><name>Lauren Tilton</name></author><author><name>Jasmijn Van Gorp</name></author><published>2021-03-05T00:00:00+00:00</published><updated>2021-03-05T00:00:00+00:00</updated><content type="html"><![CDATA[



























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000541/resources/images/figure01.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000541/resources/images/figure01_hu3d9904c8e9eaa40b88c7869ae7fe80aa_1026015_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000541/resources/images/figure01_hu3d9904c8e9eaa40b88c7869ae7fe80aa_1026015_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000541/resources/images/figure01.png 800w" 
     class="portrait"
     ><figcaption>
        <p>A mosaic of article figures included in the special issue.
        </p>
    </figcaption>
</figure>
<h2 id="towards-av-dh">Towards AV DH</h2>
<p>Many scholars have repeatedly demonstrated how expanding our areas of inquiry builds new routes for the field <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>  <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. Yet, often those interested in working with AV data have found themselves swimming upstream. Text and word culture have enjoyed a dominant position in DH, bolstered by factors such as the prominence of text analysis and the form of academic journals <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>  <sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>.<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>  Replicating the larger structures of higher education, racialized and gendered beliefs about what counts as &ldquo;rigorous&rdquo; scholarship that marginalize fields such as cultural studies and visual culture studies also permeate DH and further explain why text (analysis) has enjoyed a privileged position along with their related academic fields <sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>.<sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>  However, exciting developments are continuing to support and amplify the work of AV data in DH.</p>
<p>One of those developments is shifts in technology. The ability for computers to create, &ldquo;read&rdquo;, and store AV data followed by advances in areas such as machine learning have augmented computational image and sound analysis. Pioneering approaches such as cinemetrics that once relied on hand coding and text-based annotations can now be automated. Within DH, this has led to new theories and methods such as cultural analytics, distant listening, and distant viewing <sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>  <sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup>  <sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup>. These developments have led scholars such as Melvin Wevers and Thomas Smits <sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup> to argue that DH is seeing a &ldquo;visual, digital turn&rdquo;. Meanwhile, Mary Caton Lingold, Darren Mueller and Whitney Trettien <sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>, in the edited volume  <em>Digital Sounds Studies</em> , demonstrate how pairing digital tools with &ldquo;interpretive practices that always attend to the human&rdquo; forges new paths for sound studies and DH.</p>
<p>Another critical development is digital access to AV materials. As DH scholars increasingly think of their sources as data, they have benefited from large-scale digitization of audiovisual collections.<sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup>  At the forefront over the last three decades have been governmental and GLAM (gallery, libraries, archives, museums) institutions around the world to digitize collections, whose initiatives have often been spurred by deteriorating physical collections.<sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup>  With millions of items digitized, platforms were funded by cultural, government, and scientific organisations for providing access to audiovisual heritage collections alongside the emergence of platforms by for-profit multinational corporations, all of which enabled the circulation of digitized and digital-born materials online.<sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup>  While issues such as copyright and funding still loom large, DH scholars have greatly benefited from the significant investment in digitization over the last 30 years.</p>
<p>Finally, we turn to institutional developments.<sup id="fnref:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup>  Along with conferences specifically dedicated to DH and media [e.g. <a href="https://transformationsconference.net">Transformations Conference</a>], new journals have developed such as the  <em>International Journal of Digital Art History</em>  and  <em>Journal of Cultural Analytics</em>  featuring scholarship at the intersection of AV research and DH.<sup id="fnref:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup>  In order to further amplify AV work in DH, colleagues worked to develop a Special Interest Group (SIG) within the Alliance of Digital Humanities Organizations (ADHO). The AudioVisual in DH (AVinDH) SIG was proposed after a successful workshop on how to integrate the Audiovisual in the Digital Humanities in Lausanne at DH2014, and formalized a year later at DH2015 in Melbourne. One result of the SIG&rsquo;s work is this special issue.<sup id="fnref:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup></p>
<p>New pathways are bringing about exciting opportunities in DH as exemplified by the articles in this issue. They model how AV research can be the subject of analysis (e.g. film) or result of analysis (e.g. podcast). They highlight less visible humanities disciplines in the DH, model the importance of interdisciplinary collaboration across institutional structures, demonstrate how cutting edge scholarship comes from a plethora of positions, and offer new questions that the field is only beginning to grapple with. The contributors&rsquo; model paths for constructing entry points, building bridges, or adding intersections for engaging with audiovisual in the digital humanities. Amplifying the work of scholars with a range of disciplinary, institutional, and political commitments, the special issue constructs a more capacious configuration of DH.</p>
<h2 id="contributions">Contributions</h2>
<p>The special issue is organized into five sections. The first focuses on annotation of AV material as method and theory. The second explores analyzing (meta)data, which often includes annotation, to build and analyze AV corpora. The third focuses on creative and liberatory ways to remix AV (meta)data as a way to innovate pedagogically and methodologically while furthering discipline specific interventions. The fourth dives further into computational methods, particularly machine learning, in turn demonstrating how DH is reconfiguring these methods and ways of knowing to answer humanities questions. The special issue ends with a focus on how AV forms such as podcasts and film can also be the form of scholarly knowledge in the field, highlighting how form and argument can be mutually constituted.</p>
<p>Next, we turn to the contributions that comprise each section. The first explores annotation as a powerful way to add context and analyze AV data. A particularly prominent area of such work has been in film studies. Therefore, the first three articles offer a snapshot of different approaches and tools for film annotation. <a href="/dhqwords/vol/15/1/000515/">Cooper, Nascimento and Francis</a> present their KinoLab and discuss the opportunities and challenges of Omeka for narrative film language analysis, including the challenges related to copyrights. They argue for a universally accepted data model for film analysis. We then turn to a new annotation platform called  <em>Mediate</em>  built by a team at the University of Rochester. <a href="/dhqwords/vol/15/1/000507/">Burges et al.</a> discuss how they use the annotation tool in the classroom in three different disciplines - film and media studies, music history, and linguistics - and introduce the concept of &ldquo;audiovisualities&rdquo; as a theoretical frame for understanding remediation through annotation. Next, <a href="/dhqwords/vol/15/1/000524/">Williams and Bell</a> discuss how the Media Ecology Project is conceived as a virtuous cycle and incubator working to increase access and discovery of moving images, with a particular focus on tools such as the Semantic Annotation Tool. Zooming out, <a href="/dhqwords/vol/15/1/000512/">Clement and Fisher</a> theorize a new approach to annotation. They bring together sound and literary studies to introduce the concept of &ldquo;audiation&rdquo; as a framework for audiated annotations that increase access and discovery.</p>
<p>The next section focuses on how (meta)data can open up analytical possibilities. Using metadata to reunite AV materials, <a href="/dhqwords/vol/15/1/000509/">Sapienza et al.</a> describe the process of reuniting radio and text files virtually that belong together but ended up at different institutions. After discussing why audiovisual collections in general are heavily under described, they describe how virtual reunification and integrated access was realized through the use of linked data, minimal computing, and synced transcripts. Next, <a href="/dhqwords/vol/15/1/000519/">Hoyt et al.</a> discuss the analytical possibilities afforded by metadata. Focused on podcasting, they discuss three different methods for studying RSS feeds and podcast metadata, and point at the specificities of methods for born-digital media vis-à-vis digitized media. <a href="/dhqwords/vol/15/1/000523/">Carrivé et al.</a> then focus on the first development phase of their ANTRACT project for the transdisciplinary content analysis of 1262 newsreels containing more than 20,000 French news reports. They discuss how they dealt with the project&rsquo;s main technological challenge to process data and build tools to familiarize historians with the automated research of large audiovisual corpora in order to then use the data to pursue inquiry about Les Actualités Françaises news reports. Finally, <a href="/dhqwords/vol/15/1/000504/">Gienapp et al.</a> show what can be done when data is brought together from different sources to analyze music collaboration. They demonstrate how network analysis can reveal the contours of collaboration among musicians.</p>
<p>In the third section, the authors creatively (re)mix AV data with a focus on audio data. Using audio data, <a href="/dhqwords/vol/15/1/000516/">Tyechia and Carrera</a> demonstrate how centering Afrofuturism in DH pedagogy through mixtapes can not only realize the goals of an undergraduate composition course but realize a liberatory DH praxis. <a href="/dhqwords/vol/15/1/000505/">Bonnett et al.</a> combine data art, landscape art and augmented reality in their DataScapes Project. Departing from the premise that data can be translated into visual and sonic forms, they use protein data and texts from the bible, turn them into sequences, and translate these into visualisations and compositions. Kramer then asks what if we listened to images. Building off of previous work on &ldquo;image sonification&rdquo;, he argues that transforming the visual into audio opens up new ways of seeing and hearing the past. Next, <a href="/dhqwords/vol/15/1/000522/">Have and Enevoldsen</a> demonstrate how toggling from close to distant listening offers insights about the longue durée of Danish radio content by scrutinizing what is audible with the human ear and searching for patterns using AI. Next, we turn to work that makes field specific interventions. <a href="/dhqwords/vol/15/1/000513/">Martin</a> constructs a new path for listening to gentrification in Washington DC by combining ethnography, passive acoustic recording, and computational sound studies. The work also demonstrates how centering Black DH offers new ways to understand the relationship between embodied and computational audio analysis in DH, in turn forging new liberatory possibilities for the field.</p>
<p>The next section continues with the application and reconfiguration of computational techniques, particularly machine learning, to conduct data analysis on large collections of AV data. Looking at a large collection of artwork showcasing musical instruments, <a href="/dhqwords/vol/15/1/000517/">Sabatelli et al.</a> introduce the usage of computer vision techniques to automatically locate musical instruments in images. They investigate the algorithmic properties of their analysis and show how it leads to innovative scholarship in music iconography. A paper by <a href="/dhqwords/vol/15/1/000520/">Lupker and Turkel</a> illustrates the potential of investigating novel intersections between research in the humanities by using musical theory to guide the training and usage of machine-learning algorithms applied to a large corpus of digitized music. A born-digital collection of K-pop dance videos hosted on YouTube is analyzed using state-of-the-art computer vision techniques in a paper by <a href="/dhqwords/vol/15/1/000506/">Broadwell and Tangherlini</a>. Their article develops a typography for describing and analyzing poses and choreography to facilitate the data-driven analysis of time-based media. <a href="/dhqwords/vol/15/1/000511/">Oyallon-Koloski et al.</a> present a different approach to the study of movement in space by showing how motion capture technology can be used within film, dance, and movement studies. As with the Lupker and Turkel article, Oyallon-Koloski et al. illustrate the novel integration of theoretical frames from the humanities – in their case, Laban Movement Analysis and Bartenieff Fundamentals and Movement Studies – and the usage of computational techniques. In another take on the detection of bodies moving in space, <a href="/dhqwords/vol/15/1/000510/">Fragkiadakis et al.</a> introduce an automatic system and taxonomy for tagging and describing digital videos of sign-language usage. Together, these articles illustrate the potential for work in AV DH to infuse machine learning with analytical commitments from the humanities.</p>
<p>Finally, we turn to articles that use DH to push the boundaries of form for scholarly knowledge. In order to demonstrate how the podcast format expands our definitions of text, <a href="/dhqwords/vol/15/1/000527/">Edwards and Hershkowitz</a> reveal how podcasts can realize intersectional feminist approaches to DH. Along with demonstrating and discussing the creation of the Books Aren&rsquo;t Dead (BAD) podcast in the article, they discuss the process in a podcast for this special issue. <a href="/dhqwords/vol/15/1/000514/">Kim</a> then explores how motion caption and virtual reality can be used to record and visualize movement histories as a form of cultural heritage preservation. Through these forms, one can then use visual storytelling, she argues, to demonstrate how movement, dance, and ritual cannot be separated from a person&rsquo;s personal narration of the experience. Finally, <a href="/dhqwords/vol/15/1/000521/">Mittel&rsquo;s</a> contribution showcases twenty audiovisual deformations of the classic musical &ldquo;Singin in the Rain&rdquo; in still image, GIF, and video formats. The essay considers both what each new deformation reveals about the film and the way we engage with the by algorithmic practices derived object as a product of the &ldquo;deformed humanities.&rdquo;</p>
<p>The invitation of the authors in this special issue to think about the relationship between form and argument is one we also embraced. The publication needs, some of which weren&rsquo;t possible, of our special issue pushed the boundaries of the form and format of DHQ as a journal that is catered for linear reading of articles as written texts in XML. As a result, the articles in this special issue include 5 sound files, 52 embedded videos, and 176 gifs and images. These AV components are key elements of the authors&rsquo; argumentation. The special issue attempts to more closely mirror how scholars of AV materials in DH actually produce and create scholarship. In this context we take as a guide other pioneering initiatives in this field such as <a href="https://scalar.me/anvc/about/">Scalar</a>, <a href="https://viewjournal.eu/">VIEW journal</a>, <a href="https://vimeo.com/groups/audiovisualcy/">Audiovisualcy</a>, and <a href="http://mediacommons.org/intransition/">[in]transition</a> that question the relation between the affordances of a publication platform and the interactivity and multimodality of scholarship that increasingly embraces creative modes of production. We hope that these incremental steps within DHQ can forge exciting new possibilities for the field.</p>
<p>Finally, our decision to partner together to co-edit was driven by features of AV work in DH. First, our own areas of expertise – statistics and digital images, history and audio, american studies and photography, and media studies, television &amp; film – reflect a range of audio and visual scholarship that animates DH. Second, the inclusion of a colleague housed in a Math &amp; Computer Science department, Tayor Arnold, demonstrates how digital humanities scholarship often requires working with and giving proper credit to experts trained in computational fields. Third, we wanted to build collaborations across geopolitical boundaries and languages that might help us think critically and beyond the particular configurations of DH that shape our local, regional, or national context. We recognize that our positionalities as White able-bodied scholars living in the global west and north also brings limits. As a part of our efforts, we paid special attention to circulating the CFP beyond our immediate DH circles with particular attention to reaching beyond the US and Western Europe. However, there is still more work to do. Yet, we do hope that the issue in aggregate reveals how thinking across disciplinary, cultural, and spatial boundaries enables a more capacious configuration of the field than currently articulated.</p>
<h2 id="conclusion">Conclusion</h2>
<p>As we look toward the future, we are enthused about the possibilities and realistic about the challenges. Along with the work featured in this special issue, areas such as 3D, AR/VR, and game studies are forging exciting paths. As disciplines (albeit slowly) adopt more capacious guidelines for what counts, forms of scholarship such as films, multimodal digital projects, podcasts, and software are receiving well overdue credit. Because of the teamwork and expertise often required to access and work with AV data, this area of DH also pushes us to work across ossified divisions such as the &ldquo;Humanities&rdquo; and &ldquo;Sciences&rdquo;, &ldquo;faculty&rdquo; and &ldquo;staff&rdquo;, and &ldquo;university&rdquo; and &ldquo;cultural institution&rdquo; in ways that can help us realize a more collaborative, equitable, and generous configuration of the field.<sup id="fnref:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup></p>
<p>At the same time, challenges remain. There are major obstacles to working with AV. For example, digitized images have significantly larger file sizes than textual data making them hard to transfer and process even in light of recent technological advances <sup id="fnref:20"><a href="#fn:20" class="footnote-ref" role="doc-noteref">20</a></sup>. This makes computational analysis of large collections of digitized visual materials difficult for institutions that do not have access to extensive computational resources. Audiovisual materials are also often subject to varying degrees of copyright and access restrictions, dictated often by large multimedia producers <sup id="fnref:21"><a href="#fn:21" class="footnote-ref" role="doc-noteref">21</a></sup>. This makes it relatively difficult to work with certain collections, such as television news programs and feature films, and risks limiting the kinds of work with which digital humanists can work. Even when we do have access, the scale of AV data is growing rapidly, particularly given the rise of born digital AV content, and with this comes implications for how and who is positioned to analyze these materials. Existing audiovisual archives are heavily skewed towards European- and U.S.-centric collections. As we work through these challenges and opportunities, we need to continue to listen and engage with the cautions and critiques about computation and algorithms from scholars such as dana boyd and Kate Crawford <sup id="fnref:22"><a href="#fn:22" class="footnote-ref" role="doc-noteref">22</a></sup>, Ruha Benjamin <sup id="fnref:23"><a href="#fn:23" class="footnote-ref" role="doc-noteref">23</a></sup>, Jessica Marie Johnson <sup id="fnref:24"><a href="#fn:24" class="footnote-ref" role="doc-noteref">24</a></sup>, Catherine d&rsquo;Ignazio and Lauren Klein <sup id="fnref:25"><a href="#fn:25" class="footnote-ref" role="doc-noteref">25</a></sup>, and Safiya U. Noble <sup id="fnref:26"><a href="#fn:26" class="footnote-ref" role="doc-noteref">26</a></sup>.</p>
<p>Finally, we want to thank the contributors, reviewers, and DHQ, specifically Managing Editor Cassandra Cloutier, for their work. Even under what were once &ldquo;normal&rdquo; conditions, writing an essay for publication is demanding. The challenges quickly mounted as authors revised amidst a global pandemic that disrupted everyone&rsquo;s daily lives and affected communities unequally due to structural inequalities. As authors and our colleagues at DHQ tried to balance caregiving, jobs, and their own health, among other duties, they still carved out time to make this issue possible. This is quite an achievement, and for which we are grateful. Finally, we want to leave with an invitation. We encourage readers interested in continuing to further engage with AVinDH to <a href="https://avindhsig.wordpress.com/">join the SIG</a>. We look forward to all that lies ahead.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>McPherson, T.  “Introduction: Media Studies and the Digital Humanities”  <em>Cinema Journal</em>  48, 2: pp. 119-123 (2009).&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Manovich, L.  <em>The Language of New Media</em> , Cambridge MA: MIT Press (2002).&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Svensson P.  “Humanities computing as digital humanities”    <em>Digital Humanities Quarterly</em> , 3, 3 (2009).&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Sula, S.A. and Hill, H.V.  “The early history of digital humanities: An analysis of Computers and the Humanities (1966–2004) and Literary and Linguistic Computing (1986–2004)”    <em>Digital Scholarship in the Humanities</em> , 34, 1: pp. i190–i206 (2019), <a href="https://doi.org/10.1093/llc/fqz072">https://doi.org/10.1093/llc/fqz072</a>.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>For data about the focus on text analysis and increased presence of AV, see <sup id="fnref:27"><a href="#fn:27" class="footnote-ref" role="doc-noteref">27</a></sup> as well as Weingart&rsquo;s <a href="http://scottbot.net/tag/dhconf/">blog posts</a> about the ADHO conference. For a list of analysis and data about DH conferences and journals from DH scholars, also see Weingart&rsquo;s list on his <a href="http://scottbot.net/dh-quantified/">blog</a>&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>Losh, E., and Wernimont, J. (Eds.)  <em>Bodies of Information: Intersectional Feminism and the Digital Humanities</em> , Minneapolis, London: University of Minnesota Press (2018).&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>For example, efforts to frame computational text analysis as more rigorous and hard in part based on claims to the use of scientific methods has asserted problematic hierarchies of knowledge production in the field. It is not a coincidence that the field has been slow to acknowledge and engage with the work of humanities fields that have been feminized and racialized. Scholars from these fields have long proven how ways of knowing - such as affective, aural, embodied, and performative, and visual - may actually be the only way to recover the pasts that constitute (and often haunt) our present and through which we can imagine new futures. The work of scholars such as Kim Gallon <sup id="fnref:28"><a href="#fn:28" class="footnote-ref" role="doc-noteref">28</a></sup>, Safija U. Noble <sup id="fnref:29"><a href="#fn:29" class="footnote-ref" role="doc-noteref">29</a></sup>, Amanda Phillips <sup id="fnref:30"><a href="#fn:30" class="footnote-ref" role="doc-noteref">30</a></sup>, Roopika Risam <sup id="fnref:31"><a href="#fn:31" class="footnote-ref" role="doc-noteref">31</a></sup>, and Puthiya Purayil Sneha <sup id="fnref:32"><a href="#fn:32" class="footnote-ref" role="doc-noteref">32</a></sup> to center BlackDH, postcolonial DH, and #transformDH has made important space to think about other forms of knowledge.&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>Manovich, L.  <em>Cultural Analytics</em> , Cambridge MA: MIT Press (2020).&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>Clement, T.  “Distant listening or playing visualisations pleasantly with the eyes and ears”    <em>Digital Studies/Le champ numérique</em> , 3, 2 (2013).&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>Arnold, T. and Tilton, L.  “Distant Viewing: Analysing large visual corpora”    <em>Digital Scholarship in the Humanities</em> , 34, 1 (2019): pp. i3-i16.&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>Wevers, M. and Smits, T.  “The visual digital turn: Using neural networks to study historical images”    <em>Digital Scholarship in the Humanities</em> , 35, 1: pp. 194–207 (2020).&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p>Lingold, M. C., Mueller, D. and Trettien, W. (eds),  <em>Digital Sound Studies. Durham</em> , London: Duke University Press (2018).&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13">
<p>The transformation is quite impressive if we look back to Miriam&rsquo;s Posner&rsquo;s article on the term &ldquo;humanities data&rdquo; <sup id="fnref:33"><a href="#fn:33" class="footnote-ref" role="doc-noteref">33</a></sup>. Posner notes that this is an unusual term for humanities scholars. Fast forward five year and it is increasingly common to hear materials such as books, films, and photo called data.&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14">
<p>We give some examples of digitization initiatives from around the globe for further reading. A number of initiatives across Asia and the Pacific are mentioned in a report about digitization of Indian cultural heritage at the  <em>Indira Gandhi National Centre for the Art</em>   <sup id="fnref:34"><a href="#fn:34" class="footnote-ref" role="doc-noteref">34</a></sup>, a centre which itself has digitized a considerable proportion of its heritage resources, and made them available through its website. Other initiatives mentioned in the report are a.o. <sup id="fnref:35"><a href="#fn:35" class="footnote-ref" role="doc-noteref">35</a></sup>, <sup id="fnref:36"><a href="#fn:36" class="footnote-ref" role="doc-noteref">36</a></sup>, and <sup id="fnref:37"><a href="#fn:37" class="footnote-ref" role="doc-noteref">37</a></sup>. When considering Africa, a special issue on the impact of digitization and new media on various African societies in the  <em>Journal of Eastern African Studies</em>  sheds light on how in this part of the world the digital transformation primarily manifests itself by the uptake of social media <sup id="fnref:38"><a href="#fn:38" class="footnote-ref" role="doc-noteref">38</a></sup>. Classical ancient works are digitized in Egypt, at the remake of the ancient Library of Alexandria, the  <em>Bibliotheca Alexandrina</em>  that can be found in present day Alexandria. In a report published by the Unesco on digital culture in Spanish speaking countries <sup id="fnref:39"><a href="#fn:39" class="footnote-ref" role="doc-noteref">39</a></sup>, the core initiatives are investments in infrastructure to decrease the digital divide within the countries. In the US, the Federal Agencies Digital Guidelines Initiative FADGI established guidelines for digitization in 2007, enabling standardized digitization of cultural heritage collections by archives, museums and libraries. For more information about the EU&rsquo;s work, see <sup id="fnref:40"><a href="#fn:40" class="footnote-ref" role="doc-noteref">40</a></sup>. Given the important calls for a postcolonial DH, it is worth noting that ideas of nation and nation building are inextricably linked to many of these initiatives. Such work can be a colonial project as well as a form of resistance and decolonization. What gets digitized and by whom is a complicated, and often fraught, process, see <sup id="fnref:41"><a href="#fn:41" class="footnote-ref" role="doc-noteref">41</a></sup>  <sup id="fnref:42"><a href="#fn:42" class="footnote-ref" role="doc-noteref">42</a></sup>  <sup id="fnref:43"><a href="#fn:43" class="footnote-ref" role="doc-noteref">43</a></sup>.&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:15">
<p>For example, an early European initiative was Video Active, the predecessor of EUscreen and EUscreenXL, currently consisting of 31 organisations from 22 European countries who have come together to increase access to their materials. Together with the increased availability of digitized materials, the launch of user-generated content platforms such as YouTube in 2005 and SoundCloud in 2007 enabled the circulation of digitized and digital-born materials online.&#160;<a href="#fnref:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:16">
<p>We find Jessica Marie Johnson&rsquo;s naming &ldquo;the digital humanities in its most structural form as articulated by global academic institutions&rdquo; as &ldquo;DHDH&rdquo; to be a helpful configuration <sup id="fnref1:24"><a href="#fn:24" class="footnote-ref" role="doc-noteref">24</a></sup>&#160;<a href="#fnref:16" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:17">
<p>Indeed, parallel to this evolution, media studies and other humanities journals started to pay attention to multimedia data as a new presentation form, often in an open access format, such as <a href="http://vectors.usc.edu">Vectors</a>. In history, sound studies musicology we notice similar initiatives such as the  <em>International Journal of Digital Art</em> .&#160;<a href="#fnref:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:18">
<p>To read more about the founding, please see our <a href="/dhqwords/vol/15/1/000542/">interview</a> with the SIG&rsquo;s founders. Notably, in 2016, a SIG for Digital Humanities and Videographic Criticism was founded within the Society for Cinema and Media Studies, the largest association for film and television scholars in the world.&#160;<a href="#fnref:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:19">
<p>For more about this, see <sup id="fnref:44"><a href="#fn:44" class="footnote-ref" role="doc-noteref">44</a></sup>, <sup id="fnref:45"><a href="#fn:45" class="footnote-ref" role="doc-noteref">45</a></sup>, and <sup id="fnref:46"><a href="#fn:46" class="footnote-ref" role="doc-noteref">46</a></sup>.&#160;<a href="#fnref:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:20">
<p>Simoncelli, E. P.  “Statistical models for images: Compression, restoration and synthesis”    <em>Conference Record of the Thirty-First Asilomar Conference on Signals, Systems and Computers,</em>  1: pp. 673-678 (1997).&#160;<a href="#fnref:20" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:21">
<p>Menell, P. S.  “Envisioning Copyright Law&rsquo;s Digital Future”    <em>NYL Sch. L. Rev</em> ., 46: pp. 63-199 (2002).&#160;<a href="#fnref:21" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:22">
<p>boyd, d. and Crawford, K.  “Critical questions for big data”    <em>Information, Communication &amp; Society</em> , 15, 5 (2012): pp. 662-67.&#160;<a href="#fnref:22" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:23">
<p>Benjamin, R.  <em>Race After Technology.</em>  Cambridge: Polity Press (2019).&#160;<a href="#fnref:23" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:24">
<p>Johnson, J.M.  “Markup Bodies: Black [Life] Studies and Slavery [Death] Studies at the Digital Crossroads.”    <em>Social Text,</em>  36,4,137: pp. 57–79 (2018).&#160;<a href="#fnref:24" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:24" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:25">
<p>D&rsquo;Ignazio, C. and Klein, L.  <em>Data Feminism</em> , Cambridge MA: MIT Press (2020).&#160;<a href="#fnref:25" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:26">
<p>Noble S. U.  “Algorithms of Oppression. How Search Engines Reinforce Racism”  New York: NYU Press (2018).&#160;<a href="#fnref:26" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:27">
<p>Weingart, S. B. and Eichmann-Kalwara, N.  “What&rsquo;s Under the Big Tent?: A Study of ADHO Conference Abstracts”    <em>Digital Studies/le Champ Numérique</em> ,  <em>7</em> ,1, 6 (2017).&#160;<a href="#fnref:27" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:28">
<p>Gallon, K.  “Making a Case for the Black Digital Humanities”  In M. Gold and L. Klein (Eds.),  <em>Debates in the Digital Humanities 2016</em>  (pp. 42-49). Minneapolis, London: University of Minnesota Press (2016).&#160;<a href="#fnref:28" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:29">
<p>Noble, S. U.  “Toward a Critical Black Digital Humanities”  In M. Gold and L. Klein (Eds.),  <em>Debates in the Digital Humanities 2019</em>  (pp. 27-35), Minneapolis, London: University of Minnesota Press, http://doi:10.5749/j.ctvg251hk.5.&#160;<a href="#fnref:29" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:30">
<p>Cong-Huyen, A., Lothian, A., &amp; Philips, A.  “Reflections on a Movement: #TransformDH, Growing Up”  In M. Gold and L. Klein (Eds.),  <em>Debates in the Digital Humanities 2016</em>  (pp. 71-80), Minneapolis, London: University of Minnesota Press (2016).&#160;<a href="#fnref:30" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:31">
<p>Risam, R.  <em>New Digital Worlds: Postcolonial Digital Humanities in Theory, Praxis, and Pedagogy</em> , Evanston, Illinois: Northwestern University Press.&#160;<a href="#fnref:31" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:32">
<p>Sneha, P.P.  <em>Mapping Digital Humanities in India,</em>  Banglore: The Center for Internet and Society. December 30 (2016).&#160;<a href="#fnref:32" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:33">
<p>Posner, M.  “Humanities Data: A Necessary Contradiction”  June 25 (2015) <a href="http://miriamposner.com/blog/humanities-data-a-necessary-contradiction/">http://miriamposner.com/blog/humanities-data-a-necessary-contradiction/</a>.&#160;<a href="#fnref:33" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:34">
<p>Bakhshi, S. I.  “Digitization and Digital Preservation of Cultural Heritage in India with Special Reference to IGNCA, New Delhi”    <em>Asian Journal of Information Science and Technology,</em>  6, 2, (2016).&#160;<a href="#fnref:34" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:35">
<p>Zhizhong, L.  <em>Zhongguo guojia tushuguan guanshi</em> . Beijing Shi: NLC Press (2009).&#160;<a href="#fnref:35" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:36">
<p>Manaf, Z.A.,  “The state of digitization initiatives by cultural institutions in Malaysia: an exploratory survey”    <em>Library Review</em> , 56, 1: pp. 45-60 (2007)&#160;<a href="#fnref:36" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:37">
<p>Urgola, S.  “Archiving Egypt&rsquo;s revolution: &rsquo;the university on the square project&rsquo;, documenting January 25, 2011 and beyond”    <em>IFLA Journal</em> , 40, 1: pp. 12-16 (2014).&#160;<a href="#fnref:37" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:38">
<p>Srinivasan, S., Diepeveen, S. and Karekwaivanane, G.  “Rethinking publics in Africa in a digital age”    <em>Journal of Eastern African Studies</em> , 13, 1 (2018).&#160;<a href="#fnref:38" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:39">
<p>Kulesz, O.  <em>Culture in the digital environment: assessing impact in Latin America and Spain</em>  Paris: Unesco (2017).&#160;<a href="#fnref:39" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:40">
<p>de Jong, A., and Wintermans, V.  “Introduction”  In Y. de Lusenet and V. Wintermans (eds.)  <em>Selected papers of the international conference organized by Netherlands National Commission for UNESCO, November 4, 2005, The Hague, The Netherlands,</em>  (2007).&#160;<a href="#fnref:40" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:41">
<p>Moseley, R., and Wheatley, H.  “Is Archiving a Feminist Issue? Historical Research and the Past, Present, and Future of Television Studies”    <em>Cinema Journal,</em>    <em>47</em> , 3): pp. 152-158 (2008).&#160;<a href="#fnref:41" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:42">
<p>Collins, J.  “Doing-it-together: Public history-making and activist archiving in online popular music community archives”  In S. Baker (ed.)  <em>Preserving Popular Music Heritage: Do-it-Yourself, Do-it-Together</em>  (pp. 77-90). Abington: Taylor and Francis (2015).&#160;<a href="#fnref:42" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:43">
<p>Zaagsma, G.  “Digital History and the Politics of Digitization”  Utrecht: ADHO DH conference Utrecht (2019).&#160;<a href="#fnref:43" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:44">
<p>Spiro, L.  “&lsquo;This Is Why We Fight&rsquo;: Defining the Values of the Digital Humanities”  In M. K. Gold (Ed.),  <em>Debates in Digital Humanities</em>  (online). Minnesota: University of Minnesota Press (2012), <a href="http://dhdebates.gc.cuny.edu/debates/text/13">http://dhdebates.gc.cuny.edu/debates/text/13</a>.&#160;<a href="#fnref:44" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:45">
<p>Griffin, G. and Hayler, M. S.  “Collaboration in Digital Humanities Research - Persisting Silences”    <em>Digital Humanities Quarterly</em> , 12, 1: pp. 1-33.&#160;<a href="#fnref:45" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:46">
<p>Fitzpatrick, K.  <em>Generous Thinking: A Radical Approach to Saving the University</em> , Baltimore: John Hopkins University Press (2019).&#160;<a href="#fnref:46" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry><entry><title type="html">Leonardo, Morelli, and the Computational Mirror</title><link href="https://rlskoeser.github.io/dhqwords/vol/15/1/000540/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://rlskoeser.github.io/dhqwords/vol/15/1/000540/</id><author><name>Alison Langmead</name></author><author><name>Christopher J. Nygren</name></author><author><name>Paul Rodriguez</name></author><author><name>Alan Craig</name></author><published>2021-03-05T00:00:00+00:00</published><updated>2021-03-05T00:00:00+00:00</updated><content type="html"><![CDATA[<h2 id="heading"></h2>
<p>On 15 November 2017, the art world was rocked by a momentous event: Christie’s sold a painting for the staggering sum of $450.3 million dollars, shattering every record for an art auction and thrusting the painting well beyond every other possible claimant for the mantle of  “most expensive painting in the world”   <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. The painting is, compared to other auction house blockbusters, quite unassuming (see <a href="#figure01">Figure 1</a>).</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000540/resources/images/Figure01.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000540/resources/images/Figure01_hu9336c96c59de91d1de329a5e67a46cdb_110488_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/15/1/000540/resources/images/Figure01_hu9336c96c59de91d1de329a5e67a46cdb_110488_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/15/1/000540/resources/images/Figure01.jpg 1024w" 
     class="landscape"
     ><figcaption>
        <p>Christie&rsquo;s employees pose in front of a painting entitled <em>Salvator Mundi</em> by Italian polymath Leonardo da Vinci at a photocall at Christie&rsquo;s auction house in central London on October 22, 2017 ahead of its sale at Christie&rsquo;s New York on November 15, 2017. <a href="https://www.gettyimages.com/detail/news-photo/journalist-takes-photos-of-leonardo-da-vincis-salvator-news-photo/859782406">AFP Contributor/AFP/AFP/Getty Images</a>.
        </p>
    </figcaption>
</figure>
<p>It depicts Christ in half-length, holding a crystalline orb in his left hand while blessing with his right. Christ seems to lock eyes with the beholder, suggesting that he is in fact blessing us, which gives the picture a striking sense of immediacy. But assuming that the painting was made around the year 1500, as suggested by the auction house, this kind of interpersonal interaction with Christ was nothing new; painters like Antonello da Messina and Giovanni Bellini had at that point been experimenting with the interpersonal dynamic between images of the godhead and the individualized beholder for at least a quarter century. So, if novelty is not what drove the auction price to such extremes, what did? The social practice of art attribution. The painting has been attributed by the auction house (with the support of some art historians) to Leonardo da Vinci, one of the unrivaled masters of the Italian Renaissance.</p>
<p>There are somewhere between 10 to 20 extant paintings reasonably attributed to Leonardo, so the emergence of another picture would increase that corpus by 5 to 10 percent, a significant expansion. Quickly after the initial attribution, though, expert opinion began to divide. On the one hand, a number of scholars spearheaded by Martin Kemp, a professor at Oxford who specializes on Leonardo’s artistic production, steadfastly supported the attribution <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. On the other, curators like Carmen Bambach (Metropolitan Museum of Art) insisted that the painting is the work of one of Leonardo’s assistants, Giovanni Antonio Boltraffio <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>.<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>  Somewhere in the middle stood Luke Syson, currently Director of the Fitzwilliam Museum in Cambridge, England. In his previous position at the National Gallery in London, Syson had curated a blockbuster show on Leonardo that included the Salvator Mundi largely as a hypothesis, asking whether or not the painting held up to other autograph works by Leonardo <sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup> and <sup id="fnref1:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. Syson believed parts of the painting were done by Leonardo, but that much of it was painted by assistants, as was common practice in Renaissance workshops where works done almost entirely by workshop assistants often went out under the imprimatur of  “the master”   <sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>  <sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>  <sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>  <sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup>.</p>
<p>These contrasting opinions can seem dizzying. If three major scholars who have dedicated their lives to the study of Leonardo cannot come to a consensus about this one painting, whom do we trust? What does it say about the state of knowledge production in the humanities that such a crucial question fails to find consensus among leading experts and yet the painting still commands an astronomical price? Should there, or should there not, be a relatively straightforward answer to the question, Did Leonardo paint this picture? If so, and if human connoisseurs cannot satisfactorily answer it, perhaps computers might be enlisted to clarify the problem and address it more empirically. After all, a well-programmed computer algorithm will produce results that are based only on the inputs it is given; a computer cannot swindle in the way that unscrupulous art dealers often have. So, might it be possible to model, computationally and mathematically, a problem as difficult as the attribution of a painting to an individual? Imagine for a moment a computer algorithm that could produce consistent assertions about the authorship of the Salvator Mundi. How might that change what it means to be an art historian or, even, an artist?</p>
<p>This essay is about the possibilities and impossibilities inherent to embedding computation within a social practice like art attribution. It examines how humanists make claims to knowledge and how this process may or may not be modellable or mechanizable within the context of classical, deterministic, digital computation. The example of the recent attribution and sale of the  <em>Salvator Mundi</em>  is exceptional in its economic scope, of course, but we argue that the event pinpoints an important question about the stakes of bringing computation into the realm of art attribution: Is it possible for a computer to make an attribution in any meaningful sense, or are attributions always and only possible within the realm of human judgment? We argue that the practice of art attribution is not solely about being correct, that is, finding the answer, but is equally about being able to assign responsibility for the success and/or failure of such influential decisions to an intelligent agent. We wish to underline that such responsibility must be granted by human communities to digital computation; computers cannot take responsibility for their actions on their own.</p>
<p>To begin to shed light on these questions, this essay brings forward results from a collaborative research project that used contemporary computing methods to investigate Giovanni Morelli’s nineteenth-century method for making stylistic attributions of old master paintings <sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup>. As digital art historians, we saw in Morelli’s attributional practice a method that could be easily transposed into the language of the digital computer because it describes a set of quasi-computational operations (more on which below). But we did not begin this project expecting — or even trying — to prove Morelli’s method correct or accurate, nor were we interested in finding out if Morelli’s system could successfully and accurately attribute art works in the context of digital computing. Indeed, we knew from the start that such efforts would be a logical impossibility because of the way the notion of style has been constructed and reconstructed over a century and a half since Morelli began publishing his findings.<sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>  Instead, we wanted to use the computer to force us to come to grips with the details of this foundational, highly formative art historical practice, and to look at it fundamentally anew through the process of computational formalization. We know computers to be excellent at supporting this practice. Because computers are sticklers for details, they serve as a type of computational mirror. They are able to show only what they have been asked to reveal — which is not necessarily the same as what the researcher  <em>wants</em>  to see. We knew that the use of the computer would require us, at each step, to have an honesty chat about Morelli’s (and our own) methods.</p>
<p>It was a revelation to discover the precise ways in which the Morellian techniques unfolded themselves in response to our computational approaches. We quickly came to re-contextualize Morelli’s original procedure as a nineteenth-century form of dimensionality reduction and proxy generation that was inseparable from his native human powers of judgment rather than something that parallels a twenty-first-century computational approach to formalizing the world. This unlocked for us the potential broader significance of his method both for art history and also the art market. In doing this work, we came to recognize that what truly lies at the heart of a successful (or failed) art attribution is not simply endorsing the accuracy of formal comparisons, but the ability to participate in a community of trusted experts and to take full responsibility for one’s own inferences and judgments.</p>
<p>This paper will begin with an explanation of the rationale behind choosing the Morellian practice of attribution rather than any other. After briefly surveying another effort at computationally implementing Morelli’s method, we shall present our own computational techniques and results. We will then discuss what we have come to understand about the roles of responsibility, trust, and expertise in the social practice of art attribution, and how it would be dangerous to assume that such human entailments are native to digital computers.</p>
<h2 id="morelli-and-his-method">Morelli and His Method</h2>
<p>Giovanni Morelli (1816-1891) was one of the foremost connoisseurs — masters of art attribution — in the history of art, and the method he proposed for identifying and authenticating old master pictures has had enormous impact on the discipline <sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>  <sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup>  <sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup>  <sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup>. Morelli’s career trajectory spanned the fields of medicine, politics, and art criticism, and could perhaps have only been fostered in the bubbling cauldron that was the European nineteenth century <sup id="fnref:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup>. Born in Italy to parents of Swiss origin, Morelli moved to Munich at age 17 to study medicine at the Ludwig Maximilian University. Although Morelli never practiced medicine, his training in that field would prove foundational for the method of art attribution that he developed later in his career. After graduation, Morelli traveled throughout Europe fostering his love of art and a network of associations in the European art market. In 1848 he was swept up in the revolutionary fervor that was overtaking the Continent and, as a consequence, began a long and somewhat complicated involvement in the emergence of the Italian Republic. He was a senator in the early Republic and served on the committee that established the system of Italian national museums that is largely still intact, albeit in a modified form.</p>
<p>It was only in 1874, at the age of 58, that Morelli began publishing the rudiments of his method for attributing old master paintings. From that moment, until his death in 1891, Morelli wrote numerous volumes, which quickly gained a wide audience. His interventions were published in German under a double-pseudonym: the writings purported to be the work of a Russian man named Ivan Lermolieff, which were then translated into German by Johannes Schwartze, itself a translation of the Italian name Giovanni Morelli into German. These publications took the form of dialogues in which Lermolieff converses with an anonymous Italian aristocrat. What we now think of as the Morellian Method for attributing artworks emerges as the Italian helps Lermolieff see paintings in a new way. Importantly, the Morellian method was not laid down in normative terms; rather, it emerges as the product of dialogic exchange that allowed Morelli to give voice to the discomforts provoked by his new method. The Lermolieff/Morelli figure plays the role of the rube: he is a foreigner who has come to Italy in search of great art. The dialogue’s other character serves as his educated guide, introducing this revolutionary new mode of connoisseurial analysis based on the revealing detail.</p>
<p>The rudiments of Morelli’s method are familiar to art historians operating in all European language traditions <sup id="fnref:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup>. Prior to his work, the attribution of paintings was a chaotic enterprise entangled in questions of nationalism, capitalism, prestige, and the decline of European aristocracy. Generally speaking, claims to authorship and authenticity were made on the basis of a painting as an integral whole: every piece of the painting was believed to work together to reveal the identity of the artistic agent responsible for having made the picture. This approach is predicated on what Morelli described rather derisively as the  “general impression”  of a picture, and he developed his own method consciously in opposition to this approach <sup id="fnref1:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup>.</p>
<p>Morelli’s wager was that individual details were key, not this  “general impression.”  His method assumed that painters revealed their true identity in the small places where they — and by extension also the beholders — were paying the least attention. Anatomical features like fingernails, earlobes, toes, and noses became the locus of authorial identity. Morelli argued that painters were consistent in the way that they painted these features throughout their careers and that these elements became something akin to a personalized pattern. He came to describe these patterns as Grundformen, or ground forms, and he offered line-drawn examples in his publications (see <a href="#figure02">Figure 2</a>).<sup id="fnref:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup></p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000540/resources/images/Figure02.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000540/resources/images/Figure02_hue61435eb2a09b5035e139742b154b53b_3611668_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000540/resources/images/Figure02_hue61435eb2a09b5035e139742b154b53b_3611668_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000540/resources/images/Figure02_hue61435eb2a09b5035e139742b154b53b_3611668_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000540/resources/images/Figure02_hue61435eb2a09b5035e139742b154b53b_3611668_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/1/000540/resources/images/Figure02_hue61435eb2a09b5035e139742b154b53b_3611668_1800x0_resize_box_3.png 1800w,/dhqwords/vol/15/1/000540/resources/images/Figure02.png 1800w" 
     class="landscape"
     ><figcaption>
        <p>Line drawings showing sketches of ears and hands, from [^morelli1890]. Artwork in the public domain; image courtesy <a href="https://doi.org/10.11588/diglit.2151">Universitätsbibliothek Heidelberg</a>.
        </p>
    </figcaption>
</figure>
<p>In his 1890 (English trans., 1893) dialogue on the Italian paintings in the Borghese and Pamphili collections, Morelli offers an excellent example of how to use Grundformen to make an argument about the attribution and dating of an artwork. In this passage, Morelli stages a scene in which the anonymous Italian companion outlines his method quite clearly for Lermolieff while they are standing in the presence of a portrait of Bishop Ludovico Beccadelli, signed and dated by Titian in 1552. The solidity of this attribution allows Morelli to use the painting as a tool for recognizing the Grundformen that truly distinguish Titian’s art and might thereby be used to solidify other attributions:</p>
<blockquote>
<p>It is my object to make you notice everything in a work of art, and in time you will come to see that even details, in themselves insignificant, may lead us to the truth… Look at the hand in this portrait, particularly at the ball of the thumb, which is too strongly developed and at the round form of the ear. In all his  <em>early</em>  works, and in most of those in his  <em>middle period</em>  till between 1540-1550, Titian adheres to the same round form of ear&hellip; This peculiarity in the ball of the thumb also frequently occurs in his other paintings and in his drawings.<br>
<sup id="fnref2:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup> This discussion is illustrated by a small schematic line drawing of a hand with the caption  “Tizian&rsquo;s Daumenballen,”  which is rendered in the English translation as  “The Ball of the Thumb in Titian’s Works”  (see <a href="#figure03">Figures 3a &amp; 3b</a>) <sup id="fnref3:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup>.<sup id="fnref:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup></p>
</blockquote>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000540/resources/images/Figure03.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000540/resources/images/Figure03_hu67c2b50cf4302bd533bdd611b128f2f3_1156848_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000540/resources/images/Figure03_hu67c2b50cf4302bd533bdd611b128f2f3_1156848_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000540/resources/images/Figure03.png 1024w" 
     class="landscape"
     ><figcaption>
        <p>3a (left): Line drawing showing, “The Ball of the Thumb in Titian&rsquo;s Works,” from [^morelli1890]. Artwork in the public domain; image courtesy <a href="https://doi.org/10.11588/diglit.2151">Universitätsbibliothek Heidelberg</a>. 3b (right): The (proper) right hand in Titian’s <em>Vincenzo Cappello</em> , ca. 1540. Oil on canvas. Washington, DC, National Gallery of Art, Samuel H. Kress Collection, 1957.14.3. Artwork in the public domain; photograph provided by National Gallery of Art, Washington, DC.
        </p>
    </figcaption>
</figure>
<p>That Morelli’s texts illustrate the Grundformen through line drawings and not full illustrations that more faithfully reproduce the actual image-in-the-world under scrutiny was not a technical limitation of the day. By the time that Morelli was publishing his books, it was possible to mass produce art historical volumes with full page illustrations of original works of art using a number of techniques, including lithography, engraving, and etching. His choice emphasizes his use of abstraction as a central technique and in so doing also reveals something disconcerting in Morelli’s method, which is that it is predicated on what we might think of as a kind of aesthetic dissection of the painting itself, a shredding of its holistic integrity and a disregard for the particulars of pictorial representation as instantiated in a given painting.</p>
<p>Indeed, the reduction of the entire work of art to the telling detail is one of the more charged elements of Morelli’s method, and he recognized it as such. Rather than see full-page illustrations of the paintings, the reader, like Lermolieff, is asked to encounter an odd array of noses, earlobes, and fingers organized as a grid to facilitate the process of differentiation between artists. With this approach, Morelli’s method challenged the sensibilities of what was, at that point, traditional art history, and the Lermolieff/Morelli character expressed repulsion at the aesthetic violence to which the painting is subjected by his Italian companion, as it is subdivided into hands, ears, and mouths, bemoaning that the  “general impression”  of the overall work of art is declassed to an afterthought <sup id="fnref4:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup>.</p>
<p>The Grundformen isolated features of the artwork and was a disorientating approach at the time, but for Morelli, the cost was worth the benefit. These schematizations facilitated attribution by staging comparisons, and this step-wise workflow in part helps explain why Morelli’s method is almost irresistible to those interested in algorithmic logic.<sup id="fnref:20"><a href="#fn:20" class="footnote-ref" role="doc-noteref">20</a></sup>  Morelli’s method was to reduce paintings to a limited set of discrete patterns, and in so doing, locate proxies for authorship in meaningful details he believed had been unconsciously embedded in the pictures. The effectiveness of the Grundformen could not be revealed by a casual look at the entire picture, but instead became potent only through their isolation, extraction, and comparison.</p>
<h2 id="heading-1"></h2>
<h2 id="morellis-method-as-algorithm">Morelli’s Method as Algorithm</h2>
<p>In the same way that the results of Big Data analyses are at their most thrilling when they arise from patterns that are  <em>not</em>  obvious during data collection, Morelli also strove to reveal invisible patterns of reality that are actually hidden in plain sight.<sup id="fnref:21"><a href="#fn:21" class="footnote-ref" role="doc-noteref">21</a></sup>  The connection to Big Data here is not simply coincidental. In the twenty-first century, Morelli’s approach might read to the astute technologist like a description, although imprecise, of a set of functions that, once formalized, could be effectively and successfully performed by a digital computer. For example, a supervised system of digital Morellian classification might begin with the identification of Grundformen, that is, the image-based features that typify a particular artist. It might then computationally select and extract those detail-features from a dataset of digital images. Next, it might identify a procedure that would best group these details, and classify them into artist-based clusters. With this trained system in hand, Grundformen could then be extracted from  <em>unattributed</em>  paintings. By comparing these unattributed features to the computationally classified set, one could produce a probabilistic attribution of this unattributed work of art to an artist, or distribute that probability over a set of artists, all by directly implementing Morelli’s assumption that similarities in the appearance of the Grundformen are effective proxies for an artist’s stylistic hand.<sup id="fnref:22"><a href="#fn:22" class="footnote-ref" role="doc-noteref">22</a></sup></p>
<p>Both this brief sketch of a computational Morellian system — as well as Morelli’s original procedure — are fundamentally designed to answer one critical question, Who painted this painting? But, as we aim to show in this paper, the notion that art attribution is a problem that can be solved or a question that can be answered once and for all, misses a critical point. Art attribution is a practice fully embedded in sociality. Indeed, as contemporary art historians, we have found Morelli&rsquo;s method to be most useful not to answer the question, Who painted this? but rather as a tool for better understanding all the complex, human-facing components that make up the question itself.</p>
<h2 id="finding-the-answer-means-first-defining-the-question">Finding the Answer Means First Defining the Question</h2>
<p>Our project is not invested in attempting to operationalize Morelli’s system, in part because previous scholars have already shown that this is indeed already possible, if one proceeds with the assumption that art attribution is a problem to be solved. Adrian J. Ryan, in a 2009 dissertation produced for the Department of Classics at the University of KwaZulu-Natal in South Africa, produced just such a system, and we have found that his research offers a fruitful case study for comparison. Ryan is a computer scientist/technologist by training, who, in returning to school for his degree in Classics, created a formalized, digital system designed to imitate and reproduce the Attic vase-painting attributions of Sir John Beazley. We have found his work especially useful for revealing a number of differences between a digital-art-historical understanding of the utility of Morellian method and a computer-scientific one. Most critically, Ryan’s work highlights what it looks like when Morelli’s methods are used to look for answers, rather than to situate the work of attribution within a social, art-historical context.</p>
<p>The guiding star for Ryan’s computational system of art attribution is not Giovanni Morelli himself, but instead one of the most important scholars in the history of the humanities to have implemented a Morellian Method: Sir John Beazley (1885-1970). Beazley has long served as a disciplinary figurehead for Morelli’s style of artistic attribution, not only in his home field of Attic vase-painting, but also beyond <sup id="fnref1:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup>  <sup id="fnref:23"><a href="#fn:23" class="footnote-ref" role="doc-noteref">23</a></sup>  <sup id="fnref:24"><a href="#fn:24" class="footnote-ref" role="doc-noteref">24</a></sup>  <sup id="fnref:25"><a href="#fn:25" class="footnote-ref" role="doc-noteref">25</a></sup>. Ryan states that his overall goal in implementing a computerized system of art attribution is to discover whether or not,  “a computer may be taught to attribute in the same way that an art historian can”   <sup id="fnref:26"><a href="#fn:26" class="footnote-ref" role="doc-noteref">26</a></sup>. He is clear, both in this passage and throughout his study, that his aim is not to design a classifier that can attribute paintings in some natively computational way, or to justify  “the practice of connoisseurship by means of statistics”   <sup id="fnref1:26"><a href="#fn:26" class="footnote-ref" role="doc-noteref">26</a></sup>, but instead,  “to prove or disprove&hellip;that machines may be taught to attribute in the same way [as a human expert]”   <sup id="fnref2:26"><a href="#fn:26" class="footnote-ref" role="doc-noteref">26</a></sup>. The system Ryan would come to produce is, for all intents and purposes, a Beazley Machine, that is, a supervised machine learning system designed to mimic and/or agree with any and all of Beazley’s attributions.</p>
<p>Ryan’s data-hungry algorithms function, almost exclusively, by winkling out patterns from examples, and the more examples, the better. For this reason, Beazley seems a very sensible choice for this research. The amount of data and documentation remaining from Beazley’s attributions is impressive by the standards of the humanities. Beazley used in his practice, just as Morelli had, hand-drawn, schematic renderings of the details/features that he deemed emblematic of each Attic vase-painter’s style, drawings that he published in his articles and books (see <a href="#figure04">Figure 4</a>).</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000540/resources/images/Figure04_pub.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000540/resources/images/Figure04_pub_hu97b0a9bc03937c2107658f1ddbc5baec_157167_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/15/1/000540/resources/images/Figure04_pub_hu97b0a9bc03937c2107658f1ddbc5baec_157167_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/15/1/000540/resources/images/Figure04_pub.jpg 631w" 
     class="portrait"
     ><figcaption>
        <p>Beazley&rsquo;s sketches of an amphora attributed to the Kleophrades Painter (early 5th century BCE), from Notebook 3, 1909-1910. Pen and ink on paper. Photograph from Beazley Archive, courtesy of the Classical Art Research Centre, University of Oxford.
        </p>
    </figcaption>
</figure>
<p>Over the course of half a century Beazley came to identify hundreds of vase-painters based on detail-features like knees, hands, and costume. The existence of the large photo archive that Beazley amassed of these ceramics, plus the five published, monumental, volumes of Beazley’s attributions, as well as the archival record of Beazley’s dimensionally-reduced line drawings, convinced Ryan that there might be sufficient data to computationally model Beazley’s method <sup id="fnref:27"><a href="#fn:27" class="footnote-ref" role="doc-noteref">27</a></sup>  <sup id="fnref:28"><a href="#fn:28" class="footnote-ref" role="doc-noteref">28</a></sup>  <sup id="fnref:29"><a href="#fn:29" class="footnote-ref" role="doc-noteref">29</a></sup>.<sup id="fnref:30"><a href="#fn:30" class="footnote-ref" role="doc-noteref">30</a></sup>  Nevertheless, while the collection of data Beazley created on Attic vases is truly excellent and extensive by art historical standards, Ryan found that they proved insufficient to support the types of machine learning approaches that he had hoped to implement.</p>
<p>The method that Ryan used to solve this problem of data scarcity interests us greatly for what it reveals about the necessary role of human beings in the process of art attribution even implemented computationally. Ryan elected to address it by producing artificial data sets that increase the amount of data available for training. That is, Ryan  <em>himself</em>  sketched a number of prototypical mouths, knees, and ears in the manner of a number of Antique painters, digitized them, and included his own line drawings as part of the Beazleian training data. This technique served very specific computer-scientific ends for Ryan, namely to help discover if training the classification engine on a combination of both original and artificial data reduces the risk of overfitting to a small design set significantly enough to justify the risk of adding non-essential information <sup id="fnref3:26"><a href="#fn:26" class="footnote-ref" role="doc-noteref">26</a></sup>. But, critically, he went so far as to argue that the inclusion of his own drawings — insofar as they gesturally re-enact the work of a trained, human connoisseur — made his computational experiment  <em>more</em>  accurate.<sup id="fnref:31"><a href="#fn:31" class="footnote-ref" role="doc-noteref">31</a></sup>  This is, interestingly, in line with Beazley’s own practice. Drawing inspiration from Morelli’s method, Beazley produced schematic line drawings of the forms and figures he observed on Greek vases in the belief that the identity of the artist responsible for painting a given vase might become clear when comparing these schematic surrogates against one another. But occasionally, Beazley begged for the reader’s indulgence as he produced line drawings that were more reflective of his own hand than that of the Greek artist he was attempting to ventriloquize <sup id="fnref:32"><a href="#fn:32" class="footnote-ref" role="doc-noteref">32</a></sup>. By inserting his own humanity into his datasets, we find that Ryan is not only following Beazley’s example, but also asserting that trained human actions and decisions play an unassailable role in the process of art attribution. It is Ryan’s judgment that makes this all work, not Beazley’s and not the computer’s.</p>
<p>And yet, when Ryan offers a number of justifications for his work, we feel that most of them are more consonant with the traditions of computer science than the humanities. Crucially, he argues that one of the most fundamentally positive contributions of such a system would be to make the practice of art attribution more  <em>efficient</em> , that is, to save human time. Rather than focusing on the central importance of human expertise, Ryan suggests that his system could save connoisseurs from wasting precious time performing  “painstaking, difficult”  operations, and could free them up to focus on any larger issues at hand <sup id="fnref4:26"><a href="#fn:26" class="footnote-ref" role="doc-noteref">26</a></sup>. This move to assert that the value of computation in any field is to save time and increase efficiency may function well in domains such as engineering or business, but we find it largely unconvincing in the context of the humanities.<sup id="fnref:33"><a href="#fn:33" class="footnote-ref" role="doc-noteref">33</a></sup></p>
<p>For Ryan, the opposite of an efficient system is a slow, painstaking one. However, in the context of the humanities, this analogy simply does not hold. For a humanist, slow is not necessarily bad and it is certainly not the opposite of efficient.<sup id="fnref:34"><a href="#fn:34" class="footnote-ref" role="doc-noteref">34</a></sup>  The value of the humanities does not directly reside in how quickly it can be done, and indeed the work can be considered un-finishable: full of difficult, fundamentally unanswerable questions that each human generation reassesses anew. We believe that the value of the intellectual work of the humanities partially resides in the fact that it takes time both for human beings to gain trust in one another and also to be trained to make scholarly judgments. Will it, or even can it, save connoisseurial time to delegate the practice of, say, visual comparison, to a computer program? Ryan brings no evidence to bear to demonstrate that there are other ways to produce this kind of domain-specific knowledge other than the very practice he is automating, that is, having a human being perform these  “painstaking, difficult”  tasks, especially those that require  “very narrow expertise,”  as he himself notes <sup id="fnref5:26"><a href="#fn:26" class="footnote-ref" role="doc-noteref">26</a></sup>.</p>
<p>We do not believe that such a demonstration is possible, and it would be irresponsible to find out simply by trying. The question of whether or not a certain painter painted a given part of a picture is, ultimately, unverifiable. While there is a reality to it, we cannot replicate it as an experiment: we do not have a time machine. What an act of hubris it would be to think that we could at any one point solve the problem of art attribution once and for all! Disagreement and a lack of complete consensus in an interpretive community is not a sign of its failure, or lack of efficiency; they are signs of its correct functioning as this community goes about its work of continually producing meaningful, cogent knowledge.</p>
<p>In our digital work with the Morellian Method, we focused not on issues of efficiency, novelty, or even the extent to which we could fully mechanize art attribution, but instead on using the computer to help slow our thoughts down, pick apart what we were seeing, show us some of our assumptions in pain-staking detail, and to dive deeper into the system we were formalizing. Rather than hoping that the computer system would partially mechanize our trained judgement, we hoped the process would allow us to use it more judiciously. We wished to engage with what it would mean to implement the Morellian method within the context of digital computing while maintaining our focus on the intellectual priorities of the humanities, hoping to take advantage of the computational mirror to come to learn things about Morelli’s methods and our own. We therefore proposed a technical collaboration with the Extreme Science and Engineering Discovery Environment (XSEDE) group, an NSF-funded, computational and collaborative infrastructure built on top of the United States’ national network of supercomputers, for the development of a proof-of-concept “Morelli Machine” that would take advantage of the team’s interdisciplinary composition to use digital techniques to apply Morelli&rsquo;s method to a large set of images of artworks.</p>
<h2 id="our-engagement-with-the-morelli-machine">Our Engagement with the “Morelli Machine”</h2>
<p>We were keenly aware that the reward structure that would surround the technologist assigned to us by the XSEDE infrastructure would differ greatly from our own as academics, and we wanted to design the details of the project so that it could accommodate both our desire to create recognizable humanistic research as well as fit the requirements of grant-funded technologists who would need to show computational innovation and novel results.<sup id="fnref:35"><a href="#fn:35" class="footnote-ref" role="doc-noteref">35</a></sup>  Therefore, after our initial proposal was accepted, we designed a two-phase project plan with the input of the XSEDE technologist-consultants Paul Rodriguez and Alan Craig. First, we would apply Morelli’s method in a somewhat strict historical manner. We would ask the computer to extract the eyes, mouths and hands from a dataset of images and then cluster them into formal categories, that is, we would ask the machine to follow the same workflow detailed in Morelli’s writings.<sup id="fnref:36"><a href="#fn:36" class="footnote-ref" role="doc-noteref">36</a></sup>  We would then look to see what groupings the computer might find, in part to compare the results to the known clusters of artistic similarity, but also to see if the computer was able to group these features into any other sorts of meaningful groups that may not (yet) hold any resemblance to art-historical opinion. We chose to apply a standard computer vision technique known as Histogram of Oriented Gradients for Phase One, rather than either a supervised or unsupervised machine learning system, for two reasons: first, as we will note below, Phase Two was earmarked for such approaches, and second, Histogram of Oriented Gradients, as a feature descriptor that operates similarly to edge detectors, works by isolating dramatic shifts within digital images, an approach that seemed to map well to the process that Morelli proposed.</p>
<p>It was recognized from the very beginning that this work, like Ryan’s, would require data and a great deal of it. We had initially hoped that we could amass between two and five thousand images drawn from a handful institutions, such as the Kress Foundation, The Metropolitan Museum of Art, and the National Gallery. Our plan was to gather this relatively large number of images, or perhaps even more, from ArtSTOR’s collection of digitized and digital images of artworks. While our initial conversations with ArtSTOR seemed promising, in the end, the red tape surrounding the large-scale use of multi-institutional images proved insurmountable to solve in the time we had to address it.</p>
<p>However, Nygren and Langmead knew, because of prior digital humanities work their students Andrea Maxwell and Sarah Reiff Conell had done with the images of the Samuel H. Kress Collection, that the Kress Foundation is very generous in sharing their collection of digital images for computational use, and moreover, offered dependable metadata. After our initial conversations with ArtSTOR faltered, we approached the leadership of the Kress Foundation with a request to use their images for the purposes of this project, and they happily agreed, while noting that perhaps the easiest way to acquire the files would be, ironically, through ArtSTOR. With the help of the Foundation, ArtSTOR was able to provide us with the digitized images of the Kress Collection, a dataset that amounted to 1,866 files, not Big Data by computer-scientific standards, but too large and at too-high a resolution for a laptop to process, and was sufficient to kickstart our work.</p>
<p>For the second phase of the project — one we will not have space to fully detail in this essay — we recognized that Morelli’s approach of decomposing the images into distinct human-recognizable features would not be the way that contemporary technologists might choose to approach this work if the goal was clustering and classifying images by artist. It is currently quite difficult and time-consuming to ask a computer to do the work of extracting human-defined, visual details from images, and it only has marginal computer-scientific novelty to approach the problem in this manner. And so, Langmead, Nygren, Rodriguez and Craig collectively designed a plan for Phase Two that would compare and cluster the images in this same dataset using machine learning approaches that let the computer select the important features, rather than forcing the use of human-defined ones.<sup id="fnref:37"><a href="#fn:37" class="footnote-ref" role="doc-noteref">37</a></sup>  Even though we will not detail the process or the results of this second phase in this paper, we mention it here because we feel that it is important to present the entire scope of the collaboration and the balance we tried to strike between serving the working needs of the humanists and the technologists on this project. Our results from Phase Two will be the focus of a separate study at a later date, and has already been presented by Rodriguez to the XSEDE community and received with interest.<sup id="fnref:38"><a href="#fn:38" class="footnote-ref" role="doc-noteref">38</a></sup></p>
<p>Phase One of the  “Morelli Machine”  project began in August 2018 and concluded in January 2019. Given that the off-the-shelf digital technologies for facial recognition were, and are, far more advanced than any other type of human body-part-detection, we focused on the process of extracting facial features such as eyes and mouths from the data set, rather than hands or feet. Step one on this path was, therefore, to extract faces from the images. Rodriguez began by trying the Dlib Face Detector and OpenCV Face Recognition libraries, but found in the end that the Google Vision API was the most successful at identifying the faces within our dataset.<sup id="fnref:39"><a href="#fn:39" class="footnote-ref" role="doc-noteref">39</a></sup>  Google’s technology was then applied to the entire collection, and succeeded in extracting 3,205 faces. This API also returned facial landmarks — that is, polygons that outlined its best guesses as to where the eyes, nose and mouth were located on each face it identified — and Rodriguez used this information to post-process the results and extract our desired facial features as individual, cropped images (see <a href="#figure05">Figure 5</a>).</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000540/resources/images/Figure05.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000540/resources/images/Figure05_hu37a2fc39a041940d94006058b0ad1bb0_1823283_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000540/resources/images/Figure05_hu37a2fc39a041940d94006058b0ad1bb0_1823283_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000540/resources/images/Figure05_hu37a2fc39a041940d94006058b0ad1bb0_1823283_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000540/resources/images/Figure05_hu37a2fc39a041940d94006058b0ad1bb0_1823283_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/1/000540/resources/images/Figure05.png 1593w" 
     class="landscape"
     ><figcaption>
        <p>At left, an example face extracted from a digital image of a painting in the Kress Collection showing the red box around the part of the image that the Google Vision API has identified as a face, along with the green, red and purple polygons marking the locations of eyes, nose, and mouth. At center, the face extracted using these guideposts. At right, the extracted mouth alone. Image courtesy Paul Rodriguez.
        </p>
    </figcaption>
</figure>
<p>To fine-tune this process of feature extraction, Rodriguez applied pose (i.e., head turn) estimates, again using the Google Vision API, to adjust the size and shape of these boundary polygons. We wanted to be as careful as possible to extract only our desired facial features, knowing that applying Morelli’s method requires accurate feature extraction. Overall, to the human eye, the results appeared quite good. Rodriguez estimated that around 71% of the faces found within our image files were successfully identified, and within that result set, we were able to extract all of the eyes and mouths <sup id="fnref:40"><a href="#fn:40" class="footnote-ref" role="doc-noteref">40</a></sup>.</p>
<p>Sufficiently satisfied with the accuracy of the feature extraction, the team then moved to the task of clustering. The team began by focusing on the mouths and Rodriguez turned to the Histogram of Oriented Gradients technique to begin work. Histogram of Oriented Gradients (HOG) is a feature descriptor that operates similarly to edge detectors, that is, rather than treating images as color values, it works by identifying the direction and magnitudes of relative difference between pixels. These differences can then, in turn, be used to detect the edges of and/or identify the human-recognizable objects represented in the images.<sup id="fnref:41"><a href="#fn:41" class="footnote-ref" role="doc-noteref">41</a></sup>  At each pixel, the algorithm calculates a vector for the gradient, that is the amount of numeric change between that pixel and the other pixels adjacent to it horizontally and vertically. Those vectors are then added together to create a main direction and magnitude for each pixel, that is, its oriented gradient (see <a href="#figure06">Figure 6</a>).</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000540/resources/images/Figure06.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000540/resources/images/Figure06_hua88875394f5c084be4f7d13deacf5220_88094_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000540/resources/images/Figure06_hua88875394f5c084be4f7d13deacf5220_88094_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000540/resources/images/Figure06.png 492w" 
     class="landscape"
     ><figcaption>
        <p>At left, an image that is zoomed in on the edge of a figure’s mouth, with a black box centered on a single pixel. At right, that same area of the image marked by the black box magnified further, with the red arrows denoting the change in pixel value in the horizontal and vertical dimensions, and the purple arrow being their sum. Image courtesy Paul Rodriguez.
        </p>
    </figcaption>
</figure>
<p>When implementing HOG, the meaning of change in pixel value can be tweaked by the technologist to best identify those features of an image that may be more important to the study at hand. This could be color detection, saturation detection, or any other fundamental, numerically represented, feature of a digital image. To work effectively, therefore, a bit of data pre-processing on the images is required to highlight those characteristics that it has been decided are important to the assessment at hand, and to deemphasize those felt to be extraneous. Deciding what was and was not visually important to implementing the Morellian method computationally became something that our team needed to identify at the outset. As art historians focused on implementing Morelli’s method as closely as we could, Nygren and Langmead were very concerned with maintaining as much color information in our analysis. Rodriguez took this request into deep consideration and opted to normalize the eye-images so that the total illumination was constant but relative color values were still present.<sup id="fnref:42"><a href="#fn:42" class="footnote-ref" role="doc-noteref">42</a></sup></p>
<p>As a next step in the HOG process, all pixels in the image are clustered into blocks of four and the vectorized gradients for each of them are then used to calculate nine summary gradients as measured from the block center (see <a href="#figure07">Figure 7</a>). These nine summary vectors are then counted and visualized as a histogram showing the distribution of the sum of their magnitudes, categorized by their orientation. The greater the count in any given bin, the stronger the gradient is said to be.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000540/resources/images/Figure07.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000540/resources/images/Figure07_hu54c6a25d82b54b01a9ffb199c5572321_39153_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000540/resources/images/Figure07_hu54c6a25d82b54b01a9ffb199c5572321_39153_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000540/resources/images/Figure07.png 975w" 
     class="landscape"
     ><figcaption>
        <p>At left, a visualization of pixels being assembled into groups of four, and then from each corner, nine new vectors appear, summarizing those pixels’ individual gradient magnitudes. In center, the vectors of that pixel group are visualized in isolation. At right, a visualization of the histogram (count) of the various magnitudes represented by the vectors at center, organized by their degrees (bins are 20 degrees each). Image courtesy Paul Rodriguez.
        </p>
    </figcaption>
</figure>
<p>The penultimate result of the Histogram of Oriented Gradients feature descriptor, then, is that the images have been decomposed into a set of clustered, oriented, summarizing, nine-dimensional vectors that describe the relative numeric change in pixel values across the image, one that also retains information about the relative direction in which that change occurs (see <a href="#figure08">Figure 8</a>).</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000540/resources/images/Figure08.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000540/resources/images/Figure08_huad75311d2f186a60fc280cf5c42de533_52926_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000540/resources/images/Figure08_huad75311d2f186a60fc280cf5c42de533_52926_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000540/resources/images/Figure08.png 508w" 
     class="landscape"
     ><figcaption>
        <p>Visualization of the grid of vectors that represent the distribution of oriented gradients across a digital image. Image courtesy Paul Rodriguez.
        </p>
    </figcaption>
</figure>
<p>These vectors can then be visualized as a histogram that can be taken to assert the relative strength and weakness of the gradients across the image. To conclude the process of applying the Histogram of Oriented Gradients technique, a summarization method is applied: one final feature vector is calculated by taking all of these nine-dimensional vectors and summarizing them into a single, large, ten-dimensional vector that represents, in some ways, the image in its entirety <sup id="fnref:43"><a href="#fn:43" class="footnote-ref" role="doc-noteref">43</a></sup>.</p>
<p>Once this process had been run on our extracted features, it was time to arrange them into clusters of similarity. The ten-dimensional HOG feature vectors that summarized each image were used as input into a K-means clustering algorithm to gather the images into (hopefully meaningful) groupings. To visualize this information, the original ten-dimensional feature vectors were projected into two-dimensional space using the t-SNE technique, and each point on the resulting graph was then color-coded according to their K-means clustering assignments (<a href="#figure09">Figure 9</a>).<sup id="fnref:44"><a href="#fn:44" class="footnote-ref" role="doc-noteref">44</a></sup>  We began with mouths.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000540/resources/images/Figure09.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000540/resources/images/Figure09_hu5d4647ae206767f55dd8d8a708601373_511091_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000540/resources/images/Figure09_hu5d4647ae206767f55dd8d8a708601373_511091_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000540/resources/images/Figure09.png 1146w" 
     class="landscape"
     ><figcaption>
        <p>To the upper left, a visualization of the clustering of similarities between mouths, with the particular mouth shown to the upper right (32) highlighted. To the upper right, the images drawn from the Kress Collection of the example face 32 used in the workflow. At bottom, the histogram representing the gradients from this particular example all as produced by Paul Rodriguez in the project workflow. Image courtesy Paul Rodriguez.
        </p>
    </figcaption>
</figure>
<p>The initial clustering results for mouths did not inspire anyone on the team to assert that our computational method was successfully grouping the images by any clear, meaningful art-historical metric whatsoever (see <a href="#figure10">Figure 10</a>). Above and beyond the fact that the team had clearly produced an excellent Old-Master-Painting mouth extractor, the clustering results produced by the combination of the t-SNE and K-means procedures was not anywhere near to clustering these features by artist. None of us were even particularly struck by any amount of above-average visual similarity within the clusters. The results for eyes were no better.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000540/resources/images/Figure10.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000540/resources/images/Figure10_hu8759e5b1e1c39a4ace971ac40e79ae49_610934_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000540/resources/images/Figure10_hu8759e5b1e1c39a4ace971ac40e79ae49_610934_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000540/resources/images/Figure10_hu8759e5b1e1c39a4ace971ac40e79ae49_610934_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000540/resources/images/Figure10.png 1262w" 
     class="landscape"
     ><figcaption>
        <p>Three computer-produced similarity clusters of the mouths extracted from digital images of the Kress Collection, as produced by the analysis led by Paul Rodriguez in the project workflow. Image courtesy Paul Rodriguez.
        </p>
    </figcaption>
</figure>
<p>It was becoming clear that HOG was not sorting these images into groups that held visual interest for the team. But even so, because our own research questions asked us to make some form of art-historical, or even visual, sense of the clusters of disembodied mouths, Langmead and Nygren were afforded the time and the opportunity to acknowledge and respond to their own immediate and visceral recognition of the aesthetic violence of Morelli’s method, one perhaps magnified by the scale offered by the digital computer. This moment offered us an experiential taste of what it means to extract recognizably visual features from art objects and array them like so many specimens in a row. Moreover, it suggested how hard it can be to visually interpret these isolated images at this scale, one orders of magnitude more massive than Morelli could have possibly imagined. Morelli himself suggested that it was overwhelming to confront dozens of sketches of fragmented body parts, and seeing thousands of them proved no less disconcerting (see <a href="#figure11">Figure 11</a>).</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000540/resources/images/figure11.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000540/resources/images/figure11_huc51d4a7961220e7bb26df1eda61c02a5_2425070_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000540/resources/images/figure11_huc51d4a7961220e7bb26df1eda61c02a5_2425070_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000540/resources/images/figure11_huc51d4a7961220e7bb26df1eda61c02a5_2425070_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000540/resources/images/figure11_huc51d4a7961220e7bb26df1eda61c02a5_2425070_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/1/000540/resources/images/figure11_huc51d4a7961220e7bb26df1eda61c02a5_2425070_1800x0_resize_box_3.png 1800w,/dhqwords/vol/15/1/000540/resources/images/figure11.png 2200w" 
     class="landscape"
     ><figcaption>
        <p>Left and right: Line drawings showing sketches of ears and hands, from [^morelli1890]. Artwork in the public domain; image courtesy <a href="https://doi.org/10.11588/diglit.2151">Universitätsbibliothek Heidelberg</a>. Center: Similarity clusters 1 (bottom) and 3 (top) from <a href="#figure10">Figure 10</a>. Image courtesy Paul Rodriguez.
        </p>
    </figcaption>
</figure>
<p>When comparing Morelli’s published line drawings of disembodied hands and ears to our data, Langmead and Nygren had been considering these visually comprehensible images as the computational analogs to Morelli’s Grundformen. However, during a team meeting, as Rodriguez took a moment to narrate his process in greater detail, he also shared the set of pre-processed images created for the HOG feature descriptor. It was at that moment, with the grids of mouths in mind, that Nygren and Langmead were suddenly struck with the realization that these pre-processed images — not the disembodied, recognizable mouths that had been extracted by the Google Vision API — were the objects analogous to the Grundformen, the dimensionally-reduced line drawings used by Morelli (see <a href="#figure12">Figure 12</a>). These (to human eyes) yellow and white, almost wholly abstracted forms, were the more fundamental visual evidence of the extreme computational abstraction necessary to make claims about sameness and difference within a computer, not the grid of human-identifiable mouths.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000540/resources/images/figure12.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000540/resources/images/figure12_huc51d4a7961220e7bb26df1eda61c02a5_2157054_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000540/resources/images/figure12_huc51d4a7961220e7bb26df1eda61c02a5_2157054_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000540/resources/images/figure12_huc51d4a7961220e7bb26df1eda61c02a5_2157054_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000540/resources/images/figure12_huc51d4a7961220e7bb26df1eda61c02a5_2157054_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/1/000540/resources/images/figure12_huc51d4a7961220e7bb26df1eda61c02a5_2157054_1800x0_resize_box_3.png 1800w,/dhqwords/vol/15/1/000540/resources/images/figure12.png 2200w" 
     class="landscape"
     ><figcaption>
        <p>Left and right: Line drawings showing sketches of ears and hands, from [^morelli1890]. Image courtesy <a href="https://doi.org/10.11588/diglit.2151">Universitätsbibliothek Heidelberg</a>. Center: Visualization in four rows, each showing a different example mouth extracted from digital images of the Kress Collection. The columns represent three, dimensionally-reduced forms, at left, the mouths as directly extracted from the original digital images, at center, those same images with the digital information abstracted down to a form more useful to the HOG process, but that are difficult to recognize as mouths to the human eye, and at right, the images at center shown with more of their surrounding image context. Image courtesy Paul Rodriguez.
        </p>
    </figcaption>
</figure>
<p>To explain what we mean let us return to the Beccadelli portrait we introduced above, because it is in the illustration accompanying his discussion of that painting that the ultimate stakes of the Grundformen become clear. Through working with Morelli’s method within the context of digital computing, we came to understand that it is predicated on a misdirection that leads us away from the painting-in-the-world toward an abstract, schematic rendering conjured into existence by the connoisseur. Closely comparing Morelli’s line drawing to the Beccadelli portrait, we noticed that his sketchy rendering is quite deceptive and does not appear to duplicate either hand found in Titian’s painting. To a trained art historian, the hand seen in Morelli’s line drawing actually seems much closer to the hands found in numerous other TItian paintings executed around this same period in Titian’s career (1540-50), such as the  <em>Crowning of Thorns</em> ,  <em>The Allocution of Alfonso d’Avalos,</em>  or the  <em>Portrait of Vincenzo Cappello</em>  (see <a href="#figure13">Figure 13</a>). For a man focused on details, Morelli appears entirely uninterested in offering direct visual parallels between his line drawings and the particulars of this portrait <sup id="fnref:45"><a href="#fn:45" class="footnote-ref" role="doc-noteref">45</a></sup>  <sup id="fnref:46"><a href="#fn:46" class="footnote-ref" role="doc-noteref">46</a></sup>.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000540/resources/images/Figure13.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000540/resources/images/Figure13_hu1e9033fc8a4cb247d1a5b2b6fe7ae901_3579243_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000540/resources/images/Figure13_hu1e9033fc8a4cb247d1a5b2b6fe7ae901_3579243_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000540/resources/images/Figure13_hu1e9033fc8a4cb247d1a5b2b6fe7ae901_3579243_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000540/resources/images/Figure13_hu1e9033fc8a4cb247d1a5b2b6fe7ae901_3579243_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/1/000540/resources/images/Figure13_hu1e9033fc8a4cb247d1a5b2b6fe7ae901_3579243_1800x0_resize_box_3.png 1800w,/dhqwords/vol/15/1/000540/resources/images/Figure13.png 1920w" 
     class="landscape"
     ><figcaption>
        <p>13a. Upper left: The (proper) left hand of the figure in the foreground in red in <em>The Allocution of the Marchese del Vasto to His Troops</em> , 1540-41. Oil on canvas. Madrid, Museo del Prado. Artwork in the public domain; photograph by Francesco Mariani. 13b. Upper right: The (proper) right hand in Titian’s <em>Vincenzo Cappello</em> , ca. 1540. Oil on canvas. Washington, DC, National Gallery of Art, Samuel H. Kress Collection, 1957.14.3. Artwork in the public domain; photograph provided by National Gallery of Art, Washington, DC. 13c. Bottom right: The (proper) right hand of Ludovico Beccadelli in Titian’s <em>Portrait of Ludovico Beccadelli</em> , 1552. Oil on canvas. Florence, Uffizi Galleries. Artwork in the public domain; photograph by Laura Fenelli. 13d. Bottom left: The (proper) right hand of soldier in blue at rear of Titian’s <em>Crowning with Thorns</em> , 1543. Oil on canvas. Paris, Louvre. Artwork in the public domain; photograph ©RMN-Grand Palais / Art Resource, NY. 13e. Center: Line drawing showing, “The Ball of the Thumb in Titian&rsquo;s Works,” from [^morelli1890]. Artwork in the public domain; image courtesy <a href="https://doi.org/10.11588/diglit.2151">Universitätsbibliothek Heidelberg</a>.
        </p>
    </figcaption>
</figure>
<p>Elsewhere in his volume, Morelli suggested that the  <em>Grundformen</em>  should resolve into a  “physiological treatise”   <sup id="fnref5:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup>. Having trained as a physician, Morelli’s Grundformen  identify the author of a painting in the same way that a diagnostic manual identifies a disease: if diagnosis relies on inference based on symptoms, so too does attribution mobilize evidence in the service of inferential reasoning that is ultimately metonymic.<sup id="fnref:47"><a href="#fn:47" class="footnote-ref" role="doc-noteref">47</a></sup>  It is important to understand that despite Morelli’s insistence that the Grundformen constitute a form of ground truth on top of which he constructed his attributions, his practice made apparent that they are significant personal abstractions intended for use by trained professionals who have a great deal of pre-existing visual expertise. The Grundformen do not exist in any real sense, that is, the Grundform of a Titian hand is not the same thing as a photographic detail of Beccadelli’s hand as painted by Titian. Rather, the Grundformen were, in Morelli’s account, a wholly schematic rendering of the type of hands formulaically produced across Titian’s entire pictorial corpus, which existed firstly in the mind of the scholar and only secondarily as line drawings.</p>
<p>This realization forced us to consider Morelli’s texts in a new light. On the basis of what we learned by attempting to computationally recreate Morelli’s method, it was no longer clear to us that we were making correct assumptions about how he expected his books to be read and what role he intended the Grundformen to play in the process of attribution. On the one hand, Morelli could have expected his readers to take copies of his book to the unattributed painting and compare his line drawings to the image  <em>in vivo</em> . On the other, his books could have been taken as an invitation to the reader to produce abstracted line drawings of their own, only comparable to Morelli’s published drawings, to use in the service of their own attributions.</p>
<p>At this point we feel it is worthwhile to underline what we consider one of the major findings of this project, at the level of both art-historical and computational analysis: Morelli’s Grundformen are not simply dimensional reductions of visual data. The Grundformen are embodied renderings of human judgment, the result of hundreds (if not thousands) of hours of deliberatively looking at early modern paintings with the larger intention of developing a method for describing artistic style. Simply looking at an array of extracted images would never result in the production of a Grundform; to do this work, a human must exercise trained judgment to identify precisely what is and what is not important to their own informed understanding of the notion of artistic style and then use those concepts to extract only what is necessary to them. There is no promise that the Grundformen of one scholar would match those of another. Similarly, asking a computer to replicate this workflow cannot be achieved by training the computer to recognize, say, every human hand painted by Titian; rather, it would require the computer to produce its own abstracted rendering of Titian’s hand, which encompasses both the hands he painted and his stylistic hand, and then to deploy successfully that abstraction identifying artistic style. While current computer techniques are becoming capable of achieving the first step in this process, as represented by the yellow-and-white abstractions made for the Histogram of Oriented Gradients workflow, the higher order function of articulating how these abstractions make meaning in the context of the scholarly community remains beyond its reach without human intervention. For the machines to be able to attribute artwork on their own, we would need to allow them to join our intentional community and delegate some of the responsibility for our judgment to their mechanical processes.</p>
<h2 id="the-art-market">The Art Market</h2>
<p>We set about this collaboration because we were curious to see what it would require to implement an art historical method of connoisseurship computationally. We have determined that, for the moment at least, computational techniques cannot attribute artworks without the support of human judgement and expertise.<sup id="fnref:48"><a href="#fn:48" class="footnote-ref" role="doc-noteref">48</a></sup>  While this situation may change as our relationship to technology transforms over time, our core belief remains that looking to a computer for the answer to the question who painted this picture? is currently not only misplaced but can also have noxious implications.</p>
<p>Although we did not initiate this collaboration with the art market in mind, it quickly became clear to us that our project would not be perceived as agnostic by that market itself and the stakes of this work partake of this larger ecosystem <sup id="fnref:49"><a href="#fn:49" class="footnote-ref" role="doc-noteref">49</a></sup>.<sup id="fnref:50"><a href="#fn:50" class="footnote-ref" role="doc-noteref">50</a></sup>  This perhaps should not have come as a surprise, since Morelli’s method has had a marked impact on the art market <sup id="fnref1:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>. Because the price of an object on the market is interlinked with other concepts like historic import and artistic value (however defined), any mode of verifying the authenticity or attribution of a work of art that is deemed to be trustworthy will, almost as if through capillary motion, flow into the surplus value of the work of art. Over the course of the twentieth century, increasing preference has been given to the evidence produced through technological processes to determine authorship <sup id="fnref:51"><a href="#fn:51" class="footnote-ref" role="doc-noteref">51</a></sup>  <sup id="fnref:52"><a href="#fn:52" class="footnote-ref" role="doc-noteref">52</a></sup>  <sup id="fnref:53"><a href="#fn:53" class="footnote-ref" role="doc-noteref">53</a></sup>. In 2016, Sotheby’s purchased the firm Orion Analytical, making them the first auction house to create an in-house scientific lab dedicated to authenticating works of art <sup id="fnref:54"><a href="#fn:54" class="footnote-ref" role="doc-noteref">54</a></sup>.<sup id="fnref:55"><a href="#fn:55" class="footnote-ref" role="doc-noteref">55</a></sup>  The auction houses have sought refuge in the seemingly dispassionate results of scientists, a form of evidence that takes on the air of objectivity and thereby is granted authority, especially by those outside of the scholarly community.</p>
<p>Carrying this tradition forward, computer science researchers have recently been promoting their technical skills with computer vision to hold out the promise of art verified by AI.<sup id="fnref:56"><a href="#fn:56" class="footnote-ref" role="doc-noteref">56</a></sup>  Such interventions draw on the same desire for objective data as earlier attempts to incorporate scientific results of pigment analysis and carbon-14 dating, but this time wrapped in the rhetoric of artificial intelligence. The assertion that an attribution has been computer verified is likely to increase the value and prestige of old master paintings. Would collectors be more likely to wager the GDP of a small nation, as in the example of the  <em>Salvator Mundi</em>  above, on the conflicting opinions of embodied art historians or on the declarative output of a computer algorithm? We cannot claim to know with certainty, but it seems as likely as not that a buyer will trust the definitive answer produced by the black box of a computer algorithm over a cacophonous exchange of interpretations between the competing voices of art historians. The art market demands answers, not a series of questions.<sup id="fnref:57"><a href="#fn:57" class="footnote-ref" role="doc-noteref">57</a></sup></p>
<p>As humanists, however, we believe that such questions are truly important. What does the insertion of a computer change about, or add to, the traditional humanistic process of looking at objects and providing well-founded, inferential statements about the past? Is it the immense speed of calculation? Is it the size of the data sets? Could it be that people might trust a computer in a way that they no longer trust intellectual forebears such as Morelli? We have engaged in academic research, and to the extent possible we have attempted to sever our inquiry from the umbilical cord of gold that links art, art history, and capitalism.<sup id="fnref:58"><a href="#fn:58" class="footnote-ref" role="doc-noteref">58</a></sup>  However, our community’s efforts in this domain must remain ongoing.</p>
<p>Research in this field should not only resist instrumentalization by the art market, but also offer perspectives on the serious stakes of what it would mean for human beings to vest computers with the sole responsibility for making attributions. Let us return once again to the thought experiment proposed in the introduction in order to make a specific point. To whom would the purchaser of the work purportedly by Leonardo make recourse in the event that the algorithm that produced a computational attribution of the painting to the master was proven to be faulty? The culpable might include: the computer programmer who developed the algorithm; the art historians (if any) who contributed to the construction of the algorithm; the lines of code in the algorithm; the piece of hardware that ran the algorithm; the auction house that countenanced the results; the buyer who believed the results; or the insurance agency that indemnified the results. In this day and age, it is not possible to imagine that, from such a list, anything other than a human being can be held accountable for such an error, and yet the decision would have been “made” by a machine.<sup id="fnref:59"><a href="#fn:59" class="footnote-ref" role="doc-noteref">59</a></sup></p>
<p>The difficulties faced in attempting to respond to this scenario point, we believe, to the unavoidable truth that computers have not earned their place in the conversation about artistic attribution because the linkage between style and authorship is not some immutable law of physics that is easily computable, but rather a humanistic commitment that requires human judgment. To make the practice of art attribution anything else besides a necessary collaboration between the physical object and trained human beings would require a fundamental reassessment of the practice, not simply its mechanization. We would like to argue that all such systems will currently fail to produce convincing results, not because they are incapable of mimicking the formal methods of art attribution, but because they have not (yet) been accepted as members of the community that make up attribution’s social practice, and cannot take responsibility for their conclusions. The computer can present the art historian with a series of statistical probabilities related to authorship, but it is important to note that simply choosing between a menu of options presented by the computer is not sufficient for producing a genuine attribution: art attribution cannot be reduced to decisionism; it requires judgment. We affirm that it is necessary for the community of experts called upon to make these judgments be made up of a broad and representative population of human experience that goes beyond the demographics that have traditionally dominated the space of attribution. It is precisely because  <em>human judgment</em>  is necessary to the process of attribution that, to make effective meaning, the community must reflect the breadth of the human experience.</p>
<p>While the results of our Phase One workflow were not designed to give answers to the problem of attribution, and were certainly not successfully responding to that imperative in any way, they have illuminated the ways that the mechanical processes of abstraction that, at first, seemed similar between our computational approaches and Morelli’s, were in fact utterly different, once contextualized within a social system. When they are presented as abstract entities on the page, Morelli’s Grundformen appear reified; through rhetorical force, they seem to take on an existence that was independent of the paintings in which they may be found. This presents an occasion for “verification” and cross-reference that can be described as quasi-computational and therefore formalizable. But the process of art attribution does not begin and end with the physical materiality of the painting.</p>
<p>Attributing a work of art has always been the work of narrative explication rather than mere reckoning about stylistic markers. It currently  <em>requires</em>  human judgment, expertise, and trust to become an effective truth in the world. Morelli, too, gestured towards this state of affairs by presenting his method in his text as a product of dialogue, as a conversation embedded in expertise and interpersonal persuasion. Morelli’s Grundformen existed to help his interlocutors see the paintings  <em>as he saw them</em> , as a way to convince others of his judgments. Even granting that computational methods eventually rise to the level of being able to make believable assessments of stylistic markers, for their outputs to become truly effective, they would need to earn their place as full participants in the social process of art attribution, and human beings would need to delegate some power of judgment to them.</p>
<h2 id="coda">Coda</h2>
<p>When we began this project, we took a leap of faith that we would uncover interesting areas of art-historical inquiry by computationally mechanizing Giovanni Morelli’s method of art attribution. While it was to be expected that our system could match Morelli’s own at the level of the workflow — extract a set of details and compare them — we did not anticipate how our work would show us the complexity of Morelli’s historical process at a more fundamental level. Where Morelli used his native human powers of abstraction and judgement to reduce old master paintings to a series of published line drawings, we found ourselves using the computer’s power of abstraction to create seemingly analogous, dimensionally-reduced representations. In so doing, we recognized that Morelli’s abstractions were effective because they worked in parallel with his human judgment in a way that mechanical processes could never replicate alone.</p>
<p>Our investigation has also suggested to us that there is a fundamental question that the field of art history has not yet addressed and that computers might actually help clarify: to what extent are personalized, identifiable artistic traces, however they are defined (forms, brushstroke, pigment, craquelure, etc&hellip;), registered at the level of pixels? Because, insofar as these traces are so registered, they  <em>will</em>  be legible computationally and therefore computers  <em>can</em>  identify them, perhaps even more effectively than human beings. We nevertheless argue that pixel-level information, on its own, would be insufficient to produce a credible attribution. That work is done as a social practice; it is a matter of an interaction between the physical traces of the past on the canvas and the trained judgment of human beings. It is an ecology that includes the physical history of the work of art, the archive of historical knowledge external to the work itself, and the particular training of each expert working in a community that has the power to make their findings real and effective in the world.</p>
<p>Classical, deterministic, electronic, digital computers are designed to produce certain results based on the input given and their own physical and logical programming. They only show you what you have set them up to show you — not necessarily what you wanted, needed, or even expected to see. Humanists natively operate in a different space than this, which might suggest that the world of computation cannot speak to the world of the interpretive expert. However, we argue here that this is not the case. Working with the computer, and with trained technologists, has provided us with the opportunity to gain important perspectives by allowing us to produce and manipulate an art-historical model of the world. A truism that has become particularly self-evident during the COVID-19 pandemic is that all computer models are, in some senses, wrong, but some models are useful.<sup id="fnref:60"><a href="#fn:60" class="footnote-ref" role="doc-noteref">60</a></sup>  Recognizing the ways that our computational model did and did not implement the older art-historical approach was, for us, like both looking through a microscope and a mirror onto the history of our discipline. This exercise forced us to slow down and look at Morelli’s method in minute detail, but in the end, it also reflected our own assumptions back to us, providing us with clear critical distance. Using digital techniques to think through even historically distant modes of art history indicates one path by which our field can do the ongoing work of reshaping its own practices from within while also reaching out and collaborating with other expert communities that see the world differently. Bridging those gaps, we believe, is crucial to developing art history for the digital age.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Lewis, Ben (2019).  <em>The Last Leonardo: The Secret Lives of the World’s Most Expensive Painting</em> . London: Ballantine.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Kemp, Martin. (2019).  <em>Leonardo by Leonardo</em>  (New York: Callaway).&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Bambach, Carmen (2019).  <em>Leonardo da Vinci Rediscovered</em>  (New Haven and London: Yale University Press).&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>On the possible legal repercussions of Bambach’s opinion, see <sup id="fnref:61"><a href="#fn:61" class="footnote-ref" role="doc-noteref">61</a></sup>.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>Syson, Luke, ed. (2011).  <em>Leonardo da Vinci: Painter at the Court of Milan</em>  (London: National Gallery of Art).&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>Golden, Andrea. (2004).  “Creating and Re-Creating: The Practice of Replication in the Workshop of Giovanni Bellini,”  in  <em>Giovanni Bellini and the Art of Devotion</em> , edited by Kasl, Ronda. (Indianapolis: Indianapolis Museum of Art), 91-127.&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>Michelle O’Malley. (2007).  “Quality, Demand, and the Pressures of Reputation: Rethinking Perugino.”    <em>Art Bulletin</em>  89, 674-693.&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>Williams, Robert. (2017).  <em>Raphael and the Redefinition of Art in Renaissance Italy</em> . Cambridge: Cambridge University Press, 2017.&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>Neilson, Christina. (2019).  <em>Practice &amp; Theory in the Italian Renaissance Workshop: Verrocchio and the Epistemology of Making Art</em>  (Cambridge: Cambridge University Press).&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>Morelli, Giovanni. (1893).  <em>Italian Painters: Critical Studies of Their Works. By Giovanni Morelli (Ivan Lermolieff). The Borghese and Doria-Pamphili Galleries in Rome</em> , translated by Constance Jocelyn Ffoulkes (London: John Murray).&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref3:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref4:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref5:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>Art history has moved on from the foundational ways that Morelli envisioned the historical operation of artistic production and the value of style within the field. There are numerous studies offering analyses of this historical shift. Among them, we find the following most informative: <sup id="fnref1:23"><a href="#fn:23" class="footnote-ref" role="doc-noteref">23</a></sup>  <sup id="fnref:62"><a href="#fn:62" class="footnote-ref" role="doc-noteref">62</a></sup>  <sup id="fnref:63"><a href="#fn:63" class="footnote-ref" role="doc-noteref">63</a></sup>  <sup id="fnref:64"><a href="#fn:64" class="footnote-ref" role="doc-noteref">64</a></sup>  <sup id="fnref1:24"><a href="#fn:24" class="footnote-ref" role="doc-noteref">24</a></sup>  <sup id="fnref:65"><a href="#fn:65" class="footnote-ref" role="doc-noteref">65</a></sup>  <sup id="fnref:66"><a href="#fn:66" class="footnote-ref" role="doc-noteref">66</a></sup>  <sup id="fnref1:25"><a href="#fn:25" class="footnote-ref" role="doc-noteref">25</a></sup>  <sup id="fnref:67"><a href="#fn:67" class="footnote-ref" role="doc-noteref">67</a></sup>  <sup id="fnref1:52"><a href="#fn:52" class="footnote-ref" role="doc-noteref">52</a></sup>  <sup id="fnref:68"><a href="#fn:68" class="footnote-ref" role="doc-noteref">68</a></sup>  <sup id="fnref:69"><a href="#fn:69" class="footnote-ref" role="doc-noteref">69</a></sup>  <sup id="fnref:70"><a href="#fn:70" class="footnote-ref" role="doc-noteref">70</a></sup>.&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p>Agosti, Giacomo, Maria Elisabetta Manca, Matteo Panzeri, and Marisa Dalai Emiliani (eds.). (1993).  <em>Giovanni Morelli e la cultura dei conoscitori: atti del convegno internazionale, Bergamo, 4-7 giugno 1987</em> . Bergamo: P. Lubrina.&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13">
<p>Hinojosa, Lynne Walhout. (2009).  “The Connoisseur and the Spiritual History of Art: Morelli and Berenson,”  in  <em>idem</em> ,  <em>The Renaissance, English Cultural Nationalism, and Modernism, 1860–1920</em>  (New York: Palgrave Macmillan), 89-111.&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14">
<p>Caglioti, Francesco, Andrea De Marchi, and Alessandro Nova (eds.). (2018).  <em>I Conoscitori Tedeschi Tra Otto E Novecento</em> , edited by, (Milan: Officina Libraria)&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:15">
<p>Barni, Daniele. (2016).  <em>Lo sguardo della critica: i conoscitori d&rsquo;arte in Italia tra XIX e XX secolo</em>  (Turin: Cartman).&#160;<a href="#fnref:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:16">
<p>Anderson, Jaynie. (2020).  <em>The Life of Giovanni Morelli in Risorgimento Italy</em>  (Milan: Officina Libraria).&#160;<a href="#fnref:16" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:17">
<p>Davis, Whitney. (2011).  <em>A General Theory of Visual Culture</em>  (Princeton: Princeton University Press).&#160;<a href="#fnref:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:18">
<p>It should be noted that Morelli did not always use his own method. This has been noted by numerous scholars. While this is an important historical truth, our paper takes the computational implementation as a hypothesis and is not ultimately concerned with the question of whether or not Morelli obeyed his own method. On this question, see <sup id="fnref1:45"><a href="#fn:45" class="footnote-ref" role="doc-noteref">45</a></sup>, <sup id="fnref:71"><a href="#fn:71" class="footnote-ref" role="doc-noteref">71</a></sup>, <sup id="fnref1:46"><a href="#fn:46" class="footnote-ref" role="doc-noteref">46</a></sup>.&#160;<a href="#fnref:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:19">
<p>For the original German text, see <sup id="fnref:72"><a href="#fn:72" class="footnote-ref" role="doc-noteref">72</a></sup>.&#160;<a href="#fnref:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:20">
<p>William Vaughan, the pioneering digital art historian, also linked his work to Morelli’s name in the late 1980s and early 1990s. His research on early digital imagery and their possible role in art historical study, while very valuable and notable in our field’s history, took up a fundamentally different research direction from ours, indeed as he notes,  “&hellip;there is really no connection between [my] computer system and the method devised by the nineteenth century art historian”   <sup id="fnref:73"><a href="#fn:73" class="footnote-ref" role="doc-noteref">73</a></sup>. For more on this work, see <sup id="fnref:74"><a href="#fn:74" class="footnote-ref" role="doc-noteref">74</a></sup>.&#160;<a href="#fnref:20" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:21">
<p>This now-long-standing cultural attraction to the wonder and surprise of revealing the unseen using information in plain sight has been investigated in the context of digital computing, and even likened to a fetish <sup id="fnref:75"><a href="#fn:75" class="footnote-ref" role="doc-noteref">75</a></sup>. Revealing the invisibly visible is also related to the concept of clues that Carlo Ginzburg discusses — using Morelli himself as evidence — in the context of the humanities <sup id="fnref:76"><a href="#fn:76" class="footnote-ref" role="doc-noteref">76</a></sup>.&#160;<a href="#fnref:21" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:22">
<p>There are at least three layers to the meaning of hand. The first and most basic is what we might think of as the hand as a piece of semantic content; under discussion here is a human hand as rendered in paint and then reproduced photographically. Second, the claim to hand also refers to the fact (or inference) that Titian spread the paint on the picture surface using his own hand. Third, we have the hand as the historical construct of Titian’s style as constituted by the field of art history. It is important to disentangle these different layers of meaning in order to understand which aspects of Morelli’s method might be rendered computational. For more on the use of effective proxies in the digital humanities, see <sup id="fnref:77"><a href="#fn:77" class="footnote-ref" role="doc-noteref">77</a></sup>.&#160;<a href="#fnref:22" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:23">
<p>Driscoll, Eric. (2019).  “Beazley’s Connoisseurship: Aesthetics, Natural History, and Artistic Development,”  in  <em>Dossier. Corps antiques: morceaux choisis</em> , edited by Florence Gherchanoc and Stéphanie Wyler (Paris: Éditions de l’École des hautes études en sciences sociales): 101-120, <a href="http://books.openedition.org/editionsehess/13689">http://books.openedition.org/editionsehess/13689</a>, accessed July 1, 2020.&#160;<a href="#fnref:23" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:23" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:24">
<p>Neer, Richard. (2005).  “Connoisseurship and the Stakes of Style,”    <em>Critical Inquiry</em> , 32, 1-26.&#160;<a href="#fnref:24" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:24" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:25">
<p>Smith, Tyler Jo. (2005).  “The Beazley Archive: Inside and Out,”    <em>Art Documentation: Journal of the Art Libraries Society of North America</em> , 24, 22-25.&#160;<a href="#fnref:25" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:25" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:26">
<p>Ryan, Adrian John. (January 2009).  “Computer Aided Techniques for the Attribution of Attic Black-Figure Vase-Paintings Using the Princeton Painter as a Model.”  PhD dissertation, University of Kwazulu-Natal, South Africa.&#160;<a href="#fnref:26" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:26" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:26" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref3:26" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref4:26" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref5:26" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref6:26" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:27">
<p>Beazley, John Davidson (1942).  <em>Attic Red-Figure Vase-Painters</em> , 3 volumes (Oxford, Clarendon Press).&#160;<a href="#fnref:27" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:28">
<p>Beazley, John Davidson (1956).  <em>Attic Black-Figure Vase-Painters</em>  (Oxford, Clarendon Press).&#160;<a href="#fnref:28" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:29">
<p>Beazley, John Davidson (1971).  <em>Paralipomena: Additions to Attic Black-Figure Vase-Painters and to Attic Red-figure Vase-painters</em>  (Oxford: Oxford University Press).&#160;<a href="#fnref:29" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:30">
<p>Portions of the Beazley archive are available online thanks to the University of Oxford’s Classical Art Research Centre, <a href="https://www.beazley.ox.ac.uk/">https://www.beazley.ox.ac.uk/</a>, accessed 2 July 2020.&#160;<a href="#fnref:30" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:31">
<p>“However, since the method proposed in this chapter is meant to be, to put it colloquially, an automation of the instinct of a skilled art historian rather than an objective method for identifying painters, artificial form sets by a skilled art historian are arguably more valuable [than other forms of data], since they better encode the intuition of the art historian”   <sup id="fnref6:26"><a href="#fn:26" class="footnote-ref" role="doc-noteref">26</a></sup>.&#160;<a href="#fnref:31" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:32">
<p>Beazley, John Davidson (1922).  “Citharoedus,”    <em>The Journal of Hellenic Studies</em>  42, 70-98.&#160;<a href="#fnref:32" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:33">
<p>For example, efficiency and effectiveness are crucial to the work at the intersection of computer science and medicine, see <sup id="fnref:78"><a href="#fn:78" class="footnote-ref" role="doc-noteref">78</a></sup>. Computer science also prizes efficiency and effectiveness as an independent field, see <sup id="fnref:79"><a href="#fn:79" class="footnote-ref" role="doc-noteref">79</a></sup>. There are also works at the intersection of the humanities and computer science that prize these characteristics, however we do not believe our project is subject to those same demands, see <sup id="fnref:80"><a href="#fn:80" class="footnote-ref" role="doc-noteref">80</a></sup>.&#160;<a href="#fnref:33" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:34">
<p>In recent years, the concept of slowness has undergone something of a revival among mindful practitioners in art history and the academy more broadly, see <sup id="fnref:81"><a href="#fn:81" class="footnote-ref" role="doc-noteref">81</a></sup>, <sup id="fnref:82"><a href="#fn:82" class="footnote-ref" role="doc-noteref">82</a></sup>, <sup id="fnref:83"><a href="#fn:83" class="footnote-ref" role="doc-noteref">83</a></sup>.&#160;<a href="#fnref:34" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:35">
<p>In the computing and information sciences, novel results are the coin of the realm, even serving as a major criterion for tenure and promotion. On this, see <sup id="fnref:84"><a href="#fn:84" class="footnote-ref" role="doc-noteref">84</a></sup> and <sup id="fnref:85"><a href="#fn:85" class="footnote-ref" role="doc-noteref">85</a></sup>. On the alignment of priorities and reward structures in interdisciplinary work, see <sup id="fnref:86"><a href="#fn:86" class="footnote-ref" role="doc-noteref">86</a></sup>.&#160;<a href="#fnref:35" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:36">
<p>As detailed in the workplan of Paul Rodriguez, the lead technologist/consultant on this project, we planned to,  “apply existing image processing techniques and procedures for segmentation and object border detection, current state-of-the art methods like convolution neural networks for object detection and classification, and open source tools for face detection and facial feature identification in order to divide [a dataset of] old-master paintings into a variety of segments”   <sup id="fnref:87"><a href="#fn:87" class="footnote-ref" role="doc-noteref">87</a></sup>&#160;<a href="#fnref:36" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:37">
<p>It did not pass by us unnoticed that this second approach returns, in a sense, to looking at the painting as a whole which holds an interesting parallel to taking in the general impression of a painting to classify/attribute it — an approach directly derided by Morelli.&#160;<a href="#fnref:37" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:38">
<p>Partial results from both Phase One and Phase Two can be found in <sup id="fnref:88"><a href="#fn:88" class="footnote-ref" role="doc-noteref">88</a></sup>.&#160;<a href="#fnref:38" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:39">
<p>For more on the dlib face detector, see <sup id="fnref:89"><a href="#fn:89" class="footnote-ref" role="doc-noteref">89</a></sup>. For more on OpenCV, see <sup id="fnref:90"><a href="#fn:90" class="footnote-ref" role="doc-noteref">90</a></sup>. For more on the Google Vision Detect Faces API, see <sup id="fnref:91"><a href="#fn:91" class="footnote-ref" role="doc-noteref">91</a></sup>.&#160;<a href="#fnref:39" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:40">
<p>Rodriguez, Paul. (January 17, 2020). Email Correspondence with the Team.&#160;<a href="#fnref:40" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:41">
<p>As a technical side note, in this project, summarization was used to ensure that Histogram of Oriented Gradients was implemented in a manner that was rotational invariant <sup id="fnref:92"><a href="#fn:92" class="footnote-ref" role="doc-noteref">92</a></sup>.&#160;<a href="#fnref:41" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:42">
<p>Relative here means that, for a three-channel RGB image, the sum of all pixels in each channel will consistently add to 1 and the sum of all pixels at any given image location will also add to 1 across channels <sup id="fnref:93"><a href="#fn:93" class="footnote-ref" role="doc-noteref">93</a></sup>.&#160;<a href="#fnref:42" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:43">
<p>Rodriguez, Paul. (January 12, 2020). Internal Team Report.&#160;<a href="#fnref:43" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:44">
<p>For more on t-SNE, please see [van der Maaten n.d.]. For a useful, accessible talk on the subject, see <sup id="fnref:94"><a href="#fn:94" class="footnote-ref" role="doc-noteref">94</a></sup>. For more on K-means clustering and machine learning, see <sup id="fnref:95"><a href="#fn:95" class="footnote-ref" role="doc-noteref">95</a></sup>.&#160;<a href="#fnref:44" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:45">
<p>Uglow, Luke. (2014).  “Giovanni Morelli and his Friend Giorgione: Connoisseurship, Science and Irony.”    <em>Journal of Art Historiography</em>  11 (December 2014): <a href="https://arthistoriography.files.wordpress.com/2014/11/uglow.pdf">https://arthistoriography.files.wordpress.com/2014/11/uglow.pdf</a>, accessed 2 July 2020.&#160;<a href="#fnref:45" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:45" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:46">
<p>Brewer, John. (2018).  “Giovanni Morelli,”   <a href="https://brewersblog.org/2018/01/28/giovanni-morelli/">https://brewersblog.org/2018/01/28/giovanni-morelli/</a>, accessed 11 May 2020.&#160;<a href="#fnref:46" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:46" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:47">
<p>Carlo Ginzburg also famously addresses the  “medical semiotics”  of Morelli’s work, alongside that of Sir Arthur Conan Doyle and Sigmund Freud, in <sup id="fnref1:76"><a href="#fn:76" class="footnote-ref" role="doc-noteref">76</a></sup>.&#160;<a href="#fnref:47" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:48">
<p>It may be more successful in the realm of graphic works, as suggested by <sup id="fnref1:49"><a href="#fn:49" class="footnote-ref" role="doc-noteref">49</a></sup>.&#160;<a href="#fnref:48" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:49">
<p>Elgammal, Ahmed, Yan Kang, and Milko Den Leeuw. (April 2018).  “Picasso, Matisse, or a Fake? Automated Analysis of Drawings at the Stroke Level for Attribution and Authentication.”    <em>Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)</em> , 42-50. <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17356/15669">https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17356/15669</a>, accessed June 29, 2020.&#160;<a href="#fnref:49" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:49" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:49" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref3:49" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:50">
<p><sup id="fnref2:49"><a href="#fn:49" class="footnote-ref" role="doc-noteref">49</a></sup>, <sup id="fnref:96"><a href="#fn:96" class="footnote-ref" role="doc-noteref">96</a></sup>, and <sup id="fnref:97"><a href="#fn:97" class="footnote-ref" role="doc-noteref">97</a></sup> all demonstrate possible roles that computers might take up in this ecosystem, and what may be at stake. Ahmed Elgammal is also a co-founder of Artrendex, LLC, a company that offers  “Art Trend Analytics: Innovative AI technology for the Art Market,”  which includes a product called  “Art Verified by AI: AI for Authentication,”   <a href="http://www.artrendex.com/">http://www.artrendex.com/</a>.&#160;<a href="#fnref:50" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:51">
<p>Carrier, David. (2003).  “In Praise of Connoisseurship.”    <em>The Journal of Aesthetics and Art Criticism</em> , 61, 159-69.&#160;<a href="#fnref:51" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:52">
<p>Wollheim, Richard. (1973).  “Giovanni Morelli and the Origins of Scientific Connoisseurship.”  In  <em>On Art and the Mind: Essays and Lectures</em> , by Richard Wollheim. London: Allen Lane, 177-201.&#160;<a href="#fnref:52" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:52" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:53">
<p>Grabar, Oleg. (1988).  “Between Connoisseurship and Technology: A Review.”    <em>Muqarnas</em> , 5, 1-8.&#160;<a href="#fnref:53" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:54">
<p>Subramanian, Samanth. (June 15, 2018).  “How to Spot a Perfect Fake: The World’s Top Art Forgery Detective.”    <em>The Guardian</em> . <a href="https://www.theguardian.com/news/2018/jun/15/how-to-spot-a-perfect-fake-the-worlds-top-art-forgery-detective">https://www.theguardian.com/news/2018/jun/15/how-to-spot-a-perfect-fake-the-worlds-top-art-forgery-detective</a>, accessed June 29, 2020.&#160;<a href="#fnref:54" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:55">
<p>For more on Orion Analytical, LLC, see <a href="http://orionanalytical.com/">http://orionanalytical.com/</a>.&#160;<a href="#fnref:55" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:56">
<p>Two of the three authors of a recent study that proposed to use automated analysis of drawings to determine their authenticity listed their primary affiliation as Artrendex, LLC, a startup that promises to use computers to verify works available on the market <sup id="fnref3:49"><a href="#fn:49" class="footnote-ref" role="doc-noteref">49</a></sup>.&#160;<a href="#fnref:56" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:57">
<p>For a truly cutting assessment of the anti-ethical disposition of the market for old master paintings, see <sup id="fnref:98"><a href="#fn:98" class="footnote-ref" role="doc-noteref">98</a></sup>.&#160;<a href="#fnref:57" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:58">
<p>We are indebted to Paul Jaskot for reminding us of this phrase originally used by the art critic Clement Greenberg <sup id="fnref:99"><a href="#fn:99" class="footnote-ref" role="doc-noteref">99</a></sup>. For the original, see <sup id="fnref:100"><a href="#fn:100" class="footnote-ref" role="doc-noteref">100</a></sup>.&#160;<a href="#fnref:58" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:59">
<p>“The conclusion generalizes. We should not delegate to reckoning systems, nor trust them with, tasks that require full fledged judgment - should not inadvertently use or rely on systems that, on the one hand, would need to have judgment in order to function properly or reliably, but that, on the other hand, utterly lack any such intellectual capacity”   <sup id="fnref:101"><a href="#fn:101" class="footnote-ref" role="doc-noteref">101</a></sup>.&#160;<a href="#fnref:59" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:60">
<p>The dictum, All models are wrong, but some are useful, is associated with the statistician George Box <sup id="fnref:102"><a href="#fn:102" class="footnote-ref" role="doc-noteref">102</a></sup>. On the utility of the (wrong but useful) models used in the Covid-19 pandemic, see <sup id="fnref:103"><a href="#fn:103" class="footnote-ref" role="doc-noteref">103</a></sup>. For more on the utilities of computational models within the humanities, see <sup id="fnref1:77"><a href="#fn:77" class="footnote-ref" role="doc-noteref">77</a></sup>.&#160;<a href="#fnref:60" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:61">
<p>Alberge, Dalya. (2019).  “Leonardo da Vinci expert declines to back Salvator Mundi as his painting.”    <em>The Guardian</em> , 2 June. Accessed 30 June 2020: <a href="https://www.theguardian.com/artanddesign/2019/jun/02/leonardo-da-vinci-expert-carmen-bambach-says-she-wont-back-salvator-mundi-as-his-painting">https://www.theguardian.com/artanddesign/2019/jun/02/leonardo-da-vinci-expert-carmen-bambach-says-she-wont-back-salvator-mundi-as-his-painting</a>&#160;<a href="#fnref:61" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:62">
<p>Freedberg, Sydney J. (1989).  “Some Thoughts on Berenson, Connoisseurship, and the History of Art.”    <em>I Tatti Studies in the Italian Renaissance,</em>  3, 11-26.&#160;<a href="#fnref:62" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:63">
<p>Freedberg, David. (2006).  “Why Connoisseurship Matters,”  in  <em>Munuscula amicorum Contributions on Rubens and his Colleagues in Honour of Hans Vlieghe</em> , edited by K. Van der Stighelen (Turnhout: Brepols), 29-43.&#160;<a href="#fnref:63" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:64">
<p>Melius, Jeremy. (2011).  “Connoisseurship, Painting, and Personhood,”    <em>Art History</em> , 34, 288-309&#160;<a href="#fnref:64" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:65">
<p>Opperman, Hal. (1990).  “The Thinking Eye, the Mind That Sees: The Art Historian as Connoisseur.”    <em>Artibus Et Historiae</em> , 11, 9-13.&#160;<a href="#fnref:65" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:66">
<p>Scallen, Catherine B. (2004).  <em>Rembrandt, Reputation, and the Practice of Connoisseurship</em>  (Amsterdam: Amsterdam University Press).&#160;<a href="#fnref:66" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:67">
<p>Summers, David. (1989).  “ Form,  Nineteenth-Century Metaphysics, and the Problem of Art Historical Description,”    <em>Critical Inquiry</em> , 15, 372-406.&#160;<a href="#fnref:67" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:68">
<p>Wollheim, Richard. (1979).  “Pictorial Style: Two Views,”  in  <em>The Concept of Style</em> , edited by Berel Lang (Philadelphia: University of Pennsylvania Press), 129-145.&#160;<a href="#fnref:68" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:69">
<p>Wollheim, Richard. (1995).  “Style in Painting,”  in  <em>The Question of Style in Philosophy and the Arts</em> , edited by Caroline Van Eck, James McAllister and Renée van de Vall (Cambridge: Cambridge University Press, 1995), 37-49.&#160;<a href="#fnref:69" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:70">
<p>Henri Zerner. (2014).  “What Gave Connoisseurship Its Bad Name? (1987),”  reprinted in  <em>Historical Perspectives in the Conversation of Works of Art on Paper</em> , edited by Margaret Hoben Ellis (Los Angeles: The Getty Conservation Institute), 59-61.&#160;<a href="#fnref:70" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:71">
<p>Seybold, Dietrich. (2016).  <em>The Giovanni Morelli Monograph</em> , 2016, <a href="http://www.seybold.ch/Dietrich/TheGiovanniMorelliMonograph">http://www.seybold.ch/Dietrich/TheGiovanniMorelliMonograph</a>, accessed 11 May 2020.&#160;<a href="#fnref:71" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:72">
<p>Morelli, Giovanni. (1890).  <em>Kunstkritische Studien über italienische Malerei; die Galerien Borghese und Doria Panfili in Rom.</em>  (Leipzig F.A. Brockhaus).&#160;<a href="#fnref:72" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:73">
<p>Vaughan, William. (1992).  “Automated Picture Referencing: A Further Look at  Morelli, ”    <em>Computers and the History of Art</em>  2, no. 2, 7-18.&#160;<a href="#fnref:73" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:74">
<p>Vaughan, William. (1987).  “The Automated Connoisseur: Image Analysis and Art History,”  in  <em>History and Computing</em> , edited by Peter Denley and Deian Hopkin (Manchester: Manchester University Press), 215-221.&#160;<a href="#fnref:74" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:75">
<p>Chun, Wendy Hui Kyong. (2011).  <em>Programmed Visions: Software and Memory</em>  (Cambridge: MIT Press).&#160;<a href="#fnref:75" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:76">
<p>Ginzburg, Carlo. (Spring 1980).  “Morelli, Freud and Sherlock Holmes: Clues and Scientific Method,”  translated by Anna Davin.  <em>History Workshop</em> , 9, 5-36.&#160;<a href="#fnref:76" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:76" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:77">
<p>Langmead, Alison and David Newbury. (2020).  “Pointers and Proxies: Thoughts on the Computational Modeling of the Phenomenal World.”  In  <em>The Routledge Companion to Digital Humanities and Art History</em> , edited by Kathryn Brown. London: Routledge, 358-373.&#160;<a href="#fnref:77" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:77" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:78">
<p>Register, Shilpa, Michelle Brown, and Marjorie Lee White. (2019).  “Using Healthcare Simulation in Space Planning to Improve Efficiency and Effectiveness within the Healthcare System,”    <em>Health Systems</em> , 8, no. 3 (2019): 184-189, <a href="https://doi.org/10.1080/20476965.2019.1569482">https://doi.org/10.1080/20476965.2019.1569482</a>.&#160;<a href="#fnref:78" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:79">
<p>Shin, Shin-Shing. (2019).  “Empirical Study on the Effectiveness and Efficiency of Model-Driven Architecture Techniques.”    <em>Software &amp; Systems Modeling</em> , 18, 3083-3096. DOI: <a href="https://doi.org/10.1007/s10270-018-00711-y">https://doi.org/10.1007/s10270-018-00711-y</a>.&#160;<a href="#fnref:79" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:80">
<p>Zlabinger, Markus. (2019).  “Improving the Annotation Efficiency and Effectiveness in the Text Domain.”  In  <em>Advances in Information Retrieval, ECIR 2019</em> , edited by L. Azzopardi, et al. Cham: Springer, 343-347. DOI: <a href="https://doi.org/10.1007/978-3-030-15719-7_46">https://doi.org/10.1007/978-3-030-15719-7_46</a>.&#160;<a href="#fnref:80" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:81">
<p>Berg, Maggie and Barbara K. Seeber. (2016). <em>The Slow Professor: Challenging the Culture of Speed in the Academy</em>  (Toronto, University of Toronto Press).&#160;<a href="#fnref:81" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:82">
<p>Roberts, Jennifer. (2013).  “The Power of Patience,”    <em>Harvard Magazine</em>  (November-December, 2013), 40-43. <a href="https://harvardmagazine.com/2013/11/the-power-of-patience">https://harvardmagazine.com/2013/11/the-power-of-patience</a>, accessed June 29, 2020.&#160;<a href="#fnref:82" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:83">
<p>Tishman, Shari. (2017).  <em>Slow Looking: The Art and Practice of Learning Through Observation</em> . Milton: Routledge.&#160;<a href="#fnref:83" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:84">
<p>Computing Research Association. (n.d.).  “Evaluating Computer Scientists and Engineers for Promotion and Tenure.”   <a href="https://cra.org/resources/best-practice-memos/evaluating-computer-scientists-and-engineers-for-promotion-and-tenure/">https://cra.org/resources/best-practice-memos/evaluating-computer-scientists-and-engineers-for-promotion-and-tenure/</a>, accessed June 29, 2020.&#160;<a href="#fnref:84" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:85">
<p>Luzón Marco, José. (2000).  “The Construction of Novelty in Computer Science Papers.”    <em>Revista Alicantina de Estudios Ingleses</em> , 13, 123-140. <a href="https://core.ac.uk/download/pdf/16358965.pdf">https://core.ac.uk/download/pdf/16358965.pdf</a>, accessed June 29, 2020.&#160;<a href="#fnref:85" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:86">
<p>Berg-Fulton, Tracey, Alison Langmead, Thomas Lombardi, David Newbury, and Christopher Nygren. (2018).  “A Role-Based Model for Successful Collaboration in Digital Art History.”    <em>International Journal for Digital Art History</em> , 3, 152-80. DOI: <a href="https://doi.org/10.11588/dah.2018.3.34297">https://doi.org/10.11588/dah.2018.3.34297</a>.&#160;<a href="#fnref:86" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:87">
<p>Rodriguez, Paul. (March 8, 2018).  “ECSS Workplan: The  Morelli Machine:  A Proposal Testing a Critical, Algorithmic Approach to Art History,”  n.p.&#160;<a href="#fnref:87" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:88">
<p>Rodriguez, Paul, Alan Craig, Alison Langmead, and Christopher J. Nygren. (2020).  “Extracting and Analyzing Deep Learning Features for Discriminating Historical Art.”  In  <em>Proceedings of the Practice &amp; Experience in Advanced Research Computing Conference [PEARC] 2020</em>  (Portland, OR) [in publication].&#160;<a href="#fnref:88" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:89">
<p>Ponnusamy, Arun. (April 17, 2018).  “CNN-Based Face Detector from dlib.”    <em>Towards Data Science</em> . <a href="https://towardsdatascience.com/cnn-based-face-detector-from-dlib-c3696195e01c">https://towardsdatascience.com/cnn-based-face-detector-from-dlib-c3696195e01c</a>, accessed June 29, 2020.&#160;<a href="#fnref:89" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:90">
<p>Rosebrock, Adrian. (September 24, 2018).  “OpenCV Face Recognition.”    <em>Pyimagesearch</em> . <a href="https://www.pyimagesearch.com/2018/09/24/opencv-face-recognition/">https://www.pyimagesearch.com/2018/09/24/opencv-face-recognition/</a>, accessed June 29, 2020.&#160;<a href="#fnref:90" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:91">
<p>Google,  “Face Detection,”    <em>Firebase</em> , last updated April 8, 2020. <a href="https://firebase.google.com/docs/ml-kit/detect-faces">https://firebase.google.com/docs/ml-kit/detect-faces</a>, accessed June 29, 2020.&#160;<a href="#fnref:91" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:92">
<p>Rodriguez, Paul. (March 9, 2020). Email Correspondence with the Team.&#160;<a href="#fnref:92" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:93">
<p>Rodriguez, Paul. (December 18, 2019). Internal Team Report.&#160;<a href="#fnref:93" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:94">
<p>van der Maaten, Laurens. (June 24, 2013).  “Visualizing Data Using t-SNE.”    <em>Google Tech Talks</em> , YouTube. <a href="https://youtu.be/RJVL80Gg3lA">https://youtu.be/RJVL80Gg3lA</a>, accessed June 29, 2020.&#160;<a href="#fnref:94" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:95">
<p>Trevino, Andrea. (December 6, 2016).  “Introduction to K-means Clustering.”    <em>Oracle AI and Data Science Blog</em> . <a href="https://blogs.oracle.com/datascience/introduction-to-k-means-clustering">https://blogs.oracle.com/datascience/introduction-to-k-means-clustering</a>, accessed June 29, 2020.&#160;<a href="#fnref:95" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:96">
<p>Ellis, Margaret Holben and C. Richard Johnson Jr. (2019).  “Computational Connoisseurship: Enhanced Examination Using Automated Image Analysis.”    <em>Visual Resources</em> , 35, 125-140. DOI: <a href="https://doi.org/10.1080/01973762.2019.1556886">https://doi.org/10.1080/01973762.2019.1556886</a>.&#160;<a href="#fnref:96" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:97">
<p>Floridi, Luciano. (2018).  “Artificial Intelligence, Deepfakes and a Future of Ectypes.”    <em>Philosophy &amp; Technology</em> , 31, 317-321.&#160;<a href="#fnref:97" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:98">
<p>Burke, Jill. (2020). In  “Decolonizing Art History,”  edited by Catherine Grant and Dorothy Price.  <em>Art History</em> , 43, 17-18. DOI: <a href="https://doi.org/10.1111/1467-8365.12490">https://doi.org/10.1111/1467-8365.12490</a>.&#160;<a href="#fnref:98" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:99">
<p>Jaskot, Paul. (2019).  “Digital Art History as the Social History of Art: Towards the Disciplinary Relevance of Digital Methods.”    <em>Visual Resources</em> , 35, 21-33.&#160;<a href="#fnref:99" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:100">
<p>Greenberg, Clement. (Fall 1939).  “Avant-Garde and Kitsch.”    <em>Partisan Review</em> , 6, 34-49.&#160;<a href="#fnref:100" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:101">
<p>Cantwell Smith, Brian. (2019).  <em>The Promise of Artificial Intelligence: Reckoning and Judgment</em> . Cambridge and London: MIT Press.&#160;<a href="#fnref:101" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:102">
<p>Box, George. (1979).  “Robustness in the Strategy of Scientific Model Building: Technical Report #1954.”  Madison, Wisconsin: Mathematics Research Center, University of Wisconsin-Madison. <a href="https://apps.dtic.mil/dtic/tr/fulltext/u2/a070213.pdf">https://apps.dtic.mil/dtic/tr/fulltext/u2/a070213.pdf</a>, accessed June 29, 2020.&#160;<a href="#fnref:102" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:103">
<p>Holmdahl, Inga and Caroline Buckee. (May 15, 2020).  “Wrong but Useful — What Covid-19 Epidemiologic Models Can and Cannot Tell Us.”    <em>The New England Journal of Medicine</em> . DOI: <a href="https://www.nejm.org/doi/full/10.1056/NEJMp2016822">https://www.nejm.org/doi/full/10.1056/NEJMp2016822</a>.&#160;<a href="#fnref:103" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry><entry><title type="html">Moving Cinematic History: Filmic Analysis through Performative Research</title><link href="https://rlskoeser.github.io/dhqwords/vol/15/1/000511/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://rlskoeser.github.io/dhqwords/vol/15/1/000511/</id><author><name>Jenny Oyallon-Koloski</name></author><author><name>Dora Valkanova</name></author><author><name>Michael J. Junokas</name></author><author><name>Kayt MacMaster</name></author><author><name>Sarah Marks Mininsohn</name></author><published>2021-03-05T00:00:00+00:00</published><updated>2021-03-05T00:00:00+00:00</updated><content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>The human body is a staggeringly intricate instrument. Filmmakers have developed meaningful, complex ways of staging figure movement on screen for storytelling purposes, both aided and impeded by the industrial, technological, and cultural changes that affect the conventions of film form. Despite significant developments in the digital humanities, the infinite variability and historical specificity of the body and cinematic space defy any single tool or method’s attempts to offer an automated, comprehensive categorization of figure movement in film. Much of the work in the field prioritizes quantitative and qualitative methods for observing and communicating research findings. What knowledge could we gain by integrating the researcher into a cinematic space, using digital tools to recreate the craft of dance and film form?</p>
<p>We argue for the value of motion capture-driven research that affords us such integration and moves audiovisual analysis in a performative direction. The  <em>Movement Visualization Tool</em>  (mv tool), a virtual research environment that generates live feedback of multiple agents’ movement, makes these lines of inquiry possible. Inexpensive, lightweight motion capture technology renders an abstracted skeleton of the moving agent, providing information about movement patterns and pathways through space using color-based and historical traceform filters. The tool also allows a virtually constructed mover and camera to interact through manual or algorithmic manipulation, replicating a mobile frame effect to study patterns of camera and figure movement. Researching in such a space is therefore inherently hands-on, interactive, and driven by the creation of new audiovisual content. Like the work of videographic practitioners who study and communicate their findings through the audiovisual medium, rather than in written form, this work applies performative research methods <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>  <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> to a study of movement on screen. We posit that through a practice-led embodiment of film and dance form we can better understand the formal implications of dance’s integration into cinematic space and the material conditions that affected filmmakers’ narrative and stylistic choices. By visualizing movement patterns with the mv tool and disseminating that video data as an essential part of our research output, we hope to offer alternative modes of scholarly dissemination that enriches humanistic observation of cinematic movement.</p>
<p>Two audiovisual case studies allow us to test the methodological value of this approach and deepen our understanding of moving image form. The first explores patterns from Laban/Bartenieff Movement Studies (subsequently LBMS) to observe the compatibility of this system’s expansive taxonomy of human movement with computerized analytical methods. By recording and manipulating LBMS’ Axis and Girdle movement scales, we can better perceive how those forms are rendered in a cinematic, rather than live, space. The second builds on this foundation and involves the recreation of dance sequences from two films,  <em>Top Hat</em>  (1935, Marc Sandrich) and  <em>Beau travail</em>  (1999, Claire Denis). By isolating the body and camera from their cinematic surroundings, we can recreate core staging and figure movement elements and can generate alternative camera and figure relationships. The result is a better understanding of these film sequences’ functions and the motivations that drove the filmmakers’ choices. A performative method combined with digital capture tools sharpens formal analysis, when studying figure movement on screen in particular. This approach enhances our ability to rigorously articulate the complexities of the human body in motion and deepens our understanding of cinematic production histories.</p>
<h2 id="mapping-performative-research-in-the-digital-humanities">Mapping Performative Research in the Digital Humanities</h2>
<p>Approaching our subject from a performative research paradigm helps us to further understand the stylistic and material components of dance in narrative filmmaking. Catherine Grant draws upon Brad Haseman in describing performative research as an approach in which symbolic data and material forms of practice &ldquo;work as utterances that accomplish, by their very enunciation, an action that generates effects&rdquo; <sup id="fnref1:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>  <sup id="fnref1:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. Practice is thus a critical aspect of performative research with the performative act serving as the data.</p>
<p>Scholars in a number of disciplines have engaged with methodologies of performative or practice-led research in order to approach their subject from a more dynamic perspective <sup id="fnref2:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>, formulate new research questions in audio-visual analysis <sup id="fnref2:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>  <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>  <sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>  <sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>, illuminate hidden or implicit characteristics of audiovisual texts <sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>  <sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>, and develop new pedagogical tools <sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>. We mobilize the methodological advantages of a practice-led approach through embodied, analytical recreations of figure movement in cinematic space and through an engagement with the material constraints of cinematic form.</p>
<h2 id="studying-movement-in-motion">Studying movement in motion</h2>
<p>Film and digital humanities scholars have created theoretical and historical models that expand our understanding of how bodies can manifest on screen <sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup>  <sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup>  <sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>  <sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>  <sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup>, while others have built digital tools that enable productive methods of study through the application of computerized methods <sup id="fnref1:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>  <sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup>  <sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup>. Substantial research outside of a cinematic context has explored the value of quantifying aspects of LBMS’s movement frameworks <sup id="fnref:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup>  <sup id="fnref:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup>  <sup id="fnref:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup>  <sup id="fnref:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup> and of using computerized methods to study figure movement <sup id="fnref:20"><a href="#fn:20" class="footnote-ref" role="doc-noteref">20</a></sup>  <sup id="fnref:21"><a href="#fn:21" class="footnote-ref" role="doc-noteref">21</a></sup>  <sup id="fnref:22"><a href="#fn:22" class="footnote-ref" role="doc-noteref">22</a></sup>  <sup id="fnref:23"><a href="#fn:23" class="footnote-ref" role="doc-noteref">23</a></sup>. However, this work rarely includes its movement data as part of the published output, reducing the reader’s ability to directly engage with the object of study’s dynamic nature or to evaluate the data generated by a performative research paradigm.</p>
<p>Our research method perceives the object of study as one in motion as well as in stasis, engaging with what Henri Bergson has called an &ldquo;intuitive&rdquo; knowledge of movement <sup id="fnref:24"><a href="#fn:24" class="footnote-ref" role="doc-noteref">24</a></sup> to better understand choreographic patterns and the myriad choices filmmakers make in staging the body for the camera. Carol-Lynne Moore sees close ties between Bergson’s categorizations of intellectual and intuitive perception and Rudolf Laban’s theories of Space from LBMS, which describes where the body moves in its environment. Whereas intellectual knowledge perceives movement as snapshots in time, an intuitive understanding perceives movement as a &ldquo;flowing continuity fluctuating endlessly&rdquo; <sup id="fnref:25"><a href="#fn:25" class="footnote-ref" role="doc-noteref">25</a></sup>. This approach to perception has phenomenological implications as well. Our ability to study past traceforms of movement through the mv tool’s historical traceform filter evokes the sort of &ldquo;thickness&rdquo; that Maurice Merleau-Ponty theorizes in his evocation of the accumulation of perceptual content <sup id="fnref:26"><a href="#fn:26" class="footnote-ref" role="doc-noteref">26</a></sup>. Embodying the movement is a key part of our analytical process; the videographic content presented in this article serves both as a record of the movement recreations and as a component of the research output.</p>
<p>It can further be argued that intuitive understanding is also an implicit aspect of videographic criticism, which allows practitioners to &ldquo;enter into&rdquo; their object of study <sup id="fnref1:24"><a href="#fn:24" class="footnote-ref" role="doc-noteref">24</a></sup>, in this case an audiovisual or filmic form. As Grant elaborates, in videographic criticism the researcher engages with the logic of the medium on its own terms in a way that encourages experimentation and play <sup id="fnref3:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. Jason Mittell similarly points to the transformative value of entering moving images through nonlinear editing, arguing, &ldquo;Even if you don’t make something new with the sounds and images imported into the editing platform, you can still discover something new by exploring a film via this new interface&rdquo; <sup id="fnref1:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>. Rather than working from existing audiovisual content, we posit that the act of recreating the movement phrases leads to a deeper understanding of movement patterns and, in the case of the filmic examples, of the formal practices that led to their creation.</p>
<h2 id="performing-constraints">Performing constraints</h2>
<p>Haseman positions this kind of practice-led approach as central to a performative paradigm, but he does not discuss the potential of considering choices and constraints as a part of such an approach. All creative practices are affected by constraints or obstacles of various kinds at both the macro and micro level of production: a director makes storytelling and aesthetic choices based on the budget available to them; a cinematographer chooses how to place and move the camera based on available technology; a choreographer can only work in a particular dance style (jazz, for example) if the surrounding cultural conditions have encouraged professional dancers to also train in that form. Filmmakers are constantly using craft practices, existing models and experience, and trial and error to guide their decision-making. As a result, stylistic history can benefit from what David Bordwell calls a problem-solution model of understanding the choices filmmakers make by envisioning stylistic history as a &ldquo;network of problems and solutions&rdquo; <sup id="fnref:27"><a href="#fn:27" class="footnote-ref" role="doc-noteref">27</a></sup>.</p>
<p>A practice-led approach, through the physical and intellectual act of recreation, encourages the analyst to engage with a similar level of material specificity. Indeed, as Bordwell argues, adopting a problem-solution model to understanding film history has recreation of practical details as an inherent aspect of its approach; the task &ldquo;is one of reconstruction. On the basis of surviving films and other documents, the historian reconstructs a choice situation&rdquo; <sup id="fnref1:27"><a href="#fn:27" class="footnote-ref" role="doc-noteref">27</a></sup>. Through a performative approach, the act is one of recreation rather than reconstruction. By directly engaging with the process, we end up with a tangible creation, informed by careful study of the original, that was subject to its own series of problems and solutions. The video and motion-capture recreations of LBMS scales and dance numbers published here, in turn, communicate research findings that complement our written analysis of those same movement examples. For example, in &ldquo;No Strings (Reprise)&rdquo; from  <em>Top Hat</em>  there is a movement where Fred Astaire shifts his weight into a chair and ostensibly falls asleep. In practicing the sequence for recreation with our dancers, we discovered that using a chair for the ending led to a more constrained and cautious movement quality, due to the light chair creating an unstable situation as the movers shifted weight onto it. Choreographing a graceful final posture for Astaire’s number resulted in a series of choices, perhaps an adjustment to the choreography, a change of furniture so that a lower armchair would permit Astaire to better counterbalance into it, or the addition of the carpet which mitigates most of the instability. We similarly needed to find solutions to the problem, but in our case, different furniture or securing the chair to the floor was not an option (i.e. the obvious solution of nailing the chair in place would certainly have infuriated the building owner and prevented our ability to continue working in the studio). Since our greatest priority was maintaining the spinning choreography, the feeling of lightness, and the increasing drowsiness of Astaire’s character, we chose to have the dancers shift into a final resting pose on the floor instead, which allowed us to more effectively maintain the movement qualities and the long line of the body seen in the final pose.</p>
<p>Methodologically, by conducting research in an audiovisual medium, our work will lead us to make choices and confront obstacles in parallel to the films we were recreating.<sup id="fnref:28"><a href="#fn:28" class="footnote-ref" role="doc-noteref">28</a></sup>  Catherine Grant touches on this issue in referencing concerns from the art-scholar Barbara Bolt, articulating that &ldquo;following Haseman, the problem for the &lsquo;performative’ (or creative) academic researcher can lie in recognising and mapping the effects or &rsquo;transformations’ that have occurred in their practice-research&rdquo; <sup id="fnref4:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. In our work, we choose to approach this problem as an opportunity. Incorporating an acknowledgment and understanding of constraints and choices into the performative research method leads to a more rigorous process and, as we found with our case studies, to new and valuable insights. It also helped us refine our understanding of the Kinect’s technological advantages and limitations in motion capture.</p>
<p>Our results were affected by the resources and technology available to us, and we made deliberate, informed decisions based on those practical constraints. In choosing to work primarily with motion-capture data over video, however, we are able to move beyond the gravitational and physical constraints of reality, and our research tool is specifically designed to modify the rendering of the captured data, allowing for experimentation with the relationship between figure and frame.</p>
<h2 id="technical-description-of-the-movement-visualization-tool">Technical Description of the Movement Visualization Tool</h2>
<p>The  <em>Movement Visualization Tool</em>  (mv tool) is a modular system for figure movement data capture that facilitates a variety of processes <em>.</em>  The modular nature of the system’s components allows for a dynamic architecture. We can extract or add modules as needed to achieve tasks concurrently or independently. Users can visualize the abstracted skeleton generated by the mv tool with the addition of various forms of visual and sonic feedback to better perceive the body’s movement through space. The users’ interaction with the skeleton and the visual/sonic filters can occur in a real-time (live) environment or can be recorded and manipulated for post-collection analysis. For the purposes of this research, the system can be broken down into three modules: data collection, data analysis and preprocessing, and visualization.</p>
<h2 id="data-collection">Data Collection</h2>
<p>The  <em>data collection</em>  module is a motion capture application that utilizes remote sensors to extract abstract movement data from users. Movement data is captured using the Microsoft Kinect V2, which we chose for its relative robustness-to-cost ratio, its portability, and its lightweight network protocol coupling capability, principally with Open Sound Control (OSC). The Kinect uses infrared cameras to generate depth maps, which are then internally transformed into a variety of different generalized representations, including a 25-joint skeleton frame. Using custom developed software applications <sup id="fnref:29"><a href="#fn:29" class="footnote-ref" role="doc-noteref">29</a></sup>, users can access the Kinect’s application programming interface and construct a 3-by-25  <em>skeleton sensor array</em>  for each body captured by the Kinect (i.e. the three spatial dimensions along horizontal/x, vertical/y, and sagittal/z planes for each of the 25 joints).</p>
<p>To increase resolution and/or range, multiple Kinect data collection stations can be added to the system, generating independent skeleton sensor arrays using the same module articulated above but in multiple instances. These arrays are then sent to the next module for data analysis and preprocessing (<a href="#figure01">Figure 1</a>).</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000511/resources/images/figure01.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000511/resources/images/figure01_hu9753b934eb33d52337c91fa67fb80b35_117498_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000511/resources/images/figure01_hu9753b934eb33d52337c91fa67fb80b35_117498_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000511/resources/images/figure01_hu9753b934eb33d52337c91fa67fb80b35_117498_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000511/resources/images/figure01.png 1275w" 
     class="portrait"
     ><figcaption>
        <p>Movement visualization tool infrastructure.
        </p>
    </figcaption>
</figure>
<h2 id="data-analysis-and-preprocessing">Data Analysis and Preprocessing</h2>
<p>The skeleton sensor arrays are sent from the data collection module into the  <em>data analysis and preprocessing</em>  module <em>,</em>  where they are prepared for the next series of modules within the system. The analysis and preprocessing is done with Max/MSP/Jitter <sup id="fnref:30"><a href="#fn:30" class="footnote-ref" role="doc-noteref">30</a></sup>, which we chose due to its signal-processing based computational design, native support for real-time interaction, compatibility with the chosen protocol (i.e. OSC), and the ability to easily create object oriented visualization filters.</p>
<p>The multiple skeleton sensor arrays from the Kinects can be synced at a constant frame rate, combining the multiple data streams into one representative  <em>skeleton frame,</em>  transforming them to generate the highest resolution and largest sensor capture range possible from the input skeleton sensor arrays. For this research, while three Kinects were used, comparable resolutions were achieved using the data from one Kinect. This was largely due to the asynchronous frame rate of data capture across the three Kinects, leading to a non-unified temporal dataset that could not simply be made into a higher resolution (e.g. a frame missing from one Kinect would not necessarily be at the same frame as the other Kinects, making corrective interpolation or substitution substantially more difficult). Due to this, the ultimate skeleton frame was composed from a singular sensor’s data capture.</p>
<p>The skeleton frame is then parsed into respective body arrays for individual, positional mover analysis, taking the first and second derivative of the skeleton joint positions, generating an approximate measure of  <em>velocity</em>  and  <em>acceleration</em> . The magnitude of these derivatives is calculated, generating an approximate measure of  <em>speed</em>  and the  <em>magnitude of acceleration.</em>  Using these physical representations, kinematic limitations can be placed on the skeleton frame, setting programmatic bounds modeled on physically possible human movement. This reduces jitter and discontinuity errors between frames, making a more realistic physical representation for the digital data stream.</p>
<p>From these bounded positions, kinematic measures and relative positions are measured and collected, providing an extension to the sensor’s positioning. These relative positions measure the dimensional difference of each joint from every other joint. On top of these physically limited relative positions, infinite impulse response filters (with dynamic coefficients that can be user defined) can be applied, weighting the current frame’s position by the previous frame’s position, creating a type of smoothing on the positions, which dampens jerky frame-to-frame movements, resulting in the  <em>preprocessed skeleton array,</em>  the main source of data for the rest of the system (<a href="#figure02">Figure 2</a>). In this research, the mv tool uses the preprocessed skeleton array to drive the visualization of mover and the relative camera position.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000511/resources/images/figure02.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000511/resources/images/figure02_hud2c333f432a45e07fcc4deaf6bc141eb_110251_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000511/resources/images/figure02_hud2c333f432a45e07fcc4deaf6bc141eb_110251_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000511/resources/images/figure02_hud2c333f432a45e07fcc4deaf6bc141eb_110251_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000511/resources/images/figure02.png 1460w" 
     class="landscape"
     ><figcaption>
        <p>The preprocessed skeleton array.
        </p>
    </figcaption>
</figure>
<h2 id="visualization">Visualization</h2>
<p>The mv tool visualization principally serves as an abstract joint skeleton representation that shows the subject’s movement in relationship to a virtual camera position. Purposefully eliminating other environmental contexts, this abstracted view focuses on what is being captured by the system, ultimately informing the direction of our research. Additionally, the mv tool allows for visual filters to be placed on the abstract skeleton, highlighting several opportunities for further analysis and understanding. For the purposes of this work, we apply three filters to provoke deeper research: dynamic camera positioning based on joint movement, historical joint traceforms, and spatial color maps.</p>
<p>The mv tool has the capability to dynamically change the position of the virtual camera in the digital space, operating independently or coupled with the subject it is &ldquo;filming.&rdquo; While users can manually control the three-dimensional digital camera position in the digital space, the camera movement can be tied to track a specific joint of the subject, moving in relation to the subject figure’s movement. For example, the camera can track the pelvic joints of the subject, moving in  <em>parallel</em>  or in  <em>counterpoint</em>  to the joint, allowing the subject’s pelvis to control the position of the camera as it moves (<a href="#figure03">Figure 3</a>).</p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://player.vimeo.com/video/409178465" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="LBMS dimensional scale demonstrating parallel pelvic movement." webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div>
<p>A historical joint trail filter can also be applied to the skeleton figure, leaving a dynamic history of the subject’s movement in the scene. The persistence of the traceforms in the scene can be manipulated by the user, ranging from no historical traceforms to the entire temporal record of the captured movement. This provides the user with a visual &ldquo;history,&rdquo; giving better insight into the paths and patterns of their movement (<a href="#figure04">Figure 4</a>).</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000511/resources/images/figure04.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000511/resources/images/figure04_hu5c7157c5afa1a4b6b7481e88abe062ca_361127_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000511/resources/images/figure04_hu5c7157c5afa1a4b6b7481e88abe062ca_361127_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000511/resources/images/figure04_hu5c7157c5afa1a4b6b7481e88abe062ca_361127_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000511/resources/images/figure04.png 1200w" 
     class="landscape"
     ><figcaption>
        <p>The mv tool&rsquo;s historical joint trail filter.
        </p>
    </figcaption>
</figure>
<p>Spatial color maps can also be added to the skeleton joints, providing insight into the trajectory of a given joint’s movement from an anchored point (<a href="#figure05">Figure 5</a>). For example, the trajectory of the right wrist from the spine mid can be colored in relation to its proximity to a three dimensional unit projection from the spine mid, providing the mover with a color map relating which of twenty-seven unit projections their movement aligns closest with (i.e. the complete set of permutation that can be made using only 1 and 0 in 3D).</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000511/resources/images/figure05.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000511/resources/images/figure05_hu5c7157c5afa1a4b6b7481e88abe062ca_117485_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000511/resources/images/figure05_hu5c7157c5afa1a4b6b7481e88abe062ca_117485_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000511/resources/images/figure05_hu5c7157c5afa1a4b6b7481e88abe062ca_117485_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000511/resources/images/figure05.png 1200w" 
     class="landscape"
     ><figcaption>
        <p>The mv tool&rsquo;s spatial color maps.
        </p>
    </figcaption>
</figure>
<p>The ability to manipulate recorded movement data in these ways allows the analyst to draw attention to different aspects of the body’s movement in space. These adjustments create a &ldquo;poeticized quantification&rdquo; of data that resembles videographic deformation methods articulated by Jason Mittell, which he argues can help scholars formulate new observations about an art object’s form <sup id="fnref2:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>. By algorithmically changing our visualizations of human movement, we can amplify certain observable patterns, posit hypothetical alternatives (in the relationship between camera and figure, especially), or simply marvel at the expansive complexities of the human body.</p>
<h2 id="case-study-labanbartenieff-movement-studies-axis-and-girdle-scales">Case study: Laban/Bartenieff Movement Studies (Axis and Girdle Scales)</h2>
<p>Laban/Bartenieff Movement Studies offers a detailed taxonomy to describe, analyze, and categorize forms of human movement and encourages the analyst to take a performative approach to their workflow by both observing and embodying movement patterns. Our first case study comes from the Space category of LBMS, which seeks to understand where the body moves to in its environment. This exploration considers in a more quantitative manner the changing shape of the Kinesphere, or &ldquo;the space within the reach of the body&rdquo; <sup id="fnref:31"><a href="#fn:31" class="footnote-ref" role="doc-noteref">31</a></sup>.<sup id="fnref:32"><a href="#fn:32" class="footnote-ref" role="doc-noteref">32</a></sup>  Rudolf Laban theorized a series of movement scales designed to increase the body’s range of motion and encourage the exploration of movement through particular pathways. Like musical scales, they are designed with a theoretical rigor, follow a series of guidelines based on the necessary space between each directional pull (frequently referred to in LMBS as Spatial Pulls), and are repeatable.</p>
<p>The Space category taxonomizes multiple geometric elements to describe the body’s orientation in space <sup id="fnref1:31"><a href="#fn:31" class="footnote-ref" role="doc-noteref">31</a></sup>:</p>
<ul>
<li>Directionality, or Spatial Pulls, of movement as articulated along the three dimensions (Vertical, Horizontal, Sagittal)</li>
<li>The shape of the pathways with which the body moves from point to point (Central, Peripheral, or Transverse)</li>
<li>The position of movement relative to the body center (Near Reach Space to Far Reach Space)</li>
<li>The height (Low, Mid-height, High) of the leading movement</li>
</ul>
<p>Space draws on Laban’s research into theories of platonic solids, leading him to argue for twenty-seven dominant Spatial Pulls or directional rays <sup id="fnref2:31"><a href="#fn:31" class="footnote-ref" role="doc-noteref">31</a></sup>. Six pulls form the basis for axial movement (up, down, forward, back, right, left), rays that when plotted on a graph represent the end points of an octahedron. When sequenced together, these six pulls form the dimensional scale, which isolates each of the three axes (Vertical, Horizontal, Sagittal) and demonstrates the relative stability of this range of motion (<a href="#figure06">Figure 6</a>).</p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://player.vimeo.com/video/409182153" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="A visualization of the Dimensional scale." webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div>
<p>Laban theorized that all remaining Spatial Pulls are the result of these six fundamental pulls combining in both unequal and equal combinations. Twelve rays form the basis for planar movement; the three planes — horizontal, vertical, and sagittal — each contains four Spatial Pulls. Laban theorized movement between those twelve rays as existing in an icosahedral space. Combining all three Dimensional Pulls equally leads to highly mobile movement, which Laban saw as moving between the endpoints and diagonals of a cube: Right-Forward-High, Left-Back-Low, Left-Forward-High, Right-Back-Low, Left-Back-High, Right-Forward-Low, Right-Back-High, Left-Forward-Low. To these twenty-six rays or Spatial Pulls we add Place, Laban’s articulation of a relatively neutral starting and ending position for scales, in which the figure is standing with the arms relaxed by their side and the feet planted in parallel.</p>
<p>The two scales we chose to visualize come from movement in an icosahedral space, the Axis scale and the Girdle scale. These forms are paired and complementary; each scale includes six points, none of which are repeated in the other scale’s movement. As a result, the two scales together move the body through all twelve theorized planar directions. Both scales also interact with the Right-Forward-High to Left-Back-Low diagonal of the cube. The Axis scale moves through Transverse pathways that deflect off of the diagonal without ever crossing it. Its Spatial Pulls are as follows (<a href="#figure07">Figure 7</a>):</p>
<p>Right-High (vertical), Back-Low (sagittal), Right-Forward (horizontal), Left-Low (vertical), Forward-High (sagittal), Left-Back (horizontal)</p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://player.vimeo.com/video/" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="A visualization of the Axis scale." webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div>
<p>In contrast, the Girdle scale, which Laban calls &ldquo;a chain of six surface-lines&rdquo; <sup id="fnref3:31"><a href="#fn:31" class="footnote-ref" role="doc-noteref">31</a></sup> moves in a Peripheral pathway that orbits the diagonal, as follows (<a href="#figure08">Figure 8</a>):</p>
<p>Left-High (vertical), Back-High (sagittal), Right-Back (horizontal), Right-Low (vertical), Forward-Low (sagittal), Left-Forward (horizontal)</p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://player.vimeo.com/video/" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="A visualization of a repeating Girdle scale." webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div>
<p>Each scale results in a different gestalt movement quality and progression through pathways. As the Axis scale deflects off of the diagonal, its movement has more of a swinging feel as the body moves back and forth between High and Low, Forward and Back, and Right (open) and Left (closed). The Girdle scale, through its peripheral movement, creates a pathway that maintains a consistent distance from the body center and moves through adjacent icosahedral Spatial Pulls, creating a smoother, circular quality in the body <sup id="fnref4:31"><a href="#fn:31" class="footnote-ref" role="doc-noteref">31</a></sup>  <sup id="fnref1:25"><a href="#fn:25" class="footnote-ref" role="doc-noteref">25</a></sup>.</p>
<p>Defining twenty-seven possible directions for the body to move is inherently limiting. Laban writes that &ldquo;innumerable directions radiate from the centre of our body and its kinesphere into infinite space&rdquo; <sup id="fnref5:31"><a href="#fn:31" class="footnote-ref" role="doc-noteref">31</a></sup>. However, these limitations and geometric parallels are beneficial to a computerized visualization of movement, in which we must define each possible zone of movement. This subdivision provided the spatial segmentation for the mv tool’s color filter, in particular, which aligns closely with the maximum number of distinctions our color filter could render (<a href="#figure09">Figure 9</a>).</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000511/resources/images/figure09.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000511/resources/images/figure09_huefd4841c2c8a01dd412cd2d9cd43b369_2852254_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000511/resources/images/figure09_huefd4841c2c8a01dd412cd2d9cd43b369_2852254_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000511/resources/images/figure09_huefd4841c2c8a01dd412cd2d9cd43b369_2852254_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000511/resources/images/figure09_huefd4841c2c8a01dd412cd2d9cd43b369_2852254_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/1/000511/resources/images/figure09_huefd4841c2c8a01dd412cd2d9cd43b369_2852254_1800x0_resize_box_3.png 1800w,/dhqwords/vol/15/1/000511/resources/images/figure09.png 3763w" 
     class="landscape"
     ><figcaption>
        <p>Our chart reference for the mv tool&rsquo;s color filter, annotating Laban&rsquo;s drawing which also posits twenty-seven directional rays [^laban2011a].
        </p>
    </figcaption>
</figure>
<p>When moving through a LBMS scale, one body part initiates and leads the movement through the various directional Pulls. Our recordings demonstrate these movement pathways as led by the right hand in a Far-Reach movement pattern for greatest visibility, but one can explore these sequences led from any body part (the right hip, the left big toe, the tongue) and at a bigger or more restrained scale. In all recordings presented in this article (LBMS scales and dance recreations), the color changes are dictated by the spatial position of the right wrist, with the color changes affecting the entire skeleton. Like all the filters built for the mv tool, the body part dictating the color change can be moved to any of the skeleton’s twenty-five recorded points.</p>
<p>The color filters provide data on the spatial zones that are most activated by a recorded movement phrase, as is visible in the motion-capture data for the Axis and Girdle scales with added color changes (<a href="#figure10">Figure 10</a> and <a href="#figure11">Figure 11</a>):</p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://player.vimeo.com/video/" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="A visualization of the Axis scale with color filters." webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://player.vimeo.com/video/375558656" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="A visualization of the Girdle scale with color filters." webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div>
<p>What becomes immediately apparent are the limitations of translating Laban’s theorization of twenty-seven Spatial Pulls into a computerized environment, as the color changes are not restricted simply to the six colors corresponding with the scale Spatial Pulls. Instead, the software recognizes any of the twenty-seven zones activated by the movement of the right wrist. Similarly, the starting position of Place, with the arms relaxed at one’s sides, renders as the color for Right-Low (a dark orange) rather than Place (grey), given that the wrist is below the body center. In other work, where establishing a place of neutrality is necessary, we adjust the starting point to have the right wrist resting on the sternum. This additional data, however, is valuable. In the color visualization of the Axis scale, we can observe the skeleton activating approximately fourteen color points, whereas the Girdle scale activates approximately eight. This aligns with the complementary pathways of the two scales; while the Girdle scale moves between spatially adjacent points (Peripheral pathways), the Axis scale uses Transverse pathways to slice obliquely through the Kinesphere to get to each subsequent Spatial Pull, activating additional space in the process. This contrast is visible through the addition of historical traceforms filter, in which the greater smoothness of the Girdle pathways is apparent (<a href="#figure12">Figure 12</a> and <a href="#figure13">Figure 13</a>):</p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://player.vimeo.com/video/375558975" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="A visualization of the Axis scale with color and historical trail filters." webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://player.vimeo.com/video/" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="A visualization of the Girdle scale with color and historical trail filters." webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div>
<p>What also becomes apparent from these visualizations is that the recommended technique for performing LBMS scales is ideal for a Kinect-driven motion-capture environment. While the torso, arms, and legs frequently need to twist and reach away from a forward facing position to move through the Spatial Pulls or counterbalance the movement, the pelvis should remain frontal and forward-facing, encouraging mobility of the spine and limbs. As a result, the body center is always facing forward. Some scales, like the Girdle scale, are more easily performed by taking a few steps, but the scales are always fairly spatially contained, and often result in a relatively stationary practice in which weight is shifted between the two feet. Finally, performing LBMS scales rarely requires the mover to lie prone on the ground. All three of these conditions — a consistently forward facing, a limited range of locomotion, and standing movement patterns — are ideal for this technological infrastructure. They facilitate the Kinects’ ability to maintain a consistent skeleton rendering, minimizing the quantity of glitching and increasing the accuracy of movement pathways. Determining these conditions helped in the selection process of our cinematic movement examples.</p>
<h2 id="case-study-dance-in-narrative-cinema">Case study: Dance in narrative cinema</h2>
<p>This case study seeks to better understand how filmmakers use figure and camera movement to guide viewer attention and communicate narrative and aesthetic meaning. In selecting examples to analyze, our decision-making process was heavily influenced by technology and infrastructure. We needed to work with solo dancers, as recording multiple performers simultaneously is more difficult with the Kinects, especially if those performers cross paths or touch during the number. In order to more closely recreate the relationship between camera and figure, we chose to work with sequences that maintained a relatively consistent camera position and that included minimal editing. We also did not have a studio space large enough to capture more expansive dancing, or movement that travelled through a wider space accompanied by a follow shot.</p>
<p>Our data capture approach differed for the LBMS material and the film material. As Oyallon-Koloski is certified in Laban Movement Analysis and familiar with the LBMS movement scales, she performed those phrases herself. The scales are theoretical movement exercises without an existing audiovisual referent, so no filmic comparison was necessary. In contrast, a recreated, close approximation of the choreographed movement from our filmic examples was a central component of our analytical process for studying these dance phrases, for which more specialized dance training and rehearsal time was necessary. As a result, the dance recreations were learned and performed by two Dance MFA students at the University of Illinois, Urbana-Champaign, Catherine MacMaster and Sarah Mininsohn. MacMaster has technical training in ballet, contemporary, and modern dance forms, with a secondary focus in tap dancing, musical theater jazz, West African dance, Afro-Cuban Folkloric Dance, and Tango. Mininsohn has technical training in modern dance, improvisation, and ballet, with a choreographic focus on contemporary and improvisational styles.</p>
<p>Because Oyallon-Koloski’s broader research interests focus on musical cinema, we chose to include one number that allowed us to study how these stylistic patterns affected the musical number’s relationship to the larger narrative. MacMaster and Mininsohn collaborated with Oyallon-Koloski in the selection of appropriate dance phrases given their movement backgrounds. In selecting a number from a Hollywood musical, the most compatible numbers all came from the 1930s, with solo tap dancing numbers emerging as the most logical choice. We chose to work with Fred Astaire’s soft-shoe reprise of  “No Strings (I’m Fancy Free)”  from  <em>Top Hat</em>  (1935), an iconic number that our dancers felt was compatible with their movement training. For our second example, we chose the final dance sequence from Claire Denis’  <em>Beau travail</em>  (1999). Its construction similarly met our criteria — a single dancer (Denis Lavant), a restricted setting, a single camera set-up, and limited editing — but provided us with an example that contrasted  <em>Top Hat</em> ’s technological, industrial, and cultural context. Despite sharing numerous stylistic characteristics — an emphasis on a single performer who in both cases is white and male, a relatively static camera, a longer average shot rate, few overall edits, and long shot framings on the dancers — the choreography and overall function of each number diverge, as do their production histories and choreographic processes.</p>
<p>For both sequences, the dancers’ focus was on understanding the holistic staging patterns of the choreography, the essential movement qualities (with the language of Effort qualities from LBMS guiding much of our analysis), and the narrative motivation of the dance. One significant revelation from this work was the unusual nature of the form of movement learning; despite their extensive movement training, neither had ever learned choreography before purely from an audiovisual artifact. Choreographic recreations frequently benefit from video recordings of previous performances but are led by movement professionals who have often performed the movement themselves and who re-learn and teach the choreography to the performers. Because such prior knowledge was not available to us, MacMaster and Mininsohn learned the choreography together, in consultation with Oyallon-Koloski, working from a recording of the dance sequence flipped on the horizontal to facilitate the process of learning the steps and pathways on the correct side. Both MacMaster and Mininsohn recorded multiple takes of each movement phrase, improvising or resetting their staging position during the moments when the performer in question (i.e. Fred Astaire and Denis Lavant) was not on screen. In order to allow the dancers to focus on different aspects of narrative, staging, and choreographic detail in each take, during several they performed alongside a playback of the filmic dance, while in others they performed alongside the audio track from the film only. The video recordings, shot on a Canon EOS R, approximate the camera placement but do not fully replicate the camera movement of the originals, as the dance recreations vary somewhat as well, and serve primarily as documentation of the stylistic analysis performed by our dancers.</p>
<h2 id="no-strings-reprise----_top-hat_--1935">“No Strings (Reprise)”  |  <em>Top Hat</em>  (1935)</h2>
<p><em>Top Hat</em>  is directed by Marc Sandrich, photographed by David Abel, and choreographed collaboratively by Fred Astaire and Hermes Pan. It was the first film for which Pan served as dance director <sup id="fnref:33"><a href="#fn:33" class="footnote-ref" role="doc-noteref">33</a></sup>. Pan, not Sandrich, likely worked with Abel on the camerawork for the dance numbers and had decision-making power over the integration of the figure movement into the cinematic space. Constantine, a writer for  <em>Dance Magazine</em>  who interviewed Pan in 1945, suggests that the dance director’s job is to  “focus the camera on the most important part of the choreography and swing the camera in rhythm with the dancers”   <sup id="fnref1:33"><a href="#fn:33" class="footnote-ref" role="doc-noteref">33</a></sup>. Patrick Keating discusses this relationship with more detail in his discussion of the  “No Strings (I’m Fancy Free)”  number, in which  “the camerawork remains completely subordinate to Jerry’s [Astaire’s] movements. When Jerry dances behind some chairs, the camera dollies in to follow; when Jerry spins to the right, a pan keeps him in frame”   <sup id="fnref1:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>. Astaire had a reputation for remaining closely involved in all the stylistic elements of his dances and for wanting the camera to be an &ldquo;involved but unobtrusive spectator&rdquo; <sup id="fnref:34"><a href="#fn:34" class="footnote-ref" role="doc-noteref">34</a></sup>, a &ldquo;subservient&rdquo; form of camerawork that was nonetheless the result of a complex choreographic process <sup id="fnref2:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>.</p>
<p>This reprise occurs very soon in the plot after Jerry Mulligan’s (Astaire) &ldquo;No Strings (I’m Fancy Free),&rdquo; in which his enthusiastic tap dancing awakens and irritates Gale Tremont (Ginger Rogers). Immediately smitten with Gale, he performs a soft-shoe &ldquo;sand-man&rdquo; version of the number to help lull her back to sleep, as Edward Everett Horton’s character (Horace Hardwick) observes him. Jerry’s motivation, therefore, is to perform a soothing number that stylistically contrasts its bombastic predecessor, but his movement quality is also the result of his newly discovered feelings for Gale. In preparing the recreation, we focused on the sense of lift in Astaire’s physicality (Light Weight), particularly in the upper body, paired with a core stability evoking his ballroom training that results in him skimming the surface of the floor. MacMaster and Mininsohn also embody the deliberate weight shifting steps coming from a tap dance vocabulary that move him through both swooping lateral staging changes and circular pathways. In our recreations, MacMaster performed in tap shoes and emphasized the formal steps in her learning process given that she has tap dance training; Mininsohn, drawing on her improvisation training, performed barefoot and prioritized the movement qualities and their relationship to narrative motivations (<a href="#figure14">Figure 14</a>, <a href="#figure15">Figure 15</a>, <a href="#figure16">Figure 16</a>, and <a href="#figure17">Figure 17</a>).</p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://player.vimeo.com/video/" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="&#34;No Strings (Reprise)&#34; video documentation performed by Sarah Mininsohn, with side-by-side comparison of the original." webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://player.vimeo.com/video/" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="&#34;No Strings (Reprise)&#34; video documentation performed by Catherine MacMaster, with side-by-side comparison of the original." webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://player.vimeo.com/video/" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="Visualization of &#34;No Strings (Reprise)&#34; performed by Sarah Mininsohn." webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://player.vimeo.com/video/" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="Visualization of &#34;No Strings (Reprise)&#34; performed by Catherine MacMaster." webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div>
<p>The choreography for this number involves numerous turns and pivots that move the figure away from a frontal facing and an ending weight shift into a chair which, as discussed above, our dancers performed into the ground. Both proved difficult for the Kinect to render properly; the latter results in some glitching rather than a clean line of the body, and the rotations are more difficult to perceive in the abstracted space. This points to the Kinect’s design for home gaming use, which is adapted for standing (or sitting), front-facing postures without much simultaneous movement of the limbs across the midline. However, adding the historical traceforms to Mininsohn’s recorded skeleton allows us to perceive the graceful pathways of Astaire’s movement, the Free Flowing movement that skims the surface, and the choreographic emphasis on repeated pathways and continuous motion (<a href="#figure18">Figure 18</a>).</p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://player.vimeo.com/video/" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="Visualization of &#34;No Strings (Reprise)&#34; with historical traceforms, performed by Sarah Mininsohn." webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div>
<p>We also learn from a closer analysis of the &ldquo;No Strings (Reprise)&rdquo; number that one of the camera movements during the dancing is the result of Horton’s, as well as Astaire’s, movement. Jerry’s dancing in the beginning of the number moves him repeatedly through the horizontal space of the frame, but it isn’t until Horace walks rightward towards the couch as Jerry also slides right in front of him that the camera pans slightly right to follow, ensuring that Horace remains fully visible as he sits on the couch. The camera remains static until the end of the dance, even though Astaire’s leftward pivot turns in the middle of the dance briefly cause his arms to go out of frame. By adding our mobile frame filter to keep the skeleton centered in the frame, we can visualize what Astaire’s phrase would have looked like had the camera followed all of his lateral shifts; here the body element maintained in the center of the frame is the skeleton’s pelvis (<a href="#figure19">Figure 19</a>).</p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://player.vimeo.com/video/" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="Visualization of &#34;No Strings (Reprise)&#34; with mobile framing, performed by Sarah Mininsohn." webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div>
<p>This goes against the logical assumption that the dancers on-screen would dictate figure-motivated camera movement, as both Constantine and Keating articulate. Yet such a choice on Pan and Abel’s part does not necessarily contradict this rationale. The lack of reframing to the left ensures that Horace remains visible on the right and reinforces that the number is serving a crucial narrative purpose, one that is guiding the filmmakers’ stylistic decisions as much as the impulse to clearly capture Astaire’s artistry on-screen. Having Jerry perform the number is not only for the sake of aesthetic excess (as a demonstration of Astaire’s physical prowess) but also to make amends for disturbing Gale. In the process of the number, Horace also watches the soothing dancing and is lulled to sleep before Jerry succumbs to sleep himself. Keeping Horace in frame during the dancing allows us to see that he is watching Jerry dance, and a later cut to a closer framing of him looking tired cues us to watch the background as Jerry continues to dance, where we can observe Horace put his head down on the couch. Even in the earlier &ldquo;No Strings&rdquo; number, the choreographic and cinematographic decisions are made to either keep Astaire dancing close enough to Horton so that the latter remains in frame or have Astaire’s dancing carry him far enough through lateral space that Horton does not awkwardly reappear at unexpected moments in the background. Ultimately, the camera helps to emphasize the most important function of the choreography: narrative comprehension. Recreating and manipulating the movement for the purposes of formal analysis enhances our, and hopefully the reader’s, ability to observe these choreographic and cinematic patterns.</p>
<h2 id="galoups-final-dance-sequence---_beau-travail_--1999">Galoup’s final dance sequence |  <em>Beau travail</em>  (1999)</h2>
<p><em>Beau travail</em> , loosely adapted from Herman Melville’s  <em>Billy Budd</em> , is directed by Claire Denis and photographed by Denis’ long-time collaborator Agnès Godard. The film’s narrative portrays much choreographed movement in its focus on French Foreign Legion soldiers stationed in Djibouti, and choreographer Bernardo Montet’s creative contributions were essential to the film’s preproduction design <sup id="fnref:35"><a href="#fn:35" class="footnote-ref" role="doc-noteref">35</a></sup>.  <em>Beau travail</em>  includes several dance scenes between local Djibouti women and the Legionnaires but also emphasizes performances of military and domestic exercises, blurring the line between dance, military calisthenics, and pedestrian movement. The film relies little on dialogue or explicit narration, forcing the viewer to read the implicit meaning of the figure movement; Judith Mayne describes the film as a &ldquo;kind of choreographed ritual&rdquo; <sup id="fnref1:35"><a href="#fn:35" class="footnote-ref" role="doc-noteref">35</a></sup>. The protagonist Galoup’s final solo dance occurs after the character’s presumed suicide, which allows the viewer to read the motivation of that final dance in numerous ways: as a frenetic but ultimately futile attempt at escape <sup id="fnref2:35"><a href="#fn:35" class="footnote-ref" role="doc-noteref">35</a></sup>, as an example of the post-colonial body <sup id="fnref:36"><a href="#fn:36" class="footnote-ref" role="doc-noteref">36</a></sup>, as a marker of queer displacement <sup id="fnref:37"><a href="#fn:37" class="footnote-ref" role="doc-noteref">37</a></sup>. Denis viewed the scene as an attempt at freedom; while initially planning to include the number before the suicide, she changed the plot order to &ldquo;give the sense that Galoup could escape himself&rdquo; but also because after filming the number she realized it was the stronger ending <sup id="fnref:38"><a href="#fn:38" class="footnote-ref" role="doc-noteref">38</a></sup>. Our close analysis through recreation of the film’s final dance scene is equally revelatory in its emphasis on the explicit diegetic details present and the craft mechanics of the scene.</p>
<p>Montet was closely involved with  <em>Beau travail</em> ’s narrative development in preproduction, but his influence on Galoup’s final dance sequence, performed by Denis Lavant, is unclear. Denis recalls the number’s process in an interview with  <em>Senses of Cinema</em> :</p>
<p>But we never rehearsed the dance scene at the end of  <em>Beau Travail</em> . I told him [Denis Lavant] it’s the dance between life and death. It was written like that in the script, and he said, &lsquo;What do you mean by &ldquo;the dance between life and death&rdquo;?’ So, I let him hear that great disco music [ <em>laughs</em> ], and he said, &lsquo;This is it.’ So, we didn’t need to rehearse. . . . He said, &lsquo;You don’t want us to fix some of it?’ I thought it was better to keep the energy inside, because if we started fixing some stuff then we would have made many takes. And we made one take. But he was exhausted at the end&quot; <sup id="fnref:39"><a href="#fn:39" class="footnote-ref" role="doc-noteref">39</a></sup>.</p>
<p>Lavant already had extensive movement training at the time of filming, with a background in circus and pantomime, so it is plausible that he created the number without Montet’s assistance. Denis’ comment about a lack of rehearsal and a single take seems suspect but indicates that an improvised approach to avoid an over-rehearsed look was key to the process. Most accounts of this number describe the dancing as frenetic, but upon closer analysis we can observe that Lavant’s movement is in fact quite graceful, with increasingly controlled and complex movements originating from the body center (the result of core stability and alternating Bound and Free Flow) and reverberating into the limbs. Like Astaire’s movement, much of Lavant’s movement patterns are driven by an upward impulse and a feeling of lift, but with a greater sensation of strength (Strong Weight) in contrast to Astaire’s lightness. Unlike the fluidity and ongoing nature of the &ldquo;No Strings&rdquo; number, Lavant frequently starts and stops. The spatial relationship of camera and figure also creates an off-kilter feeling. Rather than film Lavant straight-on, the camera remains to his left (likely to avoid revealing the camera in the mirrored wall). His gaze and facing frequently focuses leftward as well, as if he is intrigued by something off-screen left that the viewer cannot see. In recreating this number, MacMaster and Mininsohn focused on this sense of increasing and halting range of motion, as if in the afterlife Galoup is discovering an ability to fully express himself for the first time. They embodied the specific pathways and gestures Galoup employs in the first half of the number (including his smoking) but chose to improvise the more expansive movement of the second half (<a href="#figure20">Figure 20</a>, <a href="#figure21">Figure 21</a>, <a href="#figure22">Figure 22</a>, and <a href="#figure23">Figure 23</a>).</p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://player.vimeo.com/video/" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="Galoup&#39;s final dance documentation performed by Sarah Mininsohn, with side-by-side comparison of the original. For fair use purposes, an extract and limited audio are included." webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://player.vimeo.com/video/" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="Galoup&#39;s final dance documentation performed by Catherine MacMaster, with side-by-side comparison of the original. For fair use purposes, an extract and limited audio are included." webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://player.vimeo.com/video/" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="Visualization of Galoup&#39;s final dance performed by Sarah Mininsohn." webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://player.vimeo.com/video/" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="Visualization of Galoup&#39;s final dance performed by Catherine MacMaster." webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div>
<p>Denis’ film does not fall within the confines of the musical genre. But the space of Galoup’s final dance feels more liminal or beyond the limits of the established diegesis the way many musical sequences would, an impression that comes from the disparate stylistic choices of staging, choreography, cinematography, and sound. Unlike the earlier club scenes, where we hear French Creole and Turkish popular music, Galoup here performs to Corona’s Eurodance hit, &ldquo;Rhythm of the Night.&rdquo; Previously we see Legionnaires and local Djibouti women dancing, but now Galoup is alone (with his mirror self). As the narrative progresses, the camera allows us to see more of the movement by adopting more distant framings. The early disco scene is shot at close proximity, allowing us to see the men and women dancing together in medium close-ups. The second disco scene in which the women dance as Galoup watches shows them in a medium long shot. It is only during Galoup’s final number where we see him dance in a full long shot, and even then we frequently lose his extremities as they extend beyond the edges of the frame. Adding historical traceforms allows us to perceive the increasing expansiveness of Galoup’s movement and release, especially after his leap to the ground (2:08 in <a href="#figure24">Figure 24</a>&rsquo;s video).</p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://player.vimeo.com/video/" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="Visualization of Galoup&#39;s final dance with historical traceforms, performed by Sarah Mininsohn." webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div>
<p>Godard’s camera in  <em>Beau travail</em> ’s final number never dollies through the space, but she frequently pans and tilts to holistically follow Lavant’s movement through the small space of the disco. A visualization of the movement with the addition of a parallel mobile camera (that reframes along the horizontal, sagittal, and vertical planes) takes this follow aesthetic to the extreme (<a href="#figure25">Figure 25</a>).</p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://player.vimeo.com/video/" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="Visualization of Galoup&#39;s final dance with mobile framing, performed by Sarah Mininsohn." webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div>
<p>A perfectly aligned, parallel camera removes the feeling of spontaneity from the number. While Godard’s cinematography calmly and subtly reframes as Lavant moves through the space, parts of Lavant’s body occasionally move out of frame, specifically his head during sudden leaps, imbuing him with a sense of freedom and release. If Lavant were always centered, the expansiveness of his movements would potentially be diminished by the parallel movements of the camera, especially as the number progresses. In seeing a perfectly centered version of the dance, we also are reminded that Lavant is not truly alone in the number, as Godard keeps his mirrored image in frame throughout, balancing the space between Lavant and his reflection. Like &ldquo;No Strings (Reprise),&rdquo; Galoup’s final solo number results in a composition that reminds the viewer of the importance of both figures through deliberate choreography and mobile framing.</p>
<h2 id="in-conclusion-what-movement-should-the-camera-follow">In conclusion: What movement should the camera follow?</h2>
<p>The mobile framing in these dance sequences from  <em>Top Hat</em>  and  <em>Beau travail</em>  are motivated by figure movement. For Keating, mastery of the follow shot requires  “timing the camera’s movement to coincide with a character’s,”  and he summarizes the spectrum of follow shot aesthetics as residing between  “invisible”  and flamboyant impulses [<a href="#keating2019">2019, 75</a>]. Our ability with the mv tool to manipulate the figure’s relationship to the mobile frame raises the question, which part of a character’s movement should the camera follow? In many instances of cinematic staging, in which the camera follows a character who is walking with an erect posture, this question may serve little purpose. However, both Astaire and Lavant expand and contract their Kinespheres by moving their limbs towards and away from their body center, and their choreography pulls them off-vertical into diagonal pathways. The visualizations of mobile framing above, in which the pelvis’ position guides the parallel mobile frame, gives an approximation of what a holistic follow shot would look like, if the cameraperson was obsessively accounting for every tiny shift of the body. An examination of the end of Astaire’s solo emphasizes the importance of the cinematography in creating an unobtrusive sightline for the viewer through the cinematic space. The camera dollies to follow Astaire as he drowsily dances towards the armchair, yet it does not also retreat in parallel as Astaire’s spins briefly move him back towards the right. Instead, the follow shot creates a smooth pathway to end the number with Astaire centered, his straight body line on the diagonal of the frame.</p>
<p>With the mv tool’s mobile camera filter and the practice-based impulses of performative research, we can hypothesize what a more flamboyant approach to the follow shot would look like in these films. If the camera focuses on following the right wrist instead of the body center, for example, the mobile framing becomes significantly more obtrusive. Readers who are easily nauseated may choose to skip these visualizations, a warning that on its own can explain why filmmakers have not traditionally chosen this cinematographic approach (<a href="#figure26">Figure 26</a> and <a href="#figure27">Figure 27</a>).</p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://player.vimeo.com/video/" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="Visualization of &#34;No Strings (Reprise)&#34; with mobile framing tied to the wrist, performed by Sarah Mininsohn." webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://player.vimeo.com/video/" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="Visualization of Galoup&#39;s final dance with mobile framing tied to the wrist, performed by Sarah Mininsohn." webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div>
<p>Using digital resources like the mv tool and a performative methodology for stylistic analysis allows us to explore innovative and experimental research questions. The discovery that  “No Strings (Reprise)”  includes a follow shot motivated by a secondary character points to new lines of inquiry. How common is this occurrence, in Astaire numbers and musical cinema more broadly? With more data, could we articulate a correlation between a norm of camera movement motivated by secondary (non-dancing) characters and an emphasis on drawing viewer attention to narrative details? The flamboyant and extreme camera movements that we can record with the mv tool may not be accessible to filmmakers working in live-action environments due to the limits of camera technology. Yet the ability to change the relationship of the figure to frame initiates questions about how subtle changes to cinematic staging affects a viewer’s perception of the figure movement and can lead us to better understand why filmmakers make the choices they do. For aspiring filmmakers and cinematic choreographers, being able to play in such an environment can empower them to find creative and experimental solutions to the problems inherent in translating an idea to the screen. Performatively engaging with these digital tools provides material insight into audiovisual objects and fosters creative discovery.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Haseman, B. (2006)  “A manifesto for performative research.”    <em>Media International Australia Incorporating Culture and Policy,</em>  118(1), pp. 98-106.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Grant, C. (2016)  “The Audiovisual Essay as Performative Research.”    <em>NECSUS</em> , p. 14. Available at: <a href="http://search.ebscohost.com.proxy2.library.illinois.edu/login.aspx?direct=true&amp;db=cms&amp;AN=123942931&amp;site=eds-live&amp;scope=site">http://search.ebscohost.com.proxy2.library.illinois.edu/login.aspx?direct=true&amp;db=cms&amp;AN=123942931&amp;site=eds-live&amp;scope=site</a> (Accessed: 25 November 2019).&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref3:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref4:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Mittell, J. (2019)  “Videographic Criticism as a Digital Humanities Method.”  In Gold, M.K. and Klein, F.L. (eds.)  <em>Debates in the Digital Humanities</em>    <em>2019.</em>  Minneapolis, MN: University of Minnesota Press, pp. 224-242.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Keathley, C and Mittell, J. (2016)  “Teaching and learning the tools of videographic criticism”    <em>The Videographic Essay: Criticism in Sound and Image.</em>  Montreal: Caboose, 2016.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>Hagedoorn, B. and Sauer, S. (2018)  “The researcher as storyteller: Using digital tools for search and storytelling with audio-visual materials.”    <em>VIEW Journal of European Television History and Culture</em> , 7(14), pp. 1-21.&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>Cox, J. and Tilton, L. (2019)  “The digital public humanities: Giving new arguments and new ways to argue” ,  <em>Review of Communication</em> , 19(2), pp. 127-146. <a href="https://doi.org/10.1080/15358593.2019.1598569">DOI: 10.1080/15358593.2019.1598569</a>.&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>Arnold et al. (2019)  “Visual Style in two network era sitcoms”    <em>Cultural Analytics</em> . Available at: <a href="https://culturalanalytics.org/2019/07/visual-style-in-two-network-era-sitcoms/">https://culturalanalytics.org/2019/07/visual-style-in-two-network-era-sitcoms/</a> (Accessed: 25 November 2019) <a href="https://culturalanalytics.org/article/11045">DOI: 10.22148/16.043</a>.&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>Proctor, J. (2019)  “Teaching avant-garde practice as videographic research.”    <em>Screen</em>  60(3), pp. 466-474, <a href="https://doi.org/10.1093/screen/hjz033">DOI:10.1093/screen/hjz033</a>.&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>Brannigan, E. (2011)  <em>Dancefilm: Choreography and the Moving Image</em> . Oxford: Oxford UP.&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>Bordwell, D. (2005).  <em>Figures Traced in Light: On Cinematic Staging</em> . Berkeley: U of California P.&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>Genné, B. (2018)  <em>Dance Me a Song: Astaire, Balanchine, Kelly, and the American Film Musical.</em>  New York: Oxford UP.&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p>Keating, P. (2019)  <em>The Dynamic Frame: Camera Movement in Classical Hollywood</em> . New York: Columbia UP.&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13">
<p>McLean, A. (2008)  <em>Dying Swans and Madmen: Ballet, the Body, and Narrative Cinema</em> . New Brunswick: Rutgers.&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14">
<p>Acland, C. R. and Hoyt, E. (eds.). (2016)  <em>The Arclight Book to Media History and the Digital Humanities</em> . Sussex: REFRAME books.&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:15">
<p>Flueckiger, B. and Halter, G. (2018)  “Building a Crowdsourcing Platform for the Analysis of Film Colors,”    <em>Moving Image: The Journal of the Association of Moving Image Archivists</em>  18(1). <a href="https://www.jstor.org/stable/10.5749/movingimage.18.issue-1">DOI: 10.5749/movingimage.18.1.0080</a>.&#160;<a href="#fnref:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:16">
<p>Bernardet U. et al. (2019)  “Assessing the reliability of the Laban Movement Analysis system.”    <em>PLoS ONE</em>  14(6). <a href="https://doi.org/10.1371/journal.pone.0218179">DOI: 10.1371/journal.pone.0218179</a>.&#160;<a href="#fnref:16" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:17">
<p>LaViers, A.; Egerstedt, M. &lsquo;Style based robotic motion’ (2012) 2012  <em>American Control Conference</em>  (ACC), American Control Conference (ACC), 2012, p. 4327. <a href="https://doi.org/10.1109/ACC.2012.6315287">DOI: 10.1109/ACC.2012.6315287</a>.&#160;<a href="#fnref:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:18">
<p>Tsachor R. P. and Shafir T. (2019)  “How Shall I Count the Ways? A Method for Quantifying the Qualitative Aspects of Unscripted Movement With Laban Movement Analysis.”    <em>Frontiers in  Psychology</em> . 10:572. <a href="https://doi.org/10.3389/fpsyg.2019.00572">DOI: 10.3389/fpsyg.2019.00572</a>.&#160;<a href="#fnref:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:19">
<p>Woolford, K. (2017)  “Breakdown Harmonica: Extending Laban Notation with Video Game Development Tools”    <em>Proceedings of the 4th International Conference on Movement Computing</em> . <a href="https://doi.org/10.1145/3077981.3078051">DOI: 10.1145/3077981.3078051</a>.&#160;<a href="#fnref:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:20">
<p>Assaf, et al. (2019)  “Multitask learning for Laban Movement Analysis”    <em>Proceedings of the 2nd International Workshop on Movement and Computing</em> . DOI: <a href="http://dx.doi.org/10.1145/2790994.2791009">http://dx.doi.org/10.1145/2790994.2791009</a>.&#160;<a href="#fnref:20" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:21">
<p>Alemi, O. et al. (2014)  “Mova: Interactive movement analytics platform.”    <em>ACM International Conference Proceeding Series</em> . <a href="https://doi.org/10.1145/2617995.2618002">DOI: 10.1145/2617995.2618002</a>.&#160;<a href="#fnref:21" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:22">
<p>Cuan, C. et al. (2019)  “Time to compile: A performance installation as human-robot interaction study examining self-evaluation and perceived control.”    <em>De Gruyter</em> , 10(1), pp. 267-285. <a href="https://doi.org/10.1515/pjbr-2019-0024">DOI: 10.1515/pjbr-2019-0024</a>.&#160;<a href="#fnref:22" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:23">
<p>Slaney, H., et al. (2018)  “Ghosts in the Machine: A Motion-Capture Experiment in Distributed Reception,”    <em>Digital Humanities Quarterly</em>  12(3). Available at: <a href="/dhqwords/vol/12/3/000395/">http://digitalhumanities.org/dhq/vol/12/3/000395/000395.html</a>  (Accessed: 25 November 2019).&#160;<a href="#fnref:23" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:24">
<p>Bergson, H. (2007)  <em>The Creative Mind: An Introduction to Metaphysics</em> , transl. M.L. Andison, Mineola: Dover.&#160;<a href="#fnref:24" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:24" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:25">
<p>Moore, C.-L. (2009)  <em>The Harmonic Structure of Movement, Music, and Dance According to Rudolf Laban : An Examination of His Unpublished Writings and Drawings</em> . Lewiston: Edwin Mellen P.&#160;<a href="#fnref:25" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:25" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:25" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:26">
<p>Merleau-Ponty, M. (2002)  <em>Phenomenology of Perception</em> . Transl. C, Smith, New York: Routledge Classics.&#160;<a href="#fnref:26" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:27">
<p>Bordwell, D. (1997).  <em>On the History of Film Style</em> . Boston: Harvard UP.&#160;<a href="#fnref:27" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:27" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:28">
<p>While techniques for teaching figure movement or film form is not the focus of this article, this approach also has obvious pedagogical value in its ability to instill a greater awareness of and respect for the complexities of any audiovisual production.&#160;<a href="#fnref:28" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:29">
<p>Junokas, M.J., Lindgren, R., Kang, J., Morphew, J.W. (2018)  “Enhancing multimodal learning through personalized gesture recognition.”    <em>Journal of Computer Assisted Learning</em>  34. pp. 350-357. DOI: <a href="https://doi.org/10.1111/jcal.12262">https://doi.org/10.1111/jcal.12262</a>.&#160;<a href="#fnref:29" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:30">
<p>Puckette, M. (2018). Max/MSP (version 7): Cycling'74.&#160;<a href="#fnref:30" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:31">
<p>Laban, R. v. (2011a)  <em>Choreutics</em> . Alton: Dance Books.&#160;<a href="#fnref:31" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:31" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:31" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref3:31" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref4:31" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref5:31" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref6:31" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:32">
<p>Capitalized terms indicate vocabulary from the LBMS framework. For more on the LBMS work, see <sup id="fnref6:31"><a href="#fn:31" class="footnote-ref" role="doc-noteref">31</a></sup>  <sup id="fnref:40"><a href="#fn:40" class="footnote-ref" role="doc-noteref">40</a></sup>  <sup id="fnref2:25"><a href="#fn:25" class="footnote-ref" role="doc-noteref">25</a></sup>. For an approach to the work using less technical language, see <sup id="fnref:41"><a href="#fn:41" class="footnote-ref" role="doc-noteref">41</a></sup>.&#160;<a href="#fnref:32" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:33">
<p>Franceschina, J. (2012)  <em>The Man Who Danced With Fred Astaire</em> . Oxford: Oxford UP.&#160;<a href="#fnref:33" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:33" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:34">
<p>Mueller, J. E. (1985)  <em>Astaire Dancing: The Musical Films</em> . Columbus: Educational Publisher.&#160;<a href="#fnref:34" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:35">
<p>Mayne, J. (2005)  <em>Claire Denis</em> . Urbana: University of Illinois P.&#160;<a href="#fnref:35" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:35" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:35" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:36">
<p>Hayward, S. (2002)  “Claire Denis’ Films and the Post- Colonial Body - with special reference to  <em>Beau travail</em>  (1999),”    <em>L&rsquo;Esprit Créateur</em>  42(3). pp. 39-49.&#160;<a href="#fnref:36" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:37">
<p>[ Sosa, C. (2011).  “ <em>Beau travail</em>  (1998) and Judith Butler: Dancing at the limits of queer melancholia,”    <em>Cultural Studies</em>  25(2). pp. 200–212.&#160;<a href="#fnref:37" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:38">
<p>Darke, C. (2000).  “Desire is Violence,”    <em>Sight and Sound</em>  10:7. pp. 16-18.&#160;<a href="#fnref:38" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:39">
<p>Hughes, D. (2009)  “Dancing Reveals So Much: An Interview with Claire Denis,”    <em>Senses of Cinema</em>  50.&#160;<a href="#fnref:39" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:40">
<p>Laban, R. v. (2011b)  <em>The Mastery of Movement</em> . 4th edn. Alton: Dance Books.&#160;<a href="#fnref:40" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:41">
<p>Studd, K. and Cox, L. (2013)  <em>EveryBody is a Body</em> . Indianapolis: Dog Ear Publishing.&#160;<a href="#fnref:41" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry><entry><title type="html">Music Theory, the Missing Link Between Music-Related Big Data and Artificial Intelligence</title><link href="https://rlskoeser.github.io/dhqwords/vol/15/1/000520/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://rlskoeser.github.io/dhqwords/vol/15/1/000520/</id><author><name>Jeffrey A. T. Lupker</name></author><author><name>William J. Turkel</name></author><published>2021-03-05T00:00:00+00:00</published><updated>2021-03-05T00:00:00+00:00</updated><content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>Research in music information retrieval has produced many possibilities for developing artificial intelligence (AI) algorithms that can perform a wide variety of musically-based tasks, including even music composition itself. The availability of vast musical datasets like the  <em>Million Song Dataset</em>   <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> and Spotify&rsquo;s  <em>Web API</em>  has made it possible for researchers to acquire algorithmically-determined characteristics of a song&rsquo;s key, mode, pitch content, and more. At the same time, the existence of these large datasets has made it possible for researchers to take a &lsquo;big data&rsquo;<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>  approach to various styles of Western music. One notable example is the work by Serrà et al <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup> which showed the changes and trends related to pitch transitions, the homogenization of the timbral palette and growing loudness levels that have shaped pop music over the past 60 years. The authors went on to suggest that past songs might be modernized into new hits by restricting pitch transitions, reducing timbral variety and making them louder. Tanya Clement further suggests how studying musical big data lends itself quite well to music related tasks, especially music composition, since the  “notion of scholars  reading  visualizations [(complete representation of the data)] relates to composers or musicians who read scores … [as] the musical score is an attempt to represent complex relationships … across time and space”   <sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>.</p>
<p>Big data can also be used to create labelled instances for training supervised learners like neural nets (which tend to be  “data-thirsty” ) and can be easily parsed by unsupervised learners to find patterns. This more recent ability to train music-related AI programs has largely been directed towards autonomously generating music in the same vein as whatever genre, style or composer that particular program has been trained on. A good example of this is  “The Bach Doodle,”  which can supply a melody with appropriate counter-melodies, accompaniment and chord changes in the style of Johann Sebastian Bach <sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>.</p>
<p>While the availability of approaches such as Spotify&rsquo;s has allowed for the development of AI algorithms in music, many previous research projects have struggled to find felicitous links between this music-related big data and music itself. In the past, a common method involving the use of Krumhansl-Kessler profiles <sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>, vectors of pitch correlation to the major or minor modes, allowed for mode or key predictability in some limited capacity. While it showed promise when applied to music of specific genres, it suffered when applied to a wider scope of genres or styles. We offer methods to alter KK-profiles, and the manner in which they are employed during preprocessing, to aid autonomous mode and key predictors ability and accuracy without being genre-specific. Without the ability to connect the intermediate dots, the overall accuracy of these algorithms diminishes and their output suffers accordingly. Indeed, AI-based autonomous generation is rarely up to the standards of composers or musicians creating new music. This paper offers preliminary solutions to two existing problems for which AI is typically used, mode and key prediction. We show that by equipping our algorithms with more background in music theory we can significantly improve their accuracy. The more the program learns about the music theoretic principles of mode and key, the better it gets. Our more general argument is that one way to help bridge the gap between music-related big data and AI is to give algorithms a strong grounding in music theory.</p>
<h2 id="mode-prediction">Mode Prediction</h2>
<p>For the purpose of this paper, we looked at only the two most common modes in Western Music, the major and minor modes. These are also the only modes analyzed by  <em>The Million Song Dataset</em>  and Spotify&rsquo;s  <em>Web API</em> . The major and minor modes are a part of the  “Diatonic Collection,”  which refers to  “any scale [or mode] where the octave is divided evenly into seven steps”   <sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>. A step can be either a whole or half step (whole tone or semitone) and the way that these are arranged in order to divide the octave will determine if the mode is major or minor. A major scale consists of the pattern  <strong>W-W-H-W-W-W-H</strong>  and the minor scale consists of  <strong>W-H-W-W-H-W-W</strong>   <sup id="fnref1:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>. <a href="#figure01">Figure 1</a> shows a major scale starting on the pitch C and <a href="#figure02">Figure 2</a> shows two types of minor scales starting on C. The seventh  “step”  in the harmonic minor scale example is raised in order to create a  “leading tone.”  The leading tone occurs when the seventh scale degree is a half step away from the first scale degree, also called the  “tonic.”  This leading tone to tonic relationship will become an important music theory principle that we use to train our AIs more accurately than previous published attempts.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000520/resources/images/figure01.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000520/resources/images/figure01_hu7321835c407e2083914b15c6ce2902bd_28916_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000520/resources/images/figure01_hu7321835c407e2083914b15c6ce2902bd_28916_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000520/resources/images/figure01_hu7321835c407e2083914b15c6ce2902bd_28916_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000520/resources/images/figure01_hu7321835c407e2083914b15c6ce2902bd_28916_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/1/000520/resources/images/figure01.png 1610w" 
     class="landscape"
     ><figcaption>
        <p>C major scale demonstrating its make-up of whole and half tones.
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000520/resources/images/figure02.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000520/resources/images/figure02_hu509cd82966bfbf51da00114d90bca0d1_59119_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000520/resources/images/figure02_hu509cd82966bfbf51da00114d90bca0d1_59119_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000520/resources/images/figure02_hu509cd82966bfbf51da00114d90bca0d1_59119_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000520/resources/images/figure02_hu509cd82966bfbf51da00114d90bca0d1_59119_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/1/000520/resources/images/figure02.png 1616w" 
     class="landscape"
     ><figcaption>
        <p>Natural and harmonic minor scales starting on “C” . This also shows the progression of a natural minor scale to a harmonic scale by raising the seventh interval to become a leading tone.
        </p>
    </figcaption>
</figure>
<p>Many previous papers which use supervised learning to determine mode or key test only against songs from specific genres or styles, and few make attempts at predicting mode regardless of genre. Even the often-cited yearly competition on musical key detection hosted by Music Information Retrieval Evaluation eXchange (MIREX) has participants&rsquo; algorithms compete at classifying 1252 classical music pieces <sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>  <sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup>. However, if we look again at <a href="#figure01">Figure 1</a> and <a href="#figure02">Figure 2</a>, we can see that mode is not exclusive to genre or style, it is simply a specific arrangement of whole and half steps. So for a supervised learner programmed to  “think”  like a musician and thus determine mode based on its understanding of these music theory principles, genre or style should not affect the outcome. While this might work in a perfect world, artists have always looked for ways to  “break away”  from the norm and this can indeed manifest itself more in certain genres than others. Taking this into consideration, in this research we only selected songs for our separate ground truth set involving various genres which obey exact specifications for what constitutes as major or minor. This ground truth set will be a separate list of 100 songs labeled by us to further check the accuracy of our AI algorithms during testing. We wish to discuss shortcomings in the accuracy of past research that uses AI algorithms for predicting major or minor mode rather than to suggest a universal method for identifying all modes and scales.</p>
<p>This is one aspect where our research differs from previous papers. An AI system which incorporates a solid understanding of the rules of music theory pertaining to mode should be able to outperform others that do not incorporate such understanding or those that focus on specific genres. While certain genres or styles may increase the difficulty of algorithmically determining mode, the same is true for a human musical analyst. When successful, an AI algorithm for determining mode will process data much faster than a musician, who would either have to look through the score or figure it out by ear in order to make their decision. For parsing music-related big data quickly and accurately, speed is imperative. Thus we suggest the following framework (<a href="#figure03">Figure 3</a>) by which a supervised learner can be trained to make predictions exclusively from pitch data in order to determine the mode of a song. The process is akin to methods used by human musical analysts. Below we also outline other areas where we apply a more musician-like approach to our methods to achieve greater accuracy.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000520/resources/images/figure03.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000520/resources/images/figure03_hu8aabe446d23de9e89f7f2b78230eb73e_27641_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000520/resources/images/figure03_hu8aabe446d23de9e89f7f2b78230eb73e_27641_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000520/resources/images/figure03.png 902w" 
     class="landscape"
     ><figcaption>
        <p>Framework for supervised learning mode prediction algorithm.
        </p>
    </figcaption>
</figure>
<p>As can be seen in <a href="#figure03">Figure 3</a>, any scale or mode that does not meet the exact specifications of major or minor we categorize as  <em>other</em> ,  <em>ambiguous</em>  or  <em>nontonal</em>  (OANs). The primary reason that past research has trained supervised learners on only one specific genre or style is to avoid OANs. When OANs are not segregated from major or minor modes, they are fit improperly, leading to misclassifications.</p>
<p><em>Other</em>  pertains to any scale separate from major or minor that still contains a tonal center. Some examples of this are: modes (except Ionian or Aeolian), whole tone scale, pentatonic scale, and non-Western scales.  <em>Nontonal</em>  refers to any song which does not center around a given pitch. A common occurrence of this can be found in various examples of  “12-tone music,”  where each pitch is given equal importance in the song and thus no tonic can be derived from it.</p>
<p>Where our paper differs from previous work is the handling of songs related to the outcome  <em>ambiguous</em> . This occurs when either major or minor can be seen as an equally correct determination from the given pitches in a song. This most often occurs when chords containing the leading tone are avoided (<a href="#figure04">Figure 4</a>) and thus the remaining chords are consistent with both the major key and its relative minor (<a href="#figure04">Figure 4</a> &amp; <a href="#figure05">Figure 5</a>). The leading tone is a  “tendency tone”  or a tone that pulls towards a given target, in this case the tonic. This specific pull is one that can often signify a given mode and is therefore avoided in songs that the composer wished to float between the two modes. This can also be accomplished by simply using the relative minor&rsquo;s natural minor scale. Since the natural minor scale does not raise any pitches, it actually has the exact same notes (and resultant triads) as its relative major scale. <a href="#figure06">Figure 6</a> gives an example of a well-known pop song  <em>Despacito</em>   <sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup> and given these rules, what mode can we say the song is in? This tough choice is a common occurrence, especially in modern pop music, which can explain why papers that focused heavily on dance music might have had accuracy issues.</p>
<p>Other authors have noted that their AI algorithms can mistake the major scale for its relative natural minor scale during prediction and it is likely that their algorithms did not account for the raised leading tone to truly distinguish major from minor. Since we focused on achieving higher accuracy than existing major vs minor mode prediction AI algorithms by incorporating music theory principles, we removed any instances of songs with an ambiguous mode from our ground truth set in order to get a clearer picture of how our system compares with the existing models. Adding other mode outcomes in order to detect OANs algorithmically is a part of our ongoing and future research.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000520/resources/images/figure04.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000520/resources/images/figure04_hu0b9e2a843105084a89c119c0fc2aa579_38158_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000520/resources/images/figure04_hu0b9e2a843105084a89c119c0fc2aa579_38158_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000520/resources/images/figure04_hu0b9e2a843105084a89c119c0fc2aa579_38158_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000520/resources/images/figure04_hu0b9e2a843105084a89c119c0fc2aa579_38158_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/1/000520/resources/images/figure04.png 1604w" 
     class="landscape"
     ><figcaption>
        <p>Triads built from a major scale contain only pitches found in that scale. The relative minor of a major scale begins on the sixth interval. Chords containing the leading tone (‘B’) have been highlighted in red.
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000520/resources/images/figure05.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000520/resources/images/figure05_hu3d0f0e1d1fefa2a0ec38b2bb708a1198_71659_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000520/resources/images/figure05_hu3d0f0e1d1fefa2a0ec38b2bb708a1198_71659_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000520/resources/images/figure05_hu3d0f0e1d1fefa2a0ec38b2bb708a1198_71659_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000520/resources/images/figure05_hu3d0f0e1d1fefa2a0ec38b2bb708a1198_71659_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/1/000520/resources/images/figure05.png 1602w" 
     class="landscape"
     ><figcaption>
        <p>Shows the relative minor scale from the C major scale in <a href="#figure04">Figure 4</a>. As seen in <a href="#figure02">Figure 2</a>, the leading tone ( “G#” ) must be added in order to make the obvious distinction from a major scale to a minor scale.
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000520/resources/images/figure06.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000520/resources/images/figure06_hu48c3d45ac4cfacad9528f2503ab5d406_44472_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000520/resources/images/figure06_hu48c3d45ac4cfacad9528f2503ab5d406_44472_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000520/resources/images/figure06_hu48c3d45ac4cfacad9528f2503ab5d406_44472_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000520/resources/images/figure06.png 1258w" 
     class="landscape"
     ><figcaption>
        <p>Chord progression for the hit song from Fonsi and Yankee’s Despacito [^fonsi2017]. What mode is it in?
        </p>
    </figcaption>
</figure>
<p>The most popular method of turning pitch data into something that can be used to train machine learners comes in the form of  “chroma features.”  Chroma feature data is available for every song found in Spotify&rsquo;s giant database of songs through the use of their  <em>Web API</em> . Chroma features are vectors containing 12 values (coded as real numbers between 0 and 1) reflecting the relative dominance of all 12 pitches over the course of a small segment, usually lasting no longer than a second <sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>. Each vector begins on the pitch C and continues in chromatic order (C#, D, D#, etc.) until all 12 pitches are reached. In order to create an AI model that could make predictions on a wide variety of musical styles, we collected the chroma features for approximately 100,000 songs over the last 40 years. Spotify&rsquo;s  <em>Web API</em>  offers its data at different temporal resolutions, from the aforementioned short segments through sections of the work to the track as a whole.</p>
<p>Beyond chroma features, the API offers Spotify&rsquo;s own algorithmic analysis of musical features such as mode within these temporal units, and provides a corresponding level of confidence for each (coded as a real number between 0 and 1). We used Spotify&rsquo;s mode confidence levels to find every section within our 100,000 song list which had a mode confidence level of 0.6 or higher. The API&rsquo;s documentation states that  “ <em>confidence</em>  indicates the reliability of its corresponding attribute&hellip; elements carrying a small confidence value should be considered speculative&hellip; there may not be sufficient data in the audio to compute the element with high certainty”   <sup id="fnref1:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup> thus giving good reason to remove sections with lower confidence levels from the dataset. Previous work such as Serrà et al <sup id="fnref1:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>, Finley and Razi <sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup> and Mahieu <sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup> also used confidence thresholds, but at the temporal resolution of a whole track rather than the sections that we used. By analyzing at the level of sections, we were able to triple our training samples from 100,000 songs to approximately 350,000 sections. Not only did this method increase the number of potential training samples, but it allowed us to focus on specific areas of each song that were more likely to provide an accurate representation of each mode as they appeared. For example, a classical piece of music in sonata form will undergo a  “development”  section whereby it passes through contrasting keys and modes to build tension before its final resolve to the home key, mode and initial material. Pop music employs a similar tactic with  “the bridge,”  a section found after the midway point of a song to add contrast to the musical material found up until this point. Both of these contrasting sections might add confusion during the training process if the song is analyzed as a whole, but removing them or analyzing them separately gives the program more accurate training samples. The ability to gain more training samples from the original list of songs has the advantage of providing more data for training a supervised learner.</p>
<p>In previous work, a central tendency vector was created by taking the mean of each of the 12 components of the chroma vectors for a whole track, and this was then labelled as either major or minor for training. In an effort to mitigate the effects of noise on our averaged vector in any given recording, we found that using the medians rather than means gave us a better representation of the actual pitch content unaffected by potential outliers in the data. One common example is due to percussive instruments, such as a drum kit&rsquo;s cymbals, that happen to emphasize pitches that are  “undesirable”  for determining the song&rsquo;s particular key or mode. If that cymbal hit only occurs every few bars of music, but the  “desirable”  pitches occur much more often, we can lessen the effect that cymbal hit will have on our outcome by using a robust estimator. A musician working with a score or by ear would also filter out any unwanted sounds that did not help in mode determination. We found the medians of every segment&rsquo;s chroma feature vector found within each of our 350,000 sections.</p>
<p>The last step in the preparation process is to transpose each chroma vector such that they are all aligned into the same key. As our neural network (NN) will only output predictions of major or minor, we want to have the exact same tonal center for each chroma vector to easily compare between their whole and half step patterns (<a href="#figure03">Figure 3</a>). We based our transposition method on the one described by Serrà et al <sup id="fnref2:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup> and also used in their 2012 work. This method determines an  “optimal transposition index”  (OTI) by creating a new vector of the dot products between a chroma vector reflecting the key they wish to transpose to and the twelve possible rotations (i.e., 12 possible keys) of a second chroma vector. Using a right circular shift operation, the chroma vector is rotated by one half step each time until the maximum of 12 is reached. Argmax, a function which returns the position of the highest value in a vector, provides the OTI from the list of dot product correlation values, thus returning the number of steps needed to transpose the key of one chroma vector to match another (see Appendix 1.1 for a more detailed formula). Our method differs slightly from Serrà et al: since our vectors are all normalized, we used cosine similarity instead of the related dot product.</p>
<p>In order to train a neural network for mode prediction, some previous studies used the mode labels from the Spotify  <em>Web API</em>  for whole tracks or for sections of tracks. When we checked these measures against our own separate ground truth set (analyzed by Lupker), we discovered that the automated mode labeling was relatively inaccurate (Table 1). Instead we adapted the less complex method of Finley &amp; Razi <sup id="fnref1:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>, which reduced the need for training NNs. They compared chroma vectors to KK-Profiles to distinguish mode and other musical elements. Krumhansl and Kessler profiles (<a href="#figure07">Figure 7</a>) come from a study where human subjects were asked to rate how well each of the twelve chromatic notes fit within a key after hearing musical elements such as scales, chords or cadences <sup id="fnref1:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>. The resulting vector can be normalized to the range between 0-1 for direct comparisons to chroma vectors using similarity measures. By incorporating both modified chroma transpositions and KK-profile similarity tests, we were able to label our training data in a novel way.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000520/resources/images/figure07.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000520/resources/images/figure07_hu51d9b8f1d39840ab5619a106a8c335ca_359311_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000520/resources/images/figure07_hu51d9b8f1d39840ab5619a106a8c335ca_359311_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000520/resources/images/figure07_hu51d9b8f1d39840ab5619a106a8c335ca_359311_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000520/resources/images/figure07.png 1332w" 
     class="landscape"
     ><figcaption>
        <p>Krumhansl and Kessler profiles for major and minor keys [^krumhansl1982].
        </p>
    </figcaption>
</figure>
<p>To combine these two approaches, we first rewrite Serrà et al&rsquo;s formula (Appendix 1.1) to incorporate Finley and Razi&rsquo;s method by making both KK-profile vectors (for major and minor modes) the new &lsquo;desired vectors&rsquo; by which we will transpose our chroma vector set. This will eventually transpose our entire set of vectors to C major and C minor since the tonic of the KK-profiles is the first value in the vector, or pitch C. Correlations between KK-profiles and each of the 12 possible rotations of any given chroma vector are determined using cosine similarity. Instead of using the function which would return the position of the vector rotation that has the highest correlation (argmax), we use a different function which tells us what that correlation value is (amax). Two new lists are created. One is a list of the highest possible correlations for each transposed chroma vector and the major KK-profile, while the other is a list of correlations between each transposed chroma vector and the minor KK-profile. Finally, to determine the mode of each chroma vector, we then simply use a function to determine the position of the higher correlated value between these two lists, position 0 for major and 1 for minor (see Appendixes 1.2.1 &amp; 1.2.2).</p>
<p>As noted by Finley &amp; Razi, the most common issue affecting accuracy levels for supervised or unsupervised machine learners attempting to detect the mode or key is  “being off by a perfect musical interval of a fifth from the true key, relative mode errors or parallel mode errors”   <sup id="fnref2:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>. Unlike papers which followed the MIREX competition rules, our algorithm does not give partial credit to miscalculations no matter how closely related they may be to the true mode or key. Instead we offer methods to reduce these errors. To attempt to correct these issues for mode detection, it is necessary to address the potential differences between a result from music psychology, like the KK-profiles, and the music theoretic concepts that they quantify. As we mentioned earlier, the leading tone in a scale is one of the most important signifiers of mode. In the  <em>Despacito</em>  example, where the leading tone is avoided, it is hard to determine major or minor mode. In the (empirically determined) KK-profiles, the leading tone seems to be ranked comparatively low relative to the importance it holds theoretically. If the pitches are ordered from greatest to lowest perceived importance, the leading tone doesn&rsquo;t even register in the top five in either KK-profile. This might be a consequence of the study design, which asked subjects to judge how well each note seemed to fit after hearing other musical elements played.</p>
<p>The distance from the tonic to the leading tone is a major seventh interval (11 semitones). Different intervals fall into groups known as consonant or dissonant. Laitz defines consonant intervals as  “stable intervals… such as the unison, the third, the fifth (perfect only)”  and dissonant intervals as  “unstable intervals… [including] the second, the seventh, and all diminished and augmented intervals”   <sup id="fnref2:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>. More dissonant intervals are perceived as having more tension. Rather than separating intervals into categories of consonant and dissonant, Hindemith ranks them on a spectrum, which represents their properties more effectively. He ranks the octave as the  “most perfect,”  the major seventh as the  “least perfect”  and all intervals in between as  “decreasing in euphony in proportion to their distance from the octave and their proximity to the major seventh”   <sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup>. While determining the best method of interval ranking is irrelevant to this paper, both theorists identify the major seventh as one of the most dissonant intervals. Thus, if the leading tone were to be played by itself (that is, without the context of a chord after a musical sequence) it might sound off, unstable or tense due to the dissonance of a major seventh interval in relation to the tonic. In a song&rsquo;s chord progression or melody, this note will often be given context by its chordal accompaniment or the note might be resolved by subsequent notes. These methods and others will &lsquo;handle the dissonance&rsquo; and make the leading tone sound less out of place. We concluded that the leading tone value found within the empirical KK-profiles should be boosted to reflect its importance in a chord progression in the major or minor mode. Our tests showed that boosting the original major KK-profile&rsquo;s 12th value from 2.88 to 3.7 and the original minor KK-profile&rsquo;s 12th value from 3.17 to 4.1 increased the accuracy of the model at determining the correct mode by removing all instances where misclassifications were made between relative major and minor keys.</p>
<p>Our training samples include a list of mode determinations labelling our 350,000 chroma vectors. However, the algorithm assumes that every vector is in a major or minor mode with no consideration for OANs. Trying to categorize every vector as either major or minor leads to highly inaccurate results during testing, and seems to be a main cause of miscalculations made by the mode prediction algorithms of Spotify&rsquo;s  <em>Web API</em>  and the  <em>Million Song Dataset</em> . To account for other or nontonal scales, we can set a threshold of acceptable correlation values (major and minor modes) and unacceptable values (other or nontonal scales). Our testing showed that a threshold of greater than or equal to 0.9 gave the best accuracy on our ground truth set for determining major or minor modes. These unacceptable vectors contain other or nontonal scales and future research will determine ways of addressing and classifying these further.</p>
<p>To address  <em>ambiguous</em>  mode determinations between relative major and minor modes, we can set another threshold for removing potentially misleading data for training samples. While observing the correlation values used to determine major or minor labels, we set a further constraint when these values are too close to pick between them confidently. If the absolute difference between the two values is less than or equal to 0.02, we determine these correlation values to be indistinguishable and thus likely to reflect an ambiguous mode. As mentioned earlier, this is likely due to the song&rsquo;s chord progression avoiding certain mode determining factors such as the leading tone, and therefore the song can fit almost evenly into either the major or minor classification.</p>
<h2 id="mode-prediction-results">Mode Prediction Results</h2>
<p>After filtering out samples determined to be OANs, our sample size was reduced to approximately 100,000. From this dataset, 75% of the samples were selected to train the model and an even split of the remaining 25% of samples being withheld for testing and cross-validation to check the model&rsquo;s accuracy and performance. On the withheld test set, our model returned a very high accuracy of 99% during testing. We found that this was much better than the results reported from other studies on withheld test sets. This level of accuracy and the ability to compute the data quickly make it useful for parsing big data for any research that makes comparisons based on modes.</p>
<p>In addition to testing using only samples withheld from our large dataset, we created a separate ground truth set of 100 songs taken from various  “top 100”  charts from different genres such as pop, country, classical and jazz. This ground truth set was labeled ourselves by looking at the score of each song and comparing it with the exact recording found on Spotify (in order to make sure it wasn&rsquo;t a version recorded in a different key). Our NN reached an accuracy of 88% on the outside ground truth set. The discrepancy is perhaps due to learning from samples that were improperly labelled during cosine similarity measures against KK-profiles. Since this model only outputs major or minor, it is hard to track the exact details of each misclassification. It could be miscalculating a result based on relative major or minors, parallel major or minors (C major vs C Minor) or just be completely off. These incorrectly identified modes will be further addressed in the next section where key is also determined, giving us a clearer understanding of the problem. It is important to note the very low accuracy of 18% in modes determined by Spotify&rsquo;s  <em>Web API</em> . Future research based on the API&rsquo;s mode labelling algorithms should be tested against ground truth datasets before being used to make musicological claims.</p>
<h2 id="key-prediction">Key Prediction</h2>
<p>With a highly accurate working model for mode detection, adding the ability to predict key becomes fairly straightforward. Our mode detection algorithm is based on the music theory principle of determining mode by first finding the tonic, then calculating the subsequent intervals. Additionally, this method assumes the tonic to be the most frequent note and therefore the tonic should always register as the most prominent note in any given median-averaged chroma vector. Thus when transposing each chroma vector in our ground truth set to C major or C minor, we must keep track of the initial position of the most prominent pitches, as these are our key determining features. This method of analyzing songs based on a non key-specific approach first, and then adding key labels afterwards is derived from the method of &lsquo;roman numeral analysis&rsquo; in musicology. This kind of analysis is used by music theorists to outline triadic chord progressions found within tonal music <sup id="fnref3:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>. In this method, uppercase numerals are used for major triads and lowercase numerals are used for minor triads (<a href="#figure08">Figure 8</a>). The method itself is not key-specific (besides a brief mention at the beginning), allowing the analysis of underlying chord relationships across multiple songs regardless of key. Considering that machine learning programs typically need large datasets for training, and that it is unlikely even large datasets will contain songs in every possible key in the same proportions, roman numeral analysis is ideal.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000520/resources/images/figure08.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000520/resources/images/figure08_hua2c5f4068ac926c59cc3527526ddcfa4_28921_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000520/resources/images/figure08_hua2c5f4068ac926c59cc3527526ddcfa4_28921_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000520/resources/images/figure08_hua2c5f4068ac926c59cc3527526ddcfa4_28921_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000520/resources/images/figure08_hua2c5f4068ac926c59cc3527526ddcfa4_28921_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/1/000520/resources/images/figure08.png 1612w" 
     class="landscape"
     ><figcaption>
        <p>A roman numeral analysis of major scale triads where uppercase numerals equal major triads and lowercase for minor. The chord built upon the seventh uses a degree sign to denote that it is a diminished chord.
        </p>
    </figcaption>
</figure>
<h2 id="key-prediction-results">Key Prediction Results</h2>
<p>As our key predictions result from a simple labelling of our mode prediction neural network&rsquo;s output, we cannot compute accuracy of training. Past testing did include a second NN where the dataset taken from the mode detection NN was rotated to each of the 12 possible keys and then trained on that with 24 outputs for each major and minor key. This training resulted in 93% accuracy but we didn&rsquo;t find any significant increase over the method of applying labels after running the mode NN when testing against the ground truth set, therefore we decided against training a second NN to detect key as it saves training time. With the key labels added to the output of our mode NN, our model returned an accuracy level of 48%. While not nearly as accurate as our mode prediction algorithm, it is much higher than the Spotify  <em>Web API</em>  which returns an accuracy of 2% on our ground truth set.</p>
<p>It is also difficult to compare our accuracy on a ground truth set with any paper which used the MIREX competition dataset. MIREX&rsquo;s dataset contains remakes of real songs using Musical Instrument Digital Interface (MIDI) synthesizers which can be recorded straight into a computer, without the need for microphones. Furthermore, it is unclear if this dataset recreates the percussion synthetically for each song or if this is removed to allow easier access to the harmonic content for contestants to test their AI systems. As our training and testing uses actual recorded music and no MIDI representations there is a higher likelihood of noise or percussive elements to throw off our algorithm during testing the outside ground truth set. As of this research, we have not entered the MIREX competition, and thus do not have access to their datasets and cannot compare the performance of our algorithm with those which did. In future research we plan to enter the 2020 MIREX key detection competition and to draw more accurate comparisons with other key detection AI models.</p>
<p>Our much lower accuracy levels for key prediction in comparison to mode prediction are consistent with misclassification noted within previous research. While we were able to almost entirely remove instances of relative major and minor mode misclassifications, the big outstanding problem are keys classified by an interval of a fifth away. We see this as a potential limitation to the ability of supervised or unsupervised learning techniques to predict key (and mode to an almost perfect degree) from pitch content alone. If we imagine this method of determining mode and key from pitch content was performed by a human musician, the equivalent would be that of writing down each pitch in the first section (or the entirety) of a song and making predictions based on a tally. Since this method relies on the tonic being the most prominent pitch in this tally, the model will likely always fail when this is not the case. For example, a song might be in C major with a melody often repeating the pitch &lsquo;G&rsquo; throughout the verse even while a tonic C major chord provides the accompaniment underneath. Since the pitch &lsquo;G&rsquo; is found in the tonic chord of C major (<a href="#figure04">Figure 4</a>), it will sound good and be a theoretically correct decision. The repeated G, however, will be the most prominent note in the chroma vector section. The scale found a fifth away from a C major scale, G Major, shares all the same notes except one, thus confusing the model. Even with all the processes aimed at removing cases of OANs, a melody focused around the fifth interval of a tonic chord will constantly skew the data and the model will make miscalculations on key.</p>
<h2 id="conclusions">Conclusions</h2>
<p>Our neural net model itself is quite simple and does not represent a novel approach to music AI models. Our research was instead focused on data preparation methods grounded in music theory helping to boost the accuracy of existing models by finding more appropriate links between music-related big data and the resultant outputs. Using our methods such as boosting the leading tone&rsquo;s prominence in the KK-profile and filtering out OANs, we were able to construct a model with a higher accuracy for mode prediction during both testing on a withheld subset of our dataset and on the external ground truth set labeled by Lupker. Our key prediction showed comparable results to previous research on a separate ground truth set and we see this as a limit to the ability of predicting key using a pitch-based tally classifier. The model&rsquo;s prediction can be too easily skewed by any song with a melody focused on the fifth interval of the tonic chord, mistaking the key for one a fifth away. Accessing the MIREX dataset would give us a better comparison with those papers which have competed in past competitions, but we predict similar results to those found in this paper.</p>
<h2 id="further-research">Further Research</h2>
<p>Further steps we could take with this research would be to identify and label OANs to create a more universal mode and key prediction NN. The similarity measures specified in this paper could be repeated given chroma vector profiles of other modes and scales to compare against. This would be useful for any research projects experimenting with big data related to non-Western music. Another area for further research would be to apply our music theory based processes to some existing chord retrieval algorithms. Our testing leads us to believe that training NNs on chord transition networks as related to mode or key is the only way to reach accuracy levels comparable to human predictions. When musicians are faced with determining mode or key of a song with less obvious features, the last resort method is to look at the chord progression and make a decision based on that. Our predictions are that a learner trained to look for mode or key determining features of a chord progression will outperform those based on averaging tally counts of pitch.</p>
<h2 id="appendix-1">Appendix 1</h2>
<h2 id="11-formula-to-find-the-optimal-transposition-index-oti">1.1 Formula to find the Optimal Transposition Index (OTI)</h2>
<p>Serrà et al.&rsquo;s formula (<a href="#serr%C3%A02008">2008</a>) to find the OTI between vector (\overrightarrow{g_{A}})(the desired key) and all 12 possible rotations of vector (\overrightarrow{g_{B}}). Rotations are accomplished using the  <em>Circshift R _  function with  <em>M</em>  being the maximum amount of rotations (12) and  <em>j</em>  being the amount to rotate by. The function  <em>argmax</em>  then selects the rotation amount  <em>(j)</em> , which rotated by vector (\overrightarrow{g</em>{B}}) to the position with the maximum correlation value with vector (\overrightarrow{g_{A}}). $$\text{OTI}\left( \overrightarrow{g_{A}},\overrightarrow{g_{B}} \right) = \underset{1 \leq j \leq M}{argmax}\left{ \overrightarrow{g_{A}} \cdot \text{Circshift}<em>{R}\left( \overrightarrow{g</em>{B}},j - 1 \right) \right}$$</p>
<h2 id="121-modified-formula-to-transpose-chroma-vectors-separated-by-major-and-minor-nodes">1.2.1 Modified formula to transpose chroma vectors separated by major and minor nodes</h2>
<p>Our modified version of Serrà et al.&rsquo;s  <em>OTI</em>  formula (<a href="#serr%C3%A02008">2008</a>) using cosine similarity instead of dot product. Instead of finding the best possible transposition amount  <em>(j)</em> , we use the function  <em>amax</em>  to find the highest correlation value between the desired vector (\overrightarrow{maj}) or (\overrightarrow{min}) and each rotation of vector (\overrightarrow{cv}). $$S_{A}\left( \overrightarrow{maj},\overrightarrow{cv} \right) = \underset{1 \leq j \leq M}{amax}\left{ \left| \overrightarrow{maj} \right|\left| Circshift_{R}\left( \overrightarrow{cv},j - 1 \right) \right|\cos\Theta \right}$$  $$S_{B}\left( \overrightarrow{min},\overrightarrow{cv} \right) = \underset{1 \leq j \leq M}{amax}\left{ \left| \overrightarrow{min} \right|\left| Circshift_{R}\left( \overrightarrow{cv},j - 1 \right) \right|\cos\Theta \right}$$</p>
<h2 id="122">1.2.2</h2>
<p>Find the mode  <em>(TS)</em>  of each vector by finding the position of the higher value using the function  <em>argmax</em> . These will become the training samples used to train the neural network. $$TS\left( S_{A},S_{B} \right) = \underset{}{argmax}\left( S_{A},S_{B} \right)$$</p>
<h2 id="appendix-2">Appendix 2</h2>
<h2 id="21computational-workflow">2.1Computational Workflow</h2>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000520/resources/images/figure12.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000520/resources/images/figure12_hu6532a51f5c39b2cf6a2a1ccf0b6173c3_105164_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000520/resources/images/figure12_hu6532a51f5c39b2cf6a2a1ccf0b6173c3_105164_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000520/resources/images/figure12.png 1052w" 
     class="portrait"
     ><figcaption>
        <p>Computational workflow regarding the collection of data and the subsequent preprocessing steps required to create training samples and labels which can be fed into the neural network.
        </p>
    </figcaption>
</figure>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Bertin-Mahieux, T., D. Ellis, B. Whitman, and P. Lamere,  “The Million Song Dataset”    <em>Proceedings of the 12th International Conference on Music Information</em> , Miami, USA, October 2011.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Our use of the term &lsquo;big data&rsquo; refers to datasets that are so large that conventional computers may have difficulty processing them. Researchers are now able to access additional computational power in the form of cloud resources such as GPUs, as we have done here. Our experiments were run using Google Colaboratory.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Serrà, J., Á. Corral, M. Boguñá, M. Haro &amp; J. Ll. Arcos.  “Supplementary Information: Measuring the Evolution of Contemporary Western Popular Music”    <em>Scientific Reports</em>  521.2 (2012).&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Clement, T.,  “Distant Listening or Playing Visualisations Pleasantly with the Eyes and Ears.”    <em>Digital Studies/le Champ Numérique</em> , 3.2. DOI: <a href="http://doi.org/10.16995/dscn.236">http://doi.org/10.16995/dscn.236</a> (2013).&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>Huang, C. A., C. Hawthorne, A. Roberts, M. Dinculescu, J. Wexler, L. Hong, J. Howcroft,  “The Bach Doodle: Approachable Music Composition with Machine Learning at Scale”    <em>Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2019</em> . <a href="https://arxiv.org/abs/1907.06637">arXiv:1907.06637</a> (2019).&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>Krumhansl, C. L., and E. J. Kessler,  “Tracing the Dynamic Changes in Perceived Tonal Organization in a Spatial Representation of Musical Keys”    <em>Psychological Review</em> , 89.4 (1982), 334-368.&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>Laitz, S. G.,  <em>The Complete Musician: An Integrated Approach to Tonal Theory, Analysis, and Listening</em> . Oxford University Press, New York, 2003.&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref3:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>İzmirli, O.,  “Audio Key Finding Using Low Dimensional Spaces”    <em>Proceedings from the 7th International Conference on Music Information Retrieval</em> , Victoria, Canada, October 2006.&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>Pauws, S.,  “Musical Key Extraction from Audio”    <em>Proceedings of the 5th International Society for Music Information Retrieval Conference</em> , Barcelona, Spain, October 2004.&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>Fonsi, Luis and Daddy Yankee,  “Despacito”  Vida. Universal Latin, Los Angeles, 2017. <a href="http://doi.org/10.16995/dscn.236">Online</a>.&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>Jehan, T.,  “Analyzer Documentation: The EchoNest.”  Somerville: The Echo Nest Corporation, 2014, 5.&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p>Finley, M. and Razi, A.,  “Musical Key Estimation with Unsupervised Pattern Recognition”  IEEE 9th Annual Computing and Communication Workshop and Conference, Las Vegas, USA, January 2019.&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13">
<p>Mahieu, R.,  “Detecting Musical Key with Supervised Learning”  unpublished manuscript, 2017.&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14">
<p>Hindemith, P.,  <em>The Craft of Musical Composition: Theoretical Part - Book 1</em> . Schott, Mainz, 1984.&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry><entry><title type="html">Networks, Maps, and Time: Visualizing Historical Networks Using Palladio</title><link href="https://rlskoeser.github.io/dhqwords/vol/15/1/000534/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://rlskoeser.github.io/dhqwords/vol/15/1/000534/</id><author><name>Melanie Conroy</name></author><published>2021-03-05T00:00:00+00:00</published><updated>2021-03-05T00:00:00+00:00</updated><content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>Data visualization is a complex process because creating a single visualization requires an understanding of principles of design and mathematics, as well as the content that is being visualized. For humanists, finding collaborators in data science and design can be a practical and financial challenge. Fortunately, there are some tools and tool sets which can aid with the design process and prototyping of diagrams. In this tool review, I revisit Palladio <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>, a digital humanities package developed in the Humanities + Design Lab at Stanford University in a collaboration with the DensityDesign Lab in Milan. Palladio has now been available for more than five years and has been used to visualize everything from the appearances of  “ladies orchestras”  in Europe from 1870 to 1918 <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> to participants in Black Tech from 2014 to 2018 <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>. I have surveyed as many projects using the tool as I could find and will include examples of use cases and feedback on the tool from projects worldwide.</p>
<p>Palladio is a tool for filtering data and quickly producing diagrams that display data spatially and alongside temporal dimensions like timelines and timespans; it lacks the network analytics of software packages built for mathematical analysis. The online app allows for the presentation of multifaceted data, such as network data with date ranges or categories like network type with location data. Palladio was developed in the Humanities + Design Lab by Dan Edelstein, Nicole Coleman, Ethan Jewett, Giorgio Caviglia, and others to fulfill partially the vision for  “Knot,”    “an interface for the study of social networks in the humanities”   <sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>. The Palladio tool suite lends itself to qualitative studies because the visualizations that it produces (maps, network graphs, tables, and galleries) are familiar to most humanists. Producing iterative versions of such diagrams for the sake of quick comparison between them is a good way to explore multifaceted data that have not been curated for one use or with one particular hypothesis in mind — a collaborative research process which has been referred to as  “design thinking”   <sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>. As such, Palladio is a powerful tool for exploring data and patterns qualitatively. Unlike many other network graph packages, like Gephi <sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>, with which it shares some network graphing features, Palladio does not have the capacity for network analytics and is limited in choices such as graph layouts, which are not customizable in Palladio but are in Gephi. Rather, Palladio presents an overall good fit for historical case studies. It is in this respect that Palladio is a kind of  “middleware”  — in the terms of Drucker and Svensson — or a software solution that makes important design decisions for the user and makes some of these design decisions less apparent to viewers of any resulting visualizations <sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>.</p>
<p>Palladio was developed for use by historians and those working in related disciplines like area studies, art history, literary history, and media and cultural studies, in which historical time and space are useful concepts. The discussions which led to the creation of the tool followed a lab model: that is,  “analyzing best practices, employing design research methods, engaging in peer critiques, involving usability testing and user evaluations, and further expanding the role of infoVis to the content of service design and the trends of design for social good”   <sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>. The lab model, which is collaborative and open-ended, has frequently gone by the trendy name design thinking but is really just an abstracted form of the design process, which, in the case of Palladio, meant an ongoing collaboration between humanists and designers. The focus on ease of use, combining types of diagrams, and quick prototyping came out of Mapping the Republic of Letters <sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup>  <sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup>, a large project of which many component projects are historical, as well as a 2012 Humanities + Design Lab workshop that focused on the visualization of time and networks,  “Early Modern Time &amp; Networks.”  While those activities did not share a single methodology or set of research questions, they, for the most part, dealt with social networks in Early Modern Europe and the Americas from 1500 to 1800. The diversity of research projects presented a variety of use cases, from correspondence networks to travel itineraries to galleries of individuals’ photos. The involvement of DensityDesign was decisive for both the use of Mike Bostock’s d3.js library in visualizations and for the focus on prototyping and flexibility of tools. Like DensityDesign’s other open source web tool, RAWGraphs <sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>, Palladio provides  “a missing link between spreadsheet applications (e.g. Microsoft Excel, Apple Numbers, OpenRefine) and vector graphics editors (e.g. Adobe Illustrator, Inkscape, Sketch)”   <sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>. Whereas Palladio generates mainly maps and network graphs, RAWGraphs can produce a wide range of charts from box plots to scatterplots to treemaps <sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup>. As Yvette Shen has observed, RAWGraphs  “adopt[s] clean and crisp aesthetics”  of modern graphic design influenced by the Bauhaus School and the International Typographic Style with  “geometric layout, orderly typography, effective use of white space, and simple color composition”   <sup id="fnref1:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>. Palladio similarly makes use of a modern graphic design style for both the interface and outputs.</p>
<p>Since historians often have set archives or pre-established datasets but do not necessarily have preconceived ideas of what patterns they will find in the data, priority was placed on the ability to rapidly prototype diagrams to see whether interesting patterns appear, as well as the ability to filter data in order to look at subsets. Simplicity of the interface allows for the quick creation of diagrams that are highly legible in print format or as static images online; visualizations can be exported as SVG files to be refined in vector graphics editors like Adobe Illustrator or Inkscape. The maps and diagrams produced in Palladio are not copyrighted and can be printed in commercial and non-commercial works, unlike maps produced in Google Maps, for example. Palladio can also be used to produce tables and galleries that allow individual records to be explored via filtering by categories, timespans, and timelines. By moving between the maps/graphs and tables/galleries, researchers can find patterns in the data and then identify which records are a part of the patterns. Thus, the focus in using Palladio is on moving between diagrams and between types of diagrams rather than drilling down on one type of visualization or graph.</p>
<h2 id="features-and-case-studies">Features and Case Studies</h2>
<p>In the interest of showing how this prototyping works and also explaining some of the features of Palladio, I will walk through some examples of diagrams produced in Palladio and discuss features like filtering that can be used to rapidly prototype diagrams. I will also give examples from other projects and comments from other researchers who have used each feature in their work. Since they are the most popular features, I will focus most of my commentary on maps and network diagrams. Other demos that review aspects of Palladio include <a href="#posner2014">Posner (2014)</a> and Marten Düring’s review in the  <em>Programming Historian</em> , where he concludes that it is  “platform-independent and particularly easy-to-use”   <sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup>. Düring’s demo includes sample data and a thorough discussion of how to structure network data for visualization in Palladio, which I will not be able to treat here. The data used in my visualizations are correspondence and demographic data from the Electronic Enlightenment Project and the Groupe d’Alembert <sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup>. The metadata are enriched according to the methods described in the article  “The French Enlightenment Network”   <sup id="fnref:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup>. The French Enlightenment Network dataset is available online in multiple formats, with examples of diagrams in various formats here: <a href="http://bit.ly/VisualizingFEN">http://bit.ly/VisualizingFEN</a>. It is worth knowing that the dataset is a list of individuals with basic demographic data (name, birth and death places and years, nationality, gender, etc.) and also placement into one or more networks such as Letters_Philosophical for writers on philosophical subjects, and Aristocracy for members of the nobility.</p>
<p>First, a few words about setting up the software and creating a dataset. Palladio is available as an installable software package or as a web tool; the web tool is available at <a href="http://hdlab.stanford.edu/palladio/">http://hdlab.stanford.edu/palladio/</a>. Either way, access is through a web browser. The first step to creating a visualization is to select Create Project and input data. There are several ways to input data into Palladio. Data can be imported from a Google spreadsheet, a proprietary tablature file (TAB) or the more common tab-separated values table (TSV) or comma-separated values table (CSV), or copied and pasted from other applications like Microsoft Excel, Numbers, TextWrangler, or TextEdit. Data that are pasted into the online form and uploaded are then automatically recognized as text, numbers, dates, coordinates. If the data type is not recognized or misrecognized, that can be corrected by clicking on the pencil icon to open the data verification window and change the data type. Columns can be hidden by clicking on the eye icon; if they are hidden, it will not be possible to visualize the data in that column until the column is made visible again. The filename and the name of the data table can be changed.</p>
<p>Within the data verification window, there is a preview of the data. There are two more functions to be aware of: 1) splitting multiple values and 2) extending the data table. Palladio supports multiple values, such as multiple tags for the same record; for example, the city of Berlin could be tagged German and Large city by creating a column for cities and a column for city characteristics; within the city characteristics column would be German;Large city if the delimiter were a semi-colon. Using multiple values in a cell is useful for unstructured datasets where the rows and columns might be pre-determined but where researchers might want the freedom to add values as the dataset is created. For instance, most people might have one nationality, but a few might have more than one. In this case, most cells would contain one value, like American, but the nationality cell of a binational could contain two values, like American;Mexican, that would be readable as separate values. The delimiter (any character, often a comma or semi-colon) can be selected within the data verification window; consecutive delimiters can be treated as one so that a comma and a space are counted as one.</p>
<p>The second feature, extending the data table, is useful if the dataset contains more than one table, or if the dataset has many repeated values which are consistently associated with the same values. One use case is for LatLong coordinates; where the primary dataset contains the same place multiple times, LatLong coordinates can be associated with those places by extending the Places column with a data table consisting of the Places as text and another column of coordinates. Similarly, metadata about texts and their authors can be kept on separate spreadsheets and combined within Palladio as long as there is one column in each data table that matches the values in a column in the other table.</p>
<p>Once the dataset is imported and verified, the next step is to choose a type of visualization. Not all visualizations work for all data types. Map View needs coordinates. The Network Graph View needs at least two columns of data and should only be used if there is a consistent logic of connection between these two columns. Since Palladio doesn’t allow for much customization of graphs, the eventual product will be a bipartite graph with two types of nodes or entities. The edges, or links, cannot be weighted. The Gallery View requires images. Tables are very versatile and work well with large datasets. For this reason, it is important to think about what to do with Palladio before producing a dataset. Palladio is very flexible in importing data, but once it is imported, extending the data table may be the only way to add missing data types, and data cannot be edited within Palladio. I will now look at how to produce the four kinds of diagrams and some notable features of each. Mark Braude produced a very brief overview of features with screenshots <sup id="fnref:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup>. A more technical overview of features is available in <a href="#hdl2014">Humanities + Design Lab (2014)</a>.</p>
<h2 id="two-kinds-of-maps-points-versus-point-to-point">Two Kinds of Maps: Points Versus Point-to-Point</h2>
<p>First, I will look at the Map View in Palladio. There are two map types: 1) maps with points sized according to defined quantities, which are useful for displaying static geographic information, and 2) point-to-point diagrams, which are more useful for showing movement or change over time. Maps can be used to compare the weight, or influence, of cities, as well as travel and communication between cities and other geographical points. The data for both of these map types can be filtered to show a subset of the data, according to categories chosen by the user.</p>
<p>The first type of diagram (Figure 1) consists of points on a map. Producing diagrams of this kind requires LatLong coordinates for all locations. Such maps can easily display quantities associated with a particular place — for example, quantities of people, events, or items. Points maps are recommended for data on population, the production of books, the number of events, and so on. The nodes can be sized by the number of records at each LatLong. Figure 1 shows the predominance of Paris as a birth place for members of the French Enlightenment network. While the size of smaller points can be hard to make out, numbers of records and the names of places can be displayed on a tool tip so that hovering over each circle reveals the name and number of records at each point. Unlike in a list, it is easy to visually compare the weight of regions. Here the weight of the southeast is notable, despite the relative insignificance of each point in comparison to the total numbers for Paris. It is also visible that Lyon, at the center of the map, is a significant location for the birth of members of this group, although the central region otherwise has few important locations.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000534/resources/images/figure01.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000534/resources/images/figure01_hu7d6fc472eefa0294d8f0354fa156109b_2741990_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000534/resources/images/figure01_hu7d6fc472eefa0294d8f0354fa156109b_2741990_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000534/resources/images/figure01_hu7d6fc472eefa0294d8f0354fa156109b_2741990_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000534/resources/images/figure01_hu7d6fc472eefa0294d8f0354fa156109b_2741990_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/1/000534/resources/images/figure01_hu7d6fc472eefa0294d8f0354fa156109b_2741990_1800x0_resize_box_3.png 1800w,/dhqwords/vol/15/1/000534/resources/images/figure01.png 2938w" 
     class="landscape"
     ><figcaption>
        <p>Birth Places of Members of the French Enlightenment Network
        </p>
    </figcaption>
</figure>
<p>The strength of the points map is how easily it generates a simple geographic visualization. A list of places with LatLongs can be inputted in the data panel and visualized within seconds. Alternatively, a dataset can be uploaded with only the names of places and a separate table with LatLongs can be linked to the dataset. Doing so is recommended for large datasets with repeated places. Finally, a list of places with LatLongs could be uploaded with numerical values, such as the number of people born there. The flexibility of the tool means that many types of data can be visualized. It also means that an identical visualization can be produced in multiple ways without modifying the data. One limitation is that latitude and longitude coordinates must be in the same cell separated by a comma; both values must be combined before importing the LatLong data into Palladio.</p>
<p>The second kind of map is a point-to-point diagram (see Figure 2). Point-to-point maps are particularly useful for humanities research because they show network connections in a way that is intuitive to humanists by overlaying them on a map.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000534/resources/images/figure02.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000534/resources/images/figure02_hu7d6fc472eefa0294d8f0354fa156109b_1509628_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000534/resources/images/figure02_hu7d6fc472eefa0294d8f0354fa156109b_1509628_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000534/resources/images/figure02_hu7d6fc472eefa0294d8f0354fa156109b_1509628_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000534/resources/images/figure02_hu7d6fc472eefa0294d8f0354fa156109b_1509628_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/1/000534/resources/images/figure02_hu7d6fc472eefa0294d8f0354fa156109b_1509628_1800x0_resize_box_3.png 1800w,/dhqwords/vol/15/1/000534/resources/images/figure02.png 2938w" 
     class="landscape"
     ><figcaption>
        <p>Linked Birth and Death Places of Members of the French Enlightenment Network with Timespan
        </p>
    </figcaption>
</figure>
<p>Point-to-point maps are ideal for displaying trajectories, such as travel, or communication between population centers, such as the sending of letters. This map type requires LatLong coordinates for two distinct locations. The edges — the connections between points — could represent any number of other network relationships between two locations. The points are sized according to the total count of the items associated with that location. Features that users found lacking include the lack of directionality in point-to-point maps <sup id="fnref:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup> and an inability to produce interactive maps and diagrams <sup id="fnref:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup>.</p>
<h2 id="filtering">Filtering</h2>
<p>While maps are not new tools for humanists, digital technology allows for more sophisticated and rapid comparisons between subsets of data. Palladio makes filtering data easy and quick; in fact, producing a different map takes only a few seconds. In the case of the French Enlightenment Network, the individuals have been coded by gender, nationality, social networks (Elite, Aristocracy, Military, Court), professional networks (Artisan, Finance-banking, Cultural, etc.), and knowledge networks (Letters, Sciences, etc). These individuals also have birth years, death years, birthplaces, and death places associated with them, where this data is known. Filtering allows us to look at a series of maps and compare them easily. Figure 3 shows the linked birth and death places of aristocrats in the French Enlightenment Network.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000534/resources/images/figure03.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000534/resources/images/figure03_hu7d6fc472eefa0294d8f0354fa156109b_1520907_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000534/resources/images/figure03_hu7d6fc472eefa0294d8f0354fa156109b_1520907_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000534/resources/images/figure03_hu7d6fc472eefa0294d8f0354fa156109b_1520907_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000534/resources/images/figure03_hu7d6fc472eefa0294d8f0354fa156109b_1520907_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/1/000534/resources/images/figure03_hu7d6fc472eefa0294d8f0354fa156109b_1520907_1800x0_resize_box_3.png 1800w,/dhqwords/vol/15/1/000534/resources/images/figure03.png 2938w" 
     class="landscape"
     ><figcaption>
        <p>Linked Birth and Death Places of Members of the French Enlightenment Network Filtered (Aristocracy)
        </p>
    </figcaption>
</figure>
<p>In order to filter data, a third column should include the categories of places to be visualized; in this case, the birth places can be filtered by the gender of the participants in the French Enlightenment or by their participation in other networks like academies and salons. By comparing maps filtered by category or timeline/timespan, we can look for patterns in subsets of the data.</p>
<p>In Figure 4, the linked birth and death places are displayed on a map alongside a timeline of death years as a bar chart. The timeline can be opened or closed with the arrows on the right side. It can also be deleted by clicking on the red icon of a garbage can.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000534/resources/images/figure04.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000534/resources/images/figure04_hu7d6fc472eefa0294d8f0354fa156109b_1493340_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000534/resources/images/figure04_hu7d6fc472eefa0294d8f0354fa156109b_1493340_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000534/resources/images/figure04_hu7d6fc472eefa0294d8f0354fa156109b_1493340_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000534/resources/images/figure04_hu7d6fc472eefa0294d8f0354fa156109b_1493340_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/1/000534/resources/images/figure04_hu7d6fc472eefa0294d8f0354fa156109b_1493340_1800x0_resize_box_3.png 1800w,/dhqwords/vol/15/1/000534/resources/images/figure04.png 2938w" 
     class="landscape"
     ><figcaption>
        <p>Linked Birth and Death Places of Members of the French Enlightenment Network with Timeline by Gender
        </p>
    </figcaption>
</figure>
<p>In this diagram, the data points are grouped by gender. The light grey bars represent the number of women’s deaths in that year and the dark grey bars represent men’s deaths in that year. As we can see, there are many more men in the dataset who died in all years, and sometimes no women, yet the women who do appear in the dataset were more likely to die in later years, and very few women died in the early part of the eighteenth century. This supports the idea of women becoming more important in correspondence networks later in the century. Within Palladio, the timeline can also be filtered by selecting part of the timeline. In that case, only some of the point-to-point trajectories will be seen on the map. The data can also be grouped by other categories like nationality and social networks so that we can compare the geographic and temporal distribution of those groups using the Time Line and the Map View together.</p>
<p>The Map View with a Time Line has been one of the most popular combinations. The Sphaera project uses Palladio (among other tools) to investigate  “the process of emergence of new epistemic communities”   <sup id="fnref:20"><a href="#fn:20" class="footnote-ref" role="doc-noteref">20</a></sup>. They use the Map View with a Time Line to visualize the  “geotemporal distribution of the production of the treatises”  in their corpus. Braude’s demo video  “Monte Carlo arrival”   <sup id="fnref:21"><a href="#fn:21" class="footnote-ref" role="doc-noteref">21</a></sup> shows how to use Palladio’s point-to-point map and a timeline to create a video showing historical growth over time. A similar effect can be used by advancing the timeline and taking snapshots of the map or network and then labelling the images. The Map View has also been used by various projects to map data from the Early Modern Letters Online data; one example is  “Samuel Hartlib’s social relationships”  on a map with a Time Line <sup id="fnref:22"><a href="#fn:22" class="footnote-ref" role="doc-noteref">22</a></sup>.</p>
<h2 id="network-graphs">Network Graphs</h2>
<p>The second type of data visualization to discuss is the Graph View, which is useful in the study of networked people or things. A network can link people, places, books, or any other entities that are connected to one another. People or groups are commonly nodes in graphs of social networks. Network graphs are helpful in seeing connections between people or groups. They also aid in understanding how groups are structured. A network graph is a set of points (called nodes), connected by links (called edges). There exist related software packages for network analysis — notably Gephi, a computer program designed for the quantitative study of networks that is a more customizable version of the network function in Palladio. Palladio&rsquo;s network feature has been used to map the movement of Chinese immigrants from the village of their origin to destinations in Canada, mostly on the West Coast, recorded between 1886 and 1949 <sup id="fnref:23"><a href="#fn:23" class="footnote-ref" role="doc-noteref">23</a></sup>. Zhang and Cho found that there was a clustering effect where immigrants to a particular destination were more likely to come from the same villages in China. That said, Palladio does not allow for the computation of clustering coefficients or other analytics that would work at a larger scale to draw out such relations in a larger dataset.</p>
<p>For this demo, I choose Graph from the top menu. Figure 5 shows the academy affiliations of members of the French Enlightenment Network with the knowledge networks of academy members. Each edge shows a knowledge network that is present in an academy. The nodes are sized according to the number of members of the academy or knowledge network. The edges represent knowledge networks that share members of academies — in other words, this network graph shows the interrelation of fields and academies, not just the disciplines which are represented in the academies but all of the major interests of academy members, including those that are not represented in the academies.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000534/resources/images/figure05.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000534/resources/images/figure05_hu7d6fc472eefa0294d8f0354fa156109b_932656_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000534/resources/images/figure05_hu7d6fc472eefa0294d8f0354fa156109b_932656_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000534/resources/images/figure05_hu7d6fc472eefa0294d8f0354fa156109b_932656_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000534/resources/images/figure05_hu7d6fc472eefa0294d8f0354fa156109b_932656_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/1/000534/resources/images/figure05_hu7d6fc472eefa0294d8f0354fa156109b_932656_1800x0_resize_box_3.png 1800w,/dhqwords/vol/15/1/000534/resources/images/figure05.png 2938w" 
     class="landscape"
     ><figcaption>
        <p>Network of Academy Affiliations and Knowledge Networks
        </p>
    </figcaption>
</figure>
<p>Finally, the network graph can be downloaded as an SVG file to further customize the vector graphics or to integrate the vector graphics into another visual document. The simple design of the Graph View highlights Palladio’s modern sense of design and a lack of scientific complexity, which can be refreshing from a design standpoint but, perhaps, frustrating if taken from the standpoint of a scientific researcher seeking to encode the maximum amount of information into the data, rather than make a clear and comprehensible diagram which follows modern design aesthetics.</p>
<h2 id="tables">Tables</h2>
<p>A third data visualization, the Table View, is used for creating lists and reordering data according to various categories. This function is available in spreadsheet software like Excel, Google Sheets, or OpenRefine, but the tables function in Palladio is easier to use and does not require the use of formulas or macros. The data table can also contain more than one column that is rearranged according to the categories in the first column. Finally, the data table can be filtered easily.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000534/resources/images/figure06.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000534/resources/images/figure06_hu10ff45fec0d2ce0ce2c9f2fb6eeff916_662733_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000534/resources/images/figure06_hu10ff45fec0d2ce0ce2c9f2fb6eeff916_662733_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000534/resources/images/figure06_hu10ff45fec0d2ce0ce2c9f2fb6eeff916_662733_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000534/resources/images/figure06_hu10ff45fec0d2ce0ce2c9f2fb6eeff916_662733_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/1/000534/resources/images/figure06_hu10ff45fec0d2ce0ce2c9f2fb6eeff916_662733_1800x0_resize_box_3.png 1800w,/dhqwords/vol/15/1/000534/resources/images/figure06.png 2470w" 
     class="landscape"
     ><figcaption>
        <p>Table of Academies and People by Knowledge Network
        </p>
    </figcaption>
</figure>
<p>The first step to creating a table in Palladio is to choose a row dimension. The row dimension provides the categories according to which the rest of the data table will be re-sorted. If the underlying data have multiple values in the same cell and have been split, the row categories will reflect that. The example in Figure 6 shows for the French Enlightenment Network the knowledge network categories (i.e. Sciences_Natural, Sciences_Medicine, Letters, Letters_Literary, etc) split by a semi-colon delimiter to create a total of 12 rows. The next step is to choose the category, or categories, for the other column dimensions; here I have chosen academies (which were split by a delimiter) and full names, reversed (which were not). While these same functions can be performed in Excel, adding and removing dimensions is quick and easy in Palladio, once the data are loaded. The primary purpose of the tables function is to examine the data up close. Having found patterns in the map or graph windows, such as connections between two places on a map or between a field of study or an academy on a network graph, we can easily see which people or entities are implicated in the table function. In fact, using filters, we can even examine simultaneously the connection between people, academies, and places.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000534/resources/images/figure07.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000534/resources/images/figure07_hu10ff45fec0d2ce0ce2c9f2fb6eeff916_738815_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000534/resources/images/figure07_hu10ff45fec0d2ce0ce2c9f2fb6eeff916_738815_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000534/resources/images/figure07_hu10ff45fec0d2ce0ce2c9f2fb6eeff916_738815_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000534/resources/images/figure07_hu10ff45fec0d2ce0ce2c9f2fb6eeff916_738815_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/1/000534/resources/images/figure07_hu10ff45fec0d2ce0ce2c9f2fb6eeff916_738815_1800x0_resize_box_3.png 1800w,/dhqwords/vol/15/1/000534/resources/images/figure07.png 2470w" 
     class="landscape"
     ><figcaption>
        <p>Table of Academies and People by Knowledge Network, Filtered by Paris as Birth Place
        </p>
    </figcaption>
</figure>
<p>Tables can be filtered by categories, timespan, or timeline to dig deeper into the data. Let’s look at filtering by categories, since that is the most flexible way of filtering. In this case, we already know who was associated with particular knowledge networks and of which academies they were members. If we want to know which of those people were Parisians, we can simply filter the data table to find only the people who were born in Paris, who died in Paris, or both (Figure 7); this is possible because the underlying data already contains that information. If we noticed interesting patterns in where individuals were born, we could reverse this process and sort the table according to the row dimension Birth Place, and then add knowledge networks and academies as additional dimensions to see lists of the academies and knowledge networks associated with those places. Moving between tables and maps and graphs allows researchers to see the larger picture and dive deeper into the data without using multiple tools, yet the ability to look at subsets of the data is more limited than in tools like OpenRefine, or, indeed, Microsoft Excel.</p>
<h2 id="galleries">Galleries</h2>
<p>The fourth kind of visualization which can be produced in Palladio is the Gallery, or an array of thumbnails. Galleries are filterable and are particularly useful for large collections of images where the visual aspects of the image are important enough to make comparing thumbnail versions of the images productive. Each record must contain the URL of an image which is openly on the internet on sites like Wikimedia, Imgur, or Google Images. The links can be to images on multiple sites, but the images must be publicly hosted; it is not possible to link to private or paywalled repositories like Artstor. It is possible to designate a column as containing the title and subtitle for each image and another as containing the URL linking the image.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000534/resources/images/figure08.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000534/resources/images/figure08_hu998f443fd015cb34609a18b83dfd9d7d_1715246_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000534/resources/images/figure08_hu998f443fd015cb34609a18b83dfd9d7d_1715246_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000534/resources/images/figure08_hu998f443fd015cb34609a18b83dfd9d7d_1715246_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000534/resources/images/figure08_hu998f443fd015cb34609a18b83dfd9d7d_1715246_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/1/000534/resources/images/figure08_hu998f443fd015cb34609a18b83dfd9d7d_1715246_1800x0_resize_box_3.png 1800w,/dhqwords/vol/15/1/000534/resources/images/figure08.png 2326w" 
     class="landscape"
     ><figcaption>
        <p>Gallery of Members of the French Enlightenment Network
        </p>
    </figcaption>
</figure>
<p>In Figure 8, the image links are Wikimedia image URLs for portraits of historical individuals, the URLs are Wikipedia for biographies, the full name (reversed) is designated as the title, and the subtitle is the person’s occupation from the Electronic Enlightenment data.</p>
<p>Galleries can be quickly filtered so that only the images associated with a specific value in another column will be shown; for instance, by gender or professional network. They can also be filtered by timeline or timespan.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000534/resources/images/figure09.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000534/resources/images/figure09_hud8f0eae174e6c08abfeb36c683b68990_1337831_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000534/resources/images/figure09_hud8f0eae174e6c08abfeb36c683b68990_1337831_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000534/resources/images/figure09_hud8f0eae174e6c08abfeb36c683b68990_1337831_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000534/resources/images/figure09_hud8f0eae174e6c08abfeb36c683b68990_1337831_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/1/000534/resources/images/figure09_hud8f0eae174e6c08abfeb36c683b68990_1337831_1800x0_resize_box_3.png 1800w,/dhqwords/vol/15/1/000534/resources/images/figure09.png 2412w" 
     class="landscape"
     ><figcaption>
        <p>Gallery of Members of the French Enlightenment Network with Filtered Timespan
        </p>
    </figcaption>
</figure>
<p>Figure 9 shows the same gallery with a timespan filter and social networks as the subtitle. The Timespan filter has three different layouts: bars, parallel, or grouped bars. Filtering by timespan requires start and end dates. It is also possible to choose a data column to group the bars and a tooltip label. The timespan can be filtered by clicking and holding to select a portion of the timespan, and only the entities whose timelines appear within that span will be displayed in the gallery. While the galleries are not very different from the tables insofar as they primarily show lists of entities which can be filtered, tables allow for easier reorganization of data and galleries foreground the visual element. Galleries do have additional uses in art history, architecture, film studies, and other disciplines where large numbers of images may be associated with dates or other time stamps and being able to filter those images by category or time could bring additional insight.</p>
<h2 id="conclusions">Conclusions</h2>
<p>As quintessential middleware for digital historians and literary/cultural historians, Palladio is ideal for producing static maps and network diagrams sequentially to explore data, especially when time and geography are important elements of the dataset. The ability in Palladio to toggle between different types of visualizations without re-importing the dataset is a benefit for prototyping diagrams and moving between the visualization and the data. The option to filter data and split multiple values means that many different data structures can be used, and the same dataset can produce many different types of diagrams. The static diagrams produced in Palladio can be used in publications, since high resolution screenshots can meet standards for publication. The SVG and TXT files can also be downloaded for further processing and incorporation into more elaborate graphics in programs like Adobe Illustrator. The same diagrams can be produced with different color and contrast settings, although the land and streets greyscale base maps do not have high enough contrast to be legible when projected; the full-color Buildings and Areas,  Terrain, or Satellite maps are usually better for presentations. Similar issues may arise with diagrams produced in the other tools, since they are optimized for greyscale publication and the contrast may need to be increased or color introduced in order to make projected diagrams visible. This emphasis on maps and time is a strength when projects have a strong geospatial element but can be a weakness when geospatial patterns are less relevant to the matter at hand.</p>
<p>Finally, Palladio is useful in teaching because the data and formatting are easily shareable. It can also be used when teaching how to create visualizations and how to use visualization to explore data, as Joel Blecher has shown in his pedagogical article on using Palladio, along with RAW graphs, in group work in a class on the transmitters of early Islamic law <sup id="fnref:24"><a href="#fn:24" class="footnote-ref" role="doc-noteref">24</a></sup>. The ability to download a formatted JSON file and share that with other researchers or students means that others can import pre-verified data — for example, a map which already has the coordinates selected and visualized or a gallery which already has the filters applied. Because the formatting is included in the JSON file, the person receiving the file, whether a student or a researcher, can open the file where the previous researcher left off and see what that person saw. Any changes to the formatting can be saved by the second person and sent back as a JSON file to the first person. This makes Palladio an ideal tool for experimentation by students for creating different diagrams, especially when there is not class time to create and verify an original dataset. Students can use the same dataset, or a selection of pre-verified datasets, and still save their work. A student who has created her own dataset can also share it with the class or instructor, and others can see both the visualizations and the underlying data.</p>
<p>While Palladio is a very flexible tool, it does have some inherent limitations, most of which come from the fact that it is highly useable. Data cannot be edited within Palladio. In practice, this often means creating a dataset, importing it into Palladio, visualizing it, and then discovering errors — such as typos which cause the value to appear twice in a network diagram or differing coordinates for the same place — only to have to restart the process to correct the data. This limitation means that Palladio is very useful for creating many diagrams quickly with a verified dataset — or, indeed, finding problems or outliers in a dataset — but Palladio is consequentially most useful at the exploratory phase. As one gets to know the dataset better and finds patterns that are worth visualizing, it may become apparent that a more complex visualization is necessary to show patterns that appear within filtered maps or network graphs. One way to show such patterns in publication is small multiples, or small diagrams with variations in what is presented, displayed side-by-side. If small multiples are insufficiently complicated, more sophisticated diagrams may be necessary. In order to create complex network diagrams or interactive maps, the researcher will have to use tools that are more customizable and have more options for visualization and design than Palladio.</p>
<p>Palladio is most useful at the exploratory stages of a project, for teaching, and for collectively working through visualization problems, and less so in the latter stages of a complex project. The core assumption of the technology is that information visualization is primarily a process of designing visualizations that make historical arguments, rather than quantitative or scientific ones. The prioritization of moving between individual data points and their whole presumes enough familiarity with the underlying dataset that both the tabular information and the visualizations will make patterns apparent to the researcher rather than providing quantitative answers.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Humanities + Design Lab.  “Tutorials and FAQs,”  Palladio (2014), <a href="https://hdlab.stanford.edu/palladio/help/">https://hdlab.stanford.edu/palladio/help/</a>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Koivisto, N.  “New Data, New Methods? Sources on Ladies’ Salon Orchestras in Europe, 1870–1918,”     <em>Музикологија / Musicology</em>  26 (2019): 41-60.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>MikeR (@nahamike01)  “Still playing around with graphs&hellip;,”  Tweet (November 28, 2019), <a href="https://twitter.com/nahamike01/status/1200179039090036736">https://twitter.com/nahamike01/status/1200179039090036736</a>&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Uboldi, G., Caviglia, G., Coleman, N., Heymann, S., Mantegari, G., and Ciuccarelli, P.  “Knot: An Interface for the Study of Social Networks in the Humanities,”  in  <em>Proceedings of the Biannual Conference of the Italian Chapter of SIGCHI</em>   (CHItaly &lsquo;13) (2013), article 15. DOI: <a href="https://doi.org/10.1145/2499149.2499174">https://doi.org/10.1145/2499149.2499174</a>.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>Ciuccarelli, P.  “Mind the Graph: From Visualization to Collaborative Network Constructions,”    <em>Leonardo</em>  47.3 (2014): 268-269. DOI: <a href="https://doi.org/10.1162/LEON_a_00772">https://doi.org/10.1162/LEON_a_00772</a>&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>Bastian, M., Heymann, S., &amp; Jacomy, M.  “Gephi: An Open Source Software for Exploring and Manipulating Networks,”  in  <em>Third International AAAI Conference on Weblogs and Social Media</em>  (2009), pp. 361-362. DOI: <a href="https://doi.org/10.1007/978-1-4614-6170-8_299">https://doi.org/10.1007/978-1-4614-6170-8_299</a>.&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>Drucker, J. &amp; Svensson, P.  “The Why and How of Middleware,”    <em>Digital Humanities Quarterly</em>  10.2 (2016), <a href="/dhqwords/vol/10/2/000248/">http://digitalhumanities.org/dhq/vol/10/2/000248/000248.html</a>&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>Shen, Y.  “Placing Graphic Design at the Intersection of Information Visualization Fields,”    <em>Digital Humanities Quarterly</em>  21.4 (2018), <a href="/dhqwords/vol/12/4/000406/">http://digitalhumanities.org/dhq/vol/12/4/000406/000406.html</a>&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>Humanities + Design Lab.  “Mapping the Republic of Letters,”  Mapping the Republic of Letters [2019], <a href="http://republicofletters.stanford.edu/">http://republicofletters.stanford.edu/</a>&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>Edelstein, D., Findlen, P., Ceserani, G., Winterer, C., and Coleman, N.  “Historical Research in a Digital Age: Reflections from the Mapping the Republic of Letters Project,”    <em>The American Historical Review</em>  122.2 (2017): 400-424. DOI:  <a href="https://doi.org/10.1093/ahr/122.2.400">https://doi.org/10.1093/ahr/122.2.400</a>.&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>DensityDesign Lab.  “RAWGraphs tool,”  RAWGraphs [2013], <a href="http://app.rawgraphs.io">http://app.rawgraphs.io</a>&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p>DensityDesign Lab.  “Research: RAWGraphs,”  DensityDesign Lab [2019], <a href="https://densitydesign.org/research/raw/">https://densitydesign.org/research/raw/</a>&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13">
<p>Mauri, M., Elli, T., Caviglia, G., Uboldi, G., and Azzi, M.  “RAWGraphs: A Visualisation Platform to Create Open Outputs,”  in  <em>Proceedings of the 12th Biannual Conference on Italian SIGCHI Chapter</em>   (CHItaly &lsquo;17) (2017), article 28. DOI: <a href="https://doi.org/10.1145/3125571.3125585">https://doi.org/10.1145/3125571.3125585</a>.&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14">
<p>Düring, M.  “From Hermeneutics to Data to Networks: Data Extraction and Network Visualization of Historical Sources,”     <em>The Programming Historian</em>   4 (2015), <a href="https://programminghistorian.org/en/lessons/creating-network-diagrams-from-historical-sources">https://programminghistorian.org/en/lessons/creating-network-diagrams-from-historical-sources</a>&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:15">
<p>Comsa, M.T., Conroy, M., Edmondson, C., Edelstein, D., Willan, C., Groupe d’Alembert, &amp; Electronic Enlightenment Project.  “French Correspondents of Major Enlightenment Figures,”  Stanford Digital Repository (2014), <a href="http://purl.stanford.edu/bc436tm1194">http://purl.stanford.edu/bc436tm1194</a>&#160;<a href="#fnref:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:16">
<p>Comsa, M.T., Conroy, M., Edmondson, C., Edelstein, D., &amp; Willan, C.  “The French Enlightenment Network,”    <em>The Journal of Modern History</em>  88.3 (2016): 495-534. DOI: <a href="https://doi.org/10.1086/687927">https://doi.org/10.1086/687927</a>.&#160;<a href="#fnref:16" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:17">
<p>Braude, M.  “Palladio: Humanities thinking about data visualization,”    <em>Hestia</em>  (October 6, 2014), <a href="https://hestia.open.ac.uk/palladio-humanities-thinking-about-data-visualization/">https://hestia.open.ac.uk/palladio-humanities-thinking-about-data-visualization/</a>&#160;<a href="#fnref:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:18">
<p>Jensen, C., Sisk, K., and Braunstein, A.  “Using Palladio to Visualize Ads,”  Digital History Methods (2014), <a href="http://ricedh.github.io/01-palladio.html">http://ricedh.github.io/01-palladio.html</a>&#160;<a href="#fnref:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:19">
<p>Posner, M.  “Getting started with Palladio,”  Miriam Posner’s Blog (2014), <a href="https://miriamposner.com/blog/getting-started-with-palladio/">https://miriamposner.com/blog/getting-started-with-palladio/</a>&#160;<a href="#fnref:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:20">
<p>Valleriani, M., Kräutli, F., Zamani, M., Tejedor, A., Sander, C., Vogl, M., Betram, S., Funke, G., &amp; Kantz, H.  “The Emergence of Epistemic Communities in the  Sphaera  Corpus: Mechanisms of Knowledge Evolution,”    <em>Journal of Historical Network Research</em>   3 (2019): 50-91. DOI: <a href="https://doi.org/10.25517/jhnr.v3i1.63">https://doi.org/10.25517/jhnr.v3i1.63</a>.&#160;<a href="#fnref:20" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:21">
<p>Braude, M.  “Monte Carlo arrival”  (2015), <a href="https://vimeo.com/122883572">https://vimeo.com/122883572</a>&#160;<a href="#fnref:21" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:22">
<p>Tuominen, J., Mäkelä, E., Hyvönen, E., Bosse, A., Lewis, M., &amp; Hotson, H.  “Reassembling the Republic of Letters – A Linked Data Approach,”  in   <em>DHN</em>   (2018), pp. 76-88.&#160;<a href="#fnref:22" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:23">
<p>Zhang, S. &amp; Cho, A.  “Untapped Potential: Mining Register of Chinese Immigrants to Canada, 1886-1949 Using R and Palladio,”  OSF Preprints [2018], <a href="https://osf.io/xg5nj">https://osf.io/xg5nj</a>.&#160;<a href="#fnref:23" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:24">
<p>Blecher, J.  “Pedagogy and the Digital Humanities: Undergraduate Exploration into the Transmitters of Early Islamic Law.”   In E. Muhanna (ed.),  <em>The Digital Humanities and Islamic &amp; Middle East Studies</em> , Berlin: De Gruyter (2016), pp. 233-249.&#160;<a href="#fnref:24" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry><entry><title type="html">PodcastRE Analytics: Using RSS to Study the Cultures and Norms of Podcasting</title><link href="https://rlskoeser.github.io/dhqwords/vol/15/1/000519/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://rlskoeser.github.io/dhqwords/vol/15/1/000519/</id><author><name>Eric Hoyt</name></author><author><name>J.J. Bersch</name></author><author><name>Susan Noh</name></author><author><name>Samuel Hansen</name></author><author><name>Jacob Mertens</name></author><author><name>Jeremy Wade Morris</name></author><published>2021-03-05T00:00:00+00:00</published><updated>2021-03-05T00:00:00+00:00</updated><content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>As a cultural form, podcasting resists easy definition. It is a highly porous medium, traveling with us over earbuds, phone speakers, and car stereos, accompanying us on commutes, jogs, errands, and road trips. It’s a sound-based media that we also experience visually through live shows, thumbnail icons, and t-shirts that say  “Friend of the Pod”  or  “Night Vale Community College.”  Despite these definitional challenges, the medium, by most measures, is booming — with the quantity of podcasts, listeners, advertising revenue, and non-profit funding increasing sharply year after year, including an  “explosive”  2018, which saw the number of U.S. people over the age of twelve who have ever listened to a podcast climb above 50% for the first time <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>  <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>.</p>
<p>If we consider podcasts from a purely technical standpoint, it is possible to narrow the definition slightly. As we have elaborated elsewhere, a podcast can be defined as a collection of downloadable files, of any format, served, with accompanying metadata, via an open updatable internet feed, primarily RSS <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>. An XML-based protocol, RSS allows for podcasters to easily publish their completed work and distribute it to audiences, who can opt to subscribe to particular feeds. In many ways, the metadata and the open feed are what separate a podcast from other media files on the internet, including other forms of on-demand audio (for example, music streaming platforms and audio book companies). Because RSS feeds are open, podcasting is platform-independent. Listeners can subscribe to feeds through a number of different podcatching apps and a variety of platforms. At least for now.</p>
<p>Both the expansive cultural meanings of podcasting and the rigid technical definition have animated our work over the past three years on the PodcastRE database and our desire to study and preserve this emerging format. Based at the University of Wisconsin-Madison and supported by grants from the university and the NEH, PodcastRE (short for Podcast Research and accessible at <a href="http://podcastre.org">http://podcastre.org</a>) is a data preservation and research initiative. As we write in April 2020, the PodcastRE database has grown to over 2.5 million podcast episodes from over 16,000 unique RSS feeds which occupy 99 terabytes of space within our RAID storage array. The collection has expanded beyond what any individual could listen to within a lifetime, and it only keeps growing.</p>
<p>What can researchers do with millions of podcast episodes and their associated metadata? This article seeks to contribute to the body of digital humanities scholarship invested in harnessing the affordances of digital technology to investigate cultural data at a large scale <sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>  <sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>  <sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>  <sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>. We argue that podcasting’s unique technical affordances (e.g. RSS and metadata) open up productive methods for exploring the cultural practices and meanings of the medium. These methods, in turn, hold broader relevance for scholars seeking to integrate media studies with computational analysis (or, as the theme of this special issue nicely puts it,  “AV in DH” ). Our study of podcasting shows the ways that metadata can function as a surrogate for studying large collections of time-based media objects. To put it simply, it’s far easier to query 2 million metadata records than it is to query 2 million media files of movies, TV episodes, or audio programs. Yet our study also shows that, when it comes to born digital media, the metadata are never fully separate from the objects they describe, nor can they fully describe, or replace the need for, returning to the media themselves during the final analysis. As a result, future work in AV in DH needs to thoughtfully delineate between methods best suited for  <em>digitized media collections</em>  compared to those most appropriate for  <em>born digital media collections</em> .</p>
<p>In this article, we share three different methods for analyzing the metadata of PodcastRE’s born digital corpus, assessing the strengths and weaknesses of each method and sharing preliminary results. First, as we will share, PodcastRE’s Term Frequency Line Graph (<a href="http://podcastre.org/lineGraph">http://podcastre.org/lineGraph</a>) allows researchers to create visualizations of trending topics and keywords. Interpreting the results of the line graphs can be challenging, however, due to the messiness of the underlying metadata and the problem of  “normalizing”  a rapidly growing corpus and medium. Second, PodcastRE’s Associated Keyword Cloud visualization tool (available at <a href="http://podcastre.org/wordCloud">http://podcastre.org/wordCloud</a>) enables researchers to query a keyword and generate a word cloud that displays the other keywords that appear alongside that keyword in podcasts. We argue that this data visualization harnesses the potential of the medium’s inconsistent and messy metadata and allows for open-ended explorations, serendipitous discovery, and new questions about the agency of podcasters in self-defining their cultural output and connecting it with particular communities and conversations. Third and finally, we share approaches for studying the duration of podcasts. As a time-based media format, podcasting is unusual in that it is not bound by the programming schedules and technical limitations that provide strict parameters for most audiovisual forms, such as movies, television, and radio. If a podcast can run anywhere from a couple of seconds to several hours in length, how do norms and common practices develop that establish optimal models for a podcast’s duration? To investigate these questions, we exported CSVs from the database (using a  “mediaFileDuration”  field generated by the individual episode files), sorted them into meaningful sample groups, and investigated the data for patterns.</p>
<p>Ultimately, our goal for this article is to share research and methods for studying the explosion of audio culture taking place in podcasting and through the sonic communities and conversations podcasting draws together. These methods are especially well suited for studying audio, but they would also be valuable for exploring online video collections and other digital media objects. As in any research study, though, it’s important to address the specific before offering broader generalizations. With that in mind, we would like to now turn to a consideration of the history and design of RSS feeds, how they inform the underlying dataset (the PodcastRE collection), and the affordances and challenges of these structures.</p>
<h2 id="rss-feeds-the-podcastre-collection-and-working-with-messy-metadata">RSS Feeds, the PodcastRE Collection, and Working with Messy Metadata</h2>
<p>As the protocol that has enabled inconsistent and idiosyncratic podcast metadata to proliferate across the internet, it is fitting that there is no singular consensus on what the initials  “RSS”  should stand for. Real Simple Syndication is the most commonly cited meaning. But  “Rich Site Summary”  and  “Resource Description Framework (RDF) Site Summary”  have also been cited as the basis of the name. There is no question, however, that the technology has played a pivotal role in the growth of podcasting and infrastructure of PodcastRE.</p>
<p>On March 15, 1999, Netscape published the first specification for RSS <sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>. Based on XML and developed by Ramanathan Guha and Dan Libby, RSS was created so that the Netscape home page  “My Netscape”  could be refreshed with new content from webpages which used the specification <sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup>. Over the next couple of years, RSS went through multiple iterations, and then on December 25, 2000, Dave Winer and UserLand software released RSS 0.92 <sup id="fnref1:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>. It was this version of RSS which is most important in the history of podcasting as it was the first version which included the <enclosure> tag, which allowed for the attachment of media files. Concocted by Winer, with strong prompting from Adam Curry, as a way to deliver high quality multimedia files over the internet without the quality and wait time issues which plagued early streaming, the first use of an <enclosure> was to distribute a set of Grateful Dead MP3 files <sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup>, presaging its dominant use in the years to come.</p>
<p>RSS continued to develop for the next decade and mostly stabilized as a specification with version 2.0.11 on March 30, 2009 <sup id="fnref2:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>. Since then the only updates to the format have come in the form of XML Namespaces, which are ways of adding outside-of-specification elements to XML documents that are commonly used by Apple, Google and other podcast distributors to expand metadata options for commercial purposes <sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>  <sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>. Even as podcasting apps, playback technologies, and the on-demand sound industry has changed throughout the 2010s, the basic structure and syntax of RSS has remained constant, keeping the circulation of most podcasts relatively open and freely downloadable.</p>
<p>The open infrastructure of RSS also became foundational for our work on PodcastRE <sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup>. Interested in studying podcasts, but worried about the vulnerability of digital audio files, we realized in early 2014 that there were few searchable databases of podcasts for studying and analyzing the booming audio culture taking place in podcasting. We began rather humbly by logging RSS feeds manually in iTunes and downloading audio files to a local hard drive, tracking as best we could podcasts that were being cited in the press as part of the renewed interest in podcasting, like  <em>Serial</em>  or  <em>Welcome to Nightvale</em>   <sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup>. As the project grew, we implemented a more coherent collection process, and since 2018, we have been saving podcasts included in discussions of podcasting’s  “golden age”  as well as interrogating what podcasts are being left out of that discussion. We’ve navigated the need to preserve the  “popular”  by automating the collection of a particular index of what’s popular: the Apple Podcasts top 100 lists for the U.S., Great Britain, France, and Australia every 24 hours. This automated approach toward collecting embraces both the affordances of the digital media and the MPLP (More Product, Less Process) model proposed by Mark A. Greene and Dennis Meissner <sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup>. Our efforts to identify and collect significant podcasts beyond the Apple Podcasts top 100 have been driven by collaborations with scholars who are researching independent podcasts produced by women, indigenous peoples, and people of color <sup id="fnref:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup>  <sup id="fnref:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup> and by following the work of other networks, directories and databases devoted to highlighting marginalized/less visible podcasts (Podcasts in Color, Women In Podcasting, PotLuck Podcast Network, etc).</p>
<p>PodcastRE’s collection of 2 million podcast episodes has thus been built by a combination of algorithmic methods and informed hand-selections. There’s also a  “submit a podcast”  feature on the project’s website that allows individuals to add the RSS feeds for podcasts they’d like preserved. The 16,000 archived podcast feeds are a fraction of the over 1,000,000 podcast feeds that, according to estimates, are currently being distributed <sup id="fnref:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup>. But the PodcastRE collection does offer a valuable and diverse cross-section of English-language podcasts from the past several years.</p>
<p>The common thread through all of this work has been RSS. To put it simply, if a podcast doesn’t have an RSS feed, then we cannot yet preserve it within our system. This is one of the reasons why the technical definition of a podcast — an open feed of downloadable files and associated metadata — has been so important to our work on PodcastRE. To achieve our goals of scale, not only did we need to be able to download podcast episode files automatically, we also needed to gather the metadata we could store automatically. For PodcastRE, the elements available through the RSS specification, and its associated namespaces, are as important as the podcast episode files themselves. RSS, in other words, defines the possible universe of metadata for the podcasts archived in PodcastRE.</p>
<p>What we did not immediately appreciate was how messy, idiosyncratic, and incomplete the world of podcasting metadata would prove to be. Podcast RSS metadata is a world away from the familiar and relatively consistent metadata fields of TEI and Dublin Core. One reason is the relatively sparse number of elements which are required for a feed to be valid. In fact, an RSS feed only needs four elements to be present in order to be valid: the <channel> parent element with associated <title>, <link>, and <description> elements. This would be a feed without content though as it would contain no <item>s <sup id="fnref:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup>. Because authors fully manage their own RSS feeds, and the entry of the metadata into them, they are directly responsible for the depth and quality of the metadata. This aspect of podcast metadata cannot be stressed too highly. With the exception of a few elements like <a href="googleplay:category">googleplay:category</a> and <a href="itunes:type">itunes:type</a>, there are almost no constraints on what podcast authors put into the various elements. Even fixed format elements like <pubDate>, which seems rather self explanatorily to mean the date on which a podcast episode was published into a feed, can end up being used by authors to mean something very different. For example, there are many <pubDate>s before 1950 in the metadata for  <em>The Reith Lectures</em>  podcast from the BBC, long before the term podcast was ever coined. Instead, the series uses <pubDate> to mean the day the lecture was originally given. RSS authors continue to have the authority to change anything they wish — including something as fundamental as the title of an episode, or even their whole podcast, at any time. Looking in PodcastRE, we see examples related to branding, as when  <em>Bookworm</em>  added their network and became  <em>KCRW&rsquo;s Bookworm</em> ; or to SEO, as when  <em>Highest Self Podcast</em>  added some terms and turned into  <em>Highest Self Podcast: Modern Spirituality, Ayurveda, Conscious Entrepreneurship, Mind-Body Balance</em> .</p>
<p>The inconsistent and incomplete metadata records created major challenges for our efforts to systematically preserve podcasts and make them easily searchable. We found it especially unfortunate that metadata fields that could have been revelatory for search faceting and social network analysis (fields such as <network>, <host>, and <contributor>) are not a part of any current podcast RSS specifications. Yet it was equally clear that authoritative approaches to metadata had their own problems and major blind spots. The inadequacies and biases of Library of Congress subject headings have received considerable attention within the discipline of information studies. For example, Juliet L. Hardesty <sup id="fnref:20"><a href="#fn:20" class="footnote-ref" role="doc-noteref">20</a></sup> has argued that the subject headings generally take the primacy of white men as a default;  “Robert Frost”  is cataloged under  “Poets, American”  without reference to gender or race, whereas Maya Angelou is listed under subjects including  “African American women authors”  and  “African American authors.”  The catalogers, in these cases, are applying a schema that upholds a white patriarchal worldview and minimizes both the needs of users and the ways in which creators and subjects would choose to define themselves.</p>
<p>In contrast, podcast creators have a tremendous amount of agency in how they define themselves and attempt to connect with users (i.e. listeners and audiences). When the creators of the PHX podcast entered the keywords  “podsincolor”  and  “women of color”  within their RSS feed, they actively chose to present themselves this way and place their work within a larger network of podcasts produced by people of color. The flexibility that characterizes metadata practices prove to be critical for marginalized podcasters in forming community, as they seek to carve out space for themselves within media production practices and platforms that consistently privilege hegemonic whiteness, accepted paradigms of masculinity, and heteronormativity. While this does not necessarily mean that self-policing within metadata production does not happen as a result of the asymmetrical power dynamics between platform and creator, it does still provide yet another avenue in which marginalized communities can stand in opposition to the individualistic neoliberal ideologies that undergird contemporary user-driven media production <sup id="fnref:21"><a href="#fn:21" class="footnote-ref" role="doc-noteref">21</a></sup>  <sup id="fnref1:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup>. It is critical to note, however, that the non-uniformity of metadata production yields ambivalent practices, both where innovative podcasters can resist the influence of various dominant ideologies, while others use this space to reinforce their centrality simultaneously.</p>
<p>For example, there are also many instances of podcasters stuffing their RSS with keywords in order to make them prominent within content aggregators and  “podcatchers.”  The internet abounds with advice and speculations for search engine optimization and strategies that can be utilized in order to gain attention to one’s content, such as the optimal number of keywords, the kinds of thumbnail images that should be connected to content, and more <sup id="fnref:22"><a href="#fn:22" class="footnote-ref" role="doc-noteref">22</a></sup>  <sup id="fnref:23"><a href="#fn:23" class="footnote-ref" role="doc-noteref">23</a></sup>. The opaque nature of how Apple Podcasts organizes its search results impacts the manner in which metadata is written, and this influences the ways that podcast creators self-define their own content. The dominating influence of Apple Podcasts categories can be observed by the fact that within the entirety of the PodcastRE database, the most used keywords lists are dominated by terms that are outlined either fully or in part by Apple Podcasts genre specifications. For example, with the exception of the words,  “podcast”  and  “radio,”  the top fifteen keywords for the podcast classification (the entire podcast series) terms all cohere to various genre classifications within Apple Podcasts. Similar patterns can be seen for the episodic classification where, with the exception of  “Talk Radio,”    “Podcast”  and a blank space/uncategorized, the top ten keywords reflect Apple Podcasts categories. The large amount of uncategorized keyword terms may gesture towards the fact that after 2013, the keywords metadata field became deprecated, meaning that it no longer affected the output of Apple Podcasts’ search engine algorithms (<a href="https://support.libsyn.com/kb/the-rss-feed/">https://support.libsyn.com/kb/the-rss-feed/</a>). After this discovery, many podcasters may have forgone the labor of adding keywords, as the fields that most influence search engine optimization are now the title, author, and description tags.</p>
<p>Even though Apple Podcasts deprecated keywords within its search algorithm, we became excited about the role keywords could play for our work on PodcastRE. What sort of data visualizations and discoveries might be possible by harnessing RSS metadata at scale? Ultimately, we built two data visualizations for the site. Perhaps not surprisingly, the more successful of the two was the one that most embraced the idiosyncratic, messy, and user-created nature of RSS.</p>
<h2 id="graphing-metadata-term-frequency-across-time">Graphing Metadata Term Frequency Across Time</h2>
<p>How do keywords and other fields used to describe podcasts change over time? Could tracking these changes prove useful for spotting trending topics within the podcasting ecosystem? To explore these questions, we created PodcastRE’s Term Frequency Line Graph (publicly available at <a href="https://podcastre.org/lineGraph">https://podcastre.org/lineGraph</a>), which tracks the frequency across time that any word or phrase within the metadata appears. The fields searched include the title, creator, synopses, and keywords. A visualization graphing the term  “money”  within PodcastRE is displayed below. If a user clicks on any point within the graph, their browser opens up a new tab displaying all of the matching podcast episodes from that month or year that contain a matching search term.</p>
<p>The Term Frequency Line Graph searches metadata included within individual podcast episodes and across the entire feeds (for example, while  “NBA”  may be a keyword that describes a podcast feed as a whole,  “China”  may be a keyword that describes a topic discussed within one episode of the podcast). By default, the X-axis of the graph is divided by years; however, users can toggle to a monthly scale. This allows for researchers to see when certain topics or keyword phrases spike on a seasonal cycle (for example,  “baseball”  consistently has an uptick during the playoffs every October) versus more macro-scale trends that rise and fall over a period of years.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000519/resources/images/figure01.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000519/resources/images/figure01_huc39da1dd63b871ccf7db8e653a5ab7aa_101684_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000519/resources/images/figure01_huc39da1dd63b871ccf7db8e653a5ab7aa_101684_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000519/resources/images/figure01_huc39da1dd63b871ccf7db8e653a5ab7aa_101684_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000519/resources/images/figure01.png 1200w" 
     class="landscape"
     ><figcaption>
        <p>Term Frequency Line graph of “money” , tracked over time, within PodcastRE&rsquo;s corpus. Tool is available at <a href="http://podcastre.org/lineGraph">http://podcastre.org/lineGraph</a>.
        </p>
    </figcaption>
</figure>
<p>When researchers use the Term Frequency Line Graph to look for trends across a span of years, however, they quickly encounter an interpretive challenge: almost any term they search will appear to dramatically increase in 2017 and 2018. This is because the PodcastRE collection grew exponentially over those two years, a result of the growth in the podcasting ecosystem as a whole and our own curatorial decision to automatically preserve any feed that appears on the Apple Podcasts Top 100 chart in the U.S., U.K., Australia, or France. While we give users the ability to  “normalize”  the graph results (which employs an equation to account for the larger number of podcasts from some years compared than others), we know this feature has its limits. What does it mean to  “normalize”  the number of podcasts during a period in which the medium is rapidly evolving?</p>
<p>We have tried to address this interpretive challenge through a  “Rate of Episodes Added”  button, which provides contextualization in regard to the database itself. By showing how many episodes are added per year, users can see how the rate of growth in the database can affect the numbers that are being shown for any query’s term frequency. Additionally, the  “Area Graph”  button transforms the data into a stacked graph, which allows for comparisons across multiple queries at particular moments in time and reminds users that the graphs are malleable and dynamic. Finally, the user can move to a more granular level at any point by clicking on a point in the graph, allowing them to investigate the actual podcast feeds and episodes that appear as abstractions within the graph. Users can save the data to a CSV file, a JPG, PNG or SVG vector image, so that this data can be applied to a variety of presentational contexts.</p>
<p>In many ways, PodcastRE’s Term Frequency Line Graph exemplifies the limitations digital humanists are likely to encounter when applying data visualizations built for  <em>digitized text collections</em>  to  <em>born-digital media collections</em> . We modeled the user-experience and technological framework of PodcastRE’s Term Frequency Line Graph on that of the Arclight app (<a href="http://projectarclight.org">http://projectarclight.org</a>), which searches the 2.5 million page corpus of the Media History Digital Library (MHDL) <sup id="fnref:24"><a href="#fn:24" class="footnote-ref" role="doc-noteref">24</a></sup>. The MHDL is composed of books and magazines pertaining to the histories of film, broadcasting, and recorded sound from 1915 to 1960, which is an especially robust period for the searching of named entities (such as people, film titles, or radio station call letters). Additionally, the normalization function for Arclight graphs works quite well (the most represented year of 1915-1960 is only double in size of the least represented year, avoiding PodcastRE’s challenge of grappling with exponential growth). Normalized searches for the names of movie stars, for example, generally map onto the arcs of their popularity and/or notoriety, sometimes, though not always, with surprising results. Data visualizations built for searching entities within large corpora of digitized texts are less adept at producing immediately legible results for searching the metadata keywords of a rapidly growing born-digital medium. What would it mean to design a data visualization tool that embraced the messiness of born-digital objects and their metadata, rather than trying to smooth them out?</p>
<h2 id="associated-keyword-word-cloud">Associated Keyword Word Cloud</h2>
<p>In developing PodcastRE’s Associated Keyword Word Cloud, we sought to harness and foreground the specificities and idiosyncrasies of born digital media collections. This data visualization takes the keywords that podcasters entered to describe their work and puts them into conversation with other podcasters’ keywords. A specific example is helpful for understanding how it works.</p>
<p>Using the keyword  “money” , in a search conducted in the fall of 2019, we found the term appeared in the metadata of 68,619 podcast episodes saved within PodcastRE, collected from 587 discrete RSS feeds. The other keywords that appear most frequently along with  “money”  in podcast metadata are visualized below (see <a href="#figure02">Figure 2</a>). This visualization includes predictable matches within the popular financial self-help genre (e.g.  “wealth,”    “business,”    “entrepreneur” ), as well as meaningful intersections that lay outside financially-oriented podcasts (e.g.  “spirituality,”    “Relationships &amp; Sex,”    “Fear” ). When a user clicks on the keyword value in the cloud, the user is immediately transferred to the PodcastRE database interface, where it shows all of the podcasts that used these paired keyword values. <a href="#figure03">Figure 3</a> reveals the results of the podcasts that contain both the keywords  “money”  and  “spirituality.”  The process promotes serendipitous discovery and may lead the researcher toward encounters they hadn’t anticipated. For example, modern witchcraft is better represented in the podcasts with  “money”  and  “spirituality”  as keywords than most traditional forms of organized religion.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000519/resources/images/figure02.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000519/resources/images/figure02_hu3e21c030c9b47dc14788eec1edc04fcc_293842_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000519/resources/images/figure02_hu3e21c030c9b47dc14788eec1edc04fcc_293842_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000519/resources/images/figure02.png 975w" 
     class="landscape"
     ><figcaption>
        <p>Word cloud for the query, “money,” on the “All Keywords” category. Taken using the Associated Keyword Cloud visualization tool at <a href="http://podcastre.org/wordCloud">http://podcastre.org/wordCloud</a>.
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000519/resources/images/figure03.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000519/resources/images/figure03_hu984dfb871bdc9e9a5abfe8172e558e41_251276_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000519/resources/images/figure03_hu984dfb871bdc9e9a5abfe8172e558e41_251276_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000519/resources/images/figure03.png 974w" 
     class="landscape"
     ><figcaption>
        <p>Screen shot of the results page for podcast episodes containing the keywords “money” and “spirituality.”
        </p>
    </figcaption>
</figure>
<p>The Associated Keyword Cloud visualization was built through connecting together multiple open source technologies. Like the Term Frequency Line Graph, the Associated Keyword Word Cloud uses the Highcharts Javascript library to animate the visualization. To retrieve the information it needs, we query the keyword metadata facet within PodcastRE’s Solr search index, and we return and store them as key value pairs, with the number of podcasts that maintain both the queried keyword and the additional keyword (the hit count) next to the particular word. For example, if a user queries the word  “love”  within the database, a potential key value pair that would appear would be  “[ relationships , 163],”  where  “relationships”  would be the associated keyword for  “love,”  and the  “163”  stands for how many times this keyword was added alongside the word  “love.”  The results are sorted through keywords that have the most hits down to the associated keywords that have the least hits. By targeting this metadata keyword field and assigning the  “weight”  of a word to be the number that is assigned to the hit count of the key value pair, we were able to visually represent which keywords were paired most often with the queried word, by making the word with the heaviest  “weight,”  the largest in the word cloud. Because certain topics have a range of associated keywords that spanned hundreds of words, we limited the number of keywords that can be shown on the word cloud to a maximum of 200 words. While this decision may hinder researchers from getting the full range of associated keywords, this limitation was imposed to ensure readability on the visualization. Two hundred words seemed like a reasonable count in order for researchers to gain a sense of the wide range of relational topics that podcasters were dealing with, and simultaneously allow the visualizations to be effective in showing which keywords were the most actively engaged with.</p>
<p>There are two options on the Associated Keyword Word Cloud interface that aid in isolating whether the keywords shown are related to podcasts in their entirety or exclusive to certain episodes. Additionally, if users want a merging of these two levels of metadata, they can search across both podcast and episode keywords by using the  “All Keywords”  option. In this manner, for podcasts that may deal with a wide range of topics, such as news or current events podcasts, there can be a closer examination on a micro episodic level of what kinds of keywords are used to define certain topical content. Often, the keywords that are used to describe podcasts are not uniformly applied to define episodes, so providing these two levels of analytical range gives researchers more flexibility in the kinds of questions they can ask using PodcastRE.</p>
<p>All attempts to interpret the Associated Keyword Word Clouds ultimately lead back to reflecting on the practices, norms, aspirations, and communities of the podcasters themselves.</p>
<p>As discussed earlier, keywords allow content creators to define their work to listeners and podcatcher applications. They are a space of creator agency, where podcast producers deploy keywords to create networks of ambient affiliation with other podcasts and subject matter. By making one of PodcastRE’s database visualization tools intimately connected to these creator-defined keywords and their relationships to other keywords, we provide an alternative mode of discoverability apart from the algorithms that govern commercial aggregators such as Apple Podcasts. In this manner, PodcastRE hopes to provide a different approach that foregrounds creator agency and their interactions with their own metadata through the digital archive’s organization, particularly with these metadata visualizations.</p>
<h2 id="studying-the-durations-of-podcasts">Studying the Durations of Podcasts</h2>
<p>The Term Frequency Line Graph and Associated Keyword Word Cloud can both be effectively applied toward exploratory research and achieving serendipitous discoveries. But we also wanted to use PodcastRE and the  “mediaDuration”  field to examine a more focused question. What patterns can we notice about the duration of podcasts, and what can they tell us about practitioner norms and assumptions of what makes for a good length of a podcast? Unlike most other AV forms — movies, television, and radio — podcasts are a time-based medium that are not constrained by programming schedules (broadcast schedules, movie theater showtimes) and technical limitations (reels of film and tape). If a podcast could run anywhere from a couple of seconds to several hours in length, how do norms and common practices develop around perceived ideas of a podcast’s optimal duration? We realized that metadata could help us answer this question.</p>
<p>In this section, then, we propose and share two approaches to studying podcast duration. First, we consider how duration analysis might clarify the differences between two programs of the same specification classification, in this case two popular daily programs from  <em>The New York Times</em>  and  <em>NPR</em> , using data gathered from episodes ranging from the former’s launch in early 2017 to an end point of April 2018. Second, we conduct an investigation of a much larger scale, analyzing large rosters of programs to juxtapose duration across networks and genres. Our case studies here are the comedic programs of Earwolf and the comparably more serious fare of Gimlet Media, using data gathered from episodes ranging from 2009 until early 2018. In both of these cases, the statistics were gathered by first running an SQL query on the PodcastRE database, then exporting metadata for all of the episodes into a .csv file, and finally finding averages, medians, and other numbers using Microsoft Excel. All of these approaches required us to assemble subsets of data from within the PodcastRE collection (and the .csv files), rather than treating the entire collection as a dataset.<sup id="fnref:25"><a href="#fn:25" class="footnote-ref" role="doc-noteref">25</a></sup>  The genre and network categories that we ourselves added to the spreadsheets opened the data up for more meaningful analysis, especially when paired with the duration metadata provided by the RSS feeds.</p>
<p>Our first approach to studying duration explored what has become one of the most popular contemporary podcast formats: the daily news program. How long should a daily news podcast take to consume? When  <em>The New York Times</em>  launched  <em>The Daily</em>  in February of 2017, host Michael Barbaro described the fledgling program thusly:  “This is how the news should sound. Fifteen minutes a day, five days a week. It isn’t quite a podcast — although you can listen wherever you listen to podcasts. It isn’t quite the radio — although the mechanics are largely the same. It isn’t quite the newspaper — although we’ll be drawing heavily on the journalism that powers The New York Times”   <sup id="fnref:26"><a href="#fn:26" class="footnote-ref" role="doc-noteref">26</a></sup>. Though Barbaro pegged the program as difficult to explain, it was a nearly immediate hit, gaining over five million monthly listeners by July of 2018 <sup id="fnref:27"><a href="#fn:27" class="footnote-ref" role="doc-noteref">27</a></sup>. As Barbaro told  <em>Vanity Fair</em>  that same month,  “When we started the show, we had many goals… We didn’t realize we were going to make money that was actually going to get pumped back into the company”   <sup id="fnref:28"><a href="#fn:28" class="footnote-ref" role="doc-noteref">28</a></sup>. Yet as is often the case, success breeds imitators and competitors, and  <em>The Daily</em>  witnessed the rise of its biggest challenger in June of 2017 when NPR launched  <em>Up First</em> , a daily  “10-minute morning news podcast”  that is  “designed with digital listeners in mind but will also serve as a preview of the news stories that will be treated in depth on public radio stations across the country throughout the day”   <sup id="fnref:29"><a href="#fn:29" class="footnote-ref" role="doc-noteref">29</a></sup>. That program was also a swift triumph, and as of October 2018, both  <em>The Daily</em>  and  <em>Up First</em>  sat comfortably in the top five most popular podcasts according to Podtrac’s rankings: the former tailed behind only  <em>Serial</em> , while the latter occupies the fifth spot <sup id="fnref:30"><a href="#fn:30" class="footnote-ref" role="doc-noteref">30</a></sup>.</p>
<p>Episode duration has been a central selling point for each of the two podcasts. As seen above, both of the series’ launch press releases mention episode length. Descriptions of the programs on their official websites also focus on duration.  <em>Up First</em>  has remained consistent in its advertised average runtime:  “NPR’s  <em>Up First</em>  is the news you need to start your day. The biggest stories and ideas — from politics to pop culture — in 10 minutes”   <sup id="fnref:31"><a href="#fn:31" class="footnote-ref" role="doc-noteref">31</a></sup>.  <em>The Daily</em> , meanwhile, has added five minutes to its initial announcement:  “This is how the news should sound. Twenty minutes a day, five days a week, hosted by Michael Barbaro and powered by New York Times journalism”   <sup id="fnref:32"><a href="#fn:32" class="footnote-ref" role="doc-noteref">32</a></sup>. The programs are, essentially, two different approaches to the morning commute:  <em>Up First</em> ’s proposed shorter length seems guaranteed to slot into almost any daily trip to work, while  <em>The Daily</em> ’s longer runtime requires either a lengthy commute, multiple listening sessions, or even perhaps the utilization of 1.5x or 2x speed playback options. Such duration decisions are complimented by storytelling approaches:  <em>Up First</em> ’s short length is matched with a  “greatest hits”  style compilation of short stories, while  <em>The Daily</em> ’s relatively lengthier duration is primarily spent on the discussion of a single story. In theory, then, the former aims to quickly provide its listeners with headline-style blurbs about the day’s biggest stories, while the latter seeks to exhaustively cover a single topic.</p>
<p>Such temporal differences are roughly borne out by the metadata found in PodcastRE’s database, although the story is more complicated than the descriptions of the series imply.  <em>The Daily</em>  (mean duration of 22:51, median duration of 22:12) runs nearly ten minutes longer than  <em>Up First</em>  (mean duration of 13:33, median duration of 13:17), with both programs on average running a few minutes longer than their advertised lengths. The differences between the two series is much starker when considering the range in podcast durations, as  <em>Up First</em>  is relatively consistent in episode duration (shortest episode of 11:01 and longest episode of 17:46 for a range of 6:45) while  <em>The Daily</em>  varies widely between episodes (shortest episode of 13:00 and longest episode of 41:23 for a range of 28:23). These durational differences align neatly with the programs’ content choices (i.e. multiple headlines vs. single story focus), though they provide critical additional clarifications. While both  <em>Up First</em>  and  <em>The Daily</em>  release episodes each weekday morning, the former’s tight range and shorter length ties it more closely to its proposed function as morning commute listening, while the latter’s wider range and extended runtime emphasizes delivering a full story adequately. Since podcasts do not have the same durational constraints of broadcast media, these choices in runtime are clear aesthetic and storytelling decisions – yet given the evolutionary radio approach of NPR’s daily podcast and  <em>The New York Times</em> ’s commitment to the news story, these decisions are not completely detached from their companies’ original mediums.</p>
<p>On a larger scale, podcast duration analysis can point towards divergent approaches by podcast networks and in certain genres. As an example, we conducted an analysis of thirty Earwolf programs<sup id="fnref:33"><a href="#fn:33" class="footnote-ref" role="doc-noteref">33</a></sup>  and nineteen Gimlet Media programs.<sup id="fnref:34"><a href="#fn:34" class="footnote-ref" role="doc-noteref">34</a></sup>  The former describes itself as  “the leading comedy podcast network devoted to creating the best, funniest, and most entertaining podcast shows in existence”   <sup id="fnref:35"><a href="#fn:35" class="footnote-ref" role="doc-noteref">35</a></sup>. Gimlet Media specializes in more  “serious”  fare, characterizing itself as  “the award-winning narrative podcasting company that aims to help listeners better understand the world and each other”   <sup id="fnref:36"><a href="#fn:36" class="footnote-ref" role="doc-noteref">36</a></sup>. Though both companies employ personnel who have worked or continue to work in other mediums, Earwolf and Gimlet distinguish themselves from other major podcast networks such as  <em>NPR</em> ,  <em>iHeartRadio</em> , and  <em>WNYC Studios</em>  through their podcast nativism: both companies began as strictly podcast-focused networks rather than emerging within older media companies.</p>
<p>Perhaps as a result, the two networks have markedly different approaches towards podcast episode length. Of the 30 surveyed Earwolf programs, 3 have average runtimes between 0-20 minutes, 4 have average runtimes between 20-40 minutes, 6 have average runtimes between 40-60 minutes, 10 have average runtimes between 60-80 minutes, 3 have average runtimes between 80-100 minutes, 3 have average runtimes between 100-120 minutes, and 1 has an average runtime between 120-140 minutes (See <a href="#figure04">Figure 4</a>). This means that over half of the surveyed programs have average episode durations over an hour, with programs ranging from  <em>Eardrop</em> ’s average runtime of 3:17 and  <em>Never Not Funny</em> ’s average runtime of 2:05:33. The shortest single episode was a 38-second  <em>Eardrop</em>  episode, while the longest individual episode was a  <em>Comedy Bang! Bang!</em>  that lasted 3:19:02. Earwolf’s individual shows also frequently feature drastic ranges in shortest and longest duration:  <em>Hollywood Handbook</em> , for instance, has a range of 1:53:27 between its shortest and longest episodes, while  <em>Comedy Bang! Bang!</em> ’s range is 2:39:06.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000519/resources/images/figure04.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000519/resources/images/figure04_hu9fad66afa02ed2edfa029cba9f2f7dde_1455849_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000519/resources/images/figure04_hu9fad66afa02ed2edfa029cba9f2f7dde_1455849_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000519/resources/images/figure04_hu9fad66afa02ed2edfa029cba9f2f7dde_1455849_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000519/resources/images/figure04_hu9fad66afa02ed2edfa029cba9f2f7dde_1455849_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/1/000519/resources/images/figure04_hu9fad66afa02ed2edfa029cba9f2f7dde_1455849_1800x0_resize_box_3.png 1800w,/dhqwords/vol/15/1/000519/resources/images/figure04.png 2100w" 
     class="landscape"
     ><figcaption>
        <p>Average durations of 30 surveyed Earwolf programs.
        </p>
    </figcaption>
</figure>
<p>Gimlet, on the other hand, is much more consistent in its runtimes across series, though there is still variation between individual episodes. Of the 19 surveyed programs, only 1 had an average runtime between 0-20 minutes, and that program ( <em>Chompers</em> ) serves a specific and brief function: children are meant to listen to the series as they brush their teeth. 2 of the series had an average runtime between 40-60 minutes, though both of those shows ( <em>Twice Removed</em>  and  <em>Mystery Show</em> ) are no longer producing episodes. The other 17 series, then, had average run times between 20-40 minutes, aligning Gimlet’s roster with conventional advice on podcast episode length.<sup id="fnref:37"><a href="#fn:37" class="footnote-ref" role="doc-noteref">37</a></sup>  While individual episodes still varied quite a bit ( <em>Reply All</em> , for instance, had a range of 1:35:29 between its shortest and longest episodes, while  <em>Mogul</em>  had a range of 1:16:17), these ranges were still much smaller than the largest Earwolf ranges.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000519/resources/images/figure05.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000519/resources/images/figure05_hu96728d905567300e6bd6470df4face79_1417668_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000519/resources/images/figure05_hu96728d905567300e6bd6470df4face79_1417668_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000519/resources/images/figure05_hu96728d905567300e6bd6470df4face79_1417668_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000519/resources/images/figure05_hu96728d905567300e6bd6470df4face79_1417668_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/1/000519/resources/images/figure05_hu96728d905567300e6bd6470df4face79_1417668_1800x0_resize_box_3.png 1800w,/dhqwords/vol/15/1/000519/resources/images/figure05.png 2100w" 
     class="landscape"
     ><figcaption>
        <p>Average durations of 19 surveyed Gimlet programs.
        </p>
    </figcaption>
</figure>
<p>The relative homogenization of Gimlet Media podcast duration, then, stands in stark contrast to the diverse podcast lengths of Earwolf. Every Gimlet Media podcast had an average run time under an hour, and 84.2% of the shows surveyed had average runtimes between 20-40 minutes. 56.7% of the Earwolf podcasts analyzed, meanwhile, had average runtimes over an hour long, and 76.7% of Earwolf’s podcasts had average runtimes over 40 minutes long — in other words, over three-quarters of Earwolf’s shows ran longer on average than Gimlet’s  “sweet spot.”  Individual episode lengths varied in each of the networks’ programs, but Gimlet’s programs featured smaller ranges than the large variation found in many of Earwolf’s programs. Such differences may be the result of institutional decisions, generic divergences, or series lengths. Whatever the cause, however, Earwolf and Gimlet serve as evidence that podcast networks can have wildly divergent approaches towards episode duration, and that studying duration can lead us to insights about genre conventions, production values and more.</p>
<p>On a recent episode of the  <em>Start Up</em>  podcast, the show’s host, and Gimlet CEO, Alex Blumberg was reflecting on his decision to sell Gimlet media to Spotify. He noted that Gimlet’s gambit to standardize the production of highly edited and tightly produced &lsquo;quality’ podcasts (that often followed very specific duration and other editorial decisions) had turned out to be a financially unfeasible strategy that was losing ground to cheaper and more popular chat cast style podcasts (where duration and other attributes are more flexible given the lower costs involved for editing and polishing the finished piece). His comments are a reminder that, despite the format’s substantial growth in the last two decades, there are still many lingering questions about the forms, conventions and economics of podcasting. We believe it is especially crucial during this time of flux, before podcasting stabilizes like so many other media have, to study the different approaches podcasters of all types are taking as they experiment with this emerging sonic format. Although duration numbers seem like relatively innocuous or descriptive metadata, the research from PodcastRE suggests they reveal historical relationships between new and old media formats, industrial and economic assumptions about  “ideal”  formats, and generic conventions that shape both amateur and professional podcasts.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Our work on PodcastRE has aimed to provide tools and data that account for podcasting’s complexity as a cultural form while simultaneously taking advantage of its unique technical affordances. The centering of RSS metadata and what can be mined from it through advanced search, graphing keywords over time, or visualizing word clouds of associated keywords has helped us facilitate the automated collection of a significant corpus of podcasts from a crucial period in the format’s emergence. It has also facilitated novel, fine-grained exploration of specific file characteristics as well, like duration metrics, across a variety of genres and shows.</p>
<p>The reliance on RSS, however, has also forced us to confront the messiness and intricacies of a born digital object whose metadata and descriptive features are dynamic and podcaster generated. Podcasting’s relatively open and accessible origins have helped create a vibrant environment for web-based audio – one that includes the scores of podcasts available and the multiplicity of voices behind them, but also the numerous apps, aggregator sites and distribution technologies that have emerged to support podcasting’s rise. RSS and XML have not only been important to our work on PodcastRE, but to podcasting more broadly, and to the agency and control it has provided podcasters for defining their work on their own terms as well as for listeners in terms of defining their listening practices. Recently, there have been a number of attempts to move away from the more open and accessible versions of podcasting, to more closed and profitable models (e.g. exclusive shows tied to one platform, like Spotify, or subscription-based services like Luminary). While these options may make podcasting more user-friendly and convenient, or may offer podcasters more options for monetizing their work, they also make podcasts more platform-dependent, less analyzable, and less open to research. </p>
<p>The centrality of RSS to both podcasting and PodcastRE has been a theme throughout this article. We believe our methods and findings, however, hold relevance for beyond scholars researching other topics at the intersection of media studies and DH. As this study has shown, metadata records can serve as surrogates for studying large collections of time-based media objects, allowing researchers to query the durations of millions of media objects in a fraction of the time it would take to ingest and analyze transcoded media files. Yet our work has also shown that, when it comes to born digital media, the metadata are never fully separate from the objects they describe, nor are they fully capable of replacing close listening and other media studies methods. There is a need to delineate between methods best suited for  <em>digitized media collections</em>  compared to those most appropriate for  <em>born digital media collections</em>  and for devising strategies to blend AV and DH methods. By making these distinctions, we can better apply DH to AV and identify change and continuity, at a large scale, across media history.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Edison Research,  “The Infinite Dial 2019,”  Edison Research (2019): <a href="https://www.edisonresearch.com/infinite-dial-2019/">https://www.edisonresearch.com/infinite-dial-2019/</a>.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>PodNews.  “The Total Number of Available Podcasts Is Now 700,000,”  PodNews (2019): <a href="https://podnews.net/update/700000">https://podnews.net/update/700000</a>.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Hansen, S.  “The Feed is the Thing: How RSS Defined PodcastRE and Why Podcasts May Need to Move On.”  In J.W. Morris and E. Hoyt (eds),  <em>Saving New Sounds: Dispatches from the PodcastRE Project</em> , University of Michigan Press, Ann Arbor (forthcoming 2020).&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Jockers, M. L.  <em>Macroanalysis: Digital methods and literary history</em> . University of Illinois Press, Champaign (2013).&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>Underwood, T.  <em>Distant Horizons: Digital Evidence and Literary Change</em> . University of Chicago Press, Chicago (2019).&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>Clement, T. E.  “Towards a Rationale of Audio-Text”    <em>Digital Humanities Quarterly</em> , 10.2 (2016).&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>Clement, T. E.  “When Texts of Study Are Audio Files: Digital Tools for Sound Studies in DH.”  In S. Schreibman, R. Siemens, and J. Unsworth (eds),  <em>A New Companion to Digital Humanities</em> , Chichester ; Malden, MA: John Wiley &amp; Sons, Ltd., Chichester (2016): 348-57.&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>RSS Advisory Board.  “RSS History.”  RSS Board (n.d.): <a href="http://www.rssboard.org/rss-history">http://www.rssboard.org/rss-history</a>, accessed February 23, 2019.&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>Hines, M.  “Netscape Broadens Portal Content Strategy,”  Newsbytes. (1999): <a href="http://link.galegroup.com/apps/doc/A54120248/ITOF?u=umuser&amp;sid=ITOF&amp;xid=377f45">http://link.galegroup.com/apps/doc/A54120248/ITOF?u=umuser&amp;sid=ITOF&amp;xid=377f45</a>.&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>Winer, D.  “Payloads for RSS.”  (2001): <a href="https://web.archive.org/web/20080214205403/http://www.thetwowayweb.com/payloadsforrss">https://web.archive.org/web/20080214205403/http://www.thetwowayweb.com/payloadsforrss</a>, accessed February 23, 2019.&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>Bray, T., Hollander, D., Layman, A., Tobin, R., &amp; Thompson, H. S.  “Namespaces in XML 1.0 (Third Edition),”  W3 (2009): <a href="https://www.w3.org/TR/xml-names/">https://www.w3.org/TR/xml-names/</a>, accessed February 23, 2019.&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p>Bergen, M.  “Google Brings Podcasting to Play Music, Swinging at Apple’s Dominance,”  Recode (2015).&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13">
<p>Morris, J. W., Hansen, S., &amp; Hoyt, E.  “The PodcastRE Project: Curating and Preserving Podcasts (and Their Data)”    <em>Journal of Radio &amp; Audio Media</em> , 26.1 (2019).&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14">
<p>Adams, D.  “After &lsquo;Serial,’ Sponsors Pour Money into Podcasts,”    <em>The Boston Globe</em>  (2015): <a href="https://www.bostonglobe.com/business/2015/02/13/after-serial-sponsors-pour-money-into-podcasts/OKAzhUWtqCHQbl3IuEIiBN/story.html">https://www.bostonglobe.com/business/2015/02/13/after-serial-sponsors-pour-money-into-podcasts/OKAzhUWtqCHQbl3IuEIiBN/story.html</a>, accessed November 26, 2019.&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:15">
<p>Greene, M. and Meissner, D.  “More Product, Less Process: Revamping Traditional Archival Processing,”    <em>The American Archivist</em> , 68.2 (2005): 208–63.&#160;<a href="#fnref:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:16">
<p>Wang, J. H.  “The Perils of Ladycasting: Podcasting, Gender, and Alternative Production Cultures.”  In J.W. Morris and E. Hoyt (eds),  <em>Saving New Sounds: Dispatches from the PodcastRE Project</em> , University of Michigan Press, Ann Arbor (forthcoming 2020).&#160;<a href="#fnref:16" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:17">
<p>Florini, S.  “This Week in Blackness, the George Zimmerman acquittal, and the production of a networked collective identity.”    <em>New Media &amp; Society</em>  19.3 (2017): 439-454.&#160;<a href="#fnref:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:18">
<p>Podcast Insights.  “2020 Podcast Stats &amp; Facts (New Research From Apr 2020),”  Podcast Insights: <a href="https://www.podcastinsights.com/podcast-statistics/">https://www.podcastinsights.com/podcast-statistics/</a>&#160;<a href="#fnref:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:19">
<p>Winer, D.  “RSS 2.0 Specification,”  RSS 2.0 at Harvard Law (2015): <a href="https://cyber.harvard.edu/rss/rss.html">https://cyber.harvard.edu/rss/rss.html</a>, accessed February 23, 2019.&#160;<a href="#fnref:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:20">
<p>Hardesty, J.  “Bias and Inclusivity in Metadata: Awareness and Approaches” . Indiana University Digital Collection Services.&#160;<a href="#fnref:20" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:21">
<p>Hogan, M.  “Dykes on Mykes: Podcasting and the Activist Archive.”    <em>TOPIA: Canadian Journal of Cultural Studies</em>  20 (2008): 199-215.&#160;<a href="#fnref:21" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:22">
<p>Crowe, Anne.  “101 Quick &amp; Actionable SEO Tips That Are HUGE.”    <em>Search Engine Journal.</em>  October 21, 2017. <a href="https://www.searchenginejournal.com/101-quick-seo-tips/180563/">https://www.searchenginejournal.com/101-quick-seo-tips/180563/</a>.&#160;<a href="#fnref:22" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:23">
<p>.  “10 SEO Tips For Your Podcast.”  Podcast Motor. September 22, 2015. <a href="https://www.podcastmotor.com/seo-tips-podcast/">https://www.podcastmotor.com/seo-tips-podcast/</a>.&#160;<a href="#fnref:23" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:24">
<p>Hoyt, E., Hughes, K., and Acland, C.R.  “A Guide to the Arclight Guidebook.”  In C.R. Acland and E. Hoyt (eds),  <em>The Arclight Guidebook to Media History and the Digital Humanities</em> , REFRAME/Project, Falmer (2016): pp. 1-29.&#160;<a href="#fnref:24" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:25">
<p>It should be noted that the ability to obtain duration data is not currently available to front-end users of the PodcastRE site.&#160;<a href="#fnref:25" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:26">
<p>Barbaro, M.  “Get Ready for The Daily, Your Audio News Report,”    <em>The New York Times</em>  (2017): <a href="https://www.nytimes.com/2017/01/30/podcasts/the-daily-get-ready-for-the-daily-your-audio-news-report.html">https://www.nytimes.com/2017/01/30/podcasts/the-daily-get-ready-for-the-daily-your-audio-news-report.html</a>, accessed November 30, 2018.&#160;<a href="#fnref:26" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:27">
<p>Jerde, S.  “How NYT’s The Daily Grew to 5 Million Monthly Listeners and Became a Breakout Star,”  Ad Week (2018): <a href="https://www.adweek.com/digital/how-nyts-the-daily-grew-to-5-million-monthly-listeners-and-became-a-breakout-star/">https://www.adweek.com/digital/how-nyts-the-daily-grew-to-5-million-monthly-listeners-and-became-a-breakout-star/</a>, accessed November 30, 2018.&#160;<a href="#fnref:27" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:28">
<p>Pompeo, J.  “&lsquo;We Didn’t Expect to Make Money’: How  <em>The Daily</em> ’s Michael Barbaro Unexpectedly Became the Ira Glass of  <em>The New York Times</em> ,”    <em>Vanity Fair</em>  (2018): <a href="https://www.vanityfair.com/news/2018/07/how-the-daily-michael-barbaro-became-the-ira-glass-of-new-york-times">https://www.vanityfair.com/news/2018/07/how-the-daily-michael-barbaro-became-the-ira-glass-of-new-york-times</a>, accessed November 30, 2018.&#160;<a href="#fnref:28" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:29">
<p>“Up First: The Essential Morning News Podcast From NPR,”    <em>NPR</em>  (2017): <a href="https://www.npr.org/about-npr/522211062/up-first-the-essential-morning-news-podcast-from-npr">https://www.npr.org/about-npr/522211062/up-first-the-essential-morning-news-podcast-from-npr</a>, accessed November 30, 2018.&#160;<a href="#fnref:29" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:30">
<p>Podtrac,  “Podcast Industry Audience Rankings,”  Podtrac (2018): <a href="http://analytics.podtrac.com/industry-rankings/">http://analytics.podtrac.com/industry-rankings/</a>, accessed November 30, 2018.&#160;<a href="#fnref:30" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:31">
<p>“Up First,”    <em>NPR</em>  (2018): <a href="https://www.npr.org/podcasts/510318/up-first">https://www.npr.org/podcasts/510318/up-first</a>, accessed November 30, 2018.&#160;<a href="#fnref:31" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:32">
<p>“The Daily,”    <em>The New York Times</em>  (2018): <a href="https://www.nytimes.com/column/the-daily">https://www.nytimes.com/column/the-daily</a>, accessed November 30, 2018.&#160;<a href="#fnref:32" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:33">
<p><em>Affirmation Nation</em> ,  <em>Analyze Phish</em> ,  <em>Andy Daly Podcast Pilot Project</em> ,  <em>The Apple Sisters</em> ,  <em>Bitch Sesh</em> ,  <em>Comedy Bang! Bang!</em> ,  <em>Eardrop</em> ,  <em>Earwolf Challenge</em> ,  <em>Fogelnest Files</em> ,  <em>Glitter in the Garbage</em> ,  <em>Hard Nation</em> ,  <em>Hello From The Magic Tavern</em> ,  <em>Hollywood Handbook</em> ,  <em>How Did This Get Made</em> ,  <em>Improv4Humans</em> ,  <em>Kevin Pollak’s Chat Show</em> ,  <em>Mike Detective</em> ,  <em>Never Not Funny</em> ,  <em>Off Book</em> ,  <em>Professor Blastoff</em> ,  <em>Rafflecast</em> ,  <em>Ronna and Beverly</em> ,  <em>Spontaneanation</em> ,  <em>Throwing Shade</em> ,  <em>Topics</em> ,  <em>Totally Laime</em> ,  <em>U Talkin’ U2 2 Me</em> ,  <em>Who Charted</em> ,  <em>With Special Guest Lauren Lapkus</em> , and  <em>Womp It Up!</em>&#160;<a href="#fnref:33" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:34">
<p><em>Chompers</em> ,  <em>Crimetown</em> ,  <em>Every Little Thing</em> ,  <em>The Habitat</em> ,  <em>Heavyweight</em> ,  <em>Homecoming</em> ,  <em>Mogul</em> ,  <em>Mystery Show</em> ,  <em>The Nod</em> ,  <em>The Pitch</em> ,  <em>Reply All</em> ,  <em>Sampler</em> ,  <em>Sandra</em> ,  <em>Science Vs.</em> ,  <em>StartUp</em> ,  <em>Surprisingly Awesome</em> ,  <em>Twice Removed</em> ,  <em>Uncivil</em> , and  <em>Undone</em> .&#160;<a href="#fnref:34" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:35">
<p>Earwolf,  “About Earwolf,”  Earwolf (2018): <a href="https://www.earwolf.com/about/">https://www.earwolf.com/about/</a>, accessed November 30, 2018.&#160;<a href="#fnref:35" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:36">
<p>Gimlet Media,  “About,”  Gimlet Media (2018): <a href="https://www.gimletmedia.com/about">https://www.gimletmedia.com/about</a>, accessed November 30, 2018.&#160;<a href="#fnref:36" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:37">
<p>Though most blogs on the subject recommend tying duration to whatever length your content demands, they also routinely recommend shorter average durations, with  <em>We Edit Podcasts</em> , for instance, writing,  “it is possible to become successful with a longer show, but in general, the 22 minute rule trumps all”   <sup id="fnref:38"><a href="#fn:38" class="footnote-ref" role="doc-noteref">38</a></sup>.&#160;<a href="#fnref:37" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:38">
<p>We Edit Podcasts,  “What Is the Optimal Length for a Podcast?”  We Edit Podcasts (2016): <a href="https://www.weeditpodcasts.com/what-is-the-optimal-length-for-a-podcast/n">https://www.weeditpodcasts.com/what-is-the-optimal-length-for-a-podcast/n</a>, accessed November 30, 2018.&#160;<a href="#fnref:38" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry><entry><title type="html">The Media Ecology Project: Collaborative DH Synergies to Produce New Research in Visual Culture History</title><link href="https://rlskoeser.github.io/dhqwords/vol/15/1/000524/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://rlskoeser.github.io/dhqwords/vol/15/1/000524/</id><author><name>Mark Williams</name></author><author><name>John Bell</name></author><published>2021-03-05T00:00:00+00:00</published><updated>2021-03-05T00:00:00+00:00</updated><content type="html"><![CDATA[<h2 id="heading"></h2>
<p><em>This essay is dedicated with respect to the legion of significant Film and Media Studies scholars who passed away during the time it was written: Eileen Bowser, Edward Branigan, Thomas Elsaesser, Jonathan Kahana, Paul Spehr, Bernard Stiegler, Peter Wollen.</em></p>
<h2 id="introduction">Introduction</h2>
<p>Our moving image heritage is at enormous risk. Moving image archivists and digital repository advocates are developing solutions to these problems, but we cannot sustain interest in  “preservation”  without a better sense of the historical value of these materials.  “Access”  is not enough; new knowledge production is required in order to connect archival materials with audiences and accelerate preservation efforts. The Digital Humanities must move concertedly forward to engage visual culture with the same dedication and technological ingenuity it has brought to the study of word culture.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000524/resources/images/figure01.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000524/resources/images/figure01_huc110226c90a7f381559fa64fe4412a05_380878_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000524/resources/images/figure01_huc110226c90a7f381559fa64fe4412a05_380878_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000524/resources/images/figure01_huc110226c90a7f381559fa64fe4412a05_380878_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000524/resources/images/figure01_huc110226c90a7f381559fa64fe4412a05_380878_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/1/000524/resources/images/figure01_huc110226c90a7f381559fa64fe4412a05_380878_1800x0_resize_box_3.png 1800w,/dhqwords/vol/15/1/000524/resources/images/figure01.png 3100w" 
     class="landscape"
     ><figcaption>
        <p>Logo of MEP
        </p>
    </figcaption>
</figure>
<p>The Media Ecology Project (MEP) is a digital resource at Dartmouth directed by Prof. Mark Williams that enables researchers across disciplines to access moving image collections online for scholarly use. Dr. John Bell (Dartmouth ITC) has designed and built the overall technical architecture for MEP. MEP promotes the study of archival moving image collections, enhances discovery of relevant corpora within these archives, and develops cross-disciplinary research methods. These efforts help ensure the survival of these collections via new published scholarship, plus contributions of metadata and research on studied corpora back to the archival community. The virtuous cycle of access, research, and preservation that MEP realizes is built upon a foundation of technological advance (software development) plus large-scale partnership networks that result in new practical applications of digital tools. This article will demonstrate the steady progress toward these MEP goals and designs as a DH project, present reflections about the significant emergence of visual culture DH, and posit certain directions forward.</p>
<p>With internal support at Dartmouth and especially support from the National Endowment for the Humanities, MEP has developed several digital tools that support and sustain the creation of new networked scholarship and pedagogy about archival moving image materials. These include:</p>
<ul>
<li>The Semantic Annotation Tool (SAT), which enables the creation of time-based annotations for specific geometric regions of the motion picture frame.</li>
<li>Onomy.org, which is a vocabulary-building tool that helps to grow and refine shared vocabularies for tags applied to time-based annotations.</li>
</ul>
<p>Together, these two tools support close textual analysis of moving pictures based on time-based annotations Annotations denote a start time and stop time for a subclip, a description and tags related to that clip, and attribution for its creator. This granular approach to media literacy and scholarly annotation is flexible enough to be applied to many types of research and analysis.</p>
<p>MEP is fundamentally 1) a sustainability project that 2) develops literacies of moving image and visual culture history, and 3) functions as a collaborative incubator that fosters new research questions and methods ranging from traditional Arts and Humanities close-textual analysis to computational distant reading. The deployment of close textual analysis is a critical aspect of MEP in developing media literacies within DH. It realizes a practical response to concerns about the acceleration of contemporary culture, the related speed-read dynamics of many audiences, and vacancies of historical and aesthetic insight as a factor of modern consumerist behaviors <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>  <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>  <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>.­­­ Enabling a spectrum of purposeful and reflective considerations of the mediated past is keenly recognized to be pressing and necessary.</p>
<p>At the other end of the methodological spectrum, MEP&rsquo;s work with computer scientists has produced new tools supporting machine-reading of moving images, which produce an expansion of time-based annotations that require lucid and informed evaluation. One direction of this research produces feature extraction (isolating specific formal and aesthetic features of moving images), while another uses deep learning approaches employing convolutional neural networks to identify objects and actions in motion pictures. Data from these tools can be critically assessed and collated with the  “manual”  (human-produced) annotation tools mentioned above to create synthetic and iterative research workflows that  “learn”  across the disciplines. SAT enables real-time playback of all annotations.</p>
<p>While developing MEP as a rather distinctive Digital Humanities project, we have learned first-hand several key lessons about this important and emerging field. Because we are building MEP from an Arts and Humanities perspective, we recognize that our goals must always be framed to raise awareness about the significance of cultural-critical perspectives within the various institutions that we have engaged (archives, libraries, universities, grant resources, etc.).</p>
<p>Like many in DH, we underscore the need for collegiality and connectedness in pursuing collaborative work that depends upon openness and mutual respect as well as a balanced critical eye. This corollary of the MEP profile as a virtuous cycle is echoed in Kathleen Fitzpatrick&rsquo;s recently published call for  “generous thinking”   <sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>. Everyone who engages in MEP is at some level working outside their comfort zones: across disciplines, across expertise, across vocabularies. In a very real sense we are engaged in  “translation”  work, the great benefit of which can be experimentation regarding methodologies of study but also in infrastructural designs of work-flow and output.</p>
<p>New research questions in relation to these workflows will literally transform the value of media archives and support the development of interdisciplinary research and curricular goals (e.g., media literacy) regarding the study of visual culture history and its legacies in the 21st century. These goals have grown to be especially timely during the publication process of this essay: the conceptual and ethical significance of re-imagining our collective purchase on historical imagination has been axiomatic to the socio-political demonstrations of both outrage and engagement that are iconic to 2020.</p>
<h2 id="situating-mep-in-relation-to-transnational-media-archives">Situating MEP in relation to transnational media archives</h2>
<p>MEP was conceived by Prof. Williams at an early meeting of the ambitious Project Bamboo DH initiative. Bamboo is no more, but its basic principles still inform those of MEP: most digital tools have been built for the sciences, resulting in a crying need for DH resources in the Arts and Humanities. But if every institution assumes it must fulfill all recognized needs, our goals will be doomed; we must work collaboratively and in a series of progressive arcs forward to develop both traditional and emergent methodologies of DH scholarship.</p>
<p>The more significant early institutional affiliation for MEP was Prof. Williams&rsquo; inaugural presentation to The Association of Moving Image Archivists (AMIA), which generated immediate and enduring collaborative synergies. We are poised to realize new research trajectories regarding large moving image collections as  “big data,”  and are developing institutional ties to vast digital collections of historical moving image materials. The notion of ecology is central to the project in several ways. Those of us who work on media history recognize all too well that the materiality of historical media is fated. These historic materials simply will not endure, but for the work to preserve and archive them. This work is especially important and timely in our contemporary media environment. Most media audiences and publics simply do not recognize the dilemma that contemporary archivists face. With the rise of social media, many people know that there are thousands of videos posted per hour on sites such as YouTube, and do not imagine that moving image culture is deeply imperiled, since it seems to be ubiquitous and unquenchable. Such an impression effaces the true condition of most historical media, which archivists are vigilantly working to preserve.</p>
<p>The specific platforms we initially engaged and began to bridge are 1) Mediathread, a classroom platform developed at Columbia University, that we helped develop as a research platform that supports publication of time-based annotation metadata and integration of external controlled vocabularies for tagging; 2) Scalar, a digital publishing platform developed at The University of Southern California, expanded to support import of time-based annotations and controlled vocabularies for tagging; and 3) <a href="http://onomy.org/">Onomy.org</a>, specifically designed for MEP to facilitate the collaborative creation and sharing of controlled vocabularies for annotating online media files.<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>  The Media Ecology Project has been developed to sit in between and in relation to these platforms and media collections, navigating the import, export, and production of metadata across participating archival content that has been engaged by a scholar or team of scholars. In this way we can propel capacities for search and discovery across these media, and develop capacities to realize new forms of research, scholarship, and publication.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000524/resources/images/figure02.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000524/resources/images/figure02_hud09abd5340dd339823a278ea59f73243_1530014_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000524/resources/images/figure02_hud09abd5340dd339823a278ea59f73243_1530014_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000524/resources/images/figure02_hud09abd5340dd339823a278ea59f73243_1530014_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000524/resources/images/figure02.png 1275w" 
     class="portrait"
     ><figcaption>
        <p>MEP Archival screening poster of the 2013 founding symposium at Dartmouth
        </p>
    </figcaption>
</figure>
<p>We have enjoyed the participation of multiple renowned archives in several key pilot projects essential to honing the developmental vision for MEP.<sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>  One of the goals in each pilot study is the scholarly development of taxonomies or controlled vocabularies that can be deployed regarding the assignment of tags and other metadata to specific media content areas. The application of these vocabularies will enhance the functional discoverability of archival content and augment efforts to produce new forms of digital scholarship.</p>
<p>MEP archival connections are being built on public standards such as the Open Archive Initiative and the W3C Web Annotation format. Use of these widely available standards is key to making an ecology of applications that encourage bidirectional communication and share information as peers, treating archives as not just a source of raw materials but also a consumer of new analysis and scholarship. MEP has received funding from a variety of internal sources at Dartmouth College<sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup> , which has supported software development and metadata generation but also conference travel and stakeholder meetings at Dartmouth.</p>
<h2 id="tools-to-build-mep-key-early-grants">Tools to Build MEP: Key Early Grants</h2>
<p>In addition to internal support within Dartmouth, Prof. Williams has been fortunate to share three significant start-up grants that have been formative to MEP development.</p>
<h2 id="1-neh-digital-humanities-start-up-grant-the-action-toolbox-with-1st-pi-prof-michael-casey-at-dartmouth-2011">1. NEH Digital Humanities Start-Up Grant: The ACTION Toolbox (with 1st PI Prof. Michael Casey at Dartmouth, 2011)</h2>
<p>ACTION (Audio-visual Cinematic Toolbox for Interaction, Organization, and Navigation) is an open source platform that supports the computational analysis of film and other audiovisual materials. ACTION features extraction and multi-feature pattern analysis and machine learning tools. These tools include color features, motion features, structural segmentations, audio features, and analyses based on automatic labeling of the data via machine learning. ACTION provides a work bench to study such features in combination with machine learning methods to yield latent stylistic patterns distributed among films and directors.<sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>  As such, ACTION is a platform for researching new methodologies in the study of film and media history.<sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup></p>
<h2 id="2-neh-tier-1-research-and-development-grant-semantic-annotation-tool-for-the-media-ecology-project-with-john-bell-at-dartmouth-2015">2. NEH Tier 1 Research and Development Grant: Semantic Annotation Tool for The Media Ecology Project (with John Bell at Dartmouth, 2015)</h2>
<p>The Semantic Annotation Tool (SAT) is an open-source drop-in module that facilitates the creation and sharing of time-based media annotations on the Web by researchers, students, and educators. SAT is composed of two parts: first, a jQuery plugin that wraps an existing media player to provide an intuitive authoring and presentation environment for time-based video annotations; and second, a linked-data-compliant Annotation Server that communicates with the plugin to collect and disseminate user-generated comments and tags using the W3C Web Annotation specification.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000524/resources/images/figure03.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000524/resources/images/figure03_hu11955bec71d2dbd70338dbf8f98c87c4_821869_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000524/resources/images/figure03_hu11955bec71d2dbd70338dbf8f98c87c4_821869_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000524/resources/images/figure03_hu11955bec71d2dbd70338dbf8f98c87c4_821869_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000524/resources/images/figure03_hu11955bec71d2dbd70338dbf8f98c87c4_821869_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/1/000524/resources/images/figure03_hu11955bec71d2dbd70338dbf8f98c87c4_821869_1800x0_resize_box_3.png 1800w,/dhqwords/vol/15/1/000524/resources/images/figure03.png 2050w" 
     class="landscape"
     ><figcaption>
        <p>Semantic Annotation Tool graphic
        </p>
    </figcaption>
</figure>
<p>The goal of building this system was to create an end-to-end open source video annotation workflow that can be used as either an off-the-shelf or customizable solution for a wide variety of applications. Potential uses include collaborative close reading of video for humanities research, simplified coding of time-based documentation in social science studies, enhancing impaired vision accessibility for media clips on web sites, and many others.<sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup></p>
<p>As is typical for linked data-compliant systems, annotations created by SAT are structured using a combination of multiple, type-specific data standards.<sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>  A SAT annotation consists of:</p>
<p>A Media URI describing the location (source) of the object being annotated  Basic identifying metadata for the source object (e.g., title, author) when available  Provenance information for the annotation  A textual annotation body and multiple tags that apply to the delimited media fragment. SAT annotations create relationships between these components and the media object being annotated using several standards, including subsets of <a href="http://xmlns.com/foaf/spec/">Friend of a Friend (FOAF)</a>, <a href="http://www.w3.org/2004/02/skos/">Simple Knowledge Organization System (SKOS)</a>  <a href="http://www.w3.org/TR/annotation-model/">W3C Open Annotations (OA)</a> and <a href="http://dublincore.org/documents/dcmi-terms">Dublin Core (DC)</a>. Annotations are encoded for transmission using JSON-LD.</p>
<p>Statler is the server half of the Semantic Annotation Tool. Built on a Ruby on Rails framework, Statler is a standalone linked data server that allows persistent annotations to be added to media files with minimal changes to the host platform. Statler&rsquo;s public face is an API that serves W3C Web Annotation11-compliant metadata describing arbitrary media URLs.</p>
<p>Waldorf.js is the client half of the Semantic Annotation Tool. It is a jQuery plugin that can be added to any HTML page with only a few lines of code. Once installed it searches the page for HTML5 media tags and dynamically wraps them in an interface that supports annotation of time-and geometrically-delimited media fragments. Waldorf.js was developed in collaboration with VEMI Lab<sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>  to ensure that accessibility was forefront in its development and that playback of annotations is compatible with screen reader software.<sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup></p>
<h2 id="3-expanding-sat-via-knight-news-challenge--unlocking-film-libraries-for-discovery-and-search--with-prof-lorenzo-torresani-and-john-bell-at-dartmouth-the-internet-archive-vemi-lab-at-umaine-2016">3. Expanding SAT via Knight News Challenge:  “Unlocking Film Libraries for Discovery and Search”  (with Prof. Lorenzo Torresani and John Bell at Dartmouth, The Internet Archive, VEMI Lab at UMaine, 2016)</h2>
<p>This 6-month Knight grant successfully demonstrated the potential for the Semantic Annotation Tool to help make troves of film/video housed in thousands of libraries searchable and discoverable. Working with Dartmouth College&rsquo;s Visual Learning Group (directed by Prof. Lorenzo Torresani), we collaborated to apply machine vision tools already being developed for object, action, and speech recognition to a collection of one hundred educational films held at the Internet Archive.<sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup>  The goal was to set the stage for future full-scale integration by examining the output of these tools and comparing them to one another as well as to human-generated annotations.</p>
<p>Part of the significance of our project was to enable essential first steps in object and action recognition for historical formats of film/video, thereby providing incentives for the field of computer vision to develop research capacities regarding film/video from prior eras.<sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup>  Typical library cataloguing practices provide only basic information for such content: title, subject, synopsis. Libraries have made great strides in unlocking word culture texts through optical character recognition; they are opening up audio items with voice-to-text transcription. But thus far, libraries have not found ways to unlock moving images and annotate them at scale. Developing steps to realize an automated solution to producing high-quality metadata about such historical film and video content will be critical to allowing libraries and archives of all sizes to make the collections they own available for public use. The data generated by this prototype grant provided a significant model for first steps toward developing such a system (see <a href="#figure04">Figure 4</a>). The deep learning output was not error-free, but the success rate outperformed expectations.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000524/resources/images/figure04.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000524/resources/images/figure04_hu22ba4623ef4ed6777691bbfcf8ae5582_284903_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000524/resources/images/figure04_hu22ba4623ef4ed6777691bbfcf8ae5582_284903_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000524/resources/images/figure04.png 1100w" 
     class="portrait"
     ><figcaption>
        <p>Machine Vision Search overview
        </p>
    </figcaption>
</figure>
<p>The prototype also demonstrated the utility of The Semantic Annotation Tool as the back end of such a research protocol, robust enough to host the entire iterative annotation cycle: to enable the creation of manual (curated) time-based annotations by knowledgeable scholars and scientists, and to host the subsequent machine-learning output of many times more time-based annotations. The fulfillment of the research process will allow the content curators (manual annotators) to quickly evaluate the machine-learning results, which will produce a new enlarged and sweetened training set for the algorithms, etc. The resultant iterative cycle of excellence would indeed produce game-changing results for moving image libraries and archives everywhere. The ideal interface would operate by translating in real-time the text-queries provided by users into content-based classifiers that recognize speech, audio, objects, locations, and actions in the video, in order to identify the desired segments in the film. When implicitly validated by users (by viewing), the search results and the original text queries would be fed into SAT which will add these annotations to each film for permanent semantic browsing and search.</p>
<p>With talented undergraduate students at the DALI Lab at Dartmouth, we were able to produce a Machine Vision Search prototype website that illustrates the research process and also demonstrate key research results: SAT was used to display tags generated by machine vision analysis of films as time-based annotations of those films. MVS demonstrates the flexibility of SAT by significantly changing its presentation interface, eliminating the annotation bodies and instead displaying only tags. In addition, Waldorf.js was extended to add new functions like flagging and deleting tags/annotations that the machine vision system identified incorrectly. These new functions additionally demonstrated SAT&rsquo;s flexibility, because they required no changes to the annotation server itself. Dartmouth students were able to create the custom MVS interface on a very short development timeline due to SAT&rsquo;s architecture and simplicity.</p>
<h2 id="applications-mep-advanced-neh-grant-projects">Applications: MEP Advanced NEH Grant Projects</h2>
<p>In 2018 Prof. Williams and Dr. Bell were honored to receive two advancement grants from The National Endowment for the Humanities for The Media Ecology Project. These grant projects had each been initially developed as demo pilots for MEP and are now poised to realize significant advances in Digital Humanities scholarship via further developments of SAT, both technologically and conceptually.</p>
<h2 id="1-the-paper-printbiograph-linked-data-compendium-understanding-visual-culture-through-silent-film-collections-2018-2020">1. The Paper Print/Biograph Linked Data Compendium: Understanding Visual Culture Through Silent Film Collections (2018-2020)</h2>
<p>One of our inaugural pilot projects was in conjunction with the Library of Congress regarding their early silent film era materials, with an emphasis on the historically significant Paper Print collection <sup id="fnref:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup>.<sup id="fnref:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup>  The Paper Print collection is, especially in the U.S. context, roughly the equivalent of the Rosetta Stone for those who study moving image history in relation to visual culture: a vast and inspiring series of historical objects that is unique in film history. As motion pictures were invented and experimented with, their producers applied for copyright of each film by placing a positive print of the film materials on ribbons of photosensitive paper for deposit at the Library of Congress. This has resulted in a record of the literal development of early cinema practices that no other archive can duplicate. We are extremely proud that the Library of Congress has promised to digitize the entire corpus of Paper Print titles in relation to the partnership forged by MEP and the esteemed early cinema and pre-cinema study organization DOMITOR. Early research in the pilot was represented as the plenary panel of the 2017 Women and the Silent Screen conference in Shanghai.</p>
<p>This recent advanced NEH grant project will produce a digital compendium of over 400 select films from the silent cinema era documenting the aesthetic practices of early cinema, with attention to the transition of visual culture from stage to screen. The Compendium will afford many new questions regarding historical visual culture that span the extraordinary history of early cinema from attractions to narrative, from the natural world to vaudevillian theatrics, from abstraction to realism. It will combine highly-influential and rare works archived at the Library of Congress with materials preserved at the Eye Filmmuseum in Amsterdam, The British Film Institute (BFI), and The Museum of Modern Art (MoMA) to create a digital resource for film scholars around the world. Many early films from Eye will be digitized from prints derived from films shot in the original Biograph format of 68mm, prints that offer better raw material for machine vision analysis than Paper Print versions, for example. Taking an innovative approach to annotating the digitized films with diverse types of scholarly description, archival metadata, and machine-generated annotation, the compendium will present visitors with a variety of analytic lenses embedded in a single, simple interface.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000524/resources/images/figure05.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000524/resources/images/figure05_hu397ddf097e6178d1435875506ecd916d_393382_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000524/resources/images/figure05_hu397ddf097e6178d1435875506ecd916d_393382_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000524/resources/images/figure05.png 810w" 
     class="landscape"
     ><figcaption>
        <p>Poster for NEH grant Early Cinema Linked Data Compendium
        </p>
    </figcaption>
</figure>
<p>The late Paul Spehr&rsquo;s meticulous chronological production logs of American Mutoscope &amp; Biograph films, derived from various historical collections over many decades of research, will serve as a backbone for the Compendium to provide a framing infrastructure for all of the Compendium films and foreground the 68mm films especially as neglected marvels of early cinema, ripe for rediscovery and counter-history. The corpus will also feature new digital access to the collection of Biograph exhibitor catalogs at The Museum of Modern Art, a resource that features three keyframes from each motion picture title described — rich historical information and extremely rare kernels of visual culture, in many cases for films that are otherwise considered lost. The compendium will frame each film and its historical record as a resource for rediscovery and fresh methodological interventions, central to the advancement of the digital humanities in relation to visual culture.</p>
<p>To create the compendium, we will integrate MEP&rsquo;s SAT with software developed by the Alliance for Networking Visual Culture (ANVC). ANVC&rsquo;s Scalar is a web publishing platform designed to present text, media, and data using integrated, flexible interfaces that was an early integration target for MEP when the project built data exchange tools connecting it with Mediathread. Integration of SAT into Scalar is an evolution of those earlier efforts: rather than attempting to move data between annotation tools that do not follow common standards, the new project would take the standards-compliant SAT module and drop it directly into the Scalar platform. Both SAT and Scalar are built on semantic web principles that make it easy to gather and merge diverse data from across the web using linked data, making the integration a natural fit.</p>
<p>Several types of data will illustrate the compendium&rsquo;s wide range of methods to assess and study these valuable works of cultural history (see <a href="#figure05">Figure 5</a>). These data include:</p>
<ul>
<li>The films themselves, as streamed<sup id="fnref:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup>  from the Library of Congress and Eye Filmmuseum</li>
<li>Basic metadata and select annotations identifying creative personnel and genre</li>
<li>An extensive database of film production information</li>
<li>Descriptions of performance styles and gestures of some films, encoded using Laban Movement Analysis (LMA)</li>
<li>Algorithmically-generated analysis, such as optical flow visualizations, to highlight formal characteristics of some films</li>
</ul>
<p>The compendium will also serve as an evolving source of information and scholarly possibility. Because of its open software framework, interested scholars can contribute their own analyses based on interpretative contexts of their own devising. This MEP linked data compendium, then, will not only unite a wide and growing variety of data, but offer the chance for scholars to gather and trade ideas with one another, creating fertile territory both for discussion and the sparking of new knowledge about these essential works of cinema. The Compendium will contain data describing many different aspects of films in the corpus.</p>
<p>For example, one key area of emphasis that evolved in the MEP pilot study on the Paper Print Collection is the analysis of performance styles. One of the characteristics of the era is the transition from heavily codified theatrical performance styles derived from late 19th century theater, toward the uneven development of more  “cinematic”  performance styles that evolved in relation to the proximity of the motion picture camera. An ideal case study emerged regarding the career of Florence Lawrence who, though uncredited (as were most all performers of the pre-Nickelodeon era), came to be known to audiences as  “The Biograph Girl.”  In this study, primarily developed by Prof. Jenny Oyallon-Koloski, time-based clips of Lawrence&rsquo;s onscreen appearances were demarcated via brief description and tagged according to a simplified protocol of Laban Movement Analysis (LMA). Mediathread provided the capacity to codify her performance style (gestures, facial expressions, other aspects of the expressive body) and potentially contrast her performance style with those of other Biograph actresses of the era such as Mary Pickford.</p>
<p>In MEP&rsquo;s pilot studies, Mediathread allowed for the documentation of delimited written descriptions and metadata relevant to each title among the archival films accessed to that point. The annotation of Lawrence&rsquo;s performances, for example, were applied via full-frame time-based annotations. Using the Semantic Annotation Tool in the Compendium will enhance the precision of our already granular annotation methodology by adding geometric targets within a frame and real-time playback of annotations with sub-second resolution. These innovations will enable the creation of time-based annotations that reference films with greater specificity, a key enhancement given the speed with which performance modalities shift.</p>
<p>Traditional cataloging techniques depend on key concepts like normalizing metadata into standardized sets of descriptive fields and ensuring consistent minimum coverage across a collection. The Compendium will instead organize annotations produced via networked scholarship using linked data concepts drawn from the semantic web. Linked data was designed to distribute data across a decentralized network: the entire Internet. Like the Internet itself, it was designed for heterogeneity and fault tolerance. Using linked data as the basis for research annotations will allow the Compendium to use highly specific data models for each type of inquiry that a scholar wishes to pursue, rather than try to force data into pre-approved ontologies. Since it contains no expectation of complete coverage, researchers can choose to annotate as many or as few films as they need for their research–new data simply adds to what is already known about a film.<sup id="fnref:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup>  The  “linked”  concept means that disparate data types can be connected to one another using either simple relationships–two annotations may refer to the same timecode in a video–or semantically rich relationships–for example, cause and effect.</p>
<p>In addition to manual annotations, we are working to apply another type of data in the Compendium: optical flow tracking based on computer vision analysis of the films.<sup id="fnref:20"><a href="#fn:20" class="footnote-ref" role="doc-noteref">20</a></sup>  SAT will allow us to pinpoint where in the frame the annotated movement takes place, which will facilitate the isolation of gestures and smaller movements and allow us to visualize actors&rsquo; movement pathways through the frame. These improved annotation strategies will strengthen our ability to document the movement patterns we are observing, will aid in our communication of these findings to research collaborators, and will enhance the complementarity between our manual annotations and computer vision analysis.</p>
<p>The films represented in this Compendium will designate a galaxy of new research inquiries, especially when placed into linked data relations with one another and with the new textual and contextual metadata we will provide. The Compendium will in a sense re-animate early cinema history, phenomenologically and conceptually, especially for audiences and users new to this material. Contemporary media theorist Bernard Stiegler&rsquo;s preferred phrase is to  “re-enchant”  our sense of history and the world <sup id="fnref:21"><a href="#fn:21" class="footnote-ref" role="doc-noteref">21</a></sup>, a necessary tonic to the information bloat and hollow exploitation of much digital media engagement today.</p>
<p>But we also will cross a new set of thresholds in a digital humanities context, critical to this grant opportunity, by re-articulating the dialectic described in the very notion of digital humanities. The tension that exists between the traditional Humanities tenets of close textual analysis versus the demand for distant reading and analysis at scale in the computational sciences will be both visualized and progressively informed by this linked data Compendium.<sup id="fnref:22"><a href="#fn:22" class="footnote-ref" role="doc-noteref">22</a></sup>  The use of optical flow visualizations and metadata in the Compendium project (previously utilized in the <a href="http://aum.dartmouth.edu/~action/index.html">ACTION toolset</a>) will hopefully contribute to both the existing data pool of optical flow research and to the fundamental experiential distinction between manually-generated granular performance annotations and machine-generated cinemetrics. Both types of data will be shown in context with one another within the Compendium, inviting new relationships between close and distant viewing. Scientists, scholars, and artists alike will be in a position to imagine and explore unique ways to further interrogate and mobilize this new experience and pursue new research questions and representational innovations.</p>
<p>Though it will contain several finished essays, the Compendium will also exist as a first draft of complex research on these films with a large multi-archive body of films and related metadata to be iteratively added. But it will also be an engine for new and previously unconsidered research questions and methods, a first draft of varied directions of inter-disciplinary DH pursuits that can directly engage the arts, historical and cultural studies, and computational analysis.</p>
<h2 id="2-the-accessible-civil-rights-heritage-project-2018-2020-expanding-the-goals-of-access">2. The Accessible Civil Rights Heritage Project (2018-2020): Expanding the Goals of Access</h2>
<p>Our MEP pilot on historical news materials (newsreels, news telecasts, newsfilm, and other associated footage) was dedicated to new scholarship on news materials from multiple archives. We gradually honed this topic and its participants into a focused address to Civil Rights content,<sup id="fnref:23"><a href="#fn:23" class="footnote-ref" role="doc-noteref">23</a></sup>  with a double purpose of developing new scholarship plus a dedicated line of research and development to enable access of these semantically rich and complex materials for blind and visually impaired (BVI) users.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000524/resources/images/figure06.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000524/resources/images/figure06_hu6f0d0eff09a98f768fb7bf380cb8b7c6_787822_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000524/resources/images/figure06_hu6f0d0eff09a98f768fb7bf380cb8b7c6_787822_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000524/resources/images/figure06_hu6f0d0eff09a98f768fb7bf380cb8b7c6_787822_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000524/resources/images/figure06.png 1366w" 
     class="landscape"
     ><figcaption>
        <p>Poster for NEH grant Civil Rights ACRH NEH grant
        </p>
    </figcaption>
</figure>
<p>The term  “newsfilm”  has evolved in relation to historical changes in media technology and media formats. Television newsfilm evolved after decades of motion picture newsreel and news magazine production that was intended for exhibition in motion picture theaters on a weekly or bi-weekly basis. Local television newsfilm–often shot on site by local television station news crews that only broadcast a fraction of what they recorded–is a largely untapped source of local and national history that captured powerful moments throughout the emotionally and politically charged American civil rights era. Television newsfilm was produced in a different media industry context and was intended for  “exhibition”  to domestic audiences on an almost daily basis in the U.S. for many decades. It was regularly produced by both local television stations and network television news divisions.</p>
<p>A key context significant to most television newsfilm collections is local television itself, which is a conspicuous, persistently ignored aspect of U.S. media history, even though the local station is the backbone and the condition of possibility for the dynamics of U.S. network television. Local television history is a common site of disavowal regarding many media histories, especially work on U.S. media history. As such, it can be the site of significant resultant capacities for historiographic depth and complexity. What is often truly compelling about sophisticated historical research is the relationship between the already-understood and what we think we know, versus the capacity to interrupt given history, perhaps even to intervene in that history. Local and network television newsfilm features the full spectrum of these historiographic capacities.</p>
<p>Television newsfilm was a primary form of extended news coverage for roughly forty years, from the evolution of television in the 1940s to the gradual adaptation to video formats in the 1980s. Much of the footage to be considered for this study is what is sometimes termed  “raw”  newsfilm, the footage that existed in-camera before it was edited and repurposed by the news professionals at a station or network.  “Newsfilm”  also describes local and network  “finished”  news stories and news programs and documentaries that utilized footage from multiple sources, aired for domestic audiences and sometimes distributed via film prints to local, national, and international markets. Collections of television newsfilm are being preserved and curated today in many archival collections across the U.S. and around the world. Most of these collections are as yet unavailable for critical and historical consideration.</p>
<p>It is important to underscore that historical newsfilm from different eras and industrial contexts have become digitized and available for study and time-based annotation only very recently. Many of the newsfilm clips and raw footage materials engaged for this study will be seen by scholars for the first time. This is a burgeoning area for both scholarly and public interest, a site for critical awareness about the (mediated) past. Also distinctive to this project is that much of the newsfilm to be studied was never screened publicly. That is, in many instances this newsfilm footage has never before been part of the public sphere, and thus has never been considered by even casual historians in any field. It has existed outside of critical inquiry and scholarship that is devoted to social history and media history. From a cultural perspective, these are indexical materials that have not yet been in a position to be remembered, let alone forgotten.</p>
<p>The conditionally absented or fugitive aspects of these civil rights materials inspire awakenings of the historical imaginary, and we expect this material will become especially relevant within our very contemporary 2020 context of global demonstrations and calls for social justice in response to the brutal murder of George Floyd and other African Americans in the U.S. The  “Black Lives Matter”  outcry in the weeks and months prior to the publication of this essay has been widespread and sustained, and seems to represent what Raymond Williams referred to as a major shift in  “structures of feeling”   <sup id="fnref:24"><a href="#fn:24" class="footnote-ref" role="doc-noteref">24</a></sup>: affective relations between consciousness and social institutions that are strongly emergent and inflect palpable pressure on commonplace rationalizations and actions.</p>
<p>The historical and historiographic potential of these materials is both vast and substantial. The Accessible Civil Rights Heritage (ACRH) project (see <a href="#figure06">Figure 6</a>) will help to expand the discoverability of these historical materials for critical consideration, by developing scholarly practices in relation to archival practices that will enhance searchable access to these historically rich items that would otherwise continue to be isolated in archival and data silos and virtually unavailable for search of any kind.</p>
<p>The hundreds of newsfilm elements made available for study in this project will serve as a representative sample of the ocean of television newsfilm collected in archives and historical societies across the U.S.<sup id="fnref:25"><a href="#fn:25" class="footnote-ref" role="doc-noteref">25</a></sup>  The study of the newsfilm era and subsequent eras of news coverage (i.e., post-celluloid eras) will be significantly enabled by the development of workflows, protocols, scholarly methods, and augmented vocabularies/ontologies to be developed in this proposed study.<sup id="fnref:26"><a href="#fn:26" class="footnote-ref" role="doc-noteref">26</a></sup></p>
<p>In order to consolidate these diverse materials we have engaged archivist Becca Bender (Rhode Island Historical Society) to advise on the creation of a common metadata spreadsheet format, and veteran professional moving image cataloguers Kathy Christensen and Laura Treat to help develop an Onomy vocabulary specific to the purpose of annotating civil rights news footage. These new cataloging and access procedures will assist in the parallel development of innovating high-quality, meaningful experiences of the collection to BVI users.</p>
<p>Given the special concerns of close textual analysis and its importance to humanities researchers, it is critical that any toolset designed to support humanities research be developed with that specific application in mind. However, any existing collection of materials and scholarship would carry with it the limitations of the tools that were originally used to create it–vocabulary and metadata that was designed to fit into a particular schema, as discussed by Owens <sup id="fnref:27"><a href="#fn:27" class="footnote-ref" role="doc-noteref">27</a></sup>.</p>
<p>For the purposes of ACRH research, annotations featuring close reading analysis of civil rights newsfilm will be merged with extant metadata from the contributing archives and scholarly essays that feature newly-generated time-based descriptions. The result will be a re-animation of sorts for these historical media documents where specific events and images are contextualized in relation to known descriptors and vocabularies. ACRH&rsquo;s research into articulating the hermeneutics of moving images using annotations will result in a synthetic process that engages archivists, scholars and students to share their experience of these key cultural heritage texts with others–even those who cannot see the original texts.</p>
<p>ACRH will repurpose a selection of assorted newsfilm to produce a corpus of material that is uniquely challenging to describe: historically charged footage laden with contextual meaning but limited extant metadata. This sub-corpus is being repurposed for research into adaptive technology for BVI users, a fundamental example of cross-disciplinary opportunities that MEP is designed to enable and investigate. Although the potential historical value of newsfilm materials for BVI access is evident, accessible delivery of online video is a challenge that higher education has struggled to meet, leading to thousands of hours of video instruction being taken offline because the schools that created it could not provide equal access to all users.</p>
<p>The state of BVI accessibility on the web is, in short, disastrous. Web browsers in general are riddled with inconsistent implementations of reference specifications and vendor-exclusive features. Adding accessibility features that are often poorly understood and costly to implement to that unstable environment has resulted in–at best–inconsistent efforts to ensure web content meets accessibility guidelines, e.g. <sup id="fnref:28"><a href="#fn:28" class="footnote-ref" role="doc-noteref">28</a></sup>. The type of content that ACRH is targeting, online videos with time-based annotations, is so new that accessibility has not yet been thoroughly considered in this context. By researching key guidelines and technologies, ACRH has an opportunity to direct the accessibility conversation about time-based annotations in positive directions.</p>
<p>As the online market has matured, the penalties for organizations that fail to make content accessible online have grown. In 2015 the Department of Justice settled with online education giant EdX for failure to comply with the Americans with Disabilities Act and forced EdX to implement a number of accessibility standards including WCAG 2.0 and WAI-ARIA.<sup id="fnref:29"><a href="#fn:29" class="footnote-ref" role="doc-noteref">29</a></sup>  UC Berkeley decided to remove thousands of hours of open educational audio and video content because it did not have the resources needed to make it ADA compliant,<sup id="fnref:30"><a href="#fn:30" class="footnote-ref" role="doc-noteref">30</a></sup>  touching off a heated back and forth between university administrators and faculty.<sup id="fnref:31"><a href="#fn:31" class="footnote-ref" role="doc-noteref">31</a></sup>  Accessibility problems are not limited to higher education either, as in the case of a 2014 lawsuit brought against Seattle School District 1 that resulted in a consent decree that was estimated to cost the district in the range of three-quarters of a million dollars.<sup id="fnref:32"><a href="#fn:32" class="footnote-ref" role="doc-noteref">32</a></sup></p>
<p>BVI students in particular may have trouble fully understanding primary video sources because the text or audio descriptions associated with them rarely convey the full meaning and context of the images on screen. BVI users cannot see content filled with small clues that may be critical to its interpretation. Humanities scholars pore over information-dense resources like video to closely read it as a primary historic text at a level of detail that goes far beyond the ability of traditional accessibility adaptations like captioning to capture. ACRH proposes that time-based annotation techniques<sup id="fnref:33"><a href="#fn:33" class="footnote-ref" role="doc-noteref">33</a></sup>  can provide support for humanistic interpretation of video far better than existing adaptive technology. Beyond the BVI community, though, researching best practices for time-based annotation will provide scholars with a new perspective on how to integrate data-centric digital heuristics with deeply cultural hermeneutics.</p>
<p>Existing accessibility guidelines for online video usually focus on creating secondary audio or caption tracks that synchronize playback with the video itself.<sup id="fnref:34"><a href="#fn:34" class="footnote-ref" role="doc-noteref">34</a></sup>  The closest these recommendations come to SAT&rsquo;s methodology is Mozilla/A11y&rsquo;s <a href="https://wiki.mozilla.org/Accessibility/Video_a11y_requirements">recommendation</a> to embed a timed text track into Ogg video. Setting aside the browser restrictions introduced by using Ogg video, timed captions have a number of drawbacks in an educational setting when compared to full annotations: they are only delimited by time, not geometric space in the frame; they do not carry additional metadata like tags that are useful for cataloging and search; and they do not include authorship information that is important to convey in a scholarly context. Additionally, SAT&rsquo;s separation of annotation data from the video file provides opportunities to readily query that data using external tools–a key feature that streamlines the workflow of digital humanities scholars.<sup id="fnref:35"><a href="#fn:35" class="footnote-ref" role="doc-noteref">35</a></sup></p>
<p>ACRH will study how to write video annotations that convey the rich content of evocative videos, and create adaptive technology that supports playing back those annotations audibly. The resulting guidelines and technology will be published as an open resource that all schools, museums, and archives can use to make their own video collections more accessible to BVI users. The grant brings together scholars, archivists, cataloging experts, and cognitive neuroscientists to research best practices for these requirements. The result will be an evidence-based set of guidelines for creating accessible video annotations, documentation on how to implement those guidelines using open-source software, and a demonstration corpus of civil rights newsfilm showing humanities scholars how to apply these guidelines to their own research. Just as there is a concept of resources that are  “born-digital,”  ACRH proposes to build a humanities corpus that includes video, annotation, and metadata so it is, as a body,  “born-accessible.”</p>
<p>Composing annotations of moving image culture that are meant to assist blind and low vision viewers redefines certain basic assumptions about visual culture among the sighted, and demands careful attention to details otherwise taken for granted. It is surprisingly difficult to capture the basic information of a shot. Our methods are experimental, in that we are near the completion of compiling a sufficiently large data set to provide our colleagues at VEMI for their qualitative social science research.</p>
<p>The participating archives have generously provided core descriptive metadata for hundreds of civil rights newsfilm clips, and assisted in selecting the dozens of clips for which we will provide more extensive time-based annotations via SAT. The methodology, process, and culminating metadata will be published and made available for open access and use. Much of the archival media will also be available for scholarly and public access from the participating archives, dependent upon archival protocols and online capacities.</p>
<p>A culminating symposium for the ACRH Project will bring together archivists, technicians, and especially scholars from across academic disciplines to critically engage and assess the results of the project and imagine next best steps to develop the ARCH Project research materials.<sup id="fnref:36"><a href="#fn:36" class="footnote-ref" role="doc-noteref">36</a></sup></p>
<h2 id="visual-culture-dh-reflections-on-the-ways-ahead">Visual Culture DH: Reflections on the Way(s) Ahead</h2>
<p>As the article has shown, MEP has a history of supporting projects exploring intersections between different methodologies of study and critical approaches to varied archival content. We have found time-based annotations and tags to be a key enabling technology that allows scholars the freedom to engage with digital media texts from multiple perspectives that can then be programmatically synthesized into a cohesive assemblage.</p>
<p>Among the methodological comfort zones to be negotiated in Digital Humanities, we are committed to the development of Visual Culture Studies in DH, which can produce tension with legacy approaches to DH that primarily focus on word culture alone. In addition, the field of Film and Media Studies often features attention to research methods that address and engage audiences and the reception of media texts, emphases that are less prominent in the historical study of word culture. Most important is the prominent DH tension between the traditions of  “close reading”  that are central to the Arts and Humanities versus the goals and practices of reading at scale that are crucial to computational approaches to vast corpora of texts under analysis. Taylor Arnold and Lauren Tilton revise the terminology regarding visual culture to  “distant viewing” , which takes into account the implicit interpretive quality of acts of viewing <sup id="fnref:37"><a href="#fn:37" class="footnote-ref" role="doc-noteref">37</a></sup>.</p>
<p>Recognizing sites of potential dissonance can help to reformulate them as sites of new inquiry and critical intervention in the pursuit of Visual Culture Studies, to produce instead a growth area for both productive research inquiry and rigorous critical evaluation. The close and distant reading/viewing dissonance of DH can be seen to work within the motivated, intentional bi-play of manual and automated annotations in MEP as a defining and iterative dialectic which allows us to better recognize an always-already evolving spiral of creative and critical exchange across manual and automated realms. Realizing a full awareness of this dynamic model may present a fundamental perspective toward progress in the emerging interdisciplinary space that is DH.</p>
<p>One signal theoretical turn to recommend in this realization of an ongoing and dynamic dialectic toward computer vision and machine-reading excellence is the historic approach to early cinema theory provided by Dziga Vertov&rsquo;s Kino-Eye or kinoglaz theory.<sup id="fnref:38"><a href="#fn:38" class="footnote-ref" role="doc-noteref">38</a></sup>  Vertov&rsquo;s theory famously consists of two major tenets, related yet distinct from one another. In tenet one,  “kino-eye”  is engaged with his great enthusiasm and devoted pursuit of new technologies of enhanced vision (e.g., the motion picture camera and related lens technologies), which represented material advances beyond human vision alone that seemed capable of both futurist and constructivist goals for enhancing society and culture. But equally if not more important is the second major tenet,  “kino-edit” , which requires the rigorous study and understanding of the world and its historical processes by the artist/scientist, in order to actively interrogate, differentiate, and recontextualize what may be recognized to be merely positivist technological outputs of the Kino-Eye (e.g., racial and gender bias in facial recognition software, etc.) <sup id="fnref:39"><a href="#fn:39" class="footnote-ref" role="doc-noteref">39</a></sup>  <sup id="fnref:40"><a href="#fn:40" class="footnote-ref" role="doc-noteref">40</a></sup>  <sup id="fnref:41"><a href="#fn:41" class="footnote-ref" role="doc-noteref">41</a></sup>  <sup id="fnref:42"><a href="#fn:42" class="footnote-ref" role="doc-noteref">42</a></sup>.</p>
<p>Vertov&rsquo;s theory is widely recognized in its motivated call for critical and aesthetic discernment and socio-political insight. Like many aspects of visual culture historiography, it is surprisingly applicable to not only early cinema but also to the rise of digital culture and its related epistemologies. We can anticipate that the development of new DH tools and platforms for Visual Culture Studies will necessitate sophisticated theoretical frameworks that will prove essential to 21st research and scholarship. The two tenets/steps of Kino-Eye theory are directly applicable to the MEP advanced NEH grants described above, especially in relation to the iterative workflows of manual close-textual annotations, vast machine-reading expansions in the number of those annotations, and manual curated evaluations of the machine-reading results (that then afford a new training set for another iteration of the cycle). Also significant to recognize is that these annotations are sometimes frame grabs, but primarily time-based sub-clips, the study of which can judiciously contribute to a new and contemporary re-understanding and elaboration of Vertov&rsquo;s elusive, unfolding, yet core concept of the moving image  “interval” , tied to the organization and elaboration of movement <sup id="fnref:43"><a href="#fn:43" class="footnote-ref" role="doc-noteref">43</a></sup>.</p>
<p>There is more to say about the conceptual and phenomenological value of manual time-based annotations. As suggested earlier, manual annotation of visual culture is a contemplative, iterative, and essential task that mirrors more innovative annotation practices across Digital Humanities endeavors. For example, annotation platforms such as hypothes.is have inspired new scholarship that promotes the generative aspects of close reading practices in relation to networked scholarship when annotating word culture texts in their platform <sup id="fnref:44"><a href="#fn:44" class="footnote-ref" role="doc-noteref">44</a></sup>  <sup id="fnref:45"><a href="#fn:45" class="footnote-ref" role="doc-noteref">45</a></sup>  <sup id="fnref:46"><a href="#fn:46" class="footnote-ref" role="doc-noteref">46</a></sup>.</p>
<p>Indeed, part of the spirit of intervention within MEP and SAT is the process of manual annotation itself, which necessarily slows down the apprehension and understanding of moving images as aesthetically expressive media. This produces a palpable countervailing force against many contemporary viewing practices of moving image culture. For example, professors and other teachers of media history and arts can attest almost uniformly to a gradual change across generations of their students in the craft of  “reading”  moving image culture beyond a stencil or scaffold of factual and narrative  “content” .</p>
<p>For media historians and artists, this mode of reception can seem as though many audiences today have been trained as pattern recognition filters, impatient for the next plainly evident delta change of attractions in the image or soundtrack, and fundamentally inattentive to basic aesthetic information provided as signal contributions to the expressive registers of the text. This results in an implicit or even explicit audience  “demand”  or drive to accelerate perceptual stimuli ( “I&rsquo;ve got it, move on; I&rsquo;ve got it, move on…” ) that eschews many of the fundamental temporal and spatial registers of visual culture, and moving images in particular.</p>
<p>This presumptive need for speed may be directly related to changing ecologies in the expansive amount of available quality mediated content, but also inter-medial changes in the patterns of consumption of time-based media (e.g., scrubbing through video, binge-watching); emergent formal practices within games and social media; and perhaps even a resultant competitive pressure for the use of one&rsquo;s time within and across mediascapes, e.g. <sup id="fnref:47"><a href="#fn:47" class="footnote-ref" role="doc-noteref">47</a></sup>. Even though these contemporary reading practices and competencies may ultimately prove to be valuable in specific contexts,<sup id="fnref:48"><a href="#fn:48" class="footnote-ref" role="doc-noteref">48</a></sup>  there is indisputable value in also learning to deepen one&rsquo;s capacity for better attending to the historical aesthetic practices in moving image craft and art (across the full range of expressivity), and also to better understand the value of such an investment.<sup id="fnref:49"><a href="#fn:49" class="footnote-ref" role="doc-noteref">49</a></sup>  The rise of the global  “slow cinema”  movement is a key index of truly widespread cultural resistance to accelerated media culture, and exists in part as a contemplative critical response to the aesthetics and underlying political economy of mediated subjectivity based upon expediency, speed, presentism, and the exploitation of a delimited attention economy, e.g. <sup id="fnref:50"><a href="#fn:50" class="footnote-ref" role="doc-noteref">50</a></sup>.</p>
<p>A broader but related perspective regarding digital culture writ large, and perhaps especially the debated value of the rise of artificial intelligence (machine-reading), comes from the technological imperatives derived by Bernard Stiegler <sup id="fnref1:21"><a href="#fn:21" class="footnote-ref" role="doc-noteref">21</a></sup>, who has been developing a series of incisive theoretical tropes concerning the rise of digital culture as a pharmakon: at once a powerful and enticing possible remedy for specific needs and demands in a socio-political system, but also a poison if used unknowingly or improperly.</p>
<p>Stiegler&rsquo;s most important intervention regarding DH resulted from his career-changing participation in the artist-scientist collaborative Ars Industrialis in 2005, which led to his committed politics of critique that makes a concerted call to  “re-enchant”  the world via new and rigorous attention to history and the arts, imbued with critical literacy about them. Central to his increasingly complex and refined theories is a renewed attention to the significance of what he terms tertiary memory (e.g., the archive), an externalized and technical extension of  “internal memory” , plus a call to return to an intentional and motivated anamnesis, a refusal to forget the vital importance of critical engagement with these memory deposits in the interest of sustainable cultural environments. He positions this essential work of critical engagement in relation to what Plato termed  “self-care” , but jettisons Plato&rsquo;s dismissal of the technological. Contemporary society, deeply imbued by technology and mediation, is now, perhaps for this very reason, threatened to become history-less and therefore uncritical about itself as a control Society <sup id="fnref2:21"><a href="#fn:21" class="footnote-ref" role="doc-noteref">21</a></sup>  <sup id="fnref:51"><a href="#fn:51" class="footnote-ref" role="doc-noteref">51</a></sup>  <sup id="fnref:52"><a href="#fn:52" class="footnote-ref" role="doc-noteref">52</a></sup>.<sup id="fnref:53"><a href="#fn:53" class="footnote-ref" role="doc-noteref">53</a></sup>  Conscientious and motivated engagement with the archive is an essential component of balancing and even responding to the pharmakon of drives and controls associated with accelerated digital culture and AI. The call for both a renewed ethic of self-care about societal memory and a rigorous set of practices that enable such motivated and critical engagements are directly parallel with the goals and practices of MEP.</p>
<p>One purposeful area of related debate in the contemporary moving image archive world circulates around the recent proliferation of high-definition  “video upgrades”  (4K, 60fps) achieved via deep learning methods and often applied to early silent film era footage, see e.g <a href="https://www.youtube.com/channel/UCD8J_xbbBuGobmw_N5ga3MA">Denis Shiryaev&rsquo;s youtube channel</a> and Simon (2020). Passions can run high when archivists rightly insist that these media entities are separate and derivative  “objects” : not archival acts of preservation or restoration, not grounded in meticulous curatorial insights and not dedicated to the indexical materiality of historical photo-chemical film prints. But there may be potential to realize a Stieglerian teachable moment in relation to these experiments that  “push”  the capacities of digital tools in order to change the experience and affect of watching historical  “cinema”  texts. Wide interest about these materials in online communities may indeed represent a re-enchantment of public imagination regarding early cinema, a literal sense of re-newed awareness and interest about history and moving images&ndash;and therefore an opportunity for archivists and scholars to direct attention toward an assortment of knowledges about early cinema (including the essential work to preserve and respect its history). In other words, the  “drive”  (compulsion?) within sectors of the AI industry to transform historical media artifacts into dramatically enhanced localized forms of  “attractions”  is itself an overdetermined project regarding desire for/within the historical imagination that is worthy of much further consideration. Without immediately casting aspersions on these considerable technological efforts, the experiential enthusiasm they inspire in audiences might be recognized as a digital framework to build upon. If the  “upgrade”  aesthetic dynamics border on a potential for mere spectacle of the hyper-real,<sup id="fnref:54"><a href="#fn:54" class="footnote-ref" role="doc-noteref">54</a></sup>  this potential may represent a techno-cultural pharmakon that is ripe to be more fully understood and historically grounded. How best to channel the many implicit investments in history of the visual arts embedded in these twin dynamics of digital production and reception, to engage these investments and further develop them in edified and conceptually insightful contexts? There is a compelling relationship to the MEP Early Cinema NEH grant, in that the great anticipation within the archival community to experience screenings of the newly restored 68mm films shot with the Biograph camera during the earliest years of cinema (restored at 4K and 8K at the BFI and the Eye Filmmuseum) offer a substantial counter-example to deep-learning  “video upgrade”  productions, and may set the table for dialectical Stieglerian discourse called for here<sup id="fnref:55"><a href="#fn:55" class="footnote-ref" role="doc-noteref">55</a></sup> .</p>
<h2 id="conclusion-emerging-contexts-for-mep">Conclusion: Emerging Contexts for MEP</h2>
<p>The time for this critical engagement has never been more necessary and opportune. The moving image archive world is keenly engaged in efforts to delineate and address the imperiled status of their collections <sup id="fnref:56"><a href="#fn:56" class="footnote-ref" role="doc-noteref">56</a></sup>. Access to archival content is becoming more foregrounded as a goal of preservation,<sup id="fnref:57"><a href="#fn:57" class="footnote-ref" role="doc-noteref">57</a></sup>  and as Giovanna Fossati points out in her standard-setting book From Grain to Pixel: The Archival Life of Film in Transition, DH is gaining momentum as scholars work to bridge the gap between digital methods, film and media studies, and media archives <sup id="fnref:58"><a href="#fn:58" class="footnote-ref" role="doc-noteref">58</a></sup>.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000524/resources/images/figure07.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000524/resources/images/figure07_huf241409f80b95b20a0c7787e63d161ed_2802772_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000524/resources/images/figure07_huf241409f80b95b20a0c7787e63d161ed_2802772_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000524/resources/images/figure07_huf241409f80b95b20a0c7787e63d161ed_2802772_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000524/resources/images/figure07_huf241409f80b95b20a0c7787e63d161ed_2802772_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/1/000524/resources/images/figure07_huf241409f80b95b20a0c7787e63d161ed_2802772_1800x0_resize_box_3.png 1800w,/dhqwords/vol/15/1/000524/resources/images/figure07.png 2400w" 
     class="portrait"
     ><figcaption>
        <p>MEP mind map: 2019 overview of the project
        </p>
    </figcaption>
</figure>
<p>The connections of MEP to existing trends in DH research are evident (see <a href="#figure07">Figure 7</a>). Formal stylometric analysis grounded in new capacities for annotation are emerging in several international centers of DH study <sup id="fnref:59"><a href="#fn:59" class="footnote-ref" role="doc-noteref">59</a></sup>, and are conversant with the fragmentation aesthetics of many artists working with large media corpora</p>
<p>We have already contributed to the conversation about artists engaging with archival content initiated by our colleagues at the Eye Filmmuseum: The Sensory Moving Image Archive (SEMIA). We anticipate and welcome opportunities to build bridges toward, for example, the <a href="https://www.jan.bot/livelog">WJAN BOT</a> at The Eye and also Brian Foo&rsquo;s  “Moving Images”  video project <sup id="fnref:60"><a href="#fn:60" class="footnote-ref" role="doc-noteref">60</a></sup>. At the same time we are inspired by and look forward to collaborating with more traditional filmmakers who work primarily with archival footage<sup id="fnref:61"><a href="#fn:61" class="footnote-ref" role="doc-noteref">61</a></sup> , and also more directly cinephilic endeavors such as scholar artists involved with the burgeoning Video Essay component of Film and Media Studies <sup id="fnref:62"><a href="#fn:62" class="footnote-ref" role="doc-noteref">62</a></sup>.</p>
<p>The analytic, annotation, and presentation tools engaged to work with audiovisual materials in DH are themselves becoming more collaborative. MEP is a member of the Video Annotation Interoperability (<a href="https://github.com/CLARIAH/video-annotation-interoperability">VAINT</a>) group that includes the makers of such tools as the CLARIAH Web Annotation tool, ELAN, Frametrail, and VIAN. While each of these tools was developed for a specific purpose, their core data all indexes back to time-based annotations. VAINT is developing a standard that will allow the tools to exchange data so scholars can, for example, directly export results from VIAN&rsquo;s color analysis tools into SAT and present them alongside textual commentary using SAT&rsquo;s integration with Scalar. Also currently under consideration by the group is a further integration with IIIF-AV, which would open up data exchange with the large set of IIIF-compliant presentation tools. This approach of common data exchange, rather than consolidating functions into a single tool, is a match for MEP&rsquo;s philosophy that tools addressing specific intellectual concerns can be put into conversation with one another to support synthetic, interdisciplinary scholarship.<sup id="fnref:63"><a href="#fn:63" class="footnote-ref" role="doc-noteref">63</a></sup></p>
<p>Prof. Williams and Dr. Bell have been invited to participate in a working group of the FIAF (International Federation of Film Archives) Cataloguing and Documentation Committee Task Force dedicated to Linked Open Data, in order to further investigate the sharing of FIAF Glossary of Filmographic Terms in an RDF structure. A related area of development is an endeavor to initiate a multi-lingual dictionary of film terms.<sup id="fnref:64"><a href="#fn:64" class="footnote-ref" role="doc-noteref">64</a></sup>  We have initiated work with the American Film Institute and the Women Film Pioneers Project to expand the recognition of women filmmakers in digital and online resources. Very recently MEP has been funded to work with the Distant Viewing team on developing a prototype digital resource at Dartmouth regarding the prestigious James Nachtwey collection of photographic journalism.</p>
<p>In continuing to realize a virtuous cycle of engagement with media art and history, we will work collaboratively and intentionally to be poised toward a spirit of critical inquiry that is engaged with innovative work at other academic and cultural institutions around the world. Our efforts to engage via DH more access to and scholarship about visual culture and moving image history presents innovative approaches to fundamental historiographic questions about media and history, and also address the danger of further losses to that history in both practical and theoretical terms. We intend the scholarship that we conduct and inspire to avoid a gloss of mere positivism in its pursuit of new research questions, and to invoke issues of the missing, the fragmentary, the occluded, the repressed, and the fugitive regarding this history.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Virilio, P.  <em>Open Sky</em> . Verso, New York (1997).&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Rosa, H. and Scheuerman, W. eds.  <em>High-Speed Society: Social Acceleration, Power, and Modernity</em> . Pennsylvania State University Press, University Park (2009).&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Rockhill, G.  “Temporal Economies and the Prison of the Present: From the Crisis of the Now to Liberation Time”    <em>Diacritics</em> , 47.1 (2019): 16-29.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Fitzpatrick, K.  <em>Generous Thinking: A Radical Approach to Saving the University</em> . Johns Hopkins University Press, Baltimore (2019).&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>At Dartmouth we were able to convene an extremely productive symposium in May 2013, which brought together representatives from multiple participating archives and institutions (<a href="http://mediaecology.dartmouth.edu/wp/news/page/2">http://mediaecology.dartmouth.edu/wp/news/page/2</a>). The symposium was successful in producing a series of agreements about the future of the project. One key outcome was a unanimous call to develop a metadata server and attendant middleware that could help to facilitate and maintain quality metadata produced in relation to archival elements. The symposium also generated significant interest outside of Dartmouth in addition to the invited members of the archival community. MEP has been significantly featured at numerous national and international conferences and symposia ever since. This includes multiple conferences of the Association of Moving Image Archivists, ADHO Digital Humanities, the Society for Cinema and Media Studies, The Orphans Film Symposium, The American Studies Association, EUScreen, the Expanded Semantic Web Conference, Open Repositories, and the International Association for Media and History.&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>These include a pilot in conjunction with the UCLA Film and Television Archive researching a ground-breaking public television program  “In the Life”  that assayed gay and lesbian experience in the U.S.; an inaugural international pilot, researching informational and documentary films produced at Films Division in India; and two pilots that have matured into advanced NEH-funded grant projects to be detailed later in this essay (Paper Print collection at The Library of Congress, and historical newsfilm materials across multiple archives).&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>Principally from The Neukom Institute for Computational Science, The Leslie Center for the Humanities, The Dean of the Faculty and Associate Deans, Office of Information, Technology, and Consulting, and The Dartmouth College Library. The Dean of the Faculty Award to Prof. Williams for Scholarly Innovation and Advancement in 2014 contributed a considerable stimulus for The Media Ecology Project.&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>Interest in the use of the ACTION Toolbox was especially marked at the Workshop on Computational Methods and Film Style in Potsdam in May, 2018.&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>The platform allows such features as access to and analysis of low-level frame-by-frame data, automated segmentation and clustering of this data, audio analysis for soundtracks, and other content analysis tools. The histogram extractor class can be used to analyze streams of images or video files. The histogram class steps through movie frames and extracts two kinds of histogram for each frame. The first is a histogram of the entire image. The second is a set of sixteen histograms, each describing a region of the image evenly arranged in a four-by-four non-overlapping grid. Histogram values describe distributions of color values in each region on the screen and can be used for analysis of both full-frame and 4-by-4 subframe grids in L<em>a</em>b* colorspace. The Optical Flow class generates analysis data of general motion on screen. Optical flow data is extracted using an implementation of the Lucas-Kanade algorithm, operating tracked features (corner detector) of monochrome image data, tracking salient points of image data across consecutive frames. Feature extractor classes allow access to audio frame-rate data including spectral and timbral features. In addition to developing the toolkit, we collected metadata for over 200 films from across the full history of cinema, focusing on a representative chronological selection of 22 films by Alfred Hitchcock in relation to works by other prominent directors across the cinematic aesthetic spectrum, from mainstream Classical Hollywood to Maya Deren, Jean-Luc Godard, Chantal Akerman, and David Lynch.&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>The SAT has been developed in response to feedback from the scholars and researchers participating in MEP pilot studies, especially regarding the generation of annotation metadata to describe media files. This feedback highlighted several shortcomings in existing time-based annotation toolsets, most notably a lack of interoperability and a set of divergent interfaces that can be difficult to learn or use collaboratively at scale. Development of a drop-in annotation tool like SAT that addresses these needs is a natural supplement to MEP&rsquo;s previous work promoting interconnected systems, and helps ensure that MEP&rsquo;s research, access, and collection development goals can be met.&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>The Annotation Server and jQuery Annotation Plugin were developed in separate but related workflows with the Virtual Environments and Multimodal Interactions (VEMI) Lab at the University of Maine. As part of the initial submission for review, VEMI released the test version of both server and client components of the SAT.&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p>The VEMI Lab at the University of Maine researches accessibility and adaptive technology with a focus on blind and low vision users. MEP and VEMI collaborated on the development of SAT. They are a primary partner in the second advanced NEH grant discussed below.&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13">
<p>Though the Annotation Server was originally intended to be a Hydra/Fedora (renamed Samvera) application, VEMI Lab developers found Hydra/Fedora to be difficult to install and maintain. We removed Hydra/Fedora from the SAT software stack and replaced it with a simple Ruby on Rails application. Statler implements the same W3C Web Annotation-compliant public interfaces that Hydra/Fedora would include, but greatly reduces the server overhead necessary to implement the system.&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14">
<p>Prof. Williams and a colleague at The Internet Archive produced manual time-based annotations that subdivided 20 of the 100 films into several sequences of short clips, about 10 seconds each. The Machine Vision Search team utilized Google image searches generated from manual search terms to partially train a neural network. The software leveraged image recognition algorithms to enable content-based search and metadata generation across the entire video collection. Once trained, the network was demonstrated to find additional short clips of the concepts and tag them in the larger collection.&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:15">
<p>It was important to develop data specific to the study of archival film, because the content of a typical film library is much different than the types of video generally used in the development of machine vision software. Most of the progress being made in applying this software is delegated to very contemporary, hi-definition video formats such as videos taken via new cell phones.&#160;<a href="#fnref:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:16">
<p>Williams, M.  “The Media Ecology Project: Library of Congress Paper Print Pilot”    <em>The Moving Image: The Journal of The Association of Moving Image Archivists</em> , 16.1 (Spring, 2016): 148-151.&#160;<a href="#fnref:16" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:17">
<p>Our pilot project was conducted with the participation of the renowned DOMITOR research society, and engaged Prof. Tami Williams (University of Wisconsin at Milwaukee) plus scholars who utilized the pilot study materials in their courses on silent cinema, including Prof. Frank Kessler (Utrecht University), Prof. Laura Horak (Carleton University), and Prof. Amy Lawrence (Dartmouth).&#160;<a href="#fnref:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:18">
<p>Streaming media is used both as a technical and legal solution. The Compendium will not need to host large amounts of streaming media or worry about intellectual property constraints since the media is already publicly available.&#160;<a href="#fnref:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:19">
<p>In December 2014, the W3C produced its first public working draft of a standardized data model for Open Annotations (OA). The model encapsulates text comments, tags, and external links as annotations that can be applied to a variety of assets embedded in web pages. Critically, it also provides for annotations that apply to  “fragments”  of assets, such as a geometric selection area within a video frame or a timecode-delimited subclip of a media file. Using a standard like W3C WA makes new applications for time-based annotations more feasible and available to a wider variety of scholars. Linked data concepts, which permit new types of structured data and incomplete coverage of a collection, will allow scholars to easily create new data facets that are specific enough to support advanced analysis in the Compendium. While that functionality had previously been available in proprietary or vendor-defined formats, OA is the first time a standards body with the weight of W3C endorsed an annotation data model that developers can depend upon for stability and interoperability. The OA model has now become part of the broader W3C Web Annotation (W3C WA) spec.&#160;<a href="#fnref:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:20">
<p>Initial attempts to apply optical flow to Paper Print copies of early films have been uneven at best, due to the poor resolution and almost ubiquitous appearance of analog  “noise”  in the images. We have not yet made tests on the early 68mm materials.&#160;<a href="#fnref:20" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:21">
<p>Stiegler, B.  <em>The Re-Enchantment of the World: The Value of Spirit Against Industrial Populism</em> . Bloomsbury, London (2014).&#160;<a href="#fnref:21" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:21" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:21" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:22">
<p>For example, research in convolutional neural networks theory has suggested research questions informed by what is called the two-streams hypothesis: the difference that exists in the human visual cortex between two pathways of understanding, between the ventral stream tied to object recognition and the dorsal stream tied to the recognition of motion <sup id="fnref:65"><a href="#fn:65" class="footnote-ref" role="doc-noteref">65</a></sup>.&#160;<a href="#fnref:22" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:23">
<p>Prof. Williams&rsquo; introduction to the genre of  “newsfilm”  was courtesy of his participation for many years in The Association of Moving Image Archivists and especially a program about UCLA archive&rsquo;s 1970s newsfilm from Los Angeles television station KTLA at the Orphans West Coast symposium in 2011. An especially poignant example depicts a 1979 public protest by dozens of concerned African-American women at Parker Center in Los Angeles after the shooting death by police of Eula Love, a recently widowed 39-year-old African-American mother. The killing of Eula Love resulted from a dispute over an unpaid gas bill, a tragic landmark in the notorious racialized encounters by the LAPD and citizens of color. The protest event captured by the KTLA newsfilm footage provides an indelible memorialization of that tragedy. It is not known if any of the footage ever aired on television, but the power and salience of the imagery is deeply instructive today regarding the value of historical newsfilm. The women collectively and elegantly performed a public demonstration of the question  “Can we speak back to power?”  For additional analysis of this footage see <sup id="fnref:66"><a href="#fn:66" class="footnote-ref" role="doc-noteref">66</a></sup>. For an inter-medial relationship of this event to the coterminous rise of New Black Cinema in Los Angeles, see <sup id="fnref:67"><a href="#fn:67" class="footnote-ref" role="doc-noteref">67</a></sup>.&#160;<a href="#fnref:23" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:24">
<p>Williams, R.  “Structures of Feeling”  in  <em>Marxism and Literature</em> . Oxford, Oxford University Press, 1977: pp. 128-135.&#160;<a href="#fnref:24" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:25">
<p>Newsfilm content will be selected from archives across the United States, including The University of Georgia (local television newsfilm plus The Peabody Archives), The Mississippi Department of Archives and History, The University of Arkansas Pryor Center, The Wolfson Archive at Miami Dade College, The Bay Area Television Archive, Media Burn Archive (Chicago), Washington University Archive (St. Louis), The National Museum of African American History and Culture (Smithsonian), The MIRC Collection at The University of South Carolina, The Minnesota Historical Society, Southern Methodist University, The American Archive of Public Broadcasting, The UCLA Film and Television Archive, WGBH, The Boston TV News Digital Library, The National Archives, and The Library of Congress. Note that archival footage used in the BVI corpus will be drawn from publicly available collections.&#160;<a href="#fnref:25" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:26">
<p>The team of consultants enlisted for this project are renowned experts in media studies and issues of racial and ethnic representation. Their varied personal and professional backgrounds will help create annotations highlighting the significance of the selected corpus of newsfilm and evaluate the value of those annotations to scholarly study. Participating scholars include Jacqueline Stewart (U Chicago) and Desirée Garcia (Dartmouth). Research into specific adaptive strategies will be led by Nicholas A. Giudice and Richard R. Corey of the Virtual Environments and Multimodal Interaction Lab (VEMI Lab) at the University of Maine. Technologies used include MEP&rsquo;s tools SAT and Onomy, ANVC&rsquo;s Scalar, and the Distant Viewing Lab&rsquo;s Distant Viewing Toolkit (DVT). Creating a pathway for DVT&rsquo;s machine-generated annotations to support and inform human-generated annotations written in SAT is an important step that extends the merged distant-and close-reading methods developed for the machine vision search project into more nuanced forms of scholarly commentary and analysis.&#160;<a href="#fnref:26" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:27">
&#160;<a href="#fnref:27" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></li>
<li id="fn:28">
<p>Clossen, A. and Proces, P.  “Rating the Accessibility of Library Tutorials from Leading Research Universities.”    <em>portal: Libraries and the Academy</em> , 17.4 (2017): 803-825.&#160;<a href="#fnref:28" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:29">
<p><a href="https://www.justice.gov/usao-ma/pr/united-states-reaches-settlement-provider-massive-open-online-coursesmake-its-content">https://www.justice.gov/usao-ma/pr/united-states-reaches-settlement-provider-massive-open-online-coursesmake-its-content</a>&#160;<a href="#fnref:29" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:30">
<p><a href="https://www.insidehighered.com/news/2017/03/06/u-california-berkeley-delete-publicly-available-educationalcontent">https://www.insidehighered.com/news/2017/03/06/u-california-berkeley-delete-publicly-available-educationalcontent</a>&#160;<a href="#fnref:30" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:31">
<p><a href="https://ucbdisabilityrights.org/2016/09/22/faculty-response-to-koshland/">https://ucbdisabilityrights.org/2016/09/22/faculty-response-to-koshland/</a>&#160;<a href="#fnref:31" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:32">
<p><a href="https://marketbrief.edweek.org/marketplace-k-12/edtech">https://marketbrief.edweek.org/marketplace-k-12/edtech</a>&#160;<a href="#fnref:32" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:33">
<p>For the purposes of ACRH, an annotation consists of a reference to a specific time and geometric region of a video, a textual body describing the content in that region, a set of tags associated with the textual body, and additional provenance metadata as needed to attribute an annotation to an author.&#160;<a href="#fnref:33" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:34">
<p>Increasing accessibility also improves the experience for other users <sup id="fnref:68"><a href="#fn:68" class="footnote-ref" role="doc-noteref">68</a></sup>.&#160;<a href="#fnref:34" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:35">
<p><a href="https://w3c.github.io/webvtt/">WebVTT</a> is also worth mentioning in this context, but it has limitations similar to A11y&rsquo;s Ogg recommendation except the data is not stored within the Ogg container file.&#160;<a href="#fnref:35" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:36">
<p>In light of the present pandemic conditions, the symposium is likely to be virtual and online.&#160;<a href="#fnref:36" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:37">
<p>Arnold, T. and Tilton, L.  “Distant Viewing: Analyzing Large Visual Corpora”    <em>Digital Scholarship in the Humanities</em>  (2019).&#160;<a href="#fnref:37" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:38">
<p>Vertov&rsquo;s films have been a point of primary focus and inspiration for landmark books about digital humanities methods such as <sup id="fnref:69"><a href="#fn:69" class="footnote-ref" role="doc-noteref">69</a></sup>  <sup id="fnref1:43"><a href="#fn:43" class="footnote-ref" role="doc-noteref">43</a></sup>. The emphasis here is on the historiographic significance of Vertov&rsquo;s theory, especially as it may be applied to contemporary and emergent issues regarding computer vision and machine learning (AI).&#160;<a href="#fnref:38" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:39">
<p>Nakamura, L.  <em>Digitizing Race</em> . University of Minnesota Press, Minneapolis (2008).&#160;<a href="#fnref:39" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:40">
<p>Noble, S. U.  <em>Algorithms of Oppression: How Search Engines Reinforce Racism</em> . New York University Press, New York (2018).&#160;<a href="#fnref:40" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:41">
<p>Benjamin, R.  <em>Race After Technology: Abolitionist Tools for the New Jim Code</em> . Polity Press, Medford, MA (2019).&#160;<a href="#fnref:41" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:42">
<p>Ng, E., White, K. and Saha, A.  “#CommunicationSoWhite: Race and Power in the Academy and Beyond”    <em>Special issue of Communication, Culture &amp; Critique</em> , 13.2 (2020).&#160;<a href="#fnref:42" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:43">
<p>Heftberger, A.  <em>Digital Humanities and Film Studies: Visualizing Dziga Vertov&rsquo;s Work</em> . Springer Nature, Cham (2018).&#160;<a href="#fnref:43" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:43" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:44">
<p>Zamora, M.  “Reading as a Social Act”    <em>Connected Learning Alliance</em>  (March 24, 2016) <a href="https://clalliance.org/blog/reading-social-act/">https://clalliance.org/blog/reading-social-act/</a>.&#160;<a href="#fnref:44" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:45">
<p>Rheingold, H.  “Annotation, Rap Genius, and Education: Howard Rheingold Interviews Jeremy Dean”    <em>Connected Learning Alliance</em>  (Feb 8, 2016) <a href="https://clalliance.org/blog/annotation-rap-genius-and-education/">https://clalliance.org/blog/annotation-rap-genius-and-education/</a>.&#160;<a href="#fnref:45" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:46">
<p>Bali, M.  “What I Like About Hypothes.is”    <em>Chronicle of Higher Education ProfHacker</em> , Jan 13 (2016).&#160;<a href="#fnref:46" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:47">
<p>Guo, J.  “I have found a new way to watch TV, and it changes everything”    <em>The Washington Post</em>  (June 22, 2016), <a href="https://www.washingtonpost.com/news/wonk/wp/2016/06/22/i-havefound-a-new-way-to-watch-tv-and-it-changes-everything/">https://www.washingtonpost.com/news/wonk/wp/2016/06/22/i-havefound-a-new-way-to-watch-tv-and-it-changes-everything/</a>.&#160;<a href="#fnref:47" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:48">
<p>Robert Samuels articulates a strategic refusal to denounce generational differences via the construction of a hyper-binary about media consumption <sup id="fnref:70"><a href="#fn:70" class="footnote-ref" role="doc-noteref">70</a></sup>.&#160;<a href="#fnref:48" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:49">
<p>Anecdotally, students participating in the creation of ACRH annotations for use in BVI research report that this work has clearly enhanced their appreciation of and capacity to read for visual culture aesthetics.&#160;<a href="#fnref:49" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:50">
<p>de Luca, T. and Jorge, N. B. eds.  <em>Slow Cinema (Traditions in World Cinema)</em> . Edinburgh University Press, Edinburgh (2016).&#160;<a href="#fnref:50" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:51">
<p>Barker, S.  “Enchantment, Disenchantment, Re-Enchantment: Toward a Critical Politics of Re-Individuation”    <em>New Formations</em>  77: Bernard Stiegler: Technics, Politics, Individuation (Autumn 2012): 21-43.&#160;<a href="#fnref:51" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:52">
<p>Abbinnett, R.  <em>The Thought of Bernard Stiegler: Capitalism, Technology, and the Politics of Spirit</em> . Routledge, New York (2018).&#160;<a href="#fnref:52" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:53">
<p>Stiegler passed away suddenly on August 5, 2020. See <a href="https://www.theguardian.com/world/2020/aug/18/bernard-stiegler-obituary">https://www.theguardian.com/world/2020/aug/18/bernard-stiegler-obituary</a>.&#160;<a href="#fnref:53" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:54">
<p>See <sup id="fnref:71"><a href="#fn:71" class="footnote-ref" role="doc-noteref">71</a></sup> for background about the inter-medial history of electronic culture, and historiographic details about uneven development toward capacities (and demand) for a digitally mediated subjunctive  “now”  in practices of representation.&#160;<a href="#fnref:54" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:55">
<p>In August, 2020, the online  “release”  of a significant 68mm film restored by The Museum of Modern Art led to viral social media enthusiasm among the archival and film fan communities: <a href="https://m.youtube.com/watch?v=2Ud1aZFE0fU"> “The Flying Train”  (1902)</a>&#160;<a href="#fnref:55" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:56">
<p>Casey, M.  “Why Media Preservation Can&rsquo;t Wait: The Gathering Storm.”    <em>International Association of Sound &amp; Audiovisual Archives Journal</em> , 44 (January 2015): 14–22.&#160;<a href="#fnref:56" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:57">
<p>The Eye Filmmuseum hosts an online channel devoted to their restoration efforts: Restoration at Eye.&#160;<a href="#fnref:57" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:58">
<p>Fossati, G.  <em>From Grain to Pixel: The Archival Life of Film in Transition, Third Revised Edition</em> . Amsterdam University Press, Amsterdam (2018).&#160;<a href="#fnref:58" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:59">
<p>Junior Research Group, Audio-Visual Rhetorics of Affect, Freie University of Berlin  “Videoannotation — Relating Precisely to Audio-Visual Images”  (2018), <a href="http://www.ada.cinepoetics.fu-berlin.de/en/Methoden/Videoannotation/index.html">http://www.ada.cinepoetics.fu-berlin.de/en/Methoden/Videoannotation/index.html</a>.&#160;<a href="#fnref:59" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:60">
<p>Foo, B.  “Moving Images”  video project (2019) <a href="https://movingarchives.brianfoo.com/?fbclid=IwAR0S8mbqdyIFHOIsM3ezRHaUvhdEr_Sl5WmF74W53_X08UhkpfJl2k8p5pQ">https://movingarchives.brianfoo.com/?fbclid=IwAR0S8mbqdyIFHOIsM3ezRHaUvhdEr_Sl5WmF74W53_X08UhkpfJl2k8p5pQ</a>.&#160;<a href="#fnref:60" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:61">
<p>Recent films include Dawson City: Frozen Time (Morrison, 2016), Apollo 11 (Miller, 2019) and Recorder: The Marion Stokes Project (Wolf, 2019). For more on this tradition see <sup id="fnref:72"><a href="#fn:72" class="footnote-ref" role="doc-noteref">72</a></sup>.&#160;<a href="#fnref:61" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:62">
<p>Keathley, C., Mittell, J. and Grant, C.  <em>The Videographic Essay: Practice and Pedagogy</em> , 2019, <a href="http://videographicessay.org">http://videographicessay.org</a>.&#160;<a href="#fnref:62" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:63">
<p>A related new pedagogical project utilizing SAT has been initiated at Dartmouth: enhancing the use value of archival motion pictures by utilizing them in basic language instruction. This project was inspired by the innovative teaching methods of Prof. Hua-Yuan Mowry at Dartmouth, who uses graphics and motion pictures in her classes to teach Mandarin. The pilot for this project uses SAT to illustrate written language as it is spoken in a famous Chinese animated short, Three Monks by Jingd Xu (1980). The SAT interface  “plays”  in real time several sequential annotations of transcribed Mandarin characters at the same time that the associated Mandarin words are spoken in the film. This basic SAT schema could enhance instruction across many languages and potentially even dynamically toggle or alternate subtitle languages as students watch foreign language films.&#160;<a href="#fnref:63" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:64">
<p>In response to a generous bequest by the Taiwan Film Institute Archive, who gifted to Prof. Williams their unique and considerable bi-lingual dictionary of film terms (English and Mandarin), the Dartmouth Library worked with MEP to OCR and transcribe this resource in order to make it available as an Onomy spreadsheet. The work to achieve this complex final spreadsheet (that features three varieties of Mandarin script) was completed by gifted Dartmouth student Janine Sun. This has inspired the internal funding of several more undergraduates at Dartmouth to help develop additional bilingual Onomy spreadsheets that will contribute to a larger multi-lingual dictionary goal.&#160;<a href="#fnref:64" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:65">
<p>Simonyan, K. and Zisserman, A.  “Two-Stream Convolutional Networks for Action Recognition in Videos”  (2014) <a href="https://arxiv.org/abs/1406.2199">https://arxiv.org/abs/1406.2199</a>.&#160;<a href="#fnref:65" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:66">
<p>Williams, M.  “Archives of Liveness: Television Newsfilm Reconsidered”  in M. G. Cooper, S. B. Levavy, R. Melnick, and M. Williams (eds.)  <em>Rediscovering U.S. Newsfilm: Cinema, Television, and the Archive Routledge AFI Film Reader series</em> , New York (2018b), pp. 288-309.&#160;<a href="#fnref:66" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:67">
<p>Field, A., Horak, J. and Stewart, J. N. (eds.)  <em>L.A. Rebellion: Creating a New Black Cinema</em> . University of California Press, Berkeley (2015).&#160;<a href="#fnref:67" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:68">
<p>Schmutz, S., Sonderegger, A., and Sauer, J.  “Implementing Recommendations From Web Accessibility Guidelines: A Comparative Study of Nondisabled Users and Users With Visual Impairments,”    <em>Human Factors</em> , 59.6 (2017): 956 – 972.&#160;<a href="#fnref:68" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:69">
<p>Manovich, L.  <em>The Language of New Media</em> . MIT Press, Cambridge, Mass (2001).&#160;<a href="#fnref:69" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:70">
<p>Samuels, R.  “Auto-Modernity after Postmodernism: Autonomy and Automation in Culture, Technology, and Education”  in T. McPherson (ed.)  <em>Digital Youth, Innovation, and the Unexpected</em> . The MIT Press, Cambridge (2008),: 219-240.&#160;<a href="#fnref:70" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:71">
<p>Williams, M.  “From  Live  to Real Time: On Future Television Studies”  in J. Sayers (ed.)  <em>The Routledge Companion to Media Studies and the Digital Humanities</em> . Routledge, New York (2018a), pp. 283-291.&#160;<a href="#fnref:71" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:72">
<p>Northrup, A.  “Honoring the Art of the Archival Film”  Hyperallergic website (2019) <a href="https://hyperallergic.com/532479/idfa-2019-re-releasinghistory/?fbclid=IwAR1h1IATzR_w6485jJt9hj3oOu9WsBjE19foUnhkcUUuzEJIhFdlH_C7oOk">https://hyperallergic.com/532479/idfa-2019-re-releasinghistory/?fbclid=IwAR1h1IATzR_w6485jJt9hj3oOu9WsBjE19foUnhkcUUuzEJIhFdlH_C7oOk</a>.&#160;<a href="#fnref:72" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry><entry><title type="html">Topological properties of music collaboration networks: The case of Jazz and Hip Hop</title><link href="https://rlskoeser.github.io/dhqwords/vol/15/1/000504/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://rlskoeser.github.io/dhqwords/vol/15/1/000504/</id><author><name>Lukas Gienapp</name></author><author><name>Clara Kruckenberg</name></author><author><name>Manuel Burghardt</name></author><published>2021-03-05T00:00:00+00:00</published><updated>2021-03-05T00:00:00+00:00</updated><content type="html"><![CDATA[<h2 id="1-introduction-collaboration-in-the-music-scene">1. Introduction: Collaboration in the Music Scene</h2>
<p>The enormous increase in machine-readable data has been one of the main drivers of the digital humanities in recent years. This new data enables new, computer-aided approaches <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>, both from an exploratory and an empirical perspective. So far, the focus of digitization has been mainly on textual sources. Still, as more and more musical data is becoming available in digital form, we are witnessing an increase in music information retrieval applications <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> within the digital humanities. Work in this area is rather heterogeneous, but may be divided into two main branches: (1) &lsquo;sound and audio studies&rsquo;, where music is processed and analyzed as an actual audio signal, and (2) &lsquo;symbolic music studies&rsquo;, where music is analyzed from a more formalized perspective, as it is written down in some kind of notation system. Typically, the latter branch is dedicated to questions such as digitization of analog music scores (optical music recognition), music representation formats (MusicXML, MEI, etc.), and the quantitative analysis of musical features (e.g., melodic similarity). As symbolic music collections are widely available, this branch of music studies seems to be particularly well suited for quantitative analyses. One specific subcategory of symbolic music is music metadata, which comes in many shapes and sizes, for instance, in the form of audio features (danceability, energy, etc.) via the Spotify API<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>  or as metadata about artists, genres, etc. (<a href="https://www.last.fm/">Last.fm</a>).</p>
<p>In this work, we propose the use of the Discogs database to study patterns and strategies of music collaboration. The larger theme that motivates this research is the importance of collaborative contexts for problem-solving, creativity, and innovation in general <sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>, the latter two of which are particularly important in the field of music production. Teitelbaum et al. <sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup> note that  “music is one of the richest sources of interaction between individuals” , i.e., music collaboration between artists is a common social phenomenon. However, the concrete strategies of collaboration, such as the role of particular artists as hubs and the formation of communities, may vary widely between genres. This is illustrated by a large body of existing studies on music collaboration across different genres, including  <em>Classical Music</em>   <sup id="fnref1:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>  <sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>   <em>Pop Music</em>   <sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>  <sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>   <em>Heavy Metal</em>   <sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup>,  <em>Rap</em> / <em>Hip Hop</em>   <sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup>  <sup id="fnref1:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>  <sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup> and, above all,  <em>Jazz</em>   <sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>  <sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup>  <sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup>  <sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup>  <sup id="fnref:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup>  <sup id="fnref:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup>  <sup id="fnref:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup>.</p>
<p>Typical research questions that are investigated in the above studies are, for instance, the analysis of structural properties of collaboration networks (e.g., small worlds) <sup id="fnref1:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>  <sup id="fnref1:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup> or the study of local scenes / regional communities in the respective genres <sup id="fnref:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup>  <sup id="fnref1:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup>. The methods used in these studies are very diverse, as they include qualitative approaches (i.e., interviews) as well as quantitative approaches. When it comes to quantitative approaches, the predominant method is network analysis, which is well known in many related fields of application. Examples include analyzing collaboration between Hollywood actors <sup id="fnref:20"><a href="#fn:20" class="footnote-ref" role="doc-noteref">20</a></sup>  <sup id="fnref:21"><a href="#fn:21" class="footnote-ref" role="doc-noteref">21</a></sup>, scientists <sup id="fnref:22"><a href="#fn:22" class="footnote-ref" role="doc-noteref">22</a></sup>, or business alliances <sup id="fnref:23"><a href="#fn:23" class="footnote-ref" role="doc-noteref">23</a></sup>. Within the sub-genre of network analyses of music collaboration, we also find a multitude of different datasets used in existing studies. However, most of them only collect small amounts of data, are limited to a single timeframe, genre, or geographic area, and work with diverse understandings of the concept of &ldquo;collaboration&rdquo;. These differences in methods and data make it very difficult to investigate collaboration strategies between different genres of music on a wider range. With a few exceptions <sup id="fnref1:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup>, there are hardly any studies that compare different genres in terms of their cooperation strategies and networks. Yet comparative studies are particularly exciting, as they allow scholars to work out specific topological patterns of different genres and discuss them in relation to artistic characteristics, especially with regard to creativity and innovation.</p>
<p>For this reason, we propose a reusable digital humanities approach for studying topological properties of music collaboration networks within and between genres that may pave the way for further, more comprehensive comparative studies in the field of music cooperation. Since we use metadata from the Discogs database, we have to limit our data set to collaborations between artists that are explicitly documented by a joint release. In this work, we also perform an actual comparison of two genres to illustrate our proposed method. We picked Jazz and Hip Hop for two reasons. The first is the existing scholarship about the two genres, which provides important context for interpreting the results of network analysis. The second is that the two genres are known for sharing a similar approach to collaboration, for single artists are often collaborating in varying constellations. Yet, their collaboration networks are rarely compared. Not only is network analysis a particularly amenable method to studying the style of collaboration that are prominent in Jazz and Hip Hop communities, but the method also provides a way to compare the two genres in new ways.</p>
<h2 id="2-collaboration-in-jazz-and-hip-hop">2. Collaboration in Jazz and Hip Hop</h2>
<p>To back up our comparison on collaboration in Jazz and Hip Hop, this section introduces some basic characteristics of both genres and also provides an overview of some of the most prominent related work in this area.</p>
<h2 id="21-jazz">2.1 Jazz</h2>
<p>Studying collaborations in the genre of Jazz seems to be the most prominent stretch of research in existing work on music collaboration. This can mainly be attributed to the extensive amount of collaboration within this genre. Unlike in other genres, such as Rock, where fixed groups are the most common form of collaboration, Jazz musicians usually play and record in constellations. Real-time Jazz performances themselves are highly interactive and collaborative, as musicians typically react to each other and improvise while sticking to a common framework with regard to tempo and harmonics <sup id="fnref1:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup>. This fluid nature makes Jazz a prime research interest for social network analysis in the music domain, as the resulting networks tend to be rather complex. In the existing related work on Jazz collaborations, researchers use very different types of data, ranging from archival and biographical sources to existing collections of metadata on the genre.</p>
<p>One notable dataset on Jazz collaborations is called <a href="https://www.linkedjazz.org"> <em>Linked Jazz</em> </a>. In an attempt to create an in-depth network dataset, Pattuelli et al. <sup id="fnref1:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup> construct an ontology from digital archive material on Jazz history, more specifically transcripts of interviews with 12 Jazz musicians. Their resulting network includes 952 connections between those interviewees and 529 other Jazz artists. In more recent versions of their dataset, over 2,000 Jazz musicians with over 3,600 connections are included. While personal recollections of Jazz musicians are probably the most accurate source of data when it comes to making connections that actually represent the real world, this approach has two major disadvantages. First, it is difficult to make generalizable statements, since only a very limited number of musicians are considered. Although such data sets may be interesting from a historical perspective, they are not suitable as a basis for empirical statements on the structure of collaboration in Jazz. Second, the actual data acquisition entails a lot of work, as the qualitative data source is not readily available in a structured format.</p>
<p>The <a href="http://redhotjazz.com"> <em>Red Hot Jazz Archive</em> </a>, as used by Gleiser and Danon <sup id="fnref1:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup>, is another example of existing resources on collaborations in Jazz. It contains information about Jazz bands between 1895 to 1929, listing the members and discographies for 1,099 bands. It also provides more detailed biographical information for 192 musicians. The data was compiled manually from a variety of sources, such as oral histories, biographies, and several historical books on the Jazz culture of that era. The  <em>Red Hot Jazz Archive</em>  was created in a similar way to the  <em>Linked Jaz</em> z database, which means it also shares some of the previously described problems, namely the lack of generalizability and the time-consuming preparation of the dataset. The authors use a two-stage process to examine Jazz collaborations based on this archive. First, they infer a network of individual artists, which are connected if they played in the same band. From there on, they create a network of bands, with two bands being connected if they share at least one member. Studying both networks topologically with respect to degree distributions, clustering, and average nearest neighbor degree, they note two major limitations of their network model: artists may be credited with different spellings or under pseudonyms, and relations between artists are not time-stamped, which makes the analysis of the evolution of networks over time impossible.</p>
<p>In addition to the use of historical documents, another popular approach for the generation of collaboration networks is to use metadata collections of music releases. Hannibal <sup id="fnref1:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup> provides an extensive overview of the evolution of Jazz as a genre and the collaboration patterns associated with it based on a network obtained from <a href="https://www.lordisco.com"> <em>Tom Lord&rsquo;s The Jazz Discography</em> </a>. While this dataset is deemed comprehensive (Phillips and Kim, 2009), as it contains data from 410,000 Jazz releases, it is not license-free and only available at a high price. In the corresponding network, musicians who recorded an album together are connected via an undirected edge. Each edge is tagged with the year of recording. This method of network creation requires very little manual work and yet results in comprehensive and metadata-rich networks. Hannibal <sup id="fnref2:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup> uses the network to explore career success and historical significance of musicians in relation to their position in the network by using different metrics such as centrality, brokerage, and closure, which he relates to measures of career success (e.g., awards and sales). He presents empirical evidence that Jazz musicians who perform in closed groups are less likely to have a successful career than those who maintain a more open structure of collaborative connections.</p>
<p>In another related research paper, Giaquinto et al. <sup id="fnref1:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup> use the <a href="https://www.allmusic.com"> <em>AllMusic</em> </a> database to infer a network that connects Jazz musicians according to their similarity as determined by the database, to study which artists were most influential. However, their network size is far from comprehensive, including only 418 artists at most. Finally, Filippova et al. <sup id="fnref1:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup> provide a web service called  “Map of Jazz” , which can be used for an explorative search for Jazz musicians based on collaboration network data.<sup id="fnref:24"><a href="#fn:24" class="footnote-ref" role="doc-noteref">24</a></sup>  They aim to provide a tool to make discographies more accessible for researchers, as large-scale discography data can be difficult to analyze in the classic textual form. Their network is based on discographical data that was manually compiled from a multitude of sources, ranging from musical records and magazines to biographical data and monographs. Overall, 11,824 musicians are included. A major difference to other works is that their network is dynamic, which includes metadata about the time and place of connections. However, their work is mostly concerned with tool development and provides little descriptive insight into the network&rsquo;s properties.</p>
<h2 id="22-hiphop">2.2 HipHop</h2>
<p>Hip Hop is another promising genre when it comes to collaborations. In her book on Hip Hop culture and the role of collaboration, Smith <sup id="fnref:25"><a href="#fn:25" class="footnote-ref" role="doc-noteref">25</a></sup> notes that  “[f]rom it&rsquo;s outset in the 1970s, hip-hop was not solely a musical genre, but encompassed dance (b-boying/b-girling) and visual art (graffiti)” . In other words, Hip Hop was and is a cultural phenomenon with many facets, of which music is only one aspect. According to Smith <sup id="fnref1:25"><a href="#fn:25" class="footnote-ref" role="doc-noteref">25</a></sup>, collaboration in Hip Hop is an essential mechanism for interaction and communication of shared ideologies, attitudes, and behaviors within the scene. Compared to Jazz, there is substantially less work that deals with music collaborations in Hip Hop.</p>
<p>The most comprehensive study of collaboration in Hip Hop is provided by Smith <sup id="fnref1:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>, who presents an interesting approach by examining the metadata of over 30,000 songs extracted from lyrics databases. Since all artists who have had a vocal part in a song are listed in the metadata of the songs, a connection can be established between any artists that appear together on a song. Similar to Gleiser and Danon <sup id="fnref2:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup>, name standardization is also a problem in this study. Smith uses a fuzzy search algorithm to partly match different spellings of names. However, if the same artist is credited under a different pseudonym altogether, the standardization fails. Another related work examines the collaboration network of a single artist <sup id="fnref1:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup>. More concretely, the authors try to model the evolution of  <em>DJ Khaled</em>  with respect to collaborations by building an incremental network for each of his albums. This network is then used to identify communities as well as influential artists who affected the music of DJ Khaled the most.</p>
<h2 id="23-interim-conclusion">2.3 Interim conclusion</h2>
<p>The related work presented in this section illustrates the multitude of different data sources that are used for studying music collaboration. These are typically accompanied by a broad range of methods for inferring network structure from the data. Several limitations arise as a result of this: (1) most datasets are only available via inference from noisy data, with associated problems in data cleaning; (2) while comprehensive network and collaboration datasets exist for Jazz, sources for other genres, for instance, Hip Hop, are rather scarce; (3) there is no common method for establishing links between musicians that is applicable to all genres and analytical settings.</p>
<p>While datasets used to study music collaboration are very diverse, we found only little variation when it comes to network metrics in the related work studies: the most prominent metrics are degree distribution, degree correlations, average nearest neighbor degree, betweenness centrality, clustering coefficient, and transitivity score. The Girvan-Newman partitioning algorithm seems to be a popular choice to identify (sub-)communities in the networks. As all of the above-mentioned papers provide different results for these metrics, the results cannot be directly compared with each other in order to gain insight about structural differences in collaboration between genres, since the underlying networks are not derived in the same way. Therefore, we will now propose a method for how to address these challenges and hope they open up a path for cross-genre network analysis.</p>
<h2 id="3-methodology">3. Methodology</h2>
<p>In this section, we present our approach for the generic study of music collaborations across different genres. First, we explain our considerations when modeling the network graph. Next, we give an overview of the network metrics used and a brief explanation of how they can be interpreted for the analysis of music collaborations. Finally, we introduce the underlying Discogs dataset.</p>
<h2 id="31-network-modeling">3.1 Network Modeling</h2>
<p>When building a network, it is essential to define what constitutes an entity (node) and what constitutes a connection (edge) between these nodes. The challenge is to find a definition that, on the one hand, is applicable to a variety of research questions and, on the other hand, correctly reflects the underlying connections in the real world. In order to establish a common methodological approach to network structure, the dataset created as part of this work is concerned with connections between individual musicians. This is common in the field <sup id="fnref3:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup>  <sup id="fnref2:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>  <sup id="fnref2:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>, as the individual artist is the &ldquo;least common denominator&rdquo; across genres in terms of organizational structure. While Big Band Jazz can have group sizes of up to 170 musicians (e.g.,  <em>Paul Whitman and his Orchestra</em> ), Hip Hop, for instance, is much more centered around the individual artist (e.g.,  <em>Snoop Dogg</em> ), and larger groupings of artists are the exception (e.g.,  <em>The Wu-Tang Clan</em> <sup id="fnref:26"><a href="#fn:26" class="footnote-ref" role="doc-noteref">26</a></sup> ), not the norm.</p>
<p>There are several options available to establish connections between nodes, all different with respect to their weighting, direction, and sparseness. When studying collaboration, the edges are supposed to represent a collaborative process shared between the two connected nodes. With music records being the main available data source to infer these connections, one choice is to establish edges between all artists that appear together on a record. The main advantage of a release-centered approach over alternative methods – such as the definition of an edge between two artists being if they are part of the same band or collective <sup id="fnref3:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup> – is the possibility to assign different weights to the connections. We may well assume that artists who recorded more tracks together have a strong creative influence on each other. Therefore, the network edges are weighted by the number of collaborations.</p>
<p>However, an apparent problem here is that a music release is usually a collection of different tracks, and these tracks are not always recorded with the same set of musicians. Therefore, if only album-wide credits are available, not all of the listed musicians can automatically be assumed to have shared a collaborative process, violating the hypothesized definition of edges. The only person that is connected to all other credited artists with a high probability is the main artists of the release. This problem can be tackled in two different ways: (1) create connections between all credited artists, on at the cost of possibly introducing unwarranted ones; or (2) only create connections between credited artists and the main artist of the album, thus only creating warranted connections at the cost of missing some. Several factors should be considered in this decision: first, it can be theorized that the amount of missed connections in the latter case is higher than the number of unwarranted ones in the former since, for a high proportion of releases, a big subset of credited artists appeared on all tracks of an album. Accordingly, the data/noise trade-off seems favorable when adding all possible connections to the network.</p>
<p>Another relevant factor is the metrics that will be used to derive conclusions from the network data, especially metrics measuring network centrality. The first option would place higher network importance on primary artists, as they would receive a disproportionally high amount of connections. Also, this approach invalidates metrics that operate on notions of transitivity, such as the clustering coefficient, since the transitive edges are explicitly excluded from the dataset if the credited artists are not interconnected. Therefore, we opt to include all possible connections between the artists associated with a release. This is also the most common approach <sup id="fnref4:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup>  <sup id="fnref4:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup>  <sup id="fnref3:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>. Since no information can be derived about the direction of collaborative influence from simple co-appearance on tracks, the network can be assumed to be undirected. However, to incorporate relevant information about the concrete role each musician has played on a record (i.e., instruments, vocals, producing roles, etc.), we relied on two parallel directed edges (see <a href="#figure01">Figure 1</a>). These connections can be interpreted as  “ <em>x</em>  played with  <em>y</em>  in role  <em>a</em> ”  and as a parallel edge  “ <em>y</em>  played with  <em>x</em>  in role  <em>b</em> ” , where  <em>x</em>  and  <em>y</em>  are two connected musicians, and  <em>a</em>  and  <em>b</em>  are instruments or other musical roles. This information about the specific roles of musicians on a record can later be used to filter the data and create tailored sub-networks that – for instance – might only include edges representing piano players or producers. Besides the option to select specific nodes in the data set via their role annotations, we treat these directed parallel edges as a single undirected edge when we apply our network metrics (see next section) for the analysis of music collaborations.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000504/resources/images/figure01.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000504/resources/images/figure01_hu534379edc1c65009afdad524fa177158_255443_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000504/resources/images/figure01_hu534379edc1c65009afdad524fa177158_255443_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000504/resources/images/figure01_hu534379edc1c65009afdad524fa177158_255443_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000504/resources/images/figure01_hu534379edc1c65009afdad524fa177158_255443_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/1/000504/resources/images/figure01_hu534379edc1c65009afdad524fa177158_255443_1800x0_resize_box_3.png 1800w,/dhqwords/vol/15/1/000504/resources/images/figure01.png 2500w" 
     class="landscape"
     ><figcaption>
        <p>The two corresponding network models. The parallel directed model is used to annotate metadata in the network, with two edges representing a single connection in the undirected model.
        </p>
    </figcaption>
</figure>
<p>As for the metadata about the connections, various additional collaboration information can be provided. To allow for the creation of dynamic networks and insights into historical questions, each edge is dated with the release year of the record. This has a drawback, as noted by Hannibal <sup id="fnref5:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup>, who time-stamped the collaboration with the year of the recording session, not the year of the album release: individual songs may have been released publicly years after the recording took place. However, detailed recording session data may not be publicly available for genres apart from Jazz. In the interest of deriving a common network modeling method across multiple genres, we decided to use the album release date.</p>
<h2 id="32-metrics">3.2 Metrics</h2>
<p>In the following section, the metrics used for network analysis are described. For each metric, we provide a short description of how the metric is calculated and what insight can be derived from it regarding the collaboration networks at hand. For a more formal and in-depth description of common network metrics, see, for example, Newman <sup id="fnref:27"><a href="#fn:27" class="footnote-ref" role="doc-noteref">27</a></sup>.</p>
<h2 id="321-degree">3.2.1 Degree</h2>
<p>The degree of a node is the number of incoming and outgoing edges a node has. The neighborhood of a node is thus given by all other nodes it is connected to. Applied to the undirected music collaboration networks at hand, the degree is the number of other musicians an individual has collaborated with, and the neighborhood is the set of all those musicians. The average degree and the overall distribution of degrees gives insight into the collaboration frequencies of musicians in that genre. In addition, this work looks at the degree variance and degree distribution in order to see whether the network mostly revolves around highly connected artists or if the whole network tends to be closely connected.</p>
<h2 id="322-power-law-scaling-exponent">3.2.2 Power-law scaling exponent</h2>
<p>For numerous types of networks, it can be observed that a small number of nodes have a large number of connections to other nodes, i.e., a high degree, while the majority of nodes have a rather low degree. Networks that exhibit this property are commonly referred to as scale-free networks . In these networks, the number of nodes that have a certain number of links is related to the degree of these nodes, such that the degree distribution of the network is a power-law distribution with a scaling exponent α. Typically, α is in the range of 2 ≤ α ≤ 3, although in some cases, values outside this range can be observed <sup id="fnref1:27"><a href="#fn:27" class="footnote-ref" role="doc-noteref">27</a></sup>.</p>
<p>Scale-free characteristics can be observed for a wide range of real-world networks, especially social and collaborative networks. It has been hypothesized that they form due to two major properties: growth and preferential attachment. Since these networks grow naturally, nodes that are introduced at an earlier stage have a higher chance of developing connections, hence boosting their degree. Similarly, preferential attachment means that establishing links reinforce a positive bias toward the connected nodes, as a node of high degree (also called &ldquo;hub&rdquo;) seems to be more attractive to establish connections with, in turn, attracting more future connections <sup id="fnref:28"><a href="#fn:28" class="footnote-ref" role="doc-noteref">28</a></sup>. In the collaborative network at hand, these central hubs signify highly influential musicians. By investigating whether the degree distribution follows a power law, we can apply these characteristics to music collaboration networks, allowing for hypotheses to be derived about the origins of success for musicians.</p>
<h2 id="323-degree-correlation">3.2.3 Degree correlation</h2>
<p>The degree correlation is defined as the Pearson correlation coefficient ρ of the degrees of nodes connected by an edge <sup id="fnref:29"><a href="#fn:29" class="footnote-ref" role="doc-noteref">29</a></sup>, specifying if edges tend to be formed between nodes of similar or dissimilar degree. This parameter provides insight into the networks connectivity structure by specifying whether the network is assortative (nodes mainly connect to nodes with a similar degree, ρ &gt; 0), disassortative (there is a systematic disparity in degree between connected nodes, ρ &lt; 0), or neutral (connections are random, ρ ≈ 0). In the networks at hand, assortative would mean that musicians with lots of connections to other musicians tend to collaborate with individuals who exhibit the same characteristic. Disassortative signifies that those well-connected musicians tend to collaborate more with less-connected musicians. Neutral would mean that musicians of high and low degrees frequently collaborate in an intermixed fashion, with no apparent general trend.</p>
<h2 id="324-average-nearest-neighbor-degree">3.2.4 Average nearest neighbor degree</h2>
<p>The average nearest neighbor degree (ANND) <sup id="fnref:30"><a href="#fn:30" class="footnote-ref" role="doc-noteref">30</a></sup> of a node is the average degree of all nodes in its direct neighborhood. In weighted networks, each neighboring nodes&rsquo; degree can be weighted by the weight of the edge connecting it. This value is computed individually for all nodes of a degree  <em>k</em> , and then averaged over all nodes of that degree, a process that is repeated for all values of  <em>k</em>  present in the network. This results in the value being a function of  <em>k</em>  and providing additional information about the assortative properties of the network.</p>
<p>In assortative networks, the average neighbor degree is increasing for larger values of  <em>k</em> , decreasing in disassortative networks, and constant when the network has neutral mixing. While the degree correlation captures the general network trend in a single value for all nodes, the ANND extends the insight into the local structure by further specifying for which nodes the correlation occurs, as the ANND is a function of  <em>k</em> . Furthermore, Pearson&rsquo;s degree correlation has been shown to depend on the size of the network, an issue that the average neighbor degree does not face <sup id="fnref:31"><a href="#fn:31" class="footnote-ref" role="doc-noteref">31</a></sup>. As we deal with large networks, we compute the average neighbor degree in addition to the degree correlation to complement the interpretation of the assortative properties of music collaboration given by the degree correlation.</p>
<h2 id="325-mean-local-clustering-coefficient">3.2.5 Mean local clustering coefficient</h2>
<p>The mean local clustering coefficient <sup id="fnref:32"><a href="#fn:32" class="footnote-ref" role="doc-noteref">32</a></sup> is a measure of how complete the nodes in a network are connected to each other. This notion of completeness is based on the idea of triangles forming in the neighborhood of a node: when two nodes in the neighborhood are in turn connected to each other, the network is more tightly knit. The local clustering of a node is thus given by the ratio of triangles that exist to all possible triangles in its neighborhood. The mean local clustering coefficient is then defined as the mean of all local clustering coefficients. Applied to the collaborative nature of our network, a higher clustering coefficient would indicate that musicians that share a common collaborator are likely to collaborate with each other too, facilitating the exchange of creative ideas throughout the genre.</p>
<h2 id="326-transitivity-score">3.2.6 Transitivity score</h2>
<p>A network&rsquo;s transitivity score, also called global clustering coefficient, is another metric describing the connectedness of the network <sup id="fnref2:27"><a href="#fn:27" class="footnote-ref" role="doc-noteref">27</a></sup>. In contrast to the local clustering coefficient, it characterizes the whole network by applying the notion of complete triangles vs. possible triangles to all edges in the graph on a global, instead of a local level. Similar to the mean local clustering coefficient, this metric provides insight into the transitive properties of the network. In our case, such a transitive relation can be expressed as &ldquo;if musician A recorded together with musician B, and musician B with musician C, chances are high that A and C also recorded music together&rdquo;. The transitivity scores in social networks tend to be rather high <sup id="fnref3:27"><a href="#fn:27" class="footnote-ref" role="doc-noteref">27</a></sup>, especially in collaborative networks: for music, in particular, Smith gives a value of  <em>C</em>  = 0.18; in other applications such as a network of film actor collaborations, we have similar values:  <em>C</em>  = 0.20 <sup id="fnref:33"><a href="#fn:33" class="footnote-ref" role="doc-noteref">33</a></sup>. For our formalization of a network, these scores (and also the local clustering coefficients) are expected to be even higher. Since we add a fully connected subgraph for each music record (which in itself has a transitivity score of 1), and if the number of artists on a given record is high, the transitivity score of the whole network is boosted in turn.</p>
<h2 id="33-data">3.3 Data</h2>
<p>The Discogs database is the largest openly accessible collection of crowdsourced metadata of music releases <sup id="fnref:34"><a href="#fn:34" class="footnote-ref" role="doc-noteref">34</a></sup>, yet it is less commonly used for research purposes. <a href="https://www.discogs.com/search">Discogs</a> currently lists over 12 million music releases from 6.7 million artists on nearly 1.5 million music labels. Metadata for existing and new releases is continuously entered, checked, and curated by over 500,000 <a href="https://www.discogs.com/stats/contributors">registered users.</a> A voting system is used to rate the accuracy and completeness of data.<sup id="fnref:35"><a href="#fn:35" class="footnote-ref" role="doc-noteref">35</a></sup>  For each item, a vast variety of metadata can be specified, ranging from standard fields such as title, label, genre, and artists to fine-grained information, such as contributing musicians by track, issued versions of a master release or trivia. This allows researchers to describe virtually any type of audio recording ever released in great detail. For many releases, a list of the contributing musicians collaborating on the release, as well as their information about their collaboration role, is provided.</p>
<p>Discogs provides complete dumps of their whole database that are updated on a monthly basis. The data is licensed under <a href="https://creativecommons.org/share-your-work/public-domain/cc0">CC0</a>, so there are no restrictions placed on the use of the data, which makes it a perfect data resource for research. All data in this project was obtained from a Discogs Data dump from October 9th, 2019.<sup id="fnref:36"><a href="#fn:36" class="footnote-ref" role="doc-noteref">36</a></sup>  Based on this data, we created a collaboration network by defining an edge between the artist and each collaborator if they appear together on the same release. This process uses the Discogs ID of each person associated with the release, so even if an artist uses a pseudonym, the correct link is created in the network. Only releases which have exactly one genre listed in their metadata were considered for the network. In this way, only intra-genre collaborations are modeled. While there is an immense amount of inter-genre collaboration, the main objective of this study was to search for structural differences within genres.</p>
<p>For every entry in the database, Discogs readily provides a label denoting its data quality. We only included releases that have the status  <em>accepted</em>  (indicating duplicates, etc.) and a data quality higher or equal to  <em>correct</em> , to ensure a high quality in our derived network dataset. All relations that have a non-musical role, such as  <em>cover design</em> , were omitted. Following the role categories defined by Discogs<sup id="fnref:37"><a href="#fn:37" class="footnote-ref" role="doc-noteref">37</a></sup> , only roles from the categories  <em>vocals</em> ,  <em>instruments,</em>    <em>dj mix</em> ,  <em>remix,</em>  and  <em>beats</em>  were deemed as valid, as these categories include the actual musicians involved in creating the music on the release. Finally, metadata about nodes (artists) and edges (collaborations) was added. Nodes with the Discogs IDs <a href="https://www.discogs.com/artist/194"> <em>194</em> </a> and <a href="https://www.discogs.com/artist/355"> <em>355</em> </a> with all associated edges were deleted manually, as these IDs were only database placeholders and did not link to any artist. For each node, the Discogs artist ID and the name were included. For each edge, the year, role, and Discogs release ID were included. All data was parsed using <a href="https://www.python.org">Python</a>, and the networks were exported to a  <em>GraphML</em>  file <sup id="fnref:38"><a href="#fn:38" class="footnote-ref" role="doc-noteref">38</a></sup>. Network metrics were calculated using the <a href="https://networkx.github.io">networkx</a> package for Python <sup id="fnref:39"><a href="#fn:39" class="footnote-ref" role="doc-noteref">39</a></sup>. In addition, the Python package  <em>power-law</em>  was used to calculate the power-law exponent <sup id="fnref:40"><a href="#fn:40" class="footnote-ref" role="doc-noteref">40</a></sup>. Finally, <a href="https://gephi.org"> <em>Gephi</em> </a>  <sup id="fnref:41"><a href="#fn:41" class="footnote-ref" role="doc-noteref">41</a></sup> was used for graph visualizations.</p>
<h2 id="4-results-and-discussion">4. Results and Discussion</h2>
<h2 id="41-dataset">4.1 Dataset</h2>
<p><a href="#table01">Table 1</a> provides an overview of the network details that were derived for the genres Jazz and Hip Hop, using the previously described method on Discogs metadata. We only include releases with the highest tiers of the data quality label provided by Discogs, which indicates correctness and completeness based on a crowdsourced voting system. Thus, we ensure that only verified information is considered. Also, Discogs counts each issue and repress of the same album as a separate release, but provides a meta grouping in the form of a so-called master release, grouping together all the versions of an album. Since the metadata (at least the data relevant to our research) is consistently the same across all versions of such a master release, we based our data extraction on those. These two facts combined account for us using only a seemingly low number of releases, given the immense amount of available data.</p>
<p>Using our proposed method, we derive the most comprehensive, freely accessible network dataset on music collaboration to date, both in terms of spanned timeframe and included artists. Before carrying out analyses and calculating metrics, each network was converted to a single-edge, undirected graph by combining all the parallel edges between two nodes into one weighted edge, with its weight being the sum of merged edges. If six parallel directed edges exist as a result of two musicians collaborating on three records, those are replaced by a single undirected edge of weight six.<br>
Network characteristics for Jazz and Hip Hop as extracted from Discogs metadata       <strong>Artists</strong>    <strong>Relations</strong>    <strong>Parsed releases</strong>    <strong>Timespan</strong>       Hip Hop  23098  338200  12301  1979 - 2019      Jazz  40296  3488662  21808  1917 - 2019</p>
<h2 id="42-jazz-collaboration">4.2 Jazz Collaboration</h2>
<p><a href="#table02">Figure 2</a> summarizes the most important descriptive network statistics for our Jazz network. To put those numbers into context, we also provide the results of the earlier cited study of Gleiser and Danon <sup id="fnref5:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup>, providing comparable statistics for their network analysis. As Gleiser and Danon&rsquo;s data is based on biographical information from 192 musicians, their network is much smaller. The newly generated Discogs network is over thirty times larger in terms of recorded musicians and includes about fourteen times more connections. The data spans nearly 100 years of Jazz history, providing much more comprehensive insight into the historical development of the genre.<br>
Comparison of descriptive statistics of different our Discogs Jazz network and the existing Linked Jazz network by <sup id="fnref6:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup>. n – number of nodes in the network, m – number of undirected edges, d̅ – average degree per node, c̅ – mean local clustering coefficient, C – transitivity score, ρ – degree correlation coefficient, α – power-law scaling exponent        <strong>n</strong>    <strong>m</strong>    <strong>d̅</strong>    <strong>c̅</strong>    <strong>C</strong>    <strong>ρ</strong>    <strong>α</strong>       Discogs Jazz network  40294  569562  28.27  0.80  0.32  0.26  1.53      Linked Jazz network <sup id="fnref7:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup>  1275  38326  60.4  0.89  -  0.05  -   <br>
As shown in <a href="#table02">Figure 2</a>, the Discogs Network, with an average degree of 28.27 connections per musician, is considerably denser compared to the Linked Jazz network. This disparity could be explained by the fact that Gleiser and Danon&rsquo;s network is generated from biographical information from the years 1895 to 1929. During this time, Jazz is characterized by larger collectives of musicians, as it coincides with the musical styles of New Orleans Jazz, Dixieland Jazz, and Chicago Jazz. In modern Jazz, a larger variety of group sizes is found, with an inclination towards solo artists and trios <sup id="fnref:42"><a href="#fn:42" class="footnote-ref" role="doc-noteref">42</a></sup>. Consequently, a node in the network of Gleiser and Danon is more likely to have more connections, as the band size at the time was comparatively large, introducing a higher number of connections. Deriving data from oral recollections further facilitates that the network consists of one large coherent component. If, in contrast, the network was based on music releases, smaller clusters only representing one album would appear that are not connected to the main network in any way. As a matter of fact, the Discogs Jazz network spans 1,100 separate components, ranging from 1 to 35,128 musicians in size. 75% of these components have a maximum of 6 nodes. Yet, 87% of musicians are included in the largest component.</p>
<p>This is corroborated by the degree distribution in <a href="#figure02">Figure 2</a>, which also shows that the Discogs Network includes a large number of musicians with only a few direct connections to other musicians, while nodes with a higher number of connections are increasingly rare. In fact, over 75% of nodes have a degree smaller than the mean. The resulting distribution therefore resembles a power-law distribution.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000504/resources/images/figure02.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000504/resources/images/figure02_hu2c07c18e7dcadf4c219a91681444cd6d_175498_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000504/resources/images/figure02_hu2c07c18e7dcadf4c219a91681444cd6d_175498_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000504/resources/images/figure02_hu2c07c18e7dcadf4c219a91681444cd6d_175498_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000504/resources/images/figure02_hu2c07c18e7dcadf4c219a91681444cd6d_175498_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/1/000504/resources/images/figure02_hu2c07c18e7dcadf4c219a91681444cd6d_175498_1800x0_resize_box_3.png 1800w,/dhqwords/vol/15/1/000504/resources/images/figure02.png 2500w" 
     class="landscape"
     ><figcaption>
        <p>Degree distribution of the Jazz network. Outliers d &gt; 200 are omitted from the distribution plot for better legibility.
        </p>
    </figcaption>
</figure>
<p>In fact, with a scaling exponent of α = 1.53, the network can be characterized as scale-free. Although the α-value is outside the 2-3 range, which is typical for scale-free networks, such outliers commonly occur and the degree distribution in <a href="#figure02">Figure 2</a> is very similar to a power-law distribution for degrees greater than approximately 10. Consequently, we can classify the Jazz collaboration network as a scale-free network. Previous work did not calculate a scaling exponent for their networks; hence no comparison can be given in that regard.</p>
<p>Another metric that is influenced by the widely compartmentalized nature of the network is the degree correlation, which once again differs considerably in comparison to existing work. At ρ = 0.26, a rather strong degree correlation is present, indicating that musicians tend to collaborate with other artists who have a similar degree. Possible reasons for this correlation effect become apparent in <a href="#figure03">Figure 3</a>, which plots ANND by degree and indicates the general trend with a linear regression.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000504/resources/images/figure03.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000504/resources/images/figure03_hu63bf93a0234976efcc9c7ec1cfcd8a8b_236042_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000504/resources/images/figure03_hu63bf93a0234976efcc9c7ec1cfcd8a8b_236042_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000504/resources/images/figure03_hu63bf93a0234976efcc9c7ec1cfcd8a8b_236042_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000504/resources/images/figure03_hu63bf93a0234976efcc9c7ec1cfcd8a8b_236042_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/1/000504/resources/images/figure03_hu63bf93a0234976efcc9c7ec1cfcd8a8b_236042_1800x0_resize_box_3.png 1800w,/dhqwords/vol/15/1/000504/resources/images/figure03.png 2040w" 
     class="landscape"
     ><figcaption>
        <p>Average nearest neighbor degree (ANND) by node degree in the Jazz Network.
        </p>
    </figcaption>
</figure>
<p>Since the regression line is monotonously rising, the degree correlation effect can be validated. We can further conclude that since 75% of nodes have a degree of 26 or less, most of the correlation effect can be attributed to the mentioned small components coming from only one record or a fixed set of people, as these share the same degree within their component, having little outside connections.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000504/resources/images/figure04.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000504/resources/images/figure04_hu098eca2cac5c05338634edad40bafa2a_139095_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000504/resources/images/figure04_hu098eca2cac5c05338634edad40bafa2a_139095_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000504/resources/images/figure04_hu098eca2cac5c05338634edad40bafa2a_139095_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000504/resources/images/figure04_hu098eca2cac5c05338634edad40bafa2a_139095_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/1/000504/resources/images/figure04_hu098eca2cac5c05338634edad40bafa2a_139095_1800x0_resize_box_3.png 1800w,/dhqwords/vol/15/1/000504/resources/images/figure04.png 2500w" 
     class="landscape"
     ><figcaption>
        <p>Local clustering coefficient distribution for the Jazz network.
        </p>
    </figcaption>
</figure>
<p>Regarding the overall distribution of the local clustering coefficient of the Discogs network in <a href="#figure04">Figure 4</a>, it can be observed that half of the regarded data have a local clustering coefficient of 1. Another spike occurs at 0, with all other values being represented in a nearly equal fashion. The two spikes at 0 and 1 could once again be due to the high number of smaller components: if a group of musicians has only released one album, or does not collaborate outside their usual setting, all the nodes inside this component have a clustering coefficient of 1. Similarly, solo artists inherently have a clustering coefficient of 0.</p>
<p>In conjunction with the degree distribution and correlation, some assumptions about the overall structure of collaboration in Jazz can be derived. The results regarding degree distribution and correlation allow some assumptions about the overall nature of Jazz collaborations in the regarded network. The new data indicates that in contrast to previous findings, not only one dominant component, which includes almost all of the network, can be found, but smaller, separate groups of artists play a significant role too. A large number of musicians only appear in a fixed set, forming their own network components, bearing no connection to the rest of the artists. However, since most nodes are included in the largest component, the existing interpretation of Jazz as a highly collaborative genre still stands, as expressed by the transitive, closely-knit network structure. As the network is scale-free, it exhibits only a few highly connected hubs. These hubs may be theorized to stand for those artists who have a huge influence on the overall genre.</p>
<h2 id="43-hip-hop-collaboration">4.3 Hip Hop Collaboration</h2>
<p><a href="#table03">Table 3</a> provides descriptive network statistics for the Discogs Hip Hop network and compares them to the data derived by Smith <sup id="fnref4:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>. Again, the networks are generated in different ways. While Smith&rsquo;s network includes only the connections between rappers/singers associated with single song lyrics, the Discogs data provides a much broader range of roles, such as musicians involved in the DJ mix, remix, and beats of an album.<br>
Comparison of descriptive statistics of different our Discogs Hip Hop network and the existing lyrics Hip Hop network by <sup id="fnref5:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>. n – number of nodes in the network, m – number of undirected edges, d̅ – average degree per node, c̅ – mean local clustering coefficient, C – transitivity score, ρ – degree correlation coefficient, α – power-law scaling exponent        <strong>n</strong>    <strong>m</strong>    <strong>d̅</strong>    <strong>c̅</strong>    <strong>C</strong>    <strong>ρ</strong>    <strong>α</strong>       Discogs Hip Hop Network  23099  81700  7.07  0.73  0.63  0.45  1.58      Lyrics Network <sup id="fnref6:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>  4433  57972  20.95  0.48  0.18  0.06  3.5   <br>
Here too, the Discogs network is considerably larger in size. There are about five times as many musicians in our network, sharing 1.4 times as many connections. Once again, with 7.07 the average degree is considerably lower in the Discogs data, with an average number of 20.95 neighbors in Smith&rsquo;s network. This could be partly due to the methods used for network inference, as already mentioned for the Jazz networks. Moreover, the granularity of the data could be assumed as a different reason for the Hip Hop genre, since the time frame and general method are similar for both Hip Hop datasets. Smith uses around 30,000 lyrics files to collect the network data, about 4.5 times more releases than were included in our study. A single song, as opposed to a whole record, usually has fewer people involved. Therefore, the number of involved artists per release is much higher in the Discogs data, but the total number of releases used to infer the network is lower. These numbers facilitate a higher ratio of edges to nodes, in turn explaining the divergence in average degrees.</p>
<p>In general, the chosen metrics deviate strongly from their counterparts in existing work. Both the average local clustering degree and the global clustering coefficient are much higher in the Discogs network than in Smith&rsquo;s analyses. Following the same line of reasoning as for the Jazz network, some of these observations may be attributed to the overall large number of components included in our network. In total, 3,076 components are present, ranging from 1 to 11,285 in size. Half the network is included in the largest component, while 75% of components consist of four nodes or less. <a href="#figure05">Figure 5</a> shows the degree distribution for the Hip Hop network, with the majority of nodes showing a rather low degree, quickly decaying for fewer nodes with higher degrees. More concretely, the distribution indicates that three out of four artists have only collaborated with eight people or less.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000504/resources/images/figure05.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000504/resources/images/figure05_hu1bb93787cc7f6ce433f7e27b77be50af_166152_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000504/resources/images/figure05_hu1bb93787cc7f6ce433f7e27b77be50af_166152_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000504/resources/images/figure05_hu1bb93787cc7f6ce433f7e27b77be50af_166152_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000504/resources/images/figure05_hu1bb93787cc7f6ce433f7e27b77be50af_166152_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/1/000504/resources/images/figure05_hu1bb93787cc7f6ce433f7e27b77be50af_166152_1800x0_resize_box_3.png 1800w,/dhqwords/vol/15/1/000504/resources/images/figure05.png 2500w" 
     class="landscape"
     ><figcaption>
        <p>Degree distribution of the Hip Hop network. Outliers d &gt; 40 from are omitted from distribution plot for better legibility.
        </p>
    </figcaption>
</figure>
<p>As already seen in the Jazz network, the scaling exponent here again α is outside the usual range. In previous work, values of α are reported <sup id="fnref7:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>. This difference in our data can mostly be attributed to the network creation approach: as the Discogs network presumably is more sparse with regard to connections (as indicated by the lower average degree), the decay of the power-law function is also smaller. Ultimately, both values lead to the same conclusion, which is to classify the collaboration structure of Hip Hop as being scale-free. The high number of disconnected components in the network, in conjunction with the diameter of these local sub-networks being small, gives a possible explanation for the high degree correlation of ρ = 0.45.</p>
<p><a href="#figure06">Figure 6</a> shows the ANND plotted by node degree, supporting the notion of a high correlation between degrees. The overall trend, as indicated by the linear regression line, is rising, with the effect being most pronounced in lower degrees. This implies that artists with a low degree tend to work together with other artists of low degree, while this trend is much more diverse in higher degrees, where no clear pattern is apparent.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000504/resources/images/figure06.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000504/resources/images/figure06_huaf182a50c8385b2e17b375a5ed813545_187201_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000504/resources/images/figure06_huaf182a50c8385b2e17b375a5ed813545_187201_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000504/resources/images/figure06_huaf182a50c8385b2e17b375a5ed813545_187201_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000504/resources/images/figure06_huaf182a50c8385b2e17b375a5ed813545_187201_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/1/000504/resources/images/figure06_huaf182a50c8385b2e17b375a5ed813545_187201_1800x0_resize_box_3.png 1800w,/dhqwords/vol/15/1/000504/resources/images/figure06.png 2012w" 
     class="landscape"
     ><figcaption>
        <p>Average nearest neighbor degree (ANND) by node degree in the Hip Hop Network.
        </p>
    </figcaption>
</figure>
<p>With regard to the clustering properties of the network, we can observe a systematic difference in previous findings. Both the average local clustering coefficient and the global clustering coefficient are much higher in the Discogs network than in Smith&rsquo;s analyses.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000504/resources/images/figure07.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000504/resources/images/figure07_hu8576b75bce885854fe390006c33df96c_130085_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000504/resources/images/figure07_hu8576b75bce885854fe390006c33df96c_130085_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000504/resources/images/figure07_hu8576b75bce885854fe390006c33df96c_130085_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000504/resources/images/figure07_hu8576b75bce885854fe390006c33df96c_130085_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/1/000504/resources/images/figure07_hu8576b75bce885854fe390006c33df96c_130085_1800x0_resize_box_3.png 1800w,/dhqwords/vol/15/1/000504/resources/images/figure07.png 2500w" 
     class="landscape"
     ><figcaption>
        <p>Local clustering coefficient distribution for the Hip Hop network.
        </p>
    </figcaption>
</figure>
<p>The two spikes at 0 and 1 in the distribution of the local clustering coefficient shown in <a href="#figure07">Figure 7</a> can be attributed to the high number of small components and some solo artists that are being included. The values in between are somewhat evenly distributed throughout the whole range, with the upper range being slightly more represented.</p>
<p>Regarding the overall structure of collaboration in Hip Hop, our data indicates that solo artists, as well as small and consistent groups of artists, play a bigger role than previously assumed since a significant number of musicians only appears in a fixed set with no connection to the rest of the artists. Still, a large portion of the community shares highly collaborative properties, forming many connections between different subnetworks. Hubs are, therefore, highly relevant when it comes to brokering between more tightly knit local groupings. This observation is characteristic of scale-free networks, which can be assumed for the case of Hip Hop collaboration.</p>
<h2 id="44-comparison-of-genres">4.4 Comparison of Genres</h2>
<p>Jazz is typically characterized as a highly dynamic and versatile genre. Especially the Jazz community of the 1940s and 1950s is shaped by constant upheavals and change, not only concerning the musical style itself but above all, the constellations of musicians in this time seemed to be in constant flux. Unlike in current Pop or Rock bands, it is rather atypical in Jazz to play in the same constellation of musicians for a longer period of time. Instead, many musicians belong to several formations simultaneously <sup id="fnref:43"><a href="#fn:43" class="footnote-ref" role="doc-noteref">43</a></sup>. This tendency is well reflected in the collaboration networks. In comparison to the Discogs Hip Hop network, the average Jazz musician has more than four times more connections than a Hip Hop artist. The data shows that collaborations in Jazz are more frequent and involve more people than collaboration in Hip Hop. The Hip Hop community is more compartmentalized than the Jazz community, with only 50% of the artists being included in the main component, and 75% in Jazz, respectively.</p>
<p>After all, we can observe that Jazz musicians in the course of their career tend to collaborate with more different musicians than hip hop artists do. A similar picture emerges with regard to the clustering coefficient: While the Discogs Hip Hop network exhibits a total global clustering coefficient of 0.63, the Jazz network is only half as transitive. Thus, while the Jazz network as a whole seems to be highly connected, in Hip Hop, we see a tendency towards the development of smaller, though very closely connected groups. The degree correlation also shows that there seems to be a higher tendency of two actors with similar degrees to work together on a project in the Hip Hop network, which appears to be less the case in Jazz.</p>
<p>In Figures 8-11, we show partial visualizations of both the Jazz and the Hip Hop networks. A complete visualization of the two networks is not possible due to their size. Instead, we opted to show the neighborhood of important hubs in each network. For Hip Hop, we chose  <em>Tupac Shakur (2Pac)</em>  and  <em>Biggie Smalls</em>    <em>(Notorious B.I.G.)</em> , two of the most influential Hip Hop artists of all time. For Jazz, we show the neighborhood of  <em>Louis Armstrong</em>  and  <em>Ella Fitzgerald.</em>  The visualizations illustrate the differences described before: a musician in Jazz is connected with many more other artists than is the case in Hip Hop, Jazz is less transitive, and the other nodes a hub connects to vary more in degree. Also, in Jazz, most nodes are in one large interconnected component, with smaller groups in the periphery, while in Hip Hop, we find smaller, tightly knit clusters.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000504/resources/images/figure08.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000504/resources/images/figure08_hu272a050d0453122039cb680191b236c7_4845070_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000504/resources/images/figure08_hu272a050d0453122039cb680191b236c7_4845070_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000504/resources/images/figure08_hu272a050d0453122039cb680191b236c7_4845070_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000504/resources/images/figure08_hu272a050d0453122039cb680191b236c7_4845070_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/1/000504/resources/images/figure08_hu272a050d0453122039cb680191b236c7_4845070_1800x0_resize_box_3.png 1800w,/dhqwords/vol/15/1/000504/resources/images/figure08.png 2500w" 
     class="landscape"
     ><figcaption>
        <p>Neighborhood of Tupac Shakur in the Hip-Hop network. Nodes are scaled by degree.
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000504/resources/images/figure09.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000504/resources/images/figure09_huda3b32cb8c804d858f4d86e1061be7f6_1939054_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000504/resources/images/figure09_huda3b32cb8c804d858f4d86e1061be7f6_1939054_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000504/resources/images/figure09_huda3b32cb8c804d858f4d86e1061be7f6_1939054_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000504/resources/images/figure09_huda3b32cb8c804d858f4d86e1061be7f6_1939054_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/1/000504/resources/images/figure09_huda3b32cb8c804d858f4d86e1061be7f6_1939054_1800x0_resize_box_3.png 1800w,/dhqwords/vol/15/1/000504/resources/images/figure09.png 2500w" 
     class="landscape"
     ><figcaption>
        <p>Neighborhood of Notorious B.I.G. in the Hip-Hop network. Nodes are scaled by degree.
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000504/resources/images/figure10.jpeg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000504/resources/images/figure10_hu252b7ae7bdeb78ad0bca67a7e6a07eac_542378_500x0_resize_q75_box.jpeg 500w,
    /dhqwords/vol/15/1/000504/resources/images/figure10_hu252b7ae7bdeb78ad0bca67a7e6a07eac_542378_800x0_resize_q75_box.jpeg 800w,/dhqwords/vol/15/1/000504/resources/images/figure10_hu252b7ae7bdeb78ad0bca67a7e6a07eac_542378_1200x0_resize_q75_box.jpeg 1200w,/dhqwords/vol/15/1/000504/resources/images/figure10.jpeg 1380w" 
     class="landscape"
     ><figcaption>
        <p>Neighborhood of Louis Armstrong in the Jazz network. Nodes are scaled by degree.
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000504/resources/images/figure11.jpeg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000504/resources/images/figure11_hu3f5b9de7eb11efbf430beb00fb8b4e80_520096_500x0_resize_q75_box.jpeg 500w,
    /dhqwords/vol/15/1/000504/resources/images/figure11_hu3f5b9de7eb11efbf430beb00fb8b4e80_520096_800x0_resize_q75_box.jpeg 800w,/dhqwords/vol/15/1/000504/resources/images/figure11_hu3f5b9de7eb11efbf430beb00fb8b4e80_520096_1200x0_resize_q75_box.jpeg 1200w,/dhqwords/vol/15/1/000504/resources/images/figure11.jpeg 1378w" 
     class="landscape"
     ><figcaption>
        <p>Neighborhood of Ella Fitzgerald in the Jazz network. Nodes are scaled by degree.
        </p>
    </figcaption>
</figure>
<h2 id="5-conclusion">5. Conclusion</h2>
<p>The study of collaboration patterns in music is a lively branch of musicological, sociological, and historical research. However, most existing research is based on very heterogeneous data collections, making it hard to compare different results to each other. Furthermore, the lack of a common collection of data and of a shared definition of collaboration makes it almost impossible to compare different genres. Accordingly, existing research on music collaboration is mainly focused on singular genre analyses, with Jazz being the most studied genre by far. To support future inter-genre comparisons, we propose an approach that makes use of the freely available, ever-growing metadata collection Discogs. We further suggest deriving collaborations between musicians by extracting information of shared music releases from Discogs. Whenever two musicians played together on a record, we treat this as an instance of collaboration. We are well aware that this particular view on music collaboration comes with some conceptual limitations, as collaboration might also take place by means of unrecorded live gigs or private jam sessions. Yet, this approach to modeling collaboration can be scaled-up very easily, allowing us to obtain networks that are much more comprehensive in size than any of the data sets of comparable existing studies. These networks further include a rich set of additional metadata, for instance the time and role of each collaborative connection. Furthermore, the data source does not suffer from noisy data, as unique identifiers for artists are used in Discogs to map different spellings and pseudonyms to the same node.</p>
<p>To illustrate the potential of the suggested approach, we also present a case study in the inter-genre comparison of Jazz and Hip Hop, applying a variety of metrics commonly used to describe topological properties in collaborative networks. In contrast to previous work on music collaboration, we were able to demonstrate the influence of smaller artist groups and lesser known musicians. Accordingly, we were able to get some novel insights in this direction, as the collaboration networks seem to consist of many more components than previously assumed. This difference accounts for most of the divergence of metrics when comparing our results to those of previous music collaboration studies. The Discogs data suggests that the networks at hand are scale-free, (although with a very low scaling exponent). In other words, very few artists are very well connected to a lot of other artists, while most of the artists only have very few relations to other artists. The collaboration patterns of the two investigated genres mainly differ in group size, number of collaborations per artist, and overall density of the network. In Jazz, collaboration is more frequent, takes place with more people at a time, and with a greater variety of people overall than in Hip Hop in formal recordings.</p>
<p>Future work could provide a more detailed insight into collaboration patterns, e.g., by interrelating network position (i.e., centrality) with popularity metrics from other sources, as shown in Smith <sup id="fnref8:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>. Another possibility would be the inclusion of a more in-depth analysis of the formed cliques using the Girvan-Newman method of betweenness centrality <sup id="fnref1:29"><a href="#fn:29" class="footnote-ref" role="doc-noteref">29</a></sup>, as previously employed by Gleiser and Danon <sup id="fnref8:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup>. Other future directions might point toward the exploration of the scale-free properties of the networks in more detail. Central questions here are whether the length of a music career or the overall degree is a reliable indicator for success. Providing an interactive exploration interface similar to Filippova et al. <sup id="fnref2:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup> could further enhance the usefulness of our approach for future research, as it allows researchers from all backgrounds to gain insight into the collaborative patterns. An example of such an application based on Discogs data is the <a href="http://discograph.mbrsi.org/"> <em>Disco\Graph</em> </a> project.</p>
<p>Finally, the network data itself could be enhanced in several ways, for instance, by adding more releases (also from lower quality levels) in order to further increase the network size. Metadata about sub-genres of a release would be a valuable addition for historical network analysis, since the evolution of those sub-genres and the musicians associated with them could be studied in more detail. Enhancing the networks to records that are listed with more than one genre would provide the grounds to facilitate research in inter-genre collaborations. On the whole, we hope that the approach described in this article will initiate many further, more detailed studies that will examine collaboration strategies and patterns between artists of different genres, thus adding to the emerging field of computational musicology as part of the digital humanities.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Jockers, M. L. (2013).  <em>Macroanalysis: Digital methods and literary history</em> . University of Illinois Press.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Burgoyne, J.A., Fujinaga, I., Downie, S. Music Information Retrieval, in: Schreibman, S., Siemens, R., Unsworth, J. (Eds.),  <em>A New Companion to Digital Humanities</em> . Wiley Blackwell, West Sussex (2016).&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Spotify API <a href="https://developer.spotify.com/documentation/web-api/reference/tracks/get-audio-features/">https://developer.spotify.com/documentation/web-api/reference/tracks/get-audio-features/</a>; Note: All URLs mentioned in this article were last checked July 7, 2020.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Mertl, V., O&rsquo;Mahony, T.K., Tyson, K., Herrenkohl, L.R., Honwad, S., Hoadley, C. Analyzing collaborative contexts: Professional musicians, corporate engineers, and communities in the Himalayas, in:  <em>Proceedings of the 8th International Conference on International Conference for the Learning Sciences-Volume 3. International Society of the Learning Sciences</em>  (2008): 282–289.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>Teitelbaum, T., Balenzuela, P., Cano, P., Buldú, J.M. Community structures and role detection in music networks.  <em>Chaos Interdiscip. J. Nonlinear Sci</em> . 18, 043105 (2008).&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>Park, D., Bae, A., Schich, M., Park, J. Topology and evolution of the network of western classical music composers.  <em>EPJ Data Sci.</em>  4, 2 (2015).&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>de Lima e Silva, D., Medeiros Soares, M., Henriques, M.V.C., Schivani Alves, M.T., de Aguiar, S.G., de Carvalho, T.P., Corso, G., Lucena, L.S. The complex network of the Brazilian Popular Music.  <em>Phys. Stat. Mech</em> . Its Appl. 332 (2004): 559–565.&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>Park, J., Celma, O., Koppenberger, M., Cano, P., Buldú, J.M. The social network of contemporary popular musicians.  <em>Int. J. Bifurc. Chaos</em>  17 (2007): 2281–2288.&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>Makkonen, T. North from here: the collaboration networks of Finnish metal music genre superstars.  <em>Creat. Ind. J.</em>  10 (2017): 104–118.&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>Araújo, C.V.S., Neto, R.M., Nakamura, F.G., Nakamura, E.F. Using Complex Networks to Assess Collaboration in Rap Music: A Study Case of DJ Khaled, in: Proceedings of the 23rd Brazillian Symposium on Multimedia and the Web - WebMedia &lsquo;17. Presented at the 23rd Brazillian Symposium on Multimedia and the Web,  <em>ACM Press</em> , Gramado, RS, Brazil (2017 ): 425-428.&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>Smith, R.D. The network of collaboration among rappers and its community structure.  <em>J. Stat. Mech. Theory Exp., P02006</em>  (2006).&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref3:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref4:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref5:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref6:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref7:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref8:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p>Filippova, D., Fitzgerald, M., Kingsford, C., Benadon, F. Dynamic Exploration of Recording Sessions between Jazz Musicians over Time, in: 2012 International Conference on Privacy, Security, Risk and Trust and 2012  <em>International Conference on Social Computing</em> , IEEE, Amsterdam, Netherlands (2012): 368–376.&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13">
<p>Giaquinto, G., Bledsoe, C., McGuirk, B. Influence and similarity between contemporary jazz artists, plus six degrees of kind of blue. PhD Thesis (2007).&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14">
<p>Gleiser, P.M., Danon, L. Community structure in jazz.  <em>Adv. Complex Syst.</em>  6 (2003): 565–573.&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref3:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref4:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref5:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref6:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref7:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref8:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:15">
<p>Hannibal, B. The Network Influences of Innovation and Lifetime Career Success in Jazz Musicians between 1945 and 1958. PhD Thesis (2015).&#160;<a href="#fnref:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref3:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref4:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref5:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:16">
<p>Macdonald, R.A.R., Wilson, G.B. Constructions of jazz: How Jazz musicians present their collaborative musical practice.  <em>Music. Sci.</em>  10 (2006): 59–83.&#160;<a href="#fnref:16" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:16" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:17">
<p>Pattuelli, C., Weller, C., Szablya, G. Linked Jazz: An Exploratory Prototype, in:  <em>International Conference on Dublin Core and Metadata Applications.</em>  Dublin, Ireland (2011): 158–164.&#160;<a href="#fnref:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:18">
<p>Seddon, F.A. Modes of communication during jazz improvisation.  <em>Br. J. Music Educ.</em>  22 (2005): 47–61.&#160;<a href="#fnref:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:19">
<p>Hammou, K. Between social worlds and local scenes: Patterns of collaboration in francophone rap music, in:  <em>Social Networks and Music Worlds.</em>  Routledge (2014): 128–145.&#160;<a href="#fnref:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:20">
<p>Amaral, L.A.N., Scala, A., Barthélémy, M., Stanley, H.E. Classes of small-world networks.  <em>Proc. Natl. Acad. Sci.</em>  97 ( 2000): 11149–11152.&#160;<a href="#fnref:20" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:21">
<p>Zhang, P.-P., Chen, K., He, Y., Zhou, T., Su, B.-B., Jin, Y., Chang, H., Zhou, Y.-P., Sun, L.-C., Wang, B.-H., others. Model and empirical study on some collaboration networks.  <em>Phys. Stat. Mech.</em>  Its Appl. 360 (2006): 599–616.&#160;<a href="#fnref:21" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:22">
<p>Newman, M. The structure of scientific collaboration networks.  <em>Proc. Natl. Acad. Sci</em> . 98 (2001): 404–409.&#160;<a href="#fnref:22" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:23">
<p>Schilling, M.A., Phelps, C.C. Interfirm collaboration networks: The impact of large-scale network structure on firm innovation.  <em>Manag. Sci.</em>  53 (2007): 1113–1126.&#160;<a href="#fnref:23" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:24">
<p><a href="https://www.mapofjazz.com">Map of Jazz</a>&#160;<a href="#fnref:24" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:25">
<p>Smith, S.  <em>Hip-Hop Turntablism, Creativity and Collaboration,</em>  Routledge (2016).&#160;<a href="#fnref:25" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:25" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:26">
<p>Note: All members of the Wu-Tang Clan are also active as solo artists.&#160;<a href="#fnref:26" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:27">
<p>Newman, M.  <em>Networks: An Introduction,</em>  1st ed. Oxford University Press (2010).&#160;<a href="#fnref:27" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:27" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:27" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref3:27" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:28">
<p>Barabási, A.-L., Bonabeau, E. Scale-free networks.  <em>Sci. Am.</em>  288 (2003): 60–69.&#160;<a href="#fnref:28" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:29">
<p>Girvan, M., Newman, M. Community structure in social and biological networks <em>. Proc. Natl. Acad. Sci.</em>  99 (2002): 7821–7826.&#160;<a href="#fnref:29" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:29" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:30">
<p>Barrat, A., Barthelemy, M., Pastor-Satorras, R., Vespignani, A. The architecture of complex weighted networks.  <em>Proc. Natl. Acad. Sci.</em>  101 (2004): 3747-3752.&#160;<a href="#fnref:30" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:31">
<p>Yao, D., van der Hoorn, P., Litvak, N. Average nearest neighbor degrees in scale-free networks.  <em>ArXiv Prepr. ArXiv170405707</em>  (2017).&#160;<a href="#fnref:31" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:32">
<p>Watts, D.J., Strogatz, S.H. Collective dynamics of &lsquo;small-world&rsquo; networks.  <em>Nature</em>  393 (1998): 440.&#160;<a href="#fnref:32" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:33">
<p>Newman, M. The structure and function of complex networks.  <em>SIAM Rev.</em>  45 (2003): 167–256.&#160;<a href="#fnref:33" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:34">
<p>Bogdanov, D., Serra, X. Quantifying music tends and facts using editorial metadata from the Discogs database, in: Hu X, Cunningham SJ, Turnbull D, Duan Z.  <em>ISMIR 2017 Proceedings of the 18th International Society for Music Information Retrieval Conference</em>  (2017): p. 89-95.&#160;<a href="#fnref:34" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:35">
<p>See: <a href="https://support.discogs.com/hc/en-us/articles/360005055593-Database-Guidelines-20-Voting-Guidelines">https://support.discogs.com/hc/en-us/articles/360005055593-Database-Guidelines-20-Voting-Guidelines</a>&#160;<a href="#fnref:35" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:36">
<p><a href="https://data.discogs.">https://data.discogs.</a>&#160;<a href="#fnref:36" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:37">
<p><a href="https://www.discogs.com/help/creditslist">https://www.discogs.com/help/creditslist</a>&#160;<a href="#fnref:37" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:38">
<p>Brandes, U., Eiglsperger, M., Herman, I., Himsolt, M., Marshall, M.S., 2001. GraphML progress report structural layer proposal, in:  <em>International Symposium on Graph Drawing</em> . Springer, pp. 501–512.&#160;<a href="#fnref:38" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:39">
<p>Hagberg, A., Swart, P., S Chult, D. Exploring network structure, dynamics, and function using NetworkX. Los Alamos National Lab. (LANL), Los Alamos, NM, United States (2008).&#160;<a href="#fnref:39" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:40">
<p>Alstott, J., Bullmore, E., Plenz, D. power-law: a Python package for analysis of heavy-tailed distributions.  <em>PloS One</em>  9, e85777 (2014).&#160;<a href="#fnref:40" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:41">
<p>Bastian, M., Heymann, S., Jacomy, M. Gephi: An Open Source Software for Exploring and Manipulating Networks, in:  <em>Proceedings of the International AAAI Conference on Weblogs and Social Media</em>  (2009)&#160;<a href="#fnref:41" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:42">
<p>Gioia, T.  <em>The History of Jazz</em> . Oxford University Press, USA (1998).&#160;<a href="#fnref:42" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:43">
<p>Hinton, M., Berger, D.G., Morgenstern, D., 1988.  <em>Bass line: the stories and photographs of Milt Hinton</em> . Temple University Press.&#160;<a href="#fnref:43" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry><entry><title type="html">Towards a User-Friendly Tool for Automated Sign Annotation: Identification and Annotation of Time Slots, Number of Hands, and Handshape</title><link href="https://rlskoeser.github.io/dhqwords/vol/15/1/000510/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://rlskoeser.github.io/dhqwords/vol/15/1/000510/</id><author><name>Manolis Fragkiadakis</name></author><author><name>Victoria Nyst</name></author><author><name>Peter van der Putten</name></author><published>2021-03-05T00:00:00+00:00</published><updated>2021-03-05T00:00:00+00:00</updated><content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>While the majority of the studies in the field of digital humanities have been mostly text oriented, the evolution in computing power and technology has resulted in a shift towards multimedia-oriented studies. Recently, advances in computer vision have started to find practical applications in study domains outside of computer and data science. Video is one of the most important time-based media as it has the ability to carry large amount of digital information in a condensed form, and hence it serves as a rich medium to capture various forms of cultural expression. Automated processing and annotation of large numbers of videos is now becoming feasible due to the evolution of computer vision and machine learning.</p>
<p>In sign language linguistics, a transition took place from paper-based materials to large video corpora to facilitate the study of the languages in question. Sign language corpora are mainly composed of video data. The primary goal of these video corpora is to study sign language functioning.</p>
<p>The processing of sign languages usually involves requires a form of textual representation <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>, most notably glosses for annotation. Sign language glosses are words from a spoken language. Uniquely identifying glosses by definition refer to a specific sign. Such ID glosses are an essential element for the quantitative analysis of a sign language corpus <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. Typically, sign language linguists add glosses and other annotations to the video recordings with the use of a software tool (namely ELAN). ELAN allows researchers to add time-aligned annotations to a video. However, this task requires a lot of time and can be prone to errors.</p>
<p>New advances in computer vision open up additional ways of studying videos containing sign language data, extracting formal representations of linguistic phenomena, and implementing these in computer applications, such as automatic recognition, generation, and translation. Using computer vision and machine learning enables quick and new ways of processing large sets of video data, which in turns makes it possible to address research questions that were not feasible before.</p>
<p>This study is the first part of a project aiming at the creation of tools to automatize part of the annotation process of sign language video data. This paper presents the methodologies, tools and implementations of three functionalities: the detection of 1) manual activation 2) the number of hands involved and 3) the handshape distribution on sign language corpora.</p>
<p>Recent developments in sign language recognition illustrate the advantages of machine and deep learning for tasks related to recognition and classification <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>  <sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>  <sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>. Nevertheless, current approaches are restricted in various ways, limiting their applicability in current sign language research. For example, training deep learning networks requires a vast amount of data as well as adequate computational power. These networks are usually trained in one sign language and they do not generalize well in other sign languages.</p>
<p>Additionally, current approaches in sign language automatic annotation need manual annotation of the hands and body joints for the training of the recognizer models <sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>  <sup id="fnref1:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. Moreover, the application of color and motion detection algorithms <sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>, as feature extraction methods, can be susceptible to errors and possibly skin color bias. Finally, several hand tracking models only work on a particular type of recordings, for example, a signer wearing gloves, or recordings made with Microsoft’s Kinect <sup id="fnref1:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>. As a result, these models are not usable for the majority of the existing sign language corpora which have been recorded with standard RGB cameras.</p>
<p>Our methods have been developed and tested on two West African sign language corpora containing natural conditions with non-Caucasian signers. While most studies in the sign language recognition field have mainly concerned signers with light skin tones, little research has been conducted using darker skin tones. With the emergence of corpora compiled in African countries under challenging real-world conditions, and their contribution to the overall sign language community, it is of utmost importance to test how methods perform in such a domain. Alleviating biases and increasing diversity should be a top priority of any computer assisted study.</p>
<p>In this study, a pre-trained deep learning pose estimation library developed by Cao et al. <sup id="fnref1:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup> named OpenPose has been used to extract body and finger key-points. OpenPose has been trained and evaluated on two large datasets for multi-person pose estimation. The MPII human multi-person dataset <sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup> and the COCO 2016 keypoints challenge dataset <sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup> contain images of people of different age groups and ethnicities in diverse scenarios. As a result, OpenPose does not have a bias toward skin color. Additionally, its easy-to-use implementation makes it an ideal framework to be used by linguists with limited coding experience.</p>
<p>The combination of the aforementioned pose estimation framework as well as the machine and deep learning architectures tested in this study, provides a robust approach towards automatic annotation. Current models and tools can be used in any sign language or gestural corpus independently of its quality, length and number of people in the video. These tools have been developed as python modules that can run automatically in a video and produce the relevant annotation files requiring minimal effort from the user. More generally, as large parts of our cultures nowadays are captured in video, our study serves as a case example of how intelligent machine learning techniques can serve digital humanities researchers by extracting semantics from large video collections.</p>
<p>This article is structured as follows: Section 2 introduces the developments on the sign language recognition and automatic annotation fields. Section 3 describes the materials used in this study and the methodologies developed and applied for each tool separately. Section 4 presents the results for each experimental setup and tool. Section 5 contains the discussion and future work while Section 6 presents our conclusions. Finally, Appendix A presents the architecture and technical details of the Long-Short-Term-Memory Network trained for this study.</p>
<h2 id="2-literature-review">2. Literature review</h2>
<p>In this section we present the studies conducted on the sign language recognition and automatic annotation field developed with depth sensors as well as standard RGB cameras. Additionally, we describe the developments of the human pose estimation field and we introduce the OpenPose framework that will be used in this article.</p>
<h2 id="21-sign-language-recognition-and-automatic-annotation">2.1 Sign Language Recognition and Automatic Annotation</h2>
<p>The primary goal of sign language recognition is to develop methods and algorithms to accurately identify a series of produced signs and to discern their meaning. The majority of studies have focused on recognizing those features and methods that can properly identify a sign out of a given set of possible signs. However, such methods can only be used on a particular set of signs and, thus, a specific sign language, which makes it harder to study the relationships between and evolution of various sign languages.</p>
<p>An additional motivation behind Sign Language Recognition (SLR) is to build automatic sign language to speech or text translation systems to assist the communication between the deaf and hearing community <sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup>. Moreover, SLR plays an important role in developing gesture-based human–computer interaction systems <sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>. Sign language linguists have used such systems to facilitate the annotation process of sign language corpora in order to discern the different signs in a video recording and further study the linguistic phenomena presented.</p>
<p>There are numerous studies dealing with the automated recognition of sign languages as clearly presented by Cooper et al. <sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>, in their review study on the state-of-the-art in sign language recognition. However, the experiments presented in most of these studies are either hard to replicate, or they pose limitations as far as their applicability is concerned. For instance, most of these studies use depth sensors, most notably MS Kinect, to capture 3D images of the environment <sup id="fnref1:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>  <sup id="fnref2:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>  <sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup>. As a result, using the frameworks developed in these studies requires a machine with similar features as the one used for testing and most probably will only work for that sign language on which they have been trained.</p>
<p>Recently, computer vision techniques have been applied to sign language recognition to overcome the aforementioned limitations. Roussos et al. <sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup> created a skin color probabilistic model to detect and track the hands of a signer on a video, while Cooper et al. <sup id="fnref1:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup> use this model to segment the hands and apply a classifier based on Markov models. However, systems based on skin color <sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup>  <sup id="fnref2:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>  <sup id="fnref:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup>  <sup id="fnref:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup> are prone to errors and have difficulties on tracking the hands and the signer’s features against cluttered backgrounds and in noisy conditions in general. Also, they do not work in videos with multiple signers.</p>
<h2 id="22-human-pose-estimation">2.2 Human Pose Estimation</h2>
<p>Human pose estimation has been extensively studied due to its numerous applications on a number of different fields <sup id="fnref:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup>. Due to low computational complexity during inference, pictorial structures have been commonly used <sup id="fnref:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup>  <sup id="fnref:20"><a href="#fn:20" class="footnote-ref" role="doc-noteref">20</a></sup>  <sup id="fnref:21"><a href="#fn:21" class="footnote-ref" role="doc-noteref">21</a></sup> to model human pose. Recently, studies have focused on improving the appearance models used in these structures by modelling the individual body parts <sup id="fnref:22"><a href="#fn:22" class="footnote-ref" role="doc-noteref">22</a></sup>  <sup id="fnref:23"><a href="#fn:23" class="footnote-ref" role="doc-noteref">23</a></sup>  <sup id="fnref:24"><a href="#fn:24" class="footnote-ref" role="doc-noteref">24</a></sup>  <sup id="fnref:25"><a href="#fn:25" class="footnote-ref" role="doc-noteref">25</a></sup>. Felzenszwalb and Huttenlocher <sup id="fnref1:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup>, relying on the pictorial structure framework recommended a deformable part-based model. Additionally, Yang and Ramanan <sup id="fnref:26"><a href="#fn:26" class="footnote-ref" role="doc-noteref">26</a></sup> showed that a tree-structured model using a combination of deformable parts can be used in order to achieve accurate pose estimation. Furthermore, Charles et al. <sup id="fnref:27"><a href="#fn:27" class="footnote-ref" role="doc-noteref">27</a></sup> showed that human body joint positions can be predicted using a random forest regressor based on a co-segmentation process over all video frames.</p>
<p>In general, most of the vision-based approaches developed for sign language recognition tasks utilizing pose estimation, have used the RWTH-PHOENIX-Weather data set <sup id="fnref:28"><a href="#fn:28" class="footnote-ref" role="doc-noteref">28</a></sup> to validate their models. This data set consists of weather forecast airings from the German public tv-station PHOENIX along with transcribed gloss annotations. However, it is a question to what extent such systems tested in this data set can be replicated with real-life conditions in the corpora. It is often the case that sign language and gestural corpora, especially the ones filmed outside of studio conditions, have bad quality, low brightness and often contain more than one person in the frame. These characteristics create an additional challenge to the tracking and prediction task.</p>
<h2 id="221-openpose">2.2.1 OpenPose</h2>
<p>OpenPose is a real-time, open-source library for academic purposes for multi-person 2D pose estimation. It can detect body, foot, hand and facial keypoints <sup id="fnref2:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>. Following a bottom-up approach (from an entire image as input to full body poses as output), it outperforms similar 2D body pose estimation libraries.</p>
<p>A major advantage of the library is that it achieves high accuracy and performance regardless of the number of people in the image. Its high accuracy is performed by using a non-parametric representation of 2D vector fields. These fields encode the position and orientation of body parts over the image domain and their degree of association in order to learn to relate them to each individual.</p>
<p>OpenPose is able to run on different operating systems and multiple hardware architectures. Additionally, it provides tools for visualization and output file generation. The output can be multiple json files containing all the pixel x, y coordinates of the body, hand and face joints. In this study the DEMO version on a CPU-only mode has been used to train our models. This choice was made in order to ensure that reproducibility can be easily achieved without the need for powerful computers from the linguist’s side.</p>
<h2 id="3-materials-and-methods">3. Materials and Methods</h2>
<p>This section describes the datasets used in our study as well as the pre-processing stage using OpenPose to extract the body joints’ pixel coordinates. Furthermore, we introduce the methods applied in the development of each tool. Special consideration is given on the handshape recognition module as an additional normalization part has been developed.</p>
<h2 id="31-data">3.1 Data</h2>
<p>A data set of 7,805 frames in total (approximately 4 minutes) labeled as signing or not signing has been compiled for the first part of the study. The dimensions of the frames were 352 by 288 pixels and were extracted from the Adamorobe and Berbey Sign Language corpora <sup id="fnref:29"><a href="#fn:29" class="footnote-ref" role="doc-noteref">29</a></sup>  <sup id="fnref:30"><a href="#fn:30" class="footnote-ref" role="doc-noteref">30</a></sup>. These corpora portray an additional challenge as the signers have been filmed in and around their homes, in natural conditions, outside of a studio, with strongly varying brightness and background noise. Furthermore, they may contain signing from one and two people at the same time. As a result, they can be considered as one of the hardest corpora to perform classification tasks. It is arguable that if the methods developed in this study can perform reasonably well on corpora of such poor conditions, then they can be applied to any sign language corpus under better settings.</p>
<p>Additional videos from YouTube with higher quality have been selected for testing purposes too. For the first task of this study, the original data set was split into a training and testing set of 6,150 and 1,655 frames respectively and the labels were one hot encoded (i.e. signing as 1 and not-signing as 0).</p>
<p>After a successful training of the first prediction model, the tool was applied on a different part of the corpora. The predicted signing sequences were manually labeled as one- or two-handed signs. Together with randomly selected not-signing sequences (as predicted by the first tool), they formed a second data set. The size of this data set was slightly larger than the previous one: 10,120 frames in total.</p>
<h2 id="32-pre-processing">3.2 Pre-processing</h2>
<p>Using OpenPose, the pixel coordinates of the hands, elbows, shoulders and head were extracted from each frame. In the case of the handshape recognition module, the fingers joints coordinates were additionally extracted. We avoided using the finger extraction module of OpenPose on the first two parts of the study as that would have increased the computational time significantly. The positions of the rest of the body joints were disregarded as most of the time they were out of the frame bounds. Although the quality of the frames was poor, it created an advantage for the pose estimation framework, reducing the computational time to a reasonable level.</p>
<h2 id="33-tool-1-manual-activation">3.3 Tool 1: Manual Activation</h2>
<p>The first tool is a temporal segmentation method to predict the begin and end frames of a sign sequence in a video sample. Thus, it is important to compare the performance of multiple different machine learning algorithms consistently. Four classification methods were used, namely: Support Vector Machines (SVM), Random Forests (RF), Artificial Neural Networks (ANN) and Extreme Gradient Boosting (XGBoost). The majority of these algorithms have been extensively used in machine learning studies as well as in sign language applications <sup id="fnref1:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>. Performance was measured using the Area Under the Receiver Operating Characteristics (AUC) to validate each model. The AUC is a performance measurement specifically designed for binary (i.e. two class) classification problems. In general, it expresses how well a model is capable of distinguishing between classes, for example whether someone is signing in a video fragment or not. A model that makes random predictions will have an AUC of 0.5, a perfect model will have an AUC of 1. AUC stands for  “Area under the Receiver Operating Characteristic (ROC) Curve” , the curve of True Positive Rate (probability of detection) versus False Positive Rate (probability of false alarms). It is better than just accuracy, i.e. percentage of correct predictions by the model, because it is not dependent on the relative amount of positives, i.e. percentage of total videos with signs in our case. We searched for the optimal setting of the various classification method parameters by exhaustive testing of the possible parameter settings and testing the performance on a validation set ( “grid search” , searching the  “grid”  of possible parameter values).</p>
<h2 id="34-tool-2-number-of-hands">3.4 Tool 2: Number of Hands</h2>
<p>The second tool’s goal is to predict not only if a person is signing or not, but also to identify the number of hands involved (one- or two-handed). We hypothesized that this task is more complex than before, thus we considered it as a time-series problem. By using a sliding window technique, the original data set was parsed to form new training sets, where different possible frame intervals (1,2,3,5 and 10) were tested. Furthermore, similar (to some extend) classification methods with Tool 1 have been used <sup id="fnref:31"><a href="#fn:31" class="footnote-ref" role="doc-noteref">31</a></sup> .</p>
<p>Moreover, recent studies in the sign language recognition field suggest that the use of Long-Short-Term-Memory (LSTM) networks can yield accurate results. LSTM is an artificial recurrent neural network (RNN) architecture used in the field of deep learning. Unlike standard feedforward neural networks (like the one tested in Tool 1) LSTM has feedback connections. It can not only process single data points, but also understand patterns in entire sequences of data, by combining its internal state resulting from previous input with a new input data item. In our case, instead of predicting whether a specific pose belongs into a class, we investigate whether a sequence of poses can be used for the same purpose. In this part of the study an LSTM network with different layer units as well as sliding window intervals has also been tested and compared with the above traditional machine learning classifiers. The overall architecture and technical details of the LSTM network can be found in Appendix A.</p>
<h2 id="35-tool-3-handshape">3.5 Tool 3: Handshape</h2>
<p>The handshape recognition module was considered a so-called unsupervised learning problem as no ground truth information regarding this feature was available prior to the experiment, i.e. in contrast to the previous two problems we did not know what classes (handshapes) to detect. Such an unsupervised learning method can be useful in other newly compiled sign language or gestural corpora where there is no information regarding the different handshapes presented by the signers in the video. Additionally, an unsupervised learning method can be useful in other newly compiled sign language or gestural corpora where there is no information regarding the different handshapes presented by the signers in the video. We approached this as a clustering task: can we find groups of signs that were similar. Two different clustering methods have been tested: K-means and DBSCAN. The first clustering method was chosen for its simplicity as well as its fast implementation on the Python library that was utilized (namely scikit-learn). However, as the complexity of the data is unknown and it is case sensitive, it was decided to employ Density-Based Spatial Clustering of Applications with Noise (DBSCAN) as an alternative option. Given a set of points in some space, DBSCAN groups together points that are closely packed together, marking as outliers the ones that lie alone in low-density regions. This clustering method is one of the most common clustering algorithms.</p>
<p>Determining the optimal number of clusters (i.e. total number of expected handshapes) is a crucial issue in clustering methods such as K-means, which requires the user to specify the number of clusters  <em>k</em>  to be generated. The definition of clusters is done so that the total within-cluster sum of square (WSS) is minimized, hence, in this study the  <em>elbow method</em>  was utilized to estimate the number of clusters.</p>
<h2 id="351-hand-normalization">3.5.1 Hand Normalization</h2>
<p>Since the output of OpenPose contains the raw x, y pixel positions for the different finger joints, it is important to normalize them before applying the clustering method. To do so, the angle of the vector between the elbow and the wrist of the right hand is calculated. Subsequently, the coordinates of the finger joints positions are rotated to be in parallel on the horizontal axis and normalized so that their averaged location is at the origin. <a href="#figure01">Figure 1</a> shows the output of the overall normalization process. All experiments were conducted using one machine with a hexa-core processor (Intel Core i7-3930K) and 4GB RAM. The models are implemented using the Python libraries scikit-learn <sup id="fnref:32"><a href="#fn:32" class="footnote-ref" role="doc-noteref">32</a></sup> and Keras <sup id="fnref:33"><a href="#fn:33" class="footnote-ref" role="doc-noteref">33</a></sup> for their fast and easy implementation.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000510/resources/images/figure01.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000510/resources/images/figure01_hu9f8c230b7774d65c696b271ab067db91_167632_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000510/resources/images/figure01_hu9f8c230b7774d65c696b271ab067db91_167632_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000510/resources/images/figure01.png 800w" 
     class="landscape"
     ><figcaption>
        <p>Signer&rsquo;s hand normalization is done based on the angle between the horizontal axis and the vector of the elbow and wrist coordinates. The finger joints are rotated according to that angle in order to be in parallel to the horizontal axis and scaled so that their average location is at the origin.
        </p>
    </figcaption>
</figure>
<h2 id="4-results">4. Results</h2>
<p>The results section consists of three parts, the first part (Section 4.1) discusses the results of the analysis regarding the manual activation prediction. ﻿Section 4.2 discusses the results regarding the classification of one- and two-handed signing sequences. Last but not least, Section 4.3 presents the result regarding the handshape distribution using different clustering methods.</p>
<h2 id="41-tool-1-manual-activation">4.1 Tool 1: Manual Activation</h2>
<p>All classifiers performed adequately well, apart from the Support Vector Machines (AUC: 0.80) (<a href="#table01">Table 1</a>). Extreme Gradient Boosting (XGBoost) showed the highest AUC score at 0.92<sup id="fnref:34"><a href="#fn:34" class="footnote-ref" role="doc-noteref">34</a></sup> . <a href="#figure02">Figure 2</a> presents the ROC curve after a 10-fold cross-validation. The Artificial Neural Network was found to perform sufficiently well (AUC: 0.88). By exploring the importance of each feature on the prediction of the model we observe that the y and x pixel coordinates of the dominant (i.e. right) hand are on the top two positions (<a href="#table02">Table 2</a>).</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000510/resources/images/figure02.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000510/resources/images/figure02_hu21d0cfc5379dd73790752d0cff712ff0_47076_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/15/1/000510/resources/images/figure02_hu21d0cfc5379dd73790752d0cff712ff0_47076_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/15/1/000510/resources/images/figure02.jpg 742w" 
     class="landscape"
     ><figcaption>
        <p>10-fold cross validation of Extreme Gradient Boosting (XGBoost) classifier for the prediction of manual activation.
        </p>
    </figcaption>
</figure>
<p>The fact that the Artificial Neural Network turned out to be a less efficient approach than the XGBoost can be accounted to the small training data set. Typically, Neural Networks require a lot more training data than traditional machine learning algorithms. Additionally, designing a network that correctly encodes a domain specific problem is challenging. In most cases, competent architectures are only reached when a whole research community is working on those problems, without short-term time constraints. Fine-tuning such a network would require time and effort that reach beyond the scope of this study.</p>
<p>To account for multiple people signing in one frame, an extra module was added. This module creates bounding boxes around each person recognized by OpenPose, normalizes the positions of the body joints and runs the classifier. This process makes it possible to classify sign occurrences for multiple people irrespective of their positions in a frame (<a href="#figure04">Figure 4</a>).</p>
<p>Once all the frames have been classified, the &ldquo;cleaning up&rdquo; and annotation phase starts. A sign occurrence is annotated only if at least 12 consecutive frames have been classified as &ldquo;signing&rdquo;. That way we account for the false positive errors. This sets the stage for the annotation step. Using the PyMpi python library <sup id="fnref:35"><a href="#fn:35" class="footnote-ref" role="doc-noteref">35</a></sup> the classifications are translated into annotations that can be imported directly to ELAN, a standard audio and video annotation tool <sup id="fnref:36"><a href="#fn:36" class="footnote-ref" role="doc-noteref">36</a></sup>. <a href="#figure03">Figure 3</a> shows the result of the overall outcome.<br>
AUC scores of all the classifiers tested for manual activation prediction     <strong>Classifier</strong>    <strong>AUC score</strong>       Artificial Neural Network (ANN)  0.88      Random Forest (RF)  0.87      Support Vector Machines (SVM)  0.78      Extreme Gradient Boosting (XGBoost)  0.92        Importance of each feature during manual activation as predicted by the Extreme Gradient Boosting classifier     <strong>Weight</strong>    <strong>Feature</strong>       0.1410  Right wrist y      0.1281  Right wrist x      0.0928  Left wrist y      0.0917  Left wrist x      0.0717  Nose x      0.0658  Left shoulder x      0.0623  Left elbow y      0.0588  Right elbow y      0.0552  Nose y      0.0517  Left shoulder y      0.0482  Left elbow x      0.0482  Right elbow x      0.0482  Right shoulder y      0.0364  Right shoulder x   <br>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000510/resources/images/figure03.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000510/resources/images/figure03_hud19d8fc5eb56e01d83da67f1493ed22c_106531_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/15/1/000510/resources/images/figure03_hud19d8fc5eb56e01d83da67f1493ed22c_106531_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/15/1/000510/resources/images/figure03.jpg 785w" 
     class="landscape"
     ><figcaption>
        <p>Final output of the manual activation tool as seen in ELAN. Signing sequences have been given a &lsquo;signing&rsquo; gloss for readability. This attribute can be easily changed to produce empty glosses.
        </p>
    </figcaption>
</figure></p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000510/resources/images/figure04.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000510/resources/images/figure04_hu3e5a910261e8e8b97f893650b15e496e_130545_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/15/1/000510/resources/images/figure04_hu3e5a910261e8e8b97f893650b15e496e_130545_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/15/1/000510/resources/images/figure04.jpg 846w" 
     class="landscape"
     ><figcaption>
        <p>Bounding boxes are calculated in order to normalize the body joint coordinates for each signer. After this process the normalized coordinates are passed to the XGBoost classifier.[^3]
        </p>
    </figcaption>
</figure>
<h2 id="42-tool-2-number-of-hands">4.2 Tool 2: Number of Hands</h2>
<p>The second tool is responsible for not only recognizing whether a person in a video is signing but also if the sign is one or two-handed. We have previously hypothesized that this is a more complex task than the previous binary classification. Results on the accuracy of all the classifiers suggest that it is not as intricate as initially thought of; the higher the sliding window interval, the lower the accuracy of the model. As seen in <a href="#figure05">Figure 5</a> of all classifiers tested, Random forest had the highest accuracy at the sliding window interval of 1 frame at a time. Similarly to the previous experiment, a frame-to-frame prediction can produce the highest results.</p>
<p>Furthermore, the results regarding the Long-Short-Term-Memory networks (<a href="#figure06">Figure 6</a>) suggest that the highest accuracy can be achieved at a sliding window interval of 56 frames and at a hidden layer size of 8 units. However, such a high window interval contains more than one sign, as the average length of a sign is approximately 14 frames. This discrepancy can be caused due to the architectural properties of the LSTM network. The average length of the signs is too small for the network to converge. The LSTM units needed more timesteps in order to prevent overfitting to the data. This property in addition to the small dataset used to train the network caused this anomaly.</p>
<p>Although the tool performs well on predicting whether a sign is one- or two-handed (using a Random Forest classifier) there are cases were the output is not as expected. In particular, cases where there is a two-handed symmetrical sign produced, the tool fails to accurately predict the correct class. It is likely that such signs were under-presented in our data set, thus resulting in poor classification.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000510/resources/images/figure05.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000510/resources/images/figure05_hu86f0a799921fbbb5f9e1f9f72f5864c0_514441_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000510/resources/images/figure05_hu86f0a799921fbbb5f9e1f9f72f5864c0_514441_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000510/resources/images/figure05_hu86f0a799921fbbb5f9e1f9f72f5864c0_514441_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000510/resources/images/figure05.png 1444w" 
     class="landscape"
     ><figcaption>
        <p>Accuracy of different classifiers in various sliding window intervals in order to predict whether a sequence contains a one- or a two- handed sign or not signing at all. At a sliding window interval of 1 (a), Random Forest shows the highest accuracy.
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000510/resources/images/figure06.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000510/resources/images/figure06_hud42e51c08394b3b5f04d8b2a07e11ce3_184688_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/15/1/000510/resources/images/figure06_hud42e51c08394b3b5f04d8b2a07e11ce3_184688_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/15/1/000510/resources/images/figure06_hud42e51c08394b3b5f04d8b2a07e11ce3_184688_1200x0_resize_q75_box.jpg 1200w,/dhqwords/vol/15/1/000510/resources/images/figure06.jpg 1202w" 
     class="landscape"
     ><figcaption>
        <p>Accuracy of Long-Short-Term-Memory networks on the test set with different sliding window intervals (x) and hidden layer sizes (8,16,32,64,256).
        </p>
    </figcaption>
</figure>
<h2 id="43-tool-3-handshape">4.3 Tool 3: Handshape</h2>
<p>In order to understand the distribution of the different handshapes presented in a video, Principal Component Analysis (PCA) was utilized on all the normalized finger joint coordinates for all the frames at once (<a href="#figure07">Figure 7a</a>). This process allows us to reduce the dimensionality of the data while retaining as much as possible of the variance in the dataset. Each multidimensional array of the extracted finger joints positions, for each frame, has been reduced to a single x,y coordinate. The result already suggests that there are regions dense enough to be considered different clusters. The utilized elbow method suggested that at k=5 the highest classification could be achieved (<a href="#figure07">Figure 7b</a>). On the video sample used in our study that number seemed to reflect the proper amount of discerned handshapes. However, as OpenPose captures all the finger configurations in each frame it is at the linguist’s discretion to decide on when a handshape is significantly different from another. Additionally, experiments to optimize the hyperparameters (eta, min samples and leaf size) for the DBSCAN failed to create an accurate clustering (<a href="#figure07">Figure 7c</a>). Subsequently, the module creates annotation slots for the different handshapes in the video and adds an overlay containing the number of the predicted cluster on each frame.</p>
<p>However, special consideration must be given to the overall handshape recognition module. Although the hand normalization process prepares the finger joints adequately enough to be used in the clustering methods, it fails to account for hands perpendicular to the camera’s point of view. Additionally, handshapes that are similar to each other but are rotated towards or outwards of the signer’s body will most probably clustered differently. Some of these limitations can be solved by manually editing the cluster numbers prior to the annotation process.</p>
<p>In its current form, this method can already be used to either fully annotate the handshapes in a video sample or be used in different samples and treated as weakly annotated data in order to be used in other handshape classifiers similarly to Koller’s et al. study <sup id="fnref:37"><a href="#fn:37" class="footnote-ref" role="doc-noteref">37</a></sup>.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000510/resources/images/figure07.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000510/resources/images/figure07_hu9df8c0dfd0aa5f4d65e16e9a6c50ccd4_604032_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000510/resources/images/figure07_hu9df8c0dfd0aa5f4d65e16e9a6c50ccd4_604032_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000510/resources/images/figure07_hu9df8c0dfd0aa5f4d65e16e9a6c50ccd4_604032_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000510/resources/images/figure07.png 1497w" 
     class="landscape"
     ><figcaption>
        <p>Visualizations produced by the handshape recognition module. Principal component analysis (a) can be used to reduce the dimensionality of the finger joints coordinates. Two clustering methods, namely K-means (b) and DBSCAN (c), can be used to detect the different handshapes presented in a video sample.
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000510/resources/images/figure08.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000510/resources/images/figure08_huf784d90c38fda426620b083bed28bea8_784951_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000510/resources/images/figure08_huf784d90c38fda426620b083bed28bea8_784951_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000510/resources/images/figure08_huf784d90c38fda426620b083bed28bea8_784951_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000510/resources/images/figure08.png 1464w" 
     class="landscape"
     ><figcaption>
        <p>Different handshapes recognized by the handshape recognition module using K-means.
        </p>
    </figcaption>
</figure>
<h2 id="5-discussion">5. Discussion</h2>
<p>In this study we have presented three different tools that can be used to assist the annotation process of sign language corpora. The first tool proved to be robust on the task of classification of manual activation even when the corpora are noisy, of poor quality and most importantly containing more than one signer. This eliminates the preprocessing stage that many sign language corpora have to endure where either dedicated cameras per signer are utilized or manually cropping the original video. As a result, a more natural filming process can be applied. One limitation regarding our methodology is that at its current state is not possible to account for individual sign temporal classification. Reaching such level would require to fuse additional information into the training sets which in most cases might be language specific. However, it is possible to get a per sign prediction when the &ldquo;number of hands involved&rdquo; feature changes.</p>
<p>The most striking observation to emerge from our methodology is that there is no necessity of having massive training sets for the classification of low-level features (such as manual activation and number of hands involved). In contrast to earlier studies using neural networks for sign language recognition <sup id="fnref2:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>  <sup id="fnref3:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>  <sup id="fnref1:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup>, we used a proportionally smaller dataset. Additionally, this is the first time to our knowledge where corpora outside of studio conditions have been used to train and most importantly test models and tools for sign language automatic annotation. Furthermore, such findings can be applied in other studies as well. It is a common misconception that only large data sets can be used for analysis. Such a trend, although true for deep learning purposes, can be daunting for digital humanities researchers without in depth data science knowledge. In our study, we have shown that even with a small and noisy dataset of visual materials, researchers can use machine learning algorithms to effectively extract meaningful information. Our testing in West African Sign Language corpora showed that such frameworks can work effectively in different skin color participants lifting possible bias by previously developed algorithms.</p>
<p>There are few limitations regarding our methodologies, particularly with respect to the handshape distribution module. Low quality video and consequently framerate seem to affect the robustness of OpenPose. As a result, finger joint prediction can be noisy and of low confidence. Additionally, we observed that finger joints could not be predicted when the elbow was not visible in the frame, and thus, losing that information. In our study we treated all predicted joints equally but it is necessary for future research to include the prediction confidence interval as an additional variable. Furthermore, on the current output from OpenPose it is difficult to extract the palm orientation attribute meaning that differently rotated handshapes might result in the same cluster. Future research will concentrate on fixing that issue as well as creating an additional tool for the annotation of this feature.</p>
<p>In the sign language domain, researchers can use our tools to recognize the times of interest and basic phonological features on newly compiled corpora. Additionally, such extracted features can be further used to measure variation on different sign languages or signers, for example, to measure the distribution of one- and two-handed signs or particular handshapes. Moreover, other machine or deep learning experiments can benefit from our tools by using them to extract only the meaningful information from the corpora during the data gathering process, thus reducing possible noise in the datasets. Our tools can also be used towards automatic gloss suggestion. A future model can search only the signing sequences predicted by our tool rather than &ldquo;scanning&rdquo; the whole video corpus, and consequently making it more efficient.</p>
<p>Outside the sign language domain, the results have further strengthened our confidence that pre-trained frameworks can be used to help extract meaningful information from audio-visual materials. In particular, OpenPose can be a useful asset when human activity needs to be tracked and recognized in a video without the need of special hardware setups. Its accurate tracking allows researchers to use it in videos compiled outside studio conditions. As a result, studies in the audio-visual domain can benefit from community-created materials involving natural and unbiased communication. Using our tools, these study areas can analyze and classify human activity beyond the sign language discipline in large scale cultural archives or specific domains such as gestural research, dance or theater and cinema related studies, to name but a few. For example, video analyses in gestural and media studies can benefit from such an automatic approach to find relevant information regarding user-generated data on social media and other popular platforms.</p>
<p>Finally, due to the cumbersome installation process of OpenPose for the majority of SL linguists, we have decided to implement part of the tools in an online collaborative environment on a cloud service provided by Google (i.e. Google Colab). In this environment a temporary instance of OpenPose can be installed along with our developed python modules. In a simple step-based manner, the researcher can upload the relevant videos and download the automatically generated annotation files. Find the link to this Colab in the footnote below <sup id="fnref:38"><a href="#fn:38" class="footnote-ref" role="doc-noteref">38</a></sup> . Additionally, we have a created another environment for re-training purposes. By doing so, the researcher can re-train the models on his or her particular data and ensure the aforementioned accuracy on them<sup id="fnref:39"><a href="#fn:39" class="footnote-ref" role="doc-noteref">39</a></sup> .</p>
<h2 id="conclusion">Conclusion</h2>
<p>To summarise, glossing sign language corpora is a cumbersome and time-consuming task. Current approaches to automatize parts of this process need special video recording devices (such as Microsoft Kinect), large amount of data in order to train deep learning architectures to recognize a set of signs and can be prone to skin-color bias. In this study we explored the use of a pre-trained pose estimation framework created by Cao et al. <sup id="fnref3:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup> in order to create three tools and methods to predict sign occurrences, number of hands involved, and handshapes. The results show that four minutes of annotated data are adequate enough to train a classifier (namely XGBoost) to predict whether one or more persons are signing or not as well as the number of hands used (using Random Forest). Additionally, we examined the use of K-means and DBSCAN as clustering methods to detect the different handshapes presented in the video. Because of the low complexity of the finger joint data extracted from the pose estimation library, K-means was found to produce accurate results.</p>
<p>The significance of this study lies in the fact that the tools created do not rely on specialized cameras nor require large amount of information to be trained. Additionally, they can be easily used by researchers without developing skills and adjusted to work in any kind of sign language corpus irrespective of its quality or the number of people in the video. Finally, they have the potential to be extended and used in other audio-visual material that involve human activity such as gestural corpora.</p>
<h2 id="appendix-a">Appendix A</h2>
<p>The input shape of the LSTM network trained to recognize the  “number of hands”  (Tool 2) feature is a three dimensional array that can be defined as: [samples × timesteps × features] where features is a 2 dimensional array of [21 × 2] containing the pixel x,y coordinates of the finger joints and timesteps are the sliding window interval. Two Dense layers of 7 and 1 unit respectively follow the previous Bidirectional LSTM layer. The activation function used is  “ReLU”  and the dropout rate at 0.4. The architecture that produced the highest results for this network can be seen in <a href="#figure09">Figure 9</a>.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000510/resources/images/figure09.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000510/resources/images/figure09_hu8c2954dd9826d997ee23f03b69e42216_9772_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000510/resources/images/figure09_hu8c2954dd9826d997ee23f03b69e42216_9772_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000510/resources/images/figure09.png 176w" 
     class="portrait"
     ><figcaption>
        <p>Architecture of the Long-Short-Term-Memory network trained for Tool 2.
        </p>
    </figcaption>
</figure>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Dreuw, P. and Ney, H. (2008).  “Towards automatic sign language annotation for the ELAN tool” .  <em>LREC Workshop: Representation and Processing of Sign Languages</em> . Marrakech, Morocco, pp. 50–53.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Johnston, T. (2010).  “From archive to corpus: Transcription and annotation in the creation of signed language corpora” .  <em>International Journal of Corpus Linguistics</em> , 15(1): 106–31 doi:10.1075/ijcl.15.1.05joh. <a href="http://www.jbe-platform.com/content/journals/10.1075/ijcl.15.1.05joh">http://www.jbe-platform.com/content/journals/10.1075/ijcl.15.1.05joh</a> (accessed 19 May 2020).&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Agha, R. A. A. R., Sefer, M. N. and Fattah, P. (2018).  “A Comprehensive Study on Sign Languages Recognition Systems Using (SVM, KNN, CNN and ANN)” .  <em>First International Conference on Data Science, E-Learning and Information Systems</em> . (DATA ’18). Madrid, Spain: ACM, pp. 28:1–28:6.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Cao, Z., Simon, T., Wei, S.-E. and Sheikh, Y. (2017).  “Realtime Multi-person 2D Pose Estimation Using Part Affinity Fields” .  <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em> . Honolulu, HI: IEEE, pp. 1302–10.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref3:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>Pigou, L., Dieleman, S., Kindermans, P.-J. and Schrauwen, B. (2015).  “Sign Language Recognition Using Convolutional Neural Networks” . In Agapito, L., Bronstein, M. M. and Rother, C. (eds),  <em>Computer Vision - ECCV 2014 Workshops</em> . (Lecture Notes in Computer Science). Cham: Springer International Publishing, pp. 572–78.&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref3:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>Aitpayev, K., Islam, S. and Imashev, A. (2016).  “Semi-automatic annotation tool for sign languages” .  <em>2016 IEEE 10th International Conference on Application of Information and Communication Technologies (AICT)</em> . Baku, Azerbaijan: IEEE, pp. 1–4.&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>Kumar, N. (2017).  “Motion trajectory based human face and hands tracking for sign language recognition” .  <em>2017 4th IEEE Uttar Pradesh Section International Conference on Electrical, Computer and Electronics (UPCON)</em> . Mathura: IEEE, pp. 211–16.&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>Andriluka, M., Pishchulin, L., Gehler, P. and Schiele, B. (2014).  “2D Human Pose Estimation: New Benchmark and State of the Art Analysis” .  <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em> .&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>Lin, T.-Y., Maire, M., Belongie, S., Bourdev, L., Girshick, R., Hays, J., Perona, P., Ramanan, D., Zitnick, C. L. and Dollár, P. (2014).  <em>Microsoft COCO: Common Objects in Context</em> .&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>Fang, G., Gao, W. and Zhao, D. (2004).  “Large Vocabulary Sign Language Recognition Based on Fuzzy Decision Trees” .  <em>Systems, Man and Cybernetics, Part A: Systems and Humans, IEEE Transactions On</em> , 34: 305–14 doi:10.1109/TSMCA.2004.824852.&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>Kelly, D., Reilly Delannoy, J., Mc Donald, J. and Markham, C. (2009).  “A framework for continuous multimodal sign language recognition” .  <em>Proceedings of the 2009 International Conference on Multimodal Interfaces</em> . pp. 351–358.&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p>Cooper, H. and Bowden, R. (2007).  “Large Lexicon Detection of Sign Language” . In Lew, M., Sebe, N., Huang, T. S. and Bakker, E. M. (eds),  <em>Human–Computer Interaction</em> . (Lecture Notes in Computer Science). Springer Berlin Heidelberg, pp. 88–97.&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13">
<p>Zhang, C., Yang, X. and Tian, Y. (2013).  “Histogram of 3D Facets: A characteristic descriptor for hand gesture recognition.”    <em>2013 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)</em> . pp. 1–8.&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14">
<p>Roussos, A., Theodorakis, S., Pitsikalis, V. and Maragos, P. (2012).  “Hand Tracking and Affine Shape-Appearance Handshape Sub-units in Continuous Sign Language Recognition” . In Kutulakos, K. N. (ed),  <em>Trends and Topics in Computer Vision</em> , vol. 6553. Berlin, Heidelberg: Springer Berlin Heidelberg, pp. 258–72.&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:15">
<p>Buehler, P., Zisserman, A. and Everingham, M. (2009).  “Learning sign language by watching TV (using weakly aligned subtitles)” .  <em>2009 IEEE Conference on Computer Vision and Pattern Recognition</em> . pp. 2961–68.&#160;<a href="#fnref:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:16">
<p>Farhadi, A., Forsyth, D. and White, R. (2007).  “Transfer Learning in Sign language” .  <em>2007 IEEE Conference on Computer Vision and Pattern Recognition</em> . Minneapolis, MN, USA, pp. 1–8.&#160;<a href="#fnref:16" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:17">
<p>Starner, T., Weaver, J. and Pentland, A. (1998).  “Real-time American sign language recognition using desk and wearable computer based video” .  <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em> , 20(12): 1371–75.&#160;<a href="#fnref:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:18">
<p>Moeslund, T. B., Hilton, A. and Krüger, V. (2006).  “A survey of advances in vision-based human motion capture and analysis” .  <em>Computer Vision and Image Understanding</em> , 104(2): 90–126.&#160;<a href="#fnref:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:19">
<p>Felzenszwalb, P. F. and Huttenlocher, D. P. (2005).  “Pictorial Structures for Object Recognition” .  <em>International Journal of Computer Vision</em> , 61(1): 55–79.&#160;<a href="#fnref:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:20">
<p>Ramanan, D., Forsyth, D. A. and Zisserman, A. (2007).  “Tracking People by Learning Their Appearance” .  <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em> , 29(1): 65–81.&#160;<a href="#fnref:20" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:21">
<p>Sivic, J., Zitnick, C. L. and Szeliski, R. (2006).  “Finding people in repeated shots of the same scene” .  <em>British Machine Vision Conference 2006</em> , vol. 3. Edinburgh: British Machine Vision Association, pp. 909–18.&#160;<a href="#fnref:21" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:22">
<p>Eichner, M. and Ferrari, V. (2009).  “Better appearance models for pictorial structures” .  <em>British Machine Vision Conference 2009</em> . London, UK: British Machine Vision Association, pp. 3.1-3.11.&#160;<a href="#fnref:22" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:23">
<p>Eichner, M., Marin-Jimenez, M., Zisserman, A. and Ferrari, V. (2012).  “2D Articulated Human Pose Estimation and Retrieval in (Almost) Unconstrained Still Images” .  <em>International Journal of Computer Vision</em> , 99(2): 190–214.&#160;<a href="#fnref:23" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:24">
<p>Johnson, S. and Everingham, M. (2009).  “Combining discriminative appearance and segmentation cues for articulated human pose estimation” .  <em>2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops</em> . pp. 405–12.&#160;<a href="#fnref:24" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:25">
<p>Sapp, B., Jordan, C. and Taskar, B. (2010). Adaptive pose priors for pictorial structures.  <em>2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</em> . pp. 422–29.&#160;<a href="#fnref:25" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:26">
<p>Yang, Y. and Ramanan, D. (2011).  “Articulated pose estimation with flexible mixtures-of-parts” .  <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em> . pp. 1385–92.&#160;<a href="#fnref:26" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:27">
<p>Charles, J., Pfister, T., Everingham, M. and Zisserman, A. (2014).  “Automatic and Efficient Human Pose Estimation for Sign Language Videos” .  <em>International Journal of Computer Vision</em> , 110(1): 70–90.&#160;<a href="#fnref:27" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:28">
<p>Forster, J., Schmidt, C., Hoyoux, T., Koller, O., Zelle, U., Piater, J. and Ney, H. (2012).  “RWTH-PHOENIX-Weather: A Large Vocabulary Sign Language Recognition and Translation Corpus” . Istanbul, Turkey, pp. 3785–3789.&#160;<a href="#fnref:28" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:29">
<p>Nyst, V. (2012).  <em>A Reference Corpus of Adamorobe Sign Language</em> . A digital, annotated video corpus of the sign language used in the village of Adamorobe, Ghana.&#160;<a href="#fnref:29" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:30">
<p>Nyst, V., Magassouba, M. M. and Sylla, K. (2012).  <em>Un Corpus de reference de la Langue des Signes Malienne II</em> . A digital, annotated video corpus of local sign language use in the Dogon area of Mali.&#160;<a href="#fnref:30" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:31">
<p>Linear Regression (LR), Decision Trees (CART), Support Vector Machines (SVM), Random Forest (RF) and Gradient Boosting (GBM).&#160;<a href="#fnref:31" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:32">
<p>Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., et al. (2011).  “Scikit-learn: Machine Learning in Python” .  <em>Journal of Machine Learning Research</em> , 12: 2825–2830.&#160;<a href="#fnref:32" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:33">
<p>Chollet, F. and others (2015).  <em>Keras</em> . <a href="https://keras.io">https://keras.io</a>.&#160;<a href="#fnref:33" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:34">
<p>eta: 0.23, gamma: 3, lambda: 2, max. delta step: 4, max. depth: 37 and min. child weight: 4&#160;<a href="#fnref:34" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:35">
<p>Lubbers, M. and Torreira, F. (2013).  <em>A Python Module for Processing ELAN and Praat Annotation Files: Dopefishh/Pympi</em> . Python <a href="https://github.com/dopefishh/pympi">https://github.com/dopefishh/pympi</a>.&#160;<a href="#fnref:35" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:36">
<p>Sloetjes, H. and Wittenburg, P. (2008).  “Annotation by category - ELAN and ISO DCR. Marrakech, Morocco” , p. 5 <a href="https://tla.mpi.nl/tools/tla-tools/elan/">https://tla.mpi.nl/tools/tla-tools/elan/</a>.&#160;<a href="#fnref:36" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:37">
<p>Koller, O., Ney, H. and Bowden, R. (2016).  “Deep Hand: How to Train a CNN on 1 Million Hand Images When Your Data is Continuous and Weakly Labelled” .  <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em> . Las Vegas, NV, USA: IEEE, pp. 3793–802.&#160;<a href="#fnref:37" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:38">
<p><a href="https://colab.research.google.com/drive/1HwXo2Tk4uHizGTpRg-simMDMD4wPOzmA">https://colab.research.google.com/drive/1HwXo2Tk4uHizGTpRg-simMDMD4wPOzmA</a>.&#160;<a href="#fnref:38" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:39">
<p><a href="https://colab.research.google.com/drive/10H2uxY7p59GrrDC5z85RugYB6PfQZs7Y">https://colab.research.google.com/drive/10H2uxY7p59GrrDC5z85RugYB6PfQZs7Y</a>.&#160;<a href="#fnref:39" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry><entry><title type="html">Transdisciplinary Analysis of a Corpus of French Newsreels: The ANTRACT Project</title><link href="https://rlskoeser.github.io/dhqwords/vol/15/1/000523/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://rlskoeser.github.io/dhqwords/vol/15/1/000523/</id><author><name>Jean Carrive</name></author><author><name>Abdelkrim Beloued</name></author><author><name>Pascale Goetschel</name></author><author><name>Serge Heiden</name></author><author><name>Antoine Laurent</name></author><author><name>Pasquale Lisena</name></author><author><name>Franck Mazuet</name></author><author><name>Sylvain Meignier</name></author><author><name>Bénédicte Pincemin</name></author><author><name>Géraldine Poels</name></author><author><name>Raphaël Troncy</name></author><published>2021-03-05T00:00:00+00:00</published><updated>2021-03-05T00:00:00+00:00</updated><content type="html"><![CDATA[<h2 id="1-implementing-a-transdisciplinary-research-apparatus-on-a-film-archive-collection-opportunities-and-challenges">1. Implementing a Transdisciplinary Research Apparatus on a Film Archive Collection: Opportunities and Challenges</h2>
<p>The ANTRACT<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>  project brings together research organizations with a dual historical and technological perspective, hence the reference to the transdisciplinary in the project&rsquo;s name. It applies to a collection of 1262 newsreels (mostly black and white footage) shown in French movie theaters between 1945 and 1969. These programs were produced by  <em>Les Actualités Françaises</em>  newsreel company during the French  <em>Trente Glorieuses</em>  era. The project develops automated tools well suited to analyze these documents: automatic speech recognition, image classification, facial recognition, natural language processing, and text mining. This software is used to produce metadata and to help organize media files and documentation resources (i.e. titles, summaries, keywords, participants, etc.) into a manageable and coherent corpus usable within a dedicated online platform.</p>
<p>Working together on these newsreels divided into 20,232 news reports, ANTRACT historians and computer scientists collaborate to optimize the research on large audiovisual corpora through the following questions:</p>
<ul>
<li>What is the best technological approach to the systematic and exhaustive study of a multimedia archive collection?</li>
<li>What instruments can compile, analyze and crosscheck the data extracted from such documents?</li>
<li>Can these extracted data be combined and integrated into versatile user interfaces?</li>
<li>Can they provide new opportunities to humanities research projects through their assistance in the processing of numerous multi-format sources?</li>
</ul>
<p>In order to implement a strong cooperation between AI experts and history scholars <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>, the key objective of the project is to provide scholars and media professionals working on extensive collections of film archives with an innovative research methodology fit to address the technological and historical questions raised by this particular corpus.</p>
<p><strong>From a technological perspective</strong> , the goal is to adapt automatic analysis tools to the specificity of the  <em>Actualités Françaises</em>  corpus, i.e. its historical context, vocabulary, image type. Adapting the language models used by the automatic transcription tools with the help of the typescripts of voice overs underlines this orientation. As a film collection including footage, sound and text produced more than half a century ago,  <em>Les Actualités Françaises</em>  corpus presents an unprecedented challenge to instruments specialized in audiovisual content extraction and identification. Far from separately considering a social and cultural history of cinema on the one hand, and the use of automatic analysis tools on the other hand, the project aims to link the two. Thus, a good understanding of the technical conditions for recording the audio leads to improved audio recognition. Shot in black and white with limited equipment and often under difficult filming conditions, these old newsreels do not meet the quality standards set by the high definition video and audio recordings feeding today&rsquo;s image and speech recognition algorithms. Moreover, several film reels of the collection digitized under high compression formats show pixelated images that cannot be processed by analysis programs and some of the commentary typescripts display printing defects caused by the typewriters used for their production.</p>
<p>Along with these material obstacles comes the problem raised by the transfiguration of film content over time. This is the case for leading figures regularly filmed by the company&rsquo;s cameramen throughout its 24 years of activity. It is also the case for the recurring topographical data caught on their film. The automatic identification of these ever-changing elements recorded on monochromatic footage requires a considerable amount of resources. As part of this process, ANTRACT historians have selected a sample of the most distinctive representations of notable characters present in  <em>Les Actualités Françaises</em>  newsreels in order to build a series of extraction models.</p>
<p><strong>From an historical perspective</strong> , ANTRACT aims to approach topics beyond the notion of newsreels as a wartime media subjected to state censorship and political ambitions <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>  <sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>  <sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>  <sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>. In the wake of existing studies, one of its primary objectives is to extend the historical scope of the cinematographic press to question its role as a vector of social, political and cultural history shaping the opinion of the public during the second half of the 20th century <sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>  <sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>  <sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup>  <sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup>  <sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>  <sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>  <sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup>. This series of cinematographic documents is not the only legacy left by a newsreel company which witnessed world history from the liberation of France to the late 1960&rsquo;s. The dope sheets filled out by its cameramen, the written commentaries of its journalists and the records left by its management give us rare insight into the content of a film collection as well as its production process. Despite its historical value,  <em>Les Actualités Françaises</em>  corpus has eluded a thorough examination of its entire content. Scattered across different inventories, the numerous films, audio records and typescripts produced by the newsreel company have forestalled such a project. In this regard, the challenge presented by an exhaustive study of  <em>Les Actualités Françaises</em>  is similar to those of other abundant multi-format collections and inspires a recurring question regarding their approach: how can one identify and index thousands of hours of film archives associated with hundreds of text files produced over an extended period of time? The tools developed by the consortium partners working on the project are intended to cast a new light on the French company newsreels through the combined treatment of data extracted from its whole collection and correlatively studied on the Okapi and TXM platforms. This apparatus should open new semantic fields previously overlooked by the fragmentary research conducted on specific inventories of the company records. Focused on film content, the project is also committed to scrutinize the production process and the different trades involved in the making of  <em>Les Actualités Françaises</em>  newsreels emphasizing the political and economic background of a company controlled by a democratic state. Underlining the notion that media participate in events <sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup>, this dual analysis - both technological and historical - will be completed with the study of the public reception of these weekly journals in light of its request patterns, i.e. audience expectation for sensational and exotic news and its interest in the daily life of renowned figures <sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup>.</p>
<p>Through audio and video analysis tools dedicated to corpus building and enrichment (section 2) and platforms for historical interactive analysis (section 3), this article presents the results from the first phase of the project, which sets the focus on the technological side of the research, specifically its data processing apparatus and tools. Nevertheless, historians are involved in most of these computational preliminary steps, by contributing to the implementation and testing tasks. At the same time, we explore temporary results of historical investigations, while the full potential for historical studies will be developed in the forthcoming second phase of the project that will be addressed in a follow-up article.</p>
<h2 id="2-corpus-building-and-enrichment">2. Corpus building and enrichment</h2>
<h2 id="21-organization-of-the-video-corpus-with-automatic-content-analysis-technologies">2.1 Organization of the video corpus with automatic content analysis technologies</h2>
<p>Automatic content analysis technologies are used to obtain the most consistent, complete and homogeneous corpus as possible, allowing historians to easily search and navigate through the documents (digitized films, documentation notes and typescripts). When considering that the whole archive would not be relevant, a preliminary step was to realize that for some tasks, we had to define how our corpus would be composed and structured. One cannot just input the data into the computer and see what happens. For instance, textometric analysis would be hindered if all the available videos were kept, because of numerous duplicates which would artificially inflate word frequencies. Duplicates could be due to either multiple copies of a single news report, or to the use of the same report in several regional editions. As a collaborative decision involving newsreel experts, corpus analysis researchers, and historians, ANTRACT&rsquo;s main corpus was restricted to the collection of all national issues of  <em>Les Actualités Françaises</em>  newsreels, each issue being composed of topical report sub-units. Then, the next goal was:</p>
<p>to get a corpus made of exactly one digital video file by edition (which was a requisite condition for TXM data import, see Section 3.1),  to get archival descriptions of the reports temporally linked to these files, as an edition is made of a succession of reports.</p>
<p>This led us to take the following actions:</p>
<p>physically segment video files initially coming from the digitization of film reels, so that each file contains exactly one edition, starting at timecode 0.  keep only archival descriptions linked to either one edition or one report included in one of the editions, namely  “summary”  and  “report”  archival descriptions. Thus, archival descriptions corresponding to other content, such as rushes or unused material, called  “isolated”  archival descriptions were discarded. Around 10,700 archival descriptions have thus been kept in this first version of the corpus.</p>
<p>The remaining of this section explains how automatic analysis has been used to temporally synchronize archival descriptions with digital video files.</p>
<p><strong>Segmentation of reports</strong> . Each one of the 1,200 editions of the newsreels corresponds to more than one digital video file, either because several digitized copies of one given edition exist in the collection, or because the film has been digitized several times, for quality reasons for instance. When they exist, timecodes of archival descriptions may refer to one or the other digital video file. One objective is to get all archival descriptions of one edition referring to the same video file, with timecodes. About 9,500 out of 10,700 archival descriptions have timecodes referring to the video file of the whole edition, which left around 1,200 archival descriptions to manage. A report with timecode is called  “segmented” . One important step is to segment each edition into its constitutive reports, by detecting report boundaries. In most cases, reports are separated by black images, easily detected by simple image analysis methods (the  <em>ffmpeg</em>  video library offers an efficient  “blackdetect”  option for instance). Reports may also be separated by sequences of a few frames to a few seconds of a motion blur shot by a camera, used as a syntactic punctuation. In some cases, when these sequences are long enough, they can be detected as a simple threshold on the horizontal dimension of the optical flow, computed with existing algorithms such as OpenCV <sup id="fnref:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup>. A more robust detection method is still under development using machine learning algorithms.</p>
<p><strong>Transfer of timecodes.</strong>  When timecodes refer to a video file different from the main video file, timecodes on the main file may be computed using copy detection techniques. The principle is illustrated by <a href="#figure01">Figure 1</a>. In the figure, reports on  “Rugby”  and  “Kennedy&rsquo;s visit”  (from the edition of May 31st, 1961) refer to two video files, both distinct from the video file corresponding to the whole edition. To identify the location of the reports within the main video file, we used the audio and video copy detection method based on fingerprinting methods developed at INA <sup id="fnref:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup>, eventually allowing the transfer of timecodes for more than 800 reports.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000523/resources/images/figure01.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000523/resources/images/figure01_hue561da95d8d04672ea2b29ac397bd1ba_37030_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000523/resources/images/figure01_hue561da95d8d04672ea2b29ac397bd1ba_37030_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000523/resources/images/figure01_hue561da95d8d04672ea2b29ac397bd1ba_37030_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000523/resources/images/figure01.png 1209w" 
     class="landscape"
     ><figcaption>
        <p>Transfer of archival description timecodes.
        </p>
    </figcaption>
</figure>
<p><strong>Timecoding reports using transcripts.</strong>  We tried to identify the temporal boundaries of the remaining 400  <em>unsegmented</em>  remaining reports by comparing the text coming from corresponding archival descriptions (title + summary + keywords for instance), with the automatic speech transcription (ASR) of segments of the video file not already corresponding to one report (see Section 2.3). Simple similarity text measures such as the Jaccard distance, or  <em>ratio</em>  metrics in the Fuzzywuzzy Python package give encouraging but not entirely satisfying results. We plan to use a corpus-specific  <em>TF-IDF</em>  measure, or embedding methods such as word2vec or BERT in the future.</p>
<h2 id="22-typescripts-from-page-scans-to-structured-textual-data">2.2 Typescripts: from page scans to structured textual data</h2>
<p>Typescripts of the voice-overs have been linked to books and typescripts of each edition and separated with pages giving the summary of the edition (see <a href="#figure02">Figure 2</a>). This represents around 9,000 pages. At the beginning of the project, these documents were scanned in a good quality format (TIFF, color, 400 dpi). An optical character recognition (OCR) tool has thus been applied (Google Vision API in  “Document”  mode), giving spatially-located digital texts.</p>
<p>Once digitized, typescripts have to be separated from summaries. In order to achieve that, an automatic classifier has been trained by specializing the state-of-the-art Inception V3 classifier <sup id="fnref:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup> with a few manually chosen examples. This gave about  <strong>2,</strong> 600 pages of summaries and 6,400 pages of voice-overs.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000523/resources/images/figure02.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000523/resources/images/figure02_hu340d4946d4688e02ca6d1cdb68465cc0_3170189_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000523/resources/images/figure02_hu340d4946d4688e02ca6d1cdb68465cc0_3170189_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000523/resources/images/figure02_hu340d4946d4688e02ca6d1cdb68465cc0_3170189_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000523/resources/images/figure02.png 1406w" 
     class="landscape"
     ><figcaption>
        <p>Typescripts of voice-overs and summaries.
        </p>
    </figcaption>
</figure>
<p><strong>Spatial and temporal alignment of transcripts</strong> . The objective of this alignment is to associate each report with the corresponding section of the typescripts. The available metadata allows processing this alignment year by year. This operation is done in two stages, by using on the one hand the result of the automatic speech transcription of the voice-over from the video files, and by using on the other hand the result of the OCR of the typescripts. The first step is done by minimizing a comparison measure between strings in order to find for each subject the corresponding typescripts page. The  <em>partial ratio</em>  method of the Fuzzywuzzy Python package allows looking for a partial inclusion of the speech-to-text into the OCR. Since topics and pages are approximately chronological, exhaustive searching is not required. The second step consists in spatially locating the text of the voice-over in the corresponding typescript page. For that, we use the alignment given by the Dynamic Time Warping algorithm (DTW), slightly modified to overcome the anchoring at the ends of the found path. The typescript area thus identified in the output of the OCR makes it possible to obtain the spatial coordinates of the commentary in the typewritten page. However, the method used does not allow locating transcripts overlapping over two pages. Additional treatment should be considered, for instance in order to get aligned text units for textometric analysis (see section 3.1).</p>
<h2 id="23-automatic-audio-analysis">2.3 Automatic audio analysis</h2>
<p>The work on the audio part consists in detecting the speakers, transcribing speech into words (ASR) and detecting named entities (NE) using the systems we have developed for contemporary radio and television news.</p>
<p>Audio analysis of an old data set is an interesting challenge for automatic analysis systems. The recording devices used between 1945 and 1969 are very different from today&rsquo;s analog or digital devices. 35-mm films, which contain both sound and image, deteriorated before being digitized in the 2000s. Moreover, the acoustic and language models are generally trained on data produced between 1998 and 2012. This 50-year time gap has consequences on the system&rsquo;s performance.</p>
<p>Technically, acoustic models for ASR and speakers were trained on about 300 hours drawn from several sources of French TV and radiophonic broadcast news<sup id="fnref:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup>  with manual transcripts. The ASR language models were trained on these manual transcripts, French newspapers, news websites, Google news and the French GigaWord corpus, for a total of 1.6 billion words. The vocabulary of the language model contains the 160k most frequent words. The NE models were trained only on a subset of manual transcripts<sup id="fnref:20"><a href="#fn:20" class="footnote-ref" role="doc-noteref">20</a></sup> .</p>
<p>Prior to the transcription process, the signal is cut into homogeneous speech segments and grouped by speakers. We refer to this process as the Speaker Diarization task. Speaker Diarization is first applied at the edition level, where each video record is separately processed. Then, the process is applied at the collection level, over all the 1,200 editions, in order to link the recurrent speakers. The system is based on the LIUM S4D toolkit <sup id="fnref:21"><a href="#fn:21" class="footnote-ref" role="doc-noteref">21</a></sup>, which has been developed to provide homogeneous speech segments and accurate segment boundaries. Purity and coverage of the speaker clusters are also one of the main objectives. The system is composed of acoustic metric-based segmentation and clustering followed by an i-vector-based clustering applied to both edition and collection levels.</p>
<p>The ASR system is developed using the Kaldi Speech Recognition Toolkit <sup id="fnref:22"><a href="#fn:22" class="footnote-ref" role="doc-noteref">22</a></sup>. Acoustic models are trained using a Deep Neural Network which can effectively deal with long temporal contexts with training times comparable to standard feed-forward DNNs (chain-TDNN <sup id="fnref:23"><a href="#fn:23" class="footnote-ref" role="doc-noteref">23</a></sup>). Generic 3 and 4-gram language models, which allow users to compute the probability of emitting one word knowing a history of 2 or 3 words, were also trained and used during decoding. To help the reading, two sequence labeling systems (Conditional Random Field models) have been trained over manual transcripts to add punctuation and upper-case letters respectively.</p>
<p>The NE system, based on the <a href="https://github.com/XuezheMax/NeuroNLP2">NeuroNLP</a> toolkit, helps the text analysis. The manually annotated transcripts are used to train a text-to-text sequence labeling system. The system detects eight main entity types: amount, event, function, location, organization, person, product and time.</p>
<p>ASR was performed on the full collection of 1,200 national editions in order to feed Okapi and TXM platforms for historians&rsquo; analyses (see Section 3): about 300 hours of video, resulting in more than 1.5 million words. A subset of 12 editions from 1945 to 1969 were manually transcribed to evaluate the audio analysis systems. Due to the 50-year time gap, human annotators had some difficulties with the spelling of NE, especially regarding people and foreign NE. Thanks to Wikipedia and INA thesaurus, most of NEs have been checked. However, speakers are very hard to identify. Most of them are male voice-overs. Their faces are never seen and their names are rarely spoken, nor displayed on the images. Only journalists performing interviews and well-known people, such as politicians, athletes and celebrities, can be accurately identified and named.</p>
<p>The quality of an ASR system is evaluated using the so-called Word Error Rate (WER). This metric consists of counting the number of insertions, deletions and substitutions of words between the transcripts automatically generated by the ASR and the human transcripts considered as an oracle. The WER is 24.27% on ANTRACT data using the generic ASR system trained on modern data. The same system evaluated on 2010 data<sup id="fnref:24"><a href="#fn:24" class="footnote-ref" role="doc-noteref">24</a></sup>  achieves 13.46%. It is known that ASR systems are sensitive to acoustic and language variations between train corpus and test corpus. Here, the WER is almost double. It is generally difficult to exploit transcripts in a robust way when WER is above 30%. Most of the errors come from unknown words (which are not listed in the 160k vocabulary). These out of vocabulary words (OOV) are confused with acoustically close words, which have a negative impact on neighboring words. The system always selects the most likely word sequence containing the word replacing the OOV.</p>
<p>Additional contemporary data, such as archival descriptions and typescripts, would be useful to adapt the language model. Therefore, abstracts, titles and descriptions have been extracted from the archival descriptions. OCR sentences (see Section 2.2) have been kept when at least 95% of the words belong to the ASR vocabulary. An  “in domain”  training corpus composed of 1.3 million words from archival descriptions and 4.7 million words from typescripts was built. The 4,000 most frequent words were selected to train the new ANTRACT language model, which reduces the error rate by half: from 24.27% to 12.06% WER. <a href="#figure03">Figure 3</a> shows a sample of automatic transcription of the July 14, 1955 edition. The gain is significant thanks to the typescripts which are very similar to manual transcriptions. This  “in domain”  training corpus is contrary to the rules usually set during the well-known ASR system evaluations: a test data set should never be used to build a training corpus. However, in our case, the main goal is to provide the best transcripts to historians.</p>
<p>Future work will focus on ASR acoustic models improvement. We plan to use an alignment of typescripts with the editions, as well as historian users&rsquo; feedback providing some manually revised transcriptions. The objective is to select zones of confidence to be added to the learning data. Evaluation of the Named Entities is the next step in the roadmap. The speaker evaluation will be more difficult because of their identities, which are not available. We plan to evaluate both the detection of voice-overs and interviewers. Furthermore, some famous persons, selected in collaboration with historians for their relevance in historical analyses, will also be identified, with the possible help of crossing results with image analysis as described in Section 2.4.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000523/resources/images/figure03.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000523/resources/images/figure03_hu8ae02a3f01b80dbe7a8ab6c57f17e632_2030419_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000523/resources/images/figure03_hu8ae02a3f01b80dbe7a8ab6c57f17e632_2030419_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000523/resources/images/figure03_hu8ae02a3f01b80dbe7a8ab6c57f17e632_2030419_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000523/resources/images/figure03_hu8ae02a3f01b80dbe7a8ab6c57f17e632_2030419_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/1/000523/resources/images/figure03_hu8ae02a3f01b80dbe7a8ab6c57f17e632_2030419_1800x0_resize_box_3.png 1800w,/dhqwords/vol/15/1/000523/resources/images/figure03.png 2034w" 
     class="landscape"
     ><figcaption>
        <p>Sample “Actualité Francaise July 14, 1955 from 6:06 to 6:49” . Subtitle is an ASR file within domain language model, automatic punctuation and upper case.
        </p>
    </figcaption>
</figure>
<h2 id="24-automatic-visual-analysis">2.4 Automatic visual analysis</h2>
<p>Identifying the people appearing in a video is undoubtedly an important cue for its understanding. Knowing who appears in a video, when and where, can also lead to learning interesting patterns of relationships among characters for historical research. Such person-related annotations could provide ground for value added content. An historical archive such as the  <em>Actualités Françaises</em>  corpus contains numerous examples of celebrities appearing in the same news segment as De Gaulle and Adenauer (see <a href="#figure04">Figure 4</a>). However, the annotations produced manually by archivists do not always identify with precision those individuals in the videos. On the other side, the web offers an important number of pictures of those persons, easily accessible through Search Engines using their full name as search terms. In ANTRACT, we aim to leverage these pictures for identifying faces of celebrities in video archives.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000523/resources/images/figure04.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000523/resources/images/figure04_hu464ee86b4de602c0e06b408672205f1c_1557229_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000523/resources/images/figure04_hu464ee86b4de602c0e06b408672205f1c_1557229_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000523/resources/images/figure04_hu464ee86b4de602c0e06b408672205f1c_1557229_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000523/resources/images/figure04_hu464ee86b4de602c0e06b408672205f1c_1557229_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/1/000523/resources/images/figure04.png 1536w" 
     class="landscape"
     ><figcaption>
        <p>De Gaulle and Adenauer together in a video from 1959.
        </p>
    </figcaption>
</figure>
<p>There has been much progress in the last decade regarding the process of automatic recognition of people. It generally includes two steps: first, the faces need to be detected (i.e. which region of the frame may contain a person face) and then recognized (i.e. to which person this face belongs to).</p>
<p>The Viola-Jones algorithm <sup id="fnref:25"><a href="#fn:25" class="footnote-ref" role="doc-noteref">25</a></sup> for face detection and Local Binary Pattern (LBP) features <sup id="fnref:26"><a href="#fn:26" class="footnote-ref" role="doc-noteref">26</a></sup> for the clustering and recognition of faces were the most famous techniques used until the advent of deep learning and convolutional neural networks (CNN). Nowadays, two main approaches are in use to detect faces in video and both are using CNNs. The Dlib library <sup id="fnref:27"><a href="#fn:27" class="footnote-ref" role="doc-noteref">27</a></sup> provides good performance for frontal images but it requires an additional alignment step (which can also be performed using the Dlib library) before face recognition can be performed. The recent Multi-task Cascaded Convolutional Networks (MTCNN) approach provides even better performance using an image-pyramid approach and integrates the detection of face landmarks in order to re-align detected faces to the frontal position <sup id="fnref:28"><a href="#fn:28" class="footnote-ref" role="doc-noteref">28</a></sup>.</p>
<p>Having located the position and orientation of the faces in the video images, the recognition process can be performed in good conditions. Several strategies have been detailed in the literature to achieve recognition. Currently, the most practical approach is to perform face comparison using a transformation space in which similar faces are close together, and to use this representation to identify the right person. Such embeddings, computed on a large collection of faces, are often available to the research community <sup id="fnref:29"><a href="#fn:29" class="footnote-ref" role="doc-noteref">29</a></sup>.</p>
<p>Within ANTRACT, we developed an open source Face Celebrity Recognition system. This application is made of the following modules:</p>
<ul>
<li>A web crawler which, given a person&rsquo;s name, automatically downloads from Google a set of k photos that will be used for training a particular face model. In our experiments, we generally use k = 50. Among the results, the images not containing any face or containing more than one face are discarded. In addition, end users (e.g. domain experts) can manually exclude wrong results, for example, corresponding to pictures that do not represent the searched person.</li>
<li>A training module where the retrieved photos can be converted to black-and-white, cropped and resized in order to obtain images only containing a face, using the MTCNN algorithm <sup id="fnref1:28"><a href="#fn:28" class="footnote-ref" role="doc-noteref">28</a></sup>. A pre-trained Facenet <sup id="fnref1:29"><a href="#fn:29" class="footnote-ref" role="doc-noteref">29</a></sup> model with Inception ResNet v1 architecture trained on VGGFace2dataset <sup id="fnref:30"><a href="#fn:30" class="footnote-ref" role="doc-noteref">30</a></sup> is applied in order to extract visual features of the faces. The embeddings are used to train an SVM classifier.</li>
<li>A recognition module where a newsreel video is received as input and from which all frames are extracted at a given skipping distance  <em>d</em>  (in our experiments, we generally set  <em>d</em>  = 25, namely 1 sample frame per second). For each frame, the faces are detected (using the MTCNN algorithm) and the embeddings computed (Facenet). The SVM classifier decides if the face matches the ones among the training images.</li>
<li>Simple Online and Realtime Tracking (SORT) is an object tracking algorithm, which can track multiple objects in real-time <sup id="fnref:31"><a href="#fn:31" class="footnote-ref" role="doc-noteref">31</a></sup>. Its implementation is inspired by the suggestion code from <a href="https://github.com/Linzaer/Face-Track-Detect-Extract">Linzaer</a>. The algorithm uses the MTCNN bounding box detection and tracks it across frames. We introduced this module to increase the robustness of the library. By introducing this module, while making the assumption that faces do not swap coordinates across consecutive frames, we aim to get a more consistent prediction.</li>
<li>Finally, the last module groups together the results coming from the classifier and the tracking modules. We observe that even though the face to recognize remains the same over consecutive frames, the face prediction sometimes changes. For this reason, we select for each tracking the most frequently occurring prediction, taking also into account the confidence score given by the classifier. In this way, the system provides a common prediction for all the frames involved in a tracking, together with an aggregated confidence score. A threshold  <em>t</em>  can be applied to this score in order to discard the low-confidence prediction. According to our experiments, t = 0.6 gives a good balance between precision and recall.</li>
</ul>
<p>In order to make the software available as a service, we wrapped it into a RESTful web API, available at <a href="http://facerec.eurecom.fr/">http://facerec.eurecom.fr/</a>. The service receives as input the URI of a video resource, as it appears in Okapi, from which it retrieves the media object encoded in MPEG-4. Two output formats are supported: a custom JSON format and a serialization format in RDF using the Turtle syntax and the Media Fragment URI syntax <sup id="fnref:32"><a href="#fn:32" class="footnote-ref" role="doc-noteref">32</a></sup>, with normal play time ( <em>npt</em> ) expressed in seconds to identify temporal fragments and  <em>xywh</em>  coordinates to identify the bounding box rectangle encompassing the face in the frame. A third format, again following the Turtle syntax, will be soon implemented so that the results can be directly integrated in the Okapi Knowledge Graph. A light cache system is also provided in order to enable serving pre-computed results, unless the no cache parameter is set which is triggering a new analysis process.</p>
<p>We run experiments using the face model of Dwight D. Eisenhower on a selection of video segments extracted from Okapi, among the ones that have been annotated with the presence of the American president using the  “ina:imageContient”  and  “ina:aPourParticipant”  properties in the knowledge graph. In the absence of a ground truth, we performed a qualitative analysis of our system on three videos. For each detected person, we manually assessed whether the correct person was found or not. Out of the 90 selected segments, the system correctly identified Eisenhower in 33 of them. However, we are not sure that Eisenhower is effectively visually present in all 90 segments. We are currently working on extracting from the ANTRACT corpus a set of annotated segments to be used as ground truth so that it is possible to measure the precision and recall of the system.</p>
<p>In addition, we made the following observations:</p>
<ul>
<li>The library generally fails in detecting people when they are in the background, or when the face is occluded.</li>
<li>When faces are perfectly aligned, they are easier to detect. Improvements on the alignment algorithm are foreseen as future work.</li>
<li>When setting a high confidence threshold, we do not encounter cases where we confuse one celebrity by another one. Most errors are about confusing an unknown face with a celebrity in the dataset.</li>
</ul>
<p>In order to easily visualize the results and to facilitate history scholars&rsquo; feedback, we developed a web application that shows the results directly on the video, leveraging on HTML5 features. The application also provides a summary of the different predictions, enabling the user to directly jump to the relative part of the video where the celebrity appears. A slider allows changing the confidence threshold value, in order to better investigate the low-confidence results.</p>
<p>The application is publicly available at <a href="http://facerec.eurecom.fr/visualizer/?project=antract">http://facerec.eurecom.fr/visualizer/?project=antract</a> (see <a href="#figure05">Figure 5</a>).</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000523/resources/images/figure05.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000523/resources/images/figure05_hu048ffbe3d16c761663e223825502327a_1367814_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000523/resources/images/figure05_hu048ffbe3d16c761663e223825502327a_1367814_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000523/resources/images/figure05_hu048ffbe3d16c761663e223825502327a_1367814_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000523/resources/images/figure05_hu048ffbe3d16c761663e223825502327a_1367814_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/1/000523/resources/images/figure05_hu048ffbe3d16c761663e223825502327a_1367814_1800x0_resize_box_3.png 1800w,/dhqwords/vol/15/1/000523/resources/images/figure05.png 2500w" 
     class="landscape"
     ><figcaption>
        <p>The visualizer of the Celebrity Face Recognition System.
        </p>
    </figcaption>
</figure>
<h2 id="3-platforms-for-historians-exploration-and-analysis-of-the-corpus">3. Platforms for historians&rsquo; exploration and analysis of the corpus</h2>
<p>The corpus built with automatic tools in section 2 is explored interactively by historians using two platforms:</p>
<ul>
<li>the TXM platform for analysis of text corpora based on quantitative and qualitative exploration tools, and augmented during the ANTRACT project to facilitate the link between textual data and audio and video data;</li>
<li>the Okapi knowledge-driven platform for the management and annotation of video corpora using semantic technologies.</li>
</ul>
<h2 id="31-the-txm-platform-for-interactive-textometric-analysis">3.1 The TXM platform for interactive textometric analysis</h2>
<p>Text analysis is achieved through a textometric approach <sup id="fnref:33"><a href="#fn:33" class="footnote-ref" role="doc-noteref">33</a></sup>. Textometry combines both quantitative statistical tools and qualitative text searching, reading and annotating. On the one hand, statistical functionalities include keyword analysis, collocations, clustering and correspondence analysis. This makes a significant analytical power addition in comparison with usual annotation and search &amp; count features in audiovisual transcription software such as CLAN <sup id="fnref:34"><a href="#fn:34" class="footnote-ref" role="doc-noteref">34</a></sup> or ELAN <sup id="fnref:35"><a href="#fn:35" class="footnote-ref" role="doc-noteref">35</a></sup>. On the other hand, yet again in the textometric approach, qualitative analysis is carried out by advanced KWIC concordancing, by placing an emphasis on easy-access to high quality of layout rendering of source documents and by providing annotation tools. Such a qualitative side is marginal if not absent in conventional text mining applications <sup id="fnref:36"><a href="#fn:36" class="footnote-ref" role="doc-noteref">36</a></sup>  <sup id="fnref:37"><a href="#fn:37" class="footnote-ref" role="doc-noteref">37</a></sup>  <sup id="fnref:38"><a href="#fn:38" class="footnote-ref" role="doc-noteref">38</a></sup>: most of them process plain text, getting rid of text body markup, if any, and aim at synthetic visualization displacing close text reading.</p>
<p>Textometry is implemented by the TXM software platform <sup id="fnref:39"><a href="#fn:39" class="footnote-ref" role="doc-noteref">39</a></sup>. TXM is produced as an open-source software, which integrates several specialized components: R <sup id="fnref:40"><a href="#fn:40" class="footnote-ref" role="doc-noteref">40</a></sup> for statistical modeling, CQP for full text search engine <sup id="fnref:41"><a href="#fn:41" class="footnote-ref" role="doc-noteref">41</a></sup>, TreeTagger <sup id="fnref:42"><a href="#fn:42" class="footnote-ref" role="doc-noteref">42</a></sup> for Natural Language Processing (morphosyntactic tagging and lemmatization). TXM is committed to data and software standardization and sharing efforts, and has notably be designed to manage richly-encoded corpora, such as XML data and TEI<sup id="fnref:43"><a href="#fn:43" class="footnote-ref" role="doc-noteref">43</a></sup>  encoded texts; for ANTRACT textual data, TXM imports tabulated data (Excel format export of tables from INA documentary databases) and files in the Transcriber XML format provided by speech-to-text software (see Section 2.3). TXM is dedicated to text analysis, but also helps to manage multimedia representations associated with the texts, whether it is scanned images of source material, audio or video recordings: actually, these representations participate in the interpretation of TXM common tools results in their full semiotic context.</p>
<p>In 2018, we began to build the AFNOTICES TXM corpus by importing the INA archival descriptions: each news report is represented by several textual fields (title, abstract, sequence description) and several lexical fields (keyword lists of different types such as topics, people, or places, and credits with names of people shown or cameramen) and labeled by a dozen metadata (identifier, broadcast date, film producer, film genre, etc.) which are useful to contextualize or categorize reports.</p>
<p>In 2019, we began the production of the AFVOIXOFFV02 TXM corpus which makes the voice-over transcripts (see Section 2.3) searchable and available for statistical analysis, synchronized at the word level for video playback and labeled by INA documentary fields.</p>
<p>These corpora may still be augmented by aligning new textual modalities: texts from narration typescripts (OCR text and corresponding regions in the page images) (see Section 2.2), annotations on videos (manual annotations added by historians through the Okapi platform) (see Section 3.2), as well as automatic annotations generated by image recognition software (see Section 2.4), named entities, etc.</p>
<p>One of the technical innovations achieved for the project has been the consolidation of TXM back-to-media component <sup id="fnref:44"><a href="#fn:44" class="footnote-ref" role="doc-noteref">44</a></sup>, so that any word or text passage found in the result of a textometric tool can be played with its original video; we have also implemented authenticated streamed access to video content from the Okapi media server, which happened to be a key development for video access given the total physical size and the security constraints of such film archive data.</p>
<p>The following screenshots illustrate typical textometric analysis moments of current studies within the ANTRACT project.</p>
<p>In <a href="#figure06">Figure 6</a> and <a href="#figure07">Figure 7</a>, we study the context of use for the word  “foule”  (crowd), through a KWIC concordance. A double-click on a concordance line opens up a new window (on the right-hand side) which displays the complete transcript in which the word occurs. Then, we click on the music note symbol at the beginning of the paragraph to play the corresponding video. A dialog box prompts for credentials before accessing the video on the Okapi online server. This opportunity to confront textual analysis with the audiovisual source is all the more important here because textual data were generated by the speech-to-text automatic component, whose output could not be fully revised. Moreover, the video may add significant context that is not rendered in plain text transcription.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000523/resources/images/figure06.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000523/resources/images/figure06_hub2758d46ebd67a8e089c11043b306724_200689_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000523/resources/images/figure06_hub2758d46ebd67a8e089c11043b306724_200689_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000523/resources/images/figure06.png 1163w" 
     class="landscape"
     ><figcaption>
        <p>CONCORDANCE of the word “foule” (crowd) in the voice-over corpus (left window), voice-over transcript EDITION corresponding to the selected concordance line (right window), and the authentication dialog box to access the Okapi video server to play the video at 0:00:06 (top left window).
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000523/resources/images/figure07.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000523/resources/images/figure07_hu2fe256f64db5f8702648efa3a12aa016_464654_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000523/resources/images/figure07_hu2fe256f64db5f8702648efa3a12aa016_464654_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000523/resources/images/figure07.png 1165w" 
     class="landscape"
     ><figcaption>
        <p>Hyperlinked windows managing results associated with the word “foule” (crowd): CONCORDANCE (left window), transcript EDITION (middle window) and synchronized video playback (right window).
        </p>
    </figcaption>
</figure>
<p>Our second example is about the place of agriculture and farmers in the  <em>Actualités françaises</em> , and how the topic is presented. It shows how one can investigate if a given word has the same meaning in documentation and in commentary, or if different words are used when dealing with the same subject. We first get (<a href="#figure08">Figure 8</a>) a comparative overview of the quantitative evolution of occurrences from two word families, derived from the stems of  “paysan” and  “agricole” /  “agriculture”  (see detailed list of words in <a href="#figure09">Figure 9</a>, left hand side window). We complete the analysis with contextual analysis through KWIC concordance views (see <a href="#figure08">Figure 8</a>, lower window) and cooccurrences computing (see <a href="#figure09">Figure 9</a>). We notice that  “paysan”  becomes less used from 1952 onwards, and that it is preferred to  “agriculteur”  when speaking of the individuals present in the newsreels extracts; conversely,  “agricole” / “agriculture”  are used in a more abstract way, to deal with new farm equipment and socio-economic transformation of this line of business.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000523/resources/images/figure08.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000523/resources/images/figure08_hu1c36cc1507433e97629556b13896aebf_198633_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000523/resources/images/figure08_hu1c36cc1507433e97629556b13896aebf_198633_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000523/resources/images/figure08.png 1168w" 
     class="landscape"
     ><figcaption>
        <p>PROGRESSION chart (upper window), and hyperlinked KWIC CONCORDANCES (lower window), to compare two word families related to farming.
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000523/resources/images/figure09.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000523/resources/images/figure09_hu0c5e0024db216cc69fd710e927faaec3_150765_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000523/resources/images/figure09_hu0c5e0024db216cc69fd710e927faaec3_150765_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000523/resources/images/figure09.png 1169w" 
     class="landscape"
     ><figcaption>
        <p>INDEX results detailing the content of two word families (left margin), and COOCCURRENCES statistical analysis to characterize their contexts.
        </p>
    </figcaption>
</figure>
<p>Combining word lists (INDEX) and morphosyntactic information is very effective to summarize phrasal contexts. For instance, in <a href="#figure08">Figure 8</a>, we can compare which adjectives qualify  “foule”  in the archival descriptions, and which ones qualify  “foule”  in the voice-over speeches. For a given phrase ( “foule immense” , huge crowd) in the voice-over, we compute its cooccurrences in order to identify in which kind of circumstances the phrase is preferred (funerals, religious meetings). In TXM, full-text search is powered by the extensive CQP search engine <sup id="fnref1:41"><a href="#fn:41" class="footnote-ref" role="doc-noteref">41</a></sup>, which allows very fine-tuned and contextualized queries.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000523/resources/images/figure10.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000523/resources/images/figure10_hu6abb0eccbdd8e86d953d18a87368a470_176843_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000523/resources/images/figure10_hu6abb0eccbdd8e86d953d18a87368a470_176843_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000523/resources/images/figure10.png 1087w" 
     class="landscape"
     ><figcaption>
        <p>INDEX of “foule” (crowd) preceded or followed by an adjective, in archival descriptions (left window) or in voice-over transcripts (middle window). COOCCURRENCES for “foule immense” (huge crowd) in voice-over transcripts (right window).
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000523/resources/images/figure11.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000523/resources/images/figure11_hu5dfa8f15466becf0eb2a18e42af7ec47_53092_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000523/resources/images/figure11_hu5dfa8f15466becf0eb2a18e42af7ec47_53092_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000523/resources/images/figure11_hu5dfa8f15466becf0eb2a18e42af7ec47_53092_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000523/resources/images/figure11.png 1234w" 
     class="landscape"
     ><figcaption>
        <p>Statistical SPECIFICITY chart for “foule” (crowd) over the years.
        </p>
    </figcaption>
</figure>
<p>For chronological investigations, we can divide the corpus into time periods in a very flexible way, such as years or groups of years. Any encoded information may be used to build corpus subdivisions. Then the SPECIFICITY command －that implements a Fisher&rsquo;s Exact Test, known as one of the best calculations to find keywords <sup id="fnref:45"><a href="#fn:45" class="footnote-ref" role="doc-noteref">45</a></sup>－ statistically measures the steady use, or the singular overuse or underuse of any word. The function can also be used to bring to light the specific terms for a given period, or for any given part of the corpus. For example, <a href="#figure09">Figure 9</a> focuses on the word  “foule”  over the years. Peak years reveal important political events (e.g. the liberation of France after WW2, the advent of the Fifth Republic), which match the high exposure of Général de Gaulle. However, the most frequent occurrences do not necessarily correspond to political upheavals.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000523/resources/images/figure12.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000523/resources/images/figure12_hu7be3ca3ffc2c6bcce90c1f152bea8c47_100978_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000523/resources/images/figure12_hu7be3ca3ffc2c6bcce90c1f152bea8c47_100978_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000523/resources/images/figure12.png 861w" 
     class="landscape"
     ><figcaption>
        <p>Example of resonance analysis (Salem, 2004): SPECIFIC terms in voice-over comments for reports showing a crowd (according to archival description) (upper window) ; then, SPECIFIC terms in voice-over comments for reports showing a crowd and having no mention of De Gaulle or “président” (president) (lower window).
        </p>
    </figcaption>
</figure>
<p>With Figure 12, we apply a statistical resonance analysis <sup id="fnref:46"><a href="#fn:46" class="footnote-ref" role="doc-noteref">46</a></sup>. When a crowd is shown (as indicated by the archival description), what are the most characteristic words said by the voice-over?  “Président”  and  “[le général De] Gaulle”  represent the main context (<a href="#figure01">Figure 12</a>, upper window). In a second step, we remove all the reports containing one of these two words and focus on the remaining reports to bring out new kinds of contexts associated with the view of a crowd (<a href="#figure12">Figure 12</a>, lower window), such as sports, commemorative events, demonstrations, festive events, etc. The recurring term  “foule”  (crowd) in the voice-overs promotes a sense of belonging to a community of fate. From a methodological perspective, this kind of cross-querying combined with statistical comparison between textual newsreel archival descriptions and commentary transcripts helps investigate correlations or discrepancies between what is shown in the newsreels and what is said in their commentaries. Such a combination of statistics across media is rarely provided by applications.</p>
<p><a href="#figure13">Figure 13</a> provides a first insight of a correspondence analysis output: we computed a 2D-map of the names of people who are present in more than 20 reports, in relation with the years in which they are mentioned. We thus get a synthetic view of the relationship between people and time in the  <em>Actualités françaises</em>  reports. In terms of calculation, as textometry often deals with frequency tables crossing words and corpus parts (here we crossed people&rsquo;s names and year divisions), it then opts for correspondence analysis, because this type of multidimensional analysis is best suited to such contingency tables <sup id="fnref1:33"><a href="#fn:33" class="footnote-ref" role="doc-noteref">33</a></sup>.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000523/resources/images/figure13.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000523/resources/images/figure13_hu5ca103dc62c0259c77102373dd9f2954_274639_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000523/resources/images/figure13_hu5ca103dc62c0259c77102373dd9f2954_274639_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000523/resources/images/figure13_hu5ca103dc62c0259c77102373dd9f2954_274639_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000523/resources/images/figure13_hu5ca103dc62c0259c77102373dd9f2954_274639_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/1/000523/resources/images/figure13.png 1594w" 
     class="landscape"
     ><figcaption>
        <p>CORRESPONDENCE ANALYSIS (first plane) of the frequency table crossing the years and the names of 51 people that are present in at least 20 reports.
        </p>
    </figcaption>
</figure>
<h2 id="32-okapi-platform-for-interactive-semantic-analysis">3.2 Okapi platform for interactive semantic analysis</h2>
<p>Okapi (Open Knowledge Annotation and Publication Interface) <sup id="fnref:47"><a href="#fn:47" class="footnote-ref" role="doc-noteref">47</a></sup> is a knowledge-based online platform for semantic management of content. It is at the intersection of three scientific domains: Indexing and description of multimedia content, knowledge management systems and Web content management systems. It takes full advantage of semantic web languages and standards (RDF, RDFS, OWL <sup id="fnref:48"><a href="#fn:48" class="footnote-ref" role="doc-noteref">48</a></sup>) to represent content as graphs of knowledge; it applies semantic inferences on these graphs and transforms them to generate new hypermedia content like web portals.</p>
<p>Okapi provides a set of tools for analyzing multimedia content (video, image, sound) and managing corpora of annotated video and sound excerpts as well as image sections. Analysis tools allow the semantic indexing and description of content using domain ontology. The corpus management tools provide services for the constitution and visualization of thematic corpora as well as their annotation and enrichment in order to generate mini-portals or thematic publications of their contents.</p>
<p>The Okapi&rsquo;s knowledge management system stores knowledge as graphs of named entities and provides services to retrieve, share and present them as linked open data. These entities can be aligned with other entities in existing knowledge bases like dbpedia and wikidata and so makes Okapi interoperable with the Linked Open Data ecosystem <sup id="fnref:49"><a href="#fn:49" class="footnote-ref" role="doc-noteref">49</a></sup>. The named entities can be of different types and categories and vary according to the studied domain. For instance, for audiovisual archives, entities may concern persons, geographical places and concepts.</p>
<p>Finally, the Okapi&rsquo;s Content Management System (Okapi&rsquo;s CMS) considers the characteristics of the studied domain and user preferences to generate web interfaces and tools for Okapi as well as content portals adapted to the domain. This publishing framework allows also authors to focus on their authoring work and to create thematic portals without any technical skills. The author can specify his thematic publication as a set of interconnected multimedia elements (video, image, sound, editorial texts). The framework applies thereafter a set of publishing rules on these elements and generates a web site.</p>
<p>The Okapi platform is used by historians to constitute thematic corpora and to publish their portals as explained in the above paragraphs. Okapi can also be used by researchers in computer science and data scientists to show and improve the results of their automatic algorithms (face detection and recognition, automatic speech recognition, etc.). The following sections show some examples of how the Okapi platform can be used on the collection  “Les Actualités Françaises”  (AF) in the context of the ANTRACT project.</p>
<p>The media analysis can be carried out manually by annotators or automatically by algorithms on several axes as shown in <a href="#figure14">Figure 14</a>. In this example, thematic analysis (the layer entitled  “strate sujets” ) of the AF program  “Journal Les Actualités Françaises : émission du 10 juillet 1968”  consists in identifying the topics addressed in this program, their temporal scope and a detailed description of the topic in terms of the subject we are talking about, the places where it happens and the persons who are involved in this subject.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000523/resources/images/figure14.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000523/resources/images/figure14_hu6f51e185d8aedd5c8296f0bfc9c8aa0b_33717_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000523/resources/images/figure14_hu6f51e185d8aedd5c8296f0bfc9c8aa0b_33717_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000523/resources/images/figure14.png 1018w" 
     class="landscape"
     ><figcaption>
        <p>Timeline for Media Analysis
        </p>
    </figcaption>
</figure>
<p>The user can create and remove analysis layers and their segments as well as the description of each segment and its timecodes. Considering the second segment in the example where we are talking about the  <strong>water sports</strong>  ( <strong>concept</strong> ) in  <strong>England</strong>  ( <strong>Place</strong> ), especially the adventures of the solo sailor  <strong>Alec Rose</strong>  ( <strong>Person</strong> ) as indicated in the following form (<a href="#figure15">Figure 15</a>): The user can edit this form to change and create new description values of the selected segment. These concepts, places and persons are a subset of named entities that are managed and suggested by Okapi to complete the description of the segment.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000523/resources/images/figure15.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000523/resources/images/figure15_hub634401e34945adebf718f29cd0afdf1_81062_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000523/resources/images/figure15_hub634401e34945adebf718f29cd0afdf1_81062_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000523/resources/images/figure15.png 901w" 
     class="landscape"
     ><figcaption>
        <p>Segment Metadata Form
        </p>
    </figcaption>
</figure>
<p>The other analysis layers (transcription, music detection, etc.) are provided by automatic algorithms. The metadata provided by these algorithms can enrich the ones created manually by users and can be used by the Okapi platform to generate a rich portal that brings value to the content and provides several access and navigation possibilities in the content as shown in <a href="#figure16">Figure 16</a>.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000523/resources/images/figure16.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000523/resources/images/figure16_hufbb569f09d17c245bc79c7a4220ec7ca_290983_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000523/resources/images/figure16_hufbb569f09d17c245bc79c7a4220ec7ca_290983_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000523/resources/images/figure16.png 1163w" 
     class="landscape"
     ><figcaption>
        <p>Okapi portal page of the AF news “Journal Les Actualités Françaises: émission du 10 Juillet 1968” .
        </p>
    </figcaption>
</figure>
<p>The generated metadata are also used as advanced criteria for looking for video excerpts and so allow users to constitute their thematic corpora focused on some topics. <a href="#figure15">Figure 15</a> shows an example of an advanced search of segments which talk about  “ <strong>Water sports</strong> ”  in  “ <strong>England</strong> ” . Like all Okapi&rsquo;s objects, a query is represented as a knowledge graph and then transformed into a SPARQL query. The results of this query, illustrated by <a href="#figure17">Figure 17</a> and <a href="#figure18">Figure 18</a>, can be used to create a corpus.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000523/resources/images/figure17.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000523/resources/images/figure17_hu05173e2db52efcb08a7e5550c343090b_32295_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000523/resources/images/figure17_hu05173e2db52efcb08a7e5550c343090b_32295_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000523/resources/images/figure17.png 909w" 
     class="landscape"
     ><figcaption>
        <p>Example of an Okapi Query.
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000523/resources/images/figure18.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000523/resources/images/figure18_hu23ea58089385dc0e7ab796fa092c1f55_28222_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000523/resources/images/figure18_hu23ea58089385dc0e7ab796fa092c1f55_28222_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000523/resources/images/figure18.png 777w" 
     class="landscape"
     ><figcaption>
        <p>Example of query results.
        </p>
    </figcaption>
</figure>
<p>The corpus itself is an object to be annotated, i.e., the user can add new metadata on the corpus itself or on its elements (video excerpts) and put rhetorical relations between them. <a href="#figure19">Figure 19</a> shows a corpus of three excerpts, retrieved from the query presented in the previous paragraph. It displays also a rhetorical relationship between the two segments:  “Robert Manry, 48 ans: Traversée solitaire de l&rsquo;océan”  which illustrates the other segment  “Alec Rose, après 354 jours sur un bateau:  la terre est ronde ” . All these metadata will be used to create a thematic portal focused on the content of the corpus or integrated into a story through the inclusion of editorial content and preferred reading paths.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000523/resources/images/figure19.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000523/resources/images/figure19_hu5e1e73c2fdde9255c128f95df56a2795_46290_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/1/000523/resources/images/figure19_hu5e1e73c2fdde9255c128f95df56a2795_46290_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000523/resources/images/figure19.png 904w" 
     class="landscape"
     ><figcaption>
        <p>Thematic Corpus “Water Sports” .
        </p>
    </figcaption>
</figure>
<p>The Okapi platform exposes a secure SPARQL endpoint and API which allows other ANTRACT tools, especially the TXM platform, to query the knowledge base and to update the stored metadata. For instance, TXM tools could retrieve metadata through the Okapi&rsquo;s endpoint in order to constitute a corpus. This corpus will then be stored in the knowledge base through the API and used by Okapi to provide thematic publications. Additional semantic descriptors produced by TXM could also be integrated into the Okapi knowledge base.</p>
<h2 id="4-conclusion">4. Conclusion</h2>
<p>Presented throughout this article, the ANTRACT project&rsquo;s challenge is to familiarize scholars with the automated research of large audiovisual corpora. Gathering instruments specialized in image, audio and text analysis into a single multimodal apparatus designed to correlate their results, the project intends to develop a transdisciplinary research model suitable to open new perspectives in the study of single or multi-format sources.</p>
<p>At this point of the project, most of the work is dedicated to the development and tuning of the automatic content analysis tools as well as the application of their results to the organization and improvement of the corpus data in connection with research provided by ANTRACT historians <sup id="fnref1:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup>. Their case studies were explored using the TXM textometry platform and the Okapi annotation and publication platform that allows its users to exploit all the data produced by the instruments developed for the project.</p>
<p>From a technological perspective, ANTRACT&rsquo;s goal is now to further adapt automatic content analysis tools to the specificity of the corpus such as its historical context, its vocabulary, its image format and quality, as it has been done, for instance, by improving the language models used by the automatic transcription tools with the help of the typescripts of voice-overs. Interactive analysis platforms should also benefit from history scholars feedback in order to improve their user interface and to develop new analytical paths.</p>
<p>At the end of the project, a comprehensive  <em>Les Actualités Françaises</em>  corpus completed with its metadata as well as the results of the research supported by automatic content analysis tools and manual annotations will be made available to the scientific community via the online Okapi platform. To this end, Okapi tutorials will be provided to the public and TXM will continue to be available as an open source software to help the analysis of corpora used in new case studies. Okapi source code will be turned to open source so that other developers can contribute to its enhancement.</p>
<p>Regarding humanities, ANTRACT tools and methodology can be adapted to various types of corpora providing historians as well as specialists from other disciplines such as sociology, anthropology and political science a renewed access to their documents supported by an exhaustive examination of their content.</p>
<h2 id="acknowledgment">Acknowledgment</h2>
<p>This work has been supported by the French National Research Agency (ANR) within the ANTRACT Project, under grant number ANR-17-CE38-0010, and by the European Union&rsquo;s Horizon 2020 research and innovation program within the MeMAD project (grant agreement No. 780069).</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>ANTRACT: ANalyse TRansdisciplinaire des ACTualités filmées (1945-1969).&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Deegan, M., and McCarty, W.  <em>Collaborative Research in the Digital Humanities</em> . Ashgate, Farnham, Burlington (2012).&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Atkinson, N. S.,  “Newsreels as Domestic Propaganda: Visual Rhetoric at the Dawn of the Cold War”    <em>Rhetoric &amp; Public Affairs</em> , 14.1 (2011): 69-100.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Bartels, U. Die Wochenschau im Dritten Reich. Entwicklung und Funktion eines Massenmediums unter besonderer Berücksichtigung völkisch-nationaler Inhalte. Peter Lang, Frankfurt am Main (2004).&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>Pozner, V.  “Les actualités soviétiques durant la Seconde Guerre mondiale : nouvelles sources, nouvelles approches”  In J.-P. Bertin-Maghit (dir.),  <em>Une histoire mondiale des cinémas de propagande</em> , Nouveau Monde Editions, Paris (2008): 421-444.&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>Veray, L.  <em>Les Films d&rsquo;actualités français de la Grande Guerre</em> . SIRPA/AFRHC, Paris (1995).&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>Fein, S.  “New Empire into Old: Making Mexican Newsreels the Cold War Way”    <em>Diplomatic History</em> , 28.5 (2004): 703-748.&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>Fein, S.  “Producing the Cold War in Mexico: The Public Limits of Covert Communications” In G. M. Joseph and D. Spenser (eds.),  <em>In from the Cold: Latin America&rsquo;s New Encounter with the Cold War</em> , Duke University Press, Durham (2008): 171-213.&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>Althaus, S., Usry, K., Richards, S., Van Thuyle, B., Aron, I., Huang, L., Leetaru, K., Muehlfeld, M., Snouffer, K., Weber, S., Zhang, Y., and Phalen, P.  “Global News Broadcasting in the Pre-Television Era: A Cross-National Comparative Analysis of World War II Newsreel Coverage”    <em>Journal of Broadcasting and Electronic Media</em> , 62.1 (2018): 147-167.&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>Chambers, C., Jönsson, M., and Vande Winkel R. (eds.)  <em>Researching Newsreels. Local, National and Transnational Case Studies</em> . Global Cinema, Palgrave Macmillan, London (2018).&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>Imesch, K., Schade, S., Sieber, S. (eds.)  <em>Constructions of cultural identities in newsreel cinema and television after 1945</em> . MediaAnalysis, 17, transcript-Verlag, Bielefeld (2016).&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p>Lindeperg, S. Clio de 5 à 7 : les actualités filmées à la Libération, archive du futur. CNRS, Paris (2000).&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13">
<p>Lindeperg, S.  “Spectacles du pouvoir gaullien: le rendez-vous manqué des actualités filmées”  In J.-P. Bertin-Maghit (dir.),  <em>Une histoire mondiale des cinémas de propagande</em> , Nouveau Monde Éditions, Paris (2008): 497-511.&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14">
<p>Goetschel, P., Granger, C. (dir.)  “Faire l&rsquo;événement, un enjeu des sociétés contemporaines”    <em>Sociétés &amp; Représentations</em> , 32 (2011): 7-23.&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:15">
<p>Maitland, S.  “Culture in translation: The case of British Pathé News”  In  <em>Culture and news translation, Perspectives: Studies in Translation Theory and Practice</em> , 23.4 (2015): 570-585.&#160;<a href="#fnref:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:16">
<p>Bradski, G.  “The OpenCV Library”    <em>Dr. Dobb&rsquo;s Journal of Software Tools</em>  (2000).&#160;<a href="#fnref:16" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:17">
<p>Chenot, J.-H., and Daigneault, G.  “A large-scale audio and video fingerprints-generated database of TV repeated contents” In 12th International Workshop on Content-Based Multimedia Indexing (CBMI), Klagenfurt, Austria (2014).&#160;<a href="#fnref:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:18">
<p>Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z.  “Rethinking the Inception Architecture for Computer Vision”  In Proceedings of  <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em> , Las Vegas (2016).&#160;<a href="#fnref:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:19">
<p>ESTER 1 &amp; 2, EPAC, ETAPE, and REPERE corpus available in ELRA catalogues (<a href="http://www.elra.info/">http://www.elra.info/</a>)&#160;<a href="#fnref:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:20">
<p>ETAPE, and QUAERO corpus available in ELRA catalogues (<a href="http://www.elra.info/">http://www.elra.info/</a>).&#160;<a href="#fnref:20" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:21">
<p>Broux, P.-A., Desnous, F., Larcher, A., Petitrenaud, S., Carrive, J., and Meignier, S.  “S4D: Speaker Diarization Toolkit in Python”  Interspeech, Hyderabad, India (2018).&#160;<a href="#fnref:21" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:22">
<p>Povey, D., Ghoshal, A., Boulianne, G., Burget, L., Glembek, O., Goel, N., Hannemann, M., Motlicek, P., Qian, Y., Schwarz, P.., Silovsky, J., Stemmer, G. and Vesely, K.  “The kaldi speech recognition toolkit”  In IEEE 2011 workshop on automatic speech recognition and understanding, IEEE Signal Processing Society, Hilton Waikoloa Village, Big Island, Hawaii, US (2011).&#160;<a href="#fnref:22" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:23">
<p>Povey, D., Peddinti, V., Galvez, D., Ghahremani, P., Manohar, V., Na, X., Wang, Y., and Khudanpur, S.  “Purely sequence-trained neural networks for ASR based on lattice-free MMI”    <em>Interspeech</em> , San Francisco (2016): 2751–2755.&#160;<a href="#fnref:23" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:24">
<p>Challenge REPERE, test data.&#160;<a href="#fnref:24" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:25">
<p>Viola, P. and Jones, M. J.  “Robust real-time face detection”    <em>International Journal of Computer Vision</em> , 57.2 (2004): 137–154.&#160;<a href="#fnref:25" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:26">
<p>Ahonen, T., Hadid, A., and Pietikainen, M.  “Face description with local binary patterns: Application to face recognition”    <em>IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</em> , 28.12 (2006): 2037–2041.&#160;<a href="#fnref:26" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:27">
<p>King, D. E.  “Dlib-ml: A machine learning toolkit”    <em>Journal of Machine Learning Research</em> , 10 (2009): 1755–1758.&#160;<a href="#fnref:27" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:28">
<p>Zhang, K., Zhang, Z., Li, Z. and Qiao, Y.  “Joint face detection and alignment using multitask cascaded convolutional networks”    <em>IEEE Signal Processing Letters</em> , 23.10 (2016): 1499–1503.&#160;<a href="#fnref:28" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:28" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:29">
<p>Schroff, F., Kalenichenko, D. and Philbin, J.  “Facenet: A unified embedding for face recognition and clustering”  In  <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em>  (2015): 815–823.&#160;<a href="#fnref:29" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:29" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:30">
<p>Cao, Q., Shen, L., Xie, W., Parkhi, O. M. and Zisserman, A.  “Vggface2: A dataset for recognising faces across pose and age”  In 13th IEEE International Conference on Automatic Face &amp; Gesture Recognition (FG) (2018): 67–74.&#160;<a href="#fnref:30" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:31">
<p>Bewley, A., Ge, Z., Ott, L., Ramos, F. and Upcroft, B.  “Simple online and realtime tracking”  In IEEE International Conference on Image Processing (ICIP) (2016): 3464–3468.&#160;<a href="#fnref:31" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:32">
<p>Troncy, R., Mannens, E., Pfeiffer, S. and van Deursen, D.  “Media Fragments URI 1.0 (basic)”  W3C Recommendation (2012).&#160;<a href="#fnref:32" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:33">
<p>Lebart, L., Salem, A. and Berry, L.  <em>Exploring textual data. Text, speech, and language technology</em> , 4, Kluwer Academic, Dordrecht, Boston (1998).&#160;<a href="#fnref:33" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:33" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:34">
<p>McWhinney, B.  <em>The CHILDES Project: Tools for Analyzing Talk.</em>  L. Erlbaum Associates, Mahwah, N.J. (2000).&#160;<a href="#fnref:34" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:35">
<p>. Max Planck Institute for Psycholinguistics, Nijmegen (2018). Retrieved from <a href="https://tla.mpi.nl/tools/tla-tools/elan">https://tla.mpi.nl/tools/tla-tools/elan</a>&#160;<a href="#fnref:35" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:36">
<p>Hotho, A., Nürnberger, A. and Paaß, G.  “A brief survey of text mining”    <em>LDV Forum</em> , 20.1 (2005): 19-62.&#160;<a href="#fnref:36" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:37">
<p>Feinerer, I., Hornik, K., and Meyer, D.  “Text Mining Infrastructure in R”    <em>Journal of Statistical Software</em> , 25.5 (2008): 1-54.&#160;<a href="#fnref:37" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:38">
<p>Weiss, S. M., Indurkhya, N., and Zhang, T.  <em>Fundamentals of Predictive Text Mining</em> . Springer-Verlag, London (2015).&#160;<a href="#fnref:38" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:39">
<p>Heiden, S.  “The TXM Platform: Building Open-Source Textual Analysis Software Compatible with the TEI Encoding Scheme” In R. Otoguro, K. Ishikawa, H. Umemoto, K. Yoshimoto, Y. Harada (eds.), 24th Pacific Asia Conference on Language, Information and Computation, Institute for Digital Enhancement of Cognitive Development, Waseda University (2010).&#160;<a href="#fnref:39" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:40">
<p>R Core Team.,  “R: A Language and Environment for Statistical Computing”  R Foundation for Statistical Computing, Vienna, Austria (2014).&#160;<a href="#fnref:40" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:41">
<p>Christ, O.  “A modular and flexible architecture for an integrated corpus query system” In Ferenc Kiefer et al. (eds.), In 3rd International Conference on Computational Lexicography, Research Institute for Linguistics, Hungarian Academy of Sciences, Budapest (1994): 23-32.&#160;<a href="#fnref:41" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:41" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:42">
<p>Schmid, H.  “Probabilistic Part-of-Speech Tagging Using Decision Trees”  In  <em>Proceedings of International Conference on New Methods in Language Processing</em> , Manchester, UK (1994).&#160;<a href="#fnref:42" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:43">
<p>Text Encoding Initiative, <a href="https://tei-c.org">https://tei-c.org</a>&#160;<a href="#fnref:43" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:44">
<p>Pincemin, B., Heiden, S. and Decorde, M.  “Textometry on Audiovisual Corpora. Experiments with TXM software”  15th International Conference on Statistical Analysis of Textual Data (JADT), Toulouse (2020).&#160;<a href="#fnref:44" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:45">
<p>McEnery, T. and Hardie, A.  <em>Corpus linguistics: method, theory and practice</em> . Cambridge University Press, Cambridge (2012).&#160;<a href="#fnref:45" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:46">
<p>Salem, A.  “Introduction à la résonance textuelle”  In G. Purnelle et al. (eds.),  <em>7èmes Journées internationales d&rsquo;Analyse statistique des Données Textuelles</em> , Presses universitaires de Louvain, Louvain (2004): 986–992.&#160;<a href="#fnref:46" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:47">
<p>Beloued, A., Stockinger, P., and Lalande, S.  “Studio Campus AAR: A Semantic Platform for Analyzing and Publishing Audiovisual Corpuses.”  In  <em>Collective Intelligence and Digital Archives</em> , John Wiley &amp; Sons Inc., Hoboken, NJ (2017): 85-133.&#160;<a href="#fnref:47" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:48">
<p>Motik, B., Patel-Schneider, P. F., Parsia, B.  “OWL 2 Web Ontology Language: Structural Specification and Functional-Style Syntax (Second Edition)”  W3C Recommendation (2012).&#160;<a href="#fnref:48" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:49">
<p>Bizer, C., Heath, T., and Berners-Lee, T.  “Linked data - the story so far”    <em>International Journal on Semantic Web and Information Systems</em> , 5 (2009): 1-22.&#160;<a href="#fnref:49" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry><entry><title type="html">What Does A Photograph Sound Like? Digital Image Sonification As Synesthetic AudioVisual Digital Humanities</title><link href="https://rlskoeser.github.io/dhqwords/vol/15/1/000508/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://rlskoeser.github.io/dhqwords/vol/15/1/000508/</id><author><name>Michael J. Kramer</name></author><published>2021-03-05T00:00:00+00:00</published><updated>2021-03-05T00:00:00+00:00</updated><content type="html"><![CDATA[<h2 id="heading"></h2>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000508/resources/images/figure01.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000508/resources/images/figure01_hu9a22ec99b37489863a81c06bd8c4e55b_31077_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/15/1/000508/resources/images/figure01_hu9a22ec99b37489863a81c06bd8c4e55b_31077_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/15/1/000508/resources/images/figure01_hu9a22ec99b37489863a81c06bd8c4e55b_31077_1200x0_resize_q75_box.jpg 1200w,/dhqwords/vol/15/1/000508/resources/images/figure01_hu9a22ec99b37489863a81c06bd8c4e55b_31077_1500x0_resize_q75_box.jpg 1500w,/dhqwords/vol/15/1/000508/resources/images/figure01.jpg 1500w" 
     class="landscape"
     ><figcaption>
        <p>Joan Baez performing at the Hearst Greek Theatre, Berkeley Folk Music Festival Collection, Charles Deering McCormick Library of Special Collections, Northwestern University Libraries, n.d. (possibly 1963), photographer unknown, <a href="https://dc.library.northwestern.edu/items/87ac8798-6aaf-456e-9c3e-dc187c796115">https://dc.library.northwestern.edu/items/87ac8798-6aaf-456e-9c3e-dc187c796115</a>.
        </p>
    </figcaption>
</figure>
<p>In the photograph (<a href="#figure01">Figure 1</a>), we see the famous folksinger Joan Baez performing at the Greek Amphitheater on the University of California-Berkeley campus, but of course, we cannot hear her. Taken on a summer night during the early 1960s, the image contains a moment when musical sound was being produced, but as an artifact, it remains silent. The photograph visually conveys a grand sense of performative scale. Snapped from high up in the audience by an unknown photographer, we look down on Baez from stage right at the 8,500-seat venue. She stands under the stark white spotlight, just a series of blurry shapes far away on stage. You can recognize her iconic blouse and knee-length skirt, her long brunette hair, and her acoustic guitar, but that is about all. She sings in a space modeled after the Ancient Theater of Epidaurus, funded by newspaper magnate William Randolph Hearst, and opened in 1903 in service of the idea that Berkeley was becoming a new fount of democracy, the  “Athens of the West.”  If you know some of the context for this image, you might think about how you are looking at a young, radical woman, barely out of her teens, who has become a folk music revival celebrity. If you also know Baez&rsquo;s music, you might hear faint echoes of Baez&rsquo;s signature soprano vibrato and fingerpicked acoustic guitar in your mind&rsquo;s ear as you look at the photograph. If you know the San Francisco Bay Area, you might remember the delicious crispness or downright chill of a spring or summer evening in that region, the fog rolling in over the bay toward the Berkeley Hills. Otherwise — or even if you do have those sonic and sensorial resources available — you are left only with a mute image.</p>
<p>Is there more here than meets the eye? If so, can AudioVisual DH help us access it, or at least think more productively about photographs of musical performance or of sound being made? We cannot magically recover the sounds of Joan Baez performing in some miraculous feat of archeological excavation (at least not yet), but the tactic of what I call digital image sonification offers one example of how computation can enhance better close reading, more compelling interpretation, and deeper contextual understanding of pictures of sound — indeed of pictures in general. While archeologists and a number of historians involved in digital humanities have been rightfully interested in recovering or modeling acoustic environments from the past, what I refer to as image sonification is drawn more from media studies concerns about what a digital photograph is.<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>  When a photograph is optically scanned and transformed into an image file, we can correlate its pixels, shapes, and other features to sound, creating an audio version of the image. As media studies scholar Wolfgang Ernst points out, &ldquo;For the computer, the difference between sound and image, if it is counted, would count only as the difference between data  <em>formats</em>  [ital. in original].&rdquo; The transpositional possibilities become useful for synesthetic modes of audiovisual DH. As Ernst puts it, &ldquo;Digital memory ignores the aesthetic differences between audio and visual data and makes one interface (to human ears and eyes) emulate another&rdquo; <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. There are many aspects of the image we could sonify, and, to be sure, there is no essential one better than another necessarily. As Taylor Arnold and Lauren Tilton note in their research on what they call &ldquo;distant viewing&rdquo; of large visual corpora, &ldquo;Raw pixel intensities hold no meaningful information out of context&rdquo; <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>. Nonetheless, even for one image — maybe especially for one image — multiple modes of experiencing data can produce new knowledge through the defamiliarization of the artifact <sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>.</p>
<p>Defamiliarization involves the purposeful alienating of a text in order to glimpse its inner workings and larger implications more perceptively. Originating as a new kind of modernist literary analysis, it is not far removed from Bertolt Brecht&rsquo;s &ldquo;distancing effect&rdquo; or tactics in surrealist art-making or Walter Benjamin&rsquo;s efforts to develop new modes of alienated aesthetic analysis in response to the &ldquo;age of technological reproducibility&rdquo; <sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>  <sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>. Image sonification also joins the long-running digital humanities interest in &ldquo;deformance,&rdquo; in which strategic acts of altering a text produce new versions with revealing differences to the original — indeed, sometimes questioning what we even mean by the concept of an &ldquo;original&rdquo; <sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>  <sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>. At some level, all digital humanities efforts, even the most committed to positivistic statistical analysis, involve this kind of distortion of the empirical record. As Lisa Gitelman has taught us, &ldquo;raw data is an oxymoron,&rdquo; and all data are &ldquo;always already new&rdquo; <sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup>  <sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup>. To be sure, there are truths to discover, but when it comes to analyses of images and sounds, the truths are often multi-perspectival, multivocal, and rarely self-evident. By actively and critically playing with the representations produced as and through data, using computers to aid in the process of hearing images as well as viewing them, we can seek out fuller, richer interpretations of the materials we study. We can listen more deeply to what we are seeing.<sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup></p>
<p>To sonify image data is to flip the contemporary obsession with data visualization, in which data of any kind are converted into what Franco Moretti famously called &ldquo;graphs, maps, trees&rdquo; <sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>. If we stop privileging the optic, the viewer — now the listener — can pivot between the visual and the audial. This places the two halves of the audiovisual binary into dialogue with each other. After all, if we already visualize sound when creating digitally produced spectrograms of frequencies, why not sonify pixel information? New details and fresh interpretive possibilities, ideas, meanings, and implications emerge when one studies photographs by combining optical and audial perception together in new synesthetic modes of analysis. Understood this way, image sonification offers an underexplored, exploratory audio tactic that can lead to discoveries about visual sources. As Kevin L. Ferguson argued in the 2019  <em>Debates in the Digital Humanities,</em>  &ldquo;Rethinking our viewing practices in a digital age…requires an investment in experimental, theoretical methods that run counter to the rationalist uses of quantitative data often employed in DH work&rdquo; <sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup>. If we shift from the aggrandizement of the statistical as well as the tyranny of the visual to more adventurous digital considerations of artefactual representation — if we embrace Fred Gibbs and Trevor Owens&rsquo; foundational digital humanities call for a more critically aware and creatively expansive &ldquo;hermeneutics of data&rdquo; — we can activate the ear as well as the eye to perceive more accurately and revealingly a fuller interpretive picture of the past <sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup>.</p>
<p>I have argued elsewhere that digital image sonification offers a provocative method for pursuing a fresh historical understanding of archival photographs. In my earlier essay, I particularly noticed in particular how image glitching and sonification produced new interpretive perspectives on race in the US folk music revival <sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup>. In this essay, I turn my eyes and ears to questions of gender, space, performance, and democracy in music performance at the height of the early 1960s folk revival <sup id="fnref:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup>. Once again, I am struck most of all by the capacity of digital image sonification to amplify the stakes of photographic representation.<sup id="fnref:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup>  The visual comes alive in a different register when transformed via its presence as pixels into sound. Hearing what we are viewing asks us to see a photograph anew. The ear aids the eye in perceptual reorientation. New perceptions provide opportunities for more perceptive interpretations.</p>
<p>As an experiment, I placed the Joan Baez image into an application, Photosounder, created by programmer Michel Rouzic.<sup id="fnref:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup>  Photosounder reads images left to right and correlates pixel brightness to frequencies of pink noise, with lower, quieter pitches generated by darker pixels and higher, chirpier pitches created by brighter areas of a digital image. There are a few parameters one can manipulate, but generally, it is a very simple image sonification program, only hinting at the variations with which one might experiment. Even so, what resulted was a kind of sonic x-ray of the image that brought out not so much the music Baez was making when the photograph was taken, but rather, to my ears, the dynamics of space and gender in her performance at the Greek Amphitheater.</p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://player.vimeo.com/video/506285503" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="Sonification using the application Photosounder of photograph of Joan Baez performing at the Hearst Greek Theatre, Berkeley Folk Music Festival Collection, Charles Deering McCormick Library of Special Collections, Northwestern University Libraries, n.d. (possibly 1963), photographer unknown, https://dc.library.northwestern.edu/items/87ac8798-6aaf-456e-9c3e-dc187c796115. Frequency scale logarithmic base set weighted toward the logarithmic at 2.0 on a proportional scale of 1 to 2." webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://player.vimeo.com/video/506366229" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="Sonification with using the application Photosounder of photograph of Joan Baez performing at the Hearst Greek Theatre, Berkeley Folk Music Festival Collection, Charles Deering McCormick Library of Special Collections, Northwestern University Libraries, n.d. (possibly 1963), photographer unknown, https://dc.library.northwestern.edu/items/87ac8798-6aaf-456e-9c3e-dc187c796115. Frequency scale logarithmic base set toward the linear, at roughly 1.2 on a proportional scale of 1 to 2." webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div>
<p>One fascinating quality of image sonification is that sound transforms space into time, creating an intensification of the spatial relations captured in the photograph by sequencing them temporally. The very dimensions of a two-dimensional photograph now have aural depth and presence.<sup id="fnref:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup>  This is a useful way in which to enter into thinking about qualities of audience-performer relationships that the photograph might quite literally flatten or even obscure or only suggest vaguely in visual form. For me, the most productive parameter to play with was the frequency scale logarithmic base. Set to a higher scale, it produced more of a whisper when the sonification reached the figure of Baez center stage. Set to a lower scale, it produced a series of chirps and whistles. Mostly, however, the darkness of the image save for Baez under the spotlight turned out to be intriguingly quiet. Lack of sound can, after all, be part of image sonification and AudioVisual DH analysis too.<sup id="fnref:20"><a href="#fn:20" class="footnote-ref" role="doc-noteref">20</a></sup>  The sound murmured across the dark areas of the photograph. Only when it arrived at the illuminated figure of Baez did it create a small bubble of sound with the frequency scale logarithmic base set to 2.0 (<a href="#figure02">Figure 2</a>) or burst out with a quick set of windy, echoing whistles with the frequency scale logarithmic base set to roughly 1.2 (<a href="#figure03">Figure 3</a>). As if to announce her spectral presence, far away from the top of the Greek Amphitheater where the photographer was, both sonifications alerted me to the dynamics of her presence on stage, a distant figure under a stark white spotlight, but also the focus of attention.</p>




























<figure ><img loading="lazy" alt="" src="/dhqwords/vol/15/1/000508/resources/images/figure04.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/1/000508/resources/images/figure04_hu58c1f097a584e3965b8e5b3078854ce4_161212_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/15/1/000508/resources/images/figure04_hu58c1f097a584e3965b8e5b3078854ce4_161212_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/15/1/000508/resources/images/figure04_hu58c1f097a584e3965b8e5b3078854ce4_161212_1200x0_resize_q75_box.jpg 1200w,/dhqwords/vol/15/1/000508/resources/images/figure04_hu58c1f097a584e3965b8e5b3078854ce4_161212_1500x0_resize_q75_box.jpg 1500w,/dhqwords/vol/15/1/000508/resources/images/figure04.jpg 1500w" 
     class="landscape"
     ><figcaption>
        <p>Audience at Joan Baez concert at the Hearst Greek Amphitheatre, Berkeley Folk Music Festival Collection, Charles Deering McCormick Library of Special Collections, Northwestern University Libraries, n.d. (possibly 1963), photographer unknown, <a href="https://dc.library.northwestern.edu/items/1c69c98a-98e0-4923-8730-19d0546480ef">https://dc.library.northwestern.edu/items/1c69c98a-98e0-4923-8730-19d0546480ef</a>.
        </p>
    </figcaption>
</figure>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://player.vimeo.com/video/506364585" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="Sonification using the application Photosounder of photograph of Audience at Joan Baez concert at the Hearst Greek Amphitheatre, Berkeley Folk Music Festival Collection, Charles Deering McCormick Library of Special Collections, Northwestern University Libraries, n.d. (possibly 1963), photographer unknown, https://dc.library.northwestern.edu/items/1c69c98a-98e0-4923-8730-19d0546480ef. Frequency scale logarithmic base set weighted toward the logarithmic at 2.0 on a proportional scale of 1 to 2." webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div>
<p>Two themes emerged the more I listened to the sonifications and looked at the photograph. First, despite the ideals of the folk revival as a decentralized, communal, democratic movement, the sonification intensified the distinction between Baez‚ the emerging celebrity on stage, and the audience. Second, I began to wonder more about the stakes of Baez as a female performer within the revival. On the first theme, the sonification amplified how this was no campfire &ldquo;Kumbaya&rdquo; session, but rather a spectacle at scale, with a star on stage who drew everyone&rsquo;s attention and the masses listening to and looking at her under the stark white spotlight. Compare, for instance, to another image sonification of the audience, probably taken from before or after the same concert (<a href="#figure04">Figure 4</a> and <a href="#figure05">Figure 5</a>). In this one, the sonification was much noisier, picking up the many patterns of heads, shirts, spotlights, and parts of the Greek Amphitheater. Here is scattered attention, the dispersed buzz of people before or after shared concentration on a conventional concert recital performance. Taken together, the image sonifications of the two images signaled how the photographs of Baez at the Greek represent key tensions in the early 1960s folk revival. On the one hand, it sought to be anti-commercial and anti-hierarchical, shifting music-making from the power differentials between entertainer and audience to a shared experience of musical communion. On the other hand, its popularity led to increasingly conventional modes of presentation, precisely entertainment, a commercial undertaking with all the imbalances between a star and passive spectators.</p>
<p>Baez, who became perhaps the iconic female figure of the revival, was particularly caught up in these tensions. A second theme about gender erupted from the image sonification. At times, she tried to follow in the footsteps of Pete Seeger, adopting progressive and radical political causes or following in his collective singalong concert tradition; at other times, she embraced the role of a distant icon of the revival. On stage at the Greek, the sonification alerts us to the contradictions she sustained as a performer. She is but a whisper in one sonification, a little burst of whistles in the other. In both, the larger darkness almost swallows her up sonically in the epic space of the Greek Amphitheater. And yet she is also the only part of the image to generate sound, especially in the whistle sonification. This reminds us that it is Baez, who focuses attention from the audience, holds forth at the sole microphone we glimpse in the photograph. She is the only figure under the spotlight. Save for a quick whisper of sound from the proscenium of the stage and an illuminated walkway to the side of the stage, we only hear her in the sonifications.</p>
<p>And what of the sound generated from the image itself in this particular experiment? The ghostly, hollow quality of one sonification and the whistle blips of the other caused me to ponder the difference between Baez, the person, and Baez, the performer, how one was always lurking within the other. A spectral absence within a charismatic performance, the actual Baez offers an embodied presentation of authenticity that is oddly disembodied. The sonifications amplified for me a young woman was thrust into the spotlight of the folk revival at the height of its popularity. She had to negotiate the constraints of gender within the folk revival milieu. As such, Baez was able to hold forth, convene an audience, speak her musical truth, and articulate progressive political ideas publicly. At the same time, Baez was reduced within the revival. One would see this a few years later in her relationship on stage and in the music business to Bob Dylan, with whom she started performing around the time this image was taken. While he pushed forward with the freedom to reinvent himself, claim the mantle of the high modernist artist, Baez was often limited to gendered roles of the girlfriend, harmony singer, or placed in a kind of virginal Madonna stereotype within the folk movement <sup id="fnref:21"><a href="#fn:21" class="footnote-ref" role="doc-noteref">21</a></sup>  <sup id="fnref:22"><a href="#fn:22" class="footnote-ref" role="doc-noteref">22</a></sup>.</p>
<p>All that just from some pink noise generated by pixel brightness? Some might complain that I am &ldquo;reading&rdquo; too much into the sonification. Others might contend the opposite: that all the sonification revealed was what, to a critical eye, the image already visualized. Precisely on both counts. The fuss of going through the process of sonification is not to get away from the contextualized interpretation of image data, but rather to read data that constitute the photographic artifact more probingly, carefully, and insightfully (or is it now insoundfully?). By shifting from visual to aural form, one can get closer to the fullness of what the artifact itself contains, what it suggests as some of its meanings and implications. Moving from merely optic investigation to a synesthetic movement between image and sound, between the optic and the visual, allows for far richer analysis, a more adventurous inquiry into even one photograph.</p>
<p>Overall, digitally listening to as well as looking at photographs allows for greater potential access to underlying performative, emotional, spatial, embodied, and contextual dimensions captured in imagery. As art historian Tina M. Campt writes about her non-digital approach to looking at photographs from the African Diaspora so too for digital image sonification:  “When the practice of listening is not just about hearing but an attunement to different levels of photographic audibility,”  what emerges is  “an attunement to sonic frequencies of affect and impact.”  The human sensorium is not neatly divided between eye and ear.<sup id="fnref:23"><a href="#fn:23" class="footnote-ref" role="doc-noteref">23</a></sup>  It is capable of new perceptions from the synesthetic interplay between the visual and the audial. Listening to images can lead to, as Campt puts it, an  “ensemble of seeing, feeling, being affected, contacted, and moved beyond the distance of sight and observer”   <sup id="fnref:24"><a href="#fn:24" class="footnote-ref" role="doc-noteref">24</a></sup>. Campt is most interested in discerning subjectivities of Black fugitivity and futurity in the photographs she examines. Image sonification expands the repertoire she outlines by bringing the power of digital computation to this project, using it to treat photographic data not as totalized empirical facts, but rather as sources of multidimensional meanings that bring one more deeply into the images less discernable aspects.</p>
<p>Digital image sonification, pursued through the method I am proposing, also offers an avenue for crisscrossing between &ldquo;distant&rdquo; and &ldquo;close&rdquo; reading in digital humanities scholarship. As Alan Liu predicted in 2012, &ldquo;one of the next frontiers for the digital humanities will be to discover technically and theoretically how to negotiate between distant and close reading&rdquo; <sup id="fnref:25"><a href="#fn:25" class="footnote-ref" role="doc-noteref">25</a></sup>. Sound can convey subtle qualities of timbre, tone, interrelationships between different elements, and other kinds of information that visual, textual, or statistical data do not display as effectively <sup id="fnref:26"><a href="#fn:26" class="footnote-ref" role="doc-noteref">26</a></sup>. So too, sound presents particular qualities for negotiating the divide between distant and close reading because it represents space as time. This allows for the compacting of large amounts of information into a quick signal, allowing one to listen &ldquo;at scale.&rdquo; It also, paradoxically, produces the ability to magnify minutiae by rendering data temporally, forcing an observer to take in details more slowly, as I did with the multiple sonifications of the Joan Baez photograph. Capable of going big or small with data, digital image sonification asks us to seek truth without resorting to overly simplistic and reductive assertions of objectivity. In the immersive &ldquo;acoustic space,&rdquo; as Edmund Carpenter and Marshall McLuhan called it, &ldquo;sight isolates&rdquo; while &ldquo;sound incorporates,&rdquo; as another scholar, Walter Ong, famously contended. &ldquo;Whereas sight situates the observer outside what he views, at a distance,&rdquo; Ong argued, &ldquo;sound pours into the hearer&rdquo; <sup id="fnref:27"><a href="#fn:27" class="footnote-ref" role="doc-noteref">27</a></sup>  <sup id="fnref:28"><a href="#fn:28" class="footnote-ref" role="doc-noteref">28</a></sup>.<sup id="fnref:29"><a href="#fn:29" class="footnote-ref" role="doc-noteref">29</a></sup>  Scholars such as Emily Thompson, Jonathan Sterne, Veit Erlmann, Alexandra Supper, and Karin Bijsterveld have since complicated simplistic compartmentalization of the senses into sight and sound as discrete entities, which is precisely why thinking synesthetically about the interplay between the two might be most productive as an AudioVisual DH tactic <sup id="fnref:30"><a href="#fn:30" class="footnote-ref" role="doc-noteref">30</a></sup>  <sup id="fnref:31"><a href="#fn:31" class="footnote-ref" role="doc-noteref">31</a></sup>  <sup id="fnref:32"><a href="#fn:32" class="footnote-ref" role="doc-noteref">32</a></sup>  <sup id="fnref:33"><a href="#fn:33" class="footnote-ref" role="doc-noteref">33</a></sup>  <sup id="fnref:34"><a href="#fn:34" class="footnote-ref" role="doc-noteref">34</a></sup>  <sup id="fnref:35"><a href="#fn:35" class="footnote-ref" role="doc-noteref">35</a></sup>  <sup id="fnref:36"><a href="#fn:36" class="footnote-ref" role="doc-noteref">36</a></sup>  <sup id="fnref:37"><a href="#fn:37" class="footnote-ref" role="doc-noteref">37</a></sup>.<sup id="fnref:38"><a href="#fn:38" class="footnote-ref" role="doc-noteref">38</a></sup>  Digital image sonification brings us into an image&rsquo;s form and content, but because we are doing so synesthetically, digital image sonification also alienates us from what we hear and see, poising us in an intermedial and intermediary space between the visual and the aural — a fruitful perceptual interzone for consideration, interpretation, and intellectual discovery.</p>
<p>Finally, digital image sonification presents an example of how AudioVisual DH can push forward research in digital history and archival thinking as part of digital humanities writ large. By heightening the perception of fleeting feelings and sensations from archives of the past, we might realize Bethany Nowviskie&rsquo;s call for  “speculative collections,”  lively play with repository holdings to activate the dormant meanings of artifacts <sup id="fnref:39"><a href="#fn:39" class="footnote-ref" role="doc-noteref">39</a></sup>. To my eye and ear, turning the Baez photograph in which music was being made back into sound summoned forth some of the  “archival liveness”  that Tom Schofield and colleagues picture digital technologies capable of accomplishing <sup id="fnref:40"><a href="#fn:40" class="footnote-ref" role="doc-noteref">40</a></sup>  <sup id="fnref:41"><a href="#fn:41" class="footnote-ref" role="doc-noteref">41</a></sup>. Rather than merely reproduce the knowledge of the archive flatly, rather than only look at the past only through the surface of the archive&rsquo;s gaze, image sonification delves more deeply and precisely into data to transpose archival knowledge into new keys. It opens up history&rsquo;s evidentiary record for what might be present — or absent — from sight alone. We cannot hear the music Joan Baez was making at the Greek Amphitheater that night in the early 1960s, but we can hear the stakes of her music-making more potently when we sonify the photograph of her. Digital image sonification invigorates both the content and the context of the image. Issues of gender, power, embodiment, spectacle, performance, hierarchy, performance, and more emerge. Synesthetically pivoting between the visual and the audio in a cyborgian dance of data, signal, image, sound, history, and human perception, image sonification activates an artifact&rsquo;s data in fresh, audiovisual ways. In doing so, it also activates the scholarly imagination.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>On archeological approaches, see <sup id="fnref:42"><a href="#fn:42" class="footnote-ref" role="doc-noteref">42</a></sup>  <sup id="fnref:43"><a href="#fn:43" class="footnote-ref" role="doc-noteref">43</a></sup>  <sup id="fnref:44"><a href="#fn:44" class="footnote-ref" role="doc-noteref">44</a></sup>. This research is particularly inspired by Wolfgang Ernst&rsquo;s theoretical insights, although he might not entirely agree with my turn back to questions of cultural history. See <sup id="fnref1:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup><sup id="fnref:45"><a href="#fn:45" class="footnote-ref" role="doc-noteref">45</a></sup>. See also, <sup id="fnref:46"><a href="#fn:46" class="footnote-ref" role="doc-noteref">46</a></sup>  <sup id="fnref:47"><a href="#fn:47" class="footnote-ref" role="doc-noteref">47</a></sup>  <sup id="fnref:48"><a href="#fn:48" class="footnote-ref" role="doc-noteref">48</a></sup>  <sup id="fnref:49"><a href="#fn:49" class="footnote-ref" role="doc-noteref">49</a></sup>. On the history of photography, see classic studies such as <sup id="fnref:50"><a href="#fn:50" class="footnote-ref" role="doc-noteref">50</a></sup>  <sup id="fnref:51"><a href="#fn:51" class="footnote-ref" role="doc-noteref">51</a></sup>.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Ernst, Wolfgang.  <em>Digital Memory and the Archive,</em>  ed. and with introduction by Jussi Parikka. Minneapolis: University of Minnesota, 2013.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Arnold, Taylor and Lauren Tilton.  “Distant Viewing: Analyzing Large Visual Corpora.”  <em>Digital Scholarship in the Humanities</em>  (March 2019): 1-14.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Shklovsky, Viktor.  “Art as Device (1917/1919).”  <em>Viktor Shklovsky: A Reader</em> , ed. and trans. Alexandra Berlina. New York: Bloomsbury, 2018.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>Brecht, Bertolt.  “Alienation Effects in Chinese Acting.”  <em>Brecht on Theatre: The Development of an Aesthetic</em> . John Willett, ed. New York: Macmillan, 1964, 91-99.&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>Benjamin, Walter.  “The Work of Art in the Age of Mechanical Reproduction.” In  <em>Iluminations.</em>  Hannah Arendt, ed. New York: Harcourt Brace Jovanovich, 1968, 217-252.&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>McGann Jerome and Lisa Samuels,  “Deformance and Interpretation.”  <em>New Literary History</em>  30 (Winter 1999): 25–56.&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>Sample, Mark.  “Notes towards a Deformed Humanities.”    <em>Sample Reality.</em>  2 May 2012. <a href="https://www.samplereality.com/2012/05/02/notes-towards-a-deformed-humanities/">www.samplereality.com/2012/05/02/notes-towards-a-deformed-humanities</a>.&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>Gitelman, Lisa.  <em>Always Already New: Media, History, and the Data of Culture</em> . Cambridge, MA: The MIT Press, 2006.&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>Gitelman, Lisa.  <em>&ldquo;Raw Data&rdquo; Is an Oxymoron</em> . Cambridge, MA: MIT Press, 2013.&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>For a discussion of deep listening and critical play in the digital humanities, literary studies, linguistics, and sound studies, see <sup id="fnref:52"><a href="#fn:52" class="footnote-ref" role="doc-noteref">52</a></sup>. Examples of the sonification of other sorts of data besides images include <sup id="fnref:53"><a href="#fn:53" class="footnote-ref" role="doc-noteref">53</a></sup>  <sup id="fnref:54"><a href="#fn:54" class="footnote-ref" role="doc-noteref">54</a></sup>  <sup id="fnref:55"><a href="#fn:55" class="footnote-ref" role="doc-noteref">55</a></sup>  <sup id="fnref1:26"><a href="#fn:26" class="footnote-ref" role="doc-noteref">26</a></sup>.&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p>Moretti, Franco.  <em>Graphs, Maps, Trees: Abstract Models for Literary History</em> . New York: Verso, 2007.&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13">
<p>Ferguson, Kevin L.  “Volumetric Cinema.”  <em>Debates in the Digital Humanities 2019,</em>  eds. Matthew Gold and Lauren Klein. Minneapolis: University of Minnesota, 2019, 335-249. <a href="https://dhdebates.gc.cuny.edu/read/untitled-f2acf72c-a469-49d8-be35-67f9ac1e3a60/section/a214af4f-2d31-4967-8686-738987c02ddf">https://dhdebates.gc.cuny.edu/read/untitled-f2acf72c-a469-49d8-be35-67f9ac1e3a60/section/a214af4f-2d31-4967-8686-738987c02ddf</a>.&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14">
<p>Gibbs, Fred and Trevor Owens.  “The Hermeneutics of Data and Historical Writing.” In   <em>Writing</em>    <em>History in the Digital Age.</em>  eds. Jack Dougherty and Kristen Nawrotzki. Ann Arbor: University of Michigan Press, 2013. <a href="https://quod.lib.umich.edu/d/dh/12230987.0001.001/1:7/--writing-history-in-the-digital-age?g=dculture;rgn=div1;view=fulltext;xc=1#7.3">https://quod.lib.umich.edu/d/dh/12230987.0001.001/1:7/&ndash;writing-history-in-the-digital-age?g=dculture;rgn=div1;view=fulltext;xc=1#7.3</a>&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:15">
<p>Kramer, Michael J.  “Glitching History: Using Image Deformance to Rethink Agency and Authenticity in the 1960s American Folk Music Revival,”  <em>Current Research in Digital History</em>    <em>2018</em> .&#160;<a href="#fnref:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:16">
<p>Kramer, Michael J.  “&lsquo;A Foreign Sound to Your Ear&rsquo;: Digital Image Sonification For Historical Interpretation,” in  <em>Digital Sound Studies,</em>  eds. Mary Caton Lingold, Darren Mueller, and Whitney Anne Trettien (Duke University Press, 2018), 178-214.&#160;<a href="#fnref:16" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:17">
<p>For overviews of the history of the folk music revival, see <sup id="fnref:56"><a href="#fn:56" class="footnote-ref" role="doc-noteref">56</a></sup>  <sup id="fnref:57"><a href="#fn:57" class="footnote-ref" role="doc-noteref">57</a></sup>  <sup id="fnref:58"><a href="#fn:58" class="footnote-ref" role="doc-noteref">58</a></sup>  <sup id="fnref:59"><a href="#fn:59" class="footnote-ref" role="doc-noteref">59</a></sup>  <sup id="fnref:60"><a href="#fn:60" class="footnote-ref" role="doc-noteref">60</a></sup>.&#160;<a href="#fnref:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:18">
<p>For more information, visit the website for <a href="http://photosounder.com"> <em>Photosounder</em> </a>.&#160;<a href="#fnref:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:19">
<p>For more on the social construction of spatial relations handled through digital methods, see <sup id="fnref:61"><a href="#fn:61" class="footnote-ref" role="doc-noteref">61</a></sup>.&#160;<a href="#fnref:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:20">
<p>One immediately thinks of John Cage&rsquo;s famous thinking about silence as useful here. See <sup id="fnref:62"><a href="#fn:62" class="footnote-ref" role="doc-noteref">62</a></sup>.&#160;<a href="#fnref:20" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:21">
<p>Baez, Joan.  <em>And a Voice to Sing With: A Memoir.</em>  1987; reprint, New York: Simon and Schuster, 2009.&#160;<a href="#fnref:21" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:22">
<p>Hadju, David. Positively  <em>4th Street The Lives and Times of Joan Baez, Bob Dylan, Mimi Baez Farina, and Richard Farina</em> . New York: Farrar Straus &amp; Giroux, 2001.&#160;<a href="#fnref:22" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:23">
<p>As Jonathan Sterne notes, the separation and classification of the human sensorium into five distinctive senses is a historical outcome of the Enlightenment, not a timeless biological phenomenon. See <sup id="fnref1:31"><a href="#fn:31" class="footnote-ref" role="doc-noteref">31</a></sup>.&#160;<a href="#fnref:23" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:24">
<p>Campt, Tina M.  <em>Listening to Images</em> . Durham: Duke University Press, 2017.&#160;<a href="#fnref:24" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:25">
<p>Liu, Alan.  “The State of the Digital Humanities: A Report and a Critique.”  <em>Arts and Humanities in Higher Education</em> , 11, 1-2 (2012): 8-41.&#160;<a href="#fnref:25" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:26">
<p>Schedel, Margaret.  “Sounds of Science: The Mystique of Sonification.”  <em>Sounding Out!,</em>  9 October 2014.&#160;<a href="#fnref:26" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:26" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:27">
<p>Ong, Walter.  <em>Orality and Literacy.</em>  New York: Routledge, 1982.&#160;<a href="#fnref:27" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:28">
<p>Carpenter, Edmund and Marshall McLuhan.  “Acoustic Space.” In  <em>Explorations in Communications</em> . Marshall McLuhan and Edmund Carpenter, eds. Boston, MA: Beacon Press, 1960, 64-69.&#160;<a href="#fnref:28" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:29">
<p>With thanks to Robert Cantwell <sup id="fnref1:56"><a href="#fn:56" class="footnote-ref" role="doc-noteref">56</a></sup> for reminding me of Ong&rsquo;s theories of sound and its significance.&#160;<a href="#fnref:29" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:30">
<p>Erlmann, Veit.  <em>Reason and Resonance: A History of Modern Aurality</em> . New York: Zone Books, 2014.&#160;<a href="#fnref:30" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:31">
<p>Sterne, Jonathan.  <em>The Audible Past: Cultural Origins of Sound Studies</em> . Durham, NC: Duke University Press, 2003.&#160;<a href="#fnref:31" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:31" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:32">
<p>Sterne, Jonathan., ed.  <em>The Sound Studies Reader.</em>  New York: Routledge, 2012.&#160;<a href="#fnref:32" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:33">
<p>Supper, Alexandra.  “Sublime Frequencies:  The Construction of Sublime Listening Experiences in the Sonification of Scientific Data.”  <em>Social Studies of Science</em>  44, 1 (February 2014): 34–58.&#160;<a href="#fnref:33" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:34">
<p>Supper, Alexandra and Karin Bijsterveld.  “Sounds Convincing: Modes of Listening and Sonic Skills in Knowledge Making.”  <em>Interdisciplinary Science Reviews</em>  40, 2 (June 2015): 124–44&#160;<a href="#fnref:34" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:35">
<p>Supper, Alexandra.  “Lobbying for the Ear, Listening with the Whole Body: The (Anti-)Visual Culture of Sonification,”  <em>Sound Studies</em>  2, 1 (January 2016): 69–80.&#160;<a href="#fnref:35" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:36">
<p>Thompson, Emily.  <em>The Soundscape of Modernity: Architectural Acoustics and the Culture of Listening in America, 1900-1933</em> . Cambridge, MA: The MIT Press, 2002.&#160;<a href="#fnref:36" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:37">
<p>Thompson, Emily.  “Author&rsquo;s Statement.” The Roaring &lsquo;Twenties.  <em>Vectors.</em>  22 September 2013. <a href="http://vectors.usc.edu/projects/index.php?project=98&amp;thread=AuthorsStatement">http://vectors.usc.edu/projects/index.php?project=98&amp;thread=AuthorsStatement</a>.&#160;<a href="#fnref:37" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:38">
<p>Many works in sound studies are useful for thinking beyond the optic without fetishizing the ear as a supposedly better sense. See also <sup id="fnref:63"><a href="#fn:63" class="footnote-ref" role="doc-noteref">63</a></sup>  <sup id="fnref:64"><a href="#fn:64" class="footnote-ref" role="doc-noteref">64</a></sup>  <sup id="fnref:65"><a href="#fn:65" class="footnote-ref" role="doc-noteref">65</a></sup>  <sup id="fnref:66"><a href="#fn:66" class="footnote-ref" role="doc-noteref">66</a></sup>  <sup id="fnref:67"><a href="#fn:67" class="footnote-ref" role="doc-noteref">67</a></sup>  <sup id="fnref:68"><a href="#fn:68" class="footnote-ref" role="doc-noteref">68</a></sup>.&#160;<a href="#fnref:38" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:39">
<p>Nowviskie, Bethany.  “Speculative Collections.”  <em>Nowviskie.org.</em>  27 October 2016. <a href="http://nowviskie.org/2016/speculative-collections/">http://nowviskie.org/2016/speculative-collections/</a>.&#160;<a href="#fnref:39" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:40">
<p>Schofield, Tom and David Kirk, Telmo Amaral, Marian Dörk, Mitchell Whitelaw, Guy Schofield, and Thomas Ploetz.  “Archival Liveness: Designing with Collections Before and During Cataloguing and Digitization.”    <em>Digital Humanities Quarterly</em>  9, 3 (2015). <a href="/dhqwords/vol/9/3/000227/">http://www.digitalhumanities.org/dhq/vol/9/3/000227/000227.html</a>.&#160;<a href="#fnref:40" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:41">
<p>Ward Megan with Adrian S. Wisnicki.  “The Archive After Theory.”  <em>Debates in the Digital Humanities 2019</em> , eds. Matthew K. Gold and Lauren F. Klein. Minneapolis: University of Minnesota Press, 2019, 200-205.&#160;<a href="#fnref:41" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:42">
<p>Wall, John N.  “Transforming the Object of our Study: The Early Modern Sermon and the Virtual Paul&rsquo;s Cross Project.”  <em>Journal of Digital Humanities</em>  3, 1 (Spring 2014).&#160;<a href="#fnref:42" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:43">
<p>Einix, Linda ed.  <em>Archaeoacoustics: The Archaeology of Sound</em> . Myakka City, FL: OTS Foundation, 2014.&#160;<a href="#fnref:43" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:44">
<p>LaFrance, Adrienne.  “Hearing the Lost Sounds of Antiquity.”  <em>The Atlantic</em> . 19 February 2006.&#160;<a href="#fnref:44" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:45">
<p>Ernst, Wolfgang.  <em>Sonic Time Machines: Explicit Sound, Sirenic Voices, and Implicit Sonicity</em> . Amsterdam: Amsterdam University Press, 2016.&#160;<a href="#fnref:45" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:46">
<p>Bolter, Jay David and Richard Grusin.  <em>Remediation: Understanding New Media</em> . Cambridge, MA: MIT Press, 2000.&#160;<a href="#fnref:46" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:47">
<p>Gitelman, Lisa.  <em>Paper Knowledge: Toward a Media History of Documents</em> . Durham: Duke University Press, 2014.&#160;<a href="#fnref:47" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:48">
<p>Sayers, Jentery, ed.  <em>The Routledge Companion to Media Studies and Digital Humanities</em> . New York: Routledge, 2018.&#160;<a href="#fnref:48" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:49">
<p>Wendy Hui Kyong Chun and Thomas Keenan, eds.,  <em>New Media, Old Media: A History and Theory Reader</em>  (New York: Routledge, 2005).&#160;<a href="#fnref:49" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:50">
<p>Sontag <em>,</em>  Susan.  <em>On Photography.</em>  New York: Farrar, Straus and Giroux, 1977.&#160;<a href="#fnref:50" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:51">
<p>Barthes, Roland.  <em>Camera Lucida: Reflections on Photography</em> . Trans. Richard Howard. New York: Farrar, Straus, and Giroux, 1981.&#160;<a href="#fnref:51" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:52">
<p>Clement, Tanya.  “When Texts of Study are Audio Files: Digital Tools for Sound Studies in DH.” In  <em>A New Companion to Digital Humanities</em> . Eds. Susan Schreibman, Ray Siemens, and John Unsworth. Hoboken, NJ: Wiley-Blackwell, 2015: 348-357.&#160;<a href="#fnref:52" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:53">
<p>Joque, Justin.  “Listening to the Dow.”   <a href="http://vimeo.com/23965023">http://vimeo.com/23965023</a>.&#160;<a href="#fnref:53" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:54">
<p>Laumeister, Maximillian.  “Realtime Bitcoin Transaction Visualizer (formerly Listen to Bitcoin).”   <a href="https://www.bitlisten.com/">www.bitlisten.com</a>.&#160;<a href="#fnref:54" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:55">
<p>Foo, Brian.  “Two Trains: Sonification of Income Inequality on the NYC Subway.” <a href="https://datadrivendj.com/tracks/subway/">https://datadrivendj.com/tracks/subway/</a>.&#160;<a href="#fnref:55" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:56">
<p>Cantwell, Robert.  <em>When We Were Good: The Folk Revival</em> . Cambridge, MA: Harvard University Press, 1997.&#160;<a href="#fnref:56" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:56" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:57">
<p>Filene, Benjamin.  <em>Romancing the Folk: Public Memory and American Roots Music</em> . Chapel Hill: The University of North Carolina Press, 2000.&#160;<a href="#fnref:57" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:58">
<p>Cohen, Ronald D.  <em>Rainbow Quest: The Folk Music Revival and American Society, 1940–1970</em> . Amherst, MA: University of Massachusetts Press, 2002.&#160;<a href="#fnref:58" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:59">
<p>Donaldson, Rachel Clare.  <em>&ldquo;I Hear America Singing&rdquo;: Folk Music and National Identity</em> . Philadelphia: Temple University Press, 2014.&#160;<a href="#fnref:59" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:60">
<p>Wald, Elijah.  <em>Dylan Goes Electric!: Newport, Seeger, Dylan, and the Night That Split the Sixties</em> . New York: Dey Street Books, 2015&#160;<a href="#fnref:60" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:61">
<p>White, Richard.  “What Is Spatial History?”  <em>Spatial History Lab Working Paper,</em>  1 February 2010.&#160;<a href="#fnref:61" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:62">
<p>Cage, John.  <em>Silence: Lectures and Writings.</em>  Middletown, CT: Wesleyan University Press, 1961.&#160;<a href="#fnref:62" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:63">
<p>Corbin, Alain.  <em>Village Bells: Sound and Meaning in the Nineteenth-Century French Countryside.</em>  Trans. Martin Thom. New York: Columbia University Press, 1998.&#160;<a href="#fnref:63" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:64">
<p>Smith, Mark M.  <em>Listening to Nineteenth-Century America</em> . Chapel Hill: The University of North Carolina Press, 2001.&#160;<a href="#fnref:64" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:65">
<p>Rath, Richard C.  <em>How Early America Sounded</em> . Ithaca, NY: Cornell University Press, 2003.&#160;<a href="#fnref:65" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:66">
<p>Bull, Michael and Les Black, eds.  <em>The Auditory Cultures Reader.</em>  New York: Bloomsbury, 2004.&#160;<a href="#fnref:66" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:67">
<p>Smith, Mark M., ed.  <em>Hearing History: A Reader</em> . Athens: University of Georgia Press, 2004.&#160;<a href="#fnref:67" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:68">
<p>2015 Novak, David and Matt Sakakeeny, eds.  <em>Keywords in Sound</em> . Durham: Duke University Press Books, 2015.&#160;<a href="#fnref:68" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry></feed>