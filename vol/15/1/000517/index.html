<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=theme-color content="#3D206C"><meta http-equiv=x-ua-compatible content="IE=edge"><link rel=apple-touch-icon sizes=180x180 href=/img/favicon/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/img/favicon/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/img/favicon/favicon-16x16.png><link rel=icon type=image/png sizes=384x384 href=/img/favicon/android-chrome-384x384.png><link rel=icon type=image/png sizes=192x192 href=/img/favicon/android-chrome-192x192.png><link rel=icon type=image/png sizes=150x150 href=/img/favicon/mstile-150x150.png><link rel="shortcut icon" href=/favicon.ico><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/img/favicon/safari-pinned-tab.svg color=#3d206c><link rel=schema.dc href=http://purl.org/DC/elements/1.0/><meta name=citation_public_url content="https://rlskoeser.github.io/dhqwords/vol/15/1/000517/"><meta name=citation_title content="Advances in Digital Music Iconography: Benchmarking the detection of musical instruments in unrestricted, non-photorealistic images from the artistic domain"><meta name=citation_date content="2021/03"><meta name=citation_author content="Matthia Sabatelli"><meta name=citation_author content="Nikolay Banar"><meta name=citation_author content="Marie Cocriamont"><meta name=citation_author content="Eva Coudyzer"><meta name=citation_author content="Karine Lasaracina"><meta name=citation_author content="Walter Daelemans"><meta name=citation_author content="Pierre Geurts"><meta name=citation_author content="Mike Kestemont"><meta name=citation_abstract content="Introduction: the era of the pixel The Digital Humanities constitute an intersectional community of praxis, in which the application of computing technologies in various subdisciplines in the Geisteswissenschaften plays a significant role. Surveys of the history of the field [hockey2004] have stressed that most of the seminal applications of computing technology were heavily, if not exclusively, text-oriented: due to the hardware and software limitations of the time, analyses of image data (but also audio or video data) remained elusive and out of practical reach until relatively late, certainly at a larger scale."><meta name=citation_journal_title content="DHQwords"><meta name=citation_issue content="15.1"><meta name=citation_publisher content="Center for Digital Humanities, Princeton University"><meta name=DC.rights content="http://creativecommons.org/licenses/by/4.0/"><meta name=author content="Matthia Sabatelli, Nikolay Banar, Marie Cocriamont, Eva Coudyzer, Karine Lasaracina, Walter Daelemans, Pierre Geurts, Mike Kestemont"><meta name=generator content="Center for Digital Humanities, Princeton University"><meta name=dcterms.created content="2021-03"><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-regular.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-italic.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-regular.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-300.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-700.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-300italic.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-500italic.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-700italic.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-500.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/Source_Sans_Pro/SourceSans3VF-Roman.otf.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/Source_Sans_Pro/SourceSans3VF-Italic.otf.woff2 crossorigin><title>Advances in Digital Music Iconography: Benchmarking the detection of musical instruments in unrestricted, non-photorealistic images from the artistic domain</title><meta name=description content="DHQwords Issue 15.1, March 2021. an experiment republishing DHQ articles with Hugo & Startwords theme. "><meta property="og:title" content="Advances in Digital Music Iconography: Benchmarking the detection of musical instruments in unrestricted, non-photorealistic images from the artistic domain"><meta property="og:description" content="Introduction: the era of the pixel The Digital Humanities constitute an intersectional community of praxis, in which the application of computing technologies in various subdisciplines in the Geisteswissenschaften plays a significant role. Surveys of the history of the field [hockey2004] have stressed that most of the seminal applications of computing technology were heavily, if not exclusively, text-oriented: due to the hardware and software limitations of the time, analyses of image data (but also audio or video data) remained elusive and out of practical reach until relatively late, certainly at a larger scale."><meta property="og:type" content="article"><meta property="og:url" content="https://rlskoeser.github.io/dhqwords/vol/15/1/000517/"><meta property="og:image" content="https://rlskoeser.github.io/dhqwords/social.png"><meta property="article:section" content="vol"><meta property="article:published_time" content="2021-03-05T00:00:00+00:00"><meta property="article:modified_time" content="2023-08-06T19:21:02-04:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://rlskoeser.github.io/dhqwords/social.png"><meta name=twitter:title content="Advances in Digital Music Iconography: Benchmarking the detection of musical instruments in unrestricted, non-photorealistic images from the artistic domain"><meta name=twitter:description content="Introduction: the era of the pixel The Digital Humanities constitute an intersectional community of praxis, in which the application of computing technologies in various subdisciplines in the Geisteswissenschaften plays a significant role. Surveys of the history of the field [hockey2004] have stressed that most of the seminal applications of computing technology were heavily, if not exclusively, text-oriented: due to the hardware and software limitations of the time, analyses of image data (but also audio or video data) remained elusive and out of practical reach until relatively late, certainly at a larger scale."><link rel=stylesheet href=/dhqwords/style.css><link rel=stylesheet href=/dhqwords/print.css media=print><script src=/dhqwords/js/polyfills.js></script><script defer src=/dhqwords/js/bundle.js></script></head><body class=article><header><nav class=main aria-label=main><ul><li class=home><a href=/dhqwords/>dhq</a></li><li class=issues><a href=/dhqwords/vol/>volumes</a></li></ul></nav></header><main><article><div class=grid><header><p class=number><a href=/dhqwords/vol/15/1/>Issue 15.1</a></p><p class=theme>AudioVisual Data in DH</p><h1>Advances in Digital Music Iconography: Benchmarking the detection of musical instruments in unrestricted, non-photorealistic images from the artistic domain</h1><p><ul class=authors><li><address>Matthia Sabatelli</address></li><li><address>Nikolay Banar</address></li><li><address>Marie Cocriamont</address></li><li><address>Eva Coudyzer</address></li><li><address>Karine Lasaracina</address></li><li><address>Walter Daelemans</address></li><li><address>Pierre Geurts</address></li><li><address>Mike Kestemont</address></li></ul></p><p><time class=pubdate datetime=2021-03>March 2021</time></p><ul class="categories tags"></ul><ul class=tags></ul><p class=formats></p></header><section class=print-only><a class=first-page-header href=/ aria-label=Startwords><img alt=Startwords src=/pdf-logotype.svg></a>
<a class=page-header href=/ aria-label=Startwords><img alt=Startwords src=/pdf-logo.svg></a>
<a href=http://doi.org/ rel=alternate class=page-doi>doi:</a></section><div class=text-container><h2 id=introduction-the-era-of-the-pixel>Introduction: the era of the pixel</h2><p>The Digital Humanities constitute an intersectional community of praxis, in which the application of computing technologies in various subdisciplines in the <em>Geisteswissenschaften</em> plays a significant role. Surveys of the history of the field<a class=footnote-ref href=#hockey2004> [hockey2004] </a>have stressed that most of the seminal applications of computing technology were heavily, if not exclusively, text-oriented: due to the hardware and software limitations of the time, analyses of image data (but also audio or video data) remained elusive and out of practical reach until relatively late, certainly at a larger scale. In the past decade, the application of deep neural networks has significantly pushed the state of the art in computer vision, leading to impressive advances in tasks such as image classification or object detection<a class=footnote-ref href=#lecun2015> [lecun2015] </a><a class=footnote-ref href=#schmidhuber2015>[schmidhuber2015] </a>. Even more recently, improvements in the field of computer vision have started to find practical applications in study domains outside of strict machine learning, such as physics, medicine or even astrology. Supported by this technology&rsquo;s (at times rather naive) coverage in the popular media, the communis opinio has been eager to herald the advent of the &ldquo;Era of the Pixel&rdquo;.</p><p>In the Digital Humanities too, the potential of computer vision is nowadays increasingly recognized. A programmatic duet of two recent articles on &ldquo;distant viewing&rdquo; in the field&rsquo;s flagship journal<a class=footnote-ref href=#wevers2020> [wevers2020] </a><a class=footnote-ref href=#arnold2019>[arnold2019] </a>leads the way in this respect, emphasizing the privileged role these new methodologies can play in the exploration of large data collections in the Humanities. The present paper too is situated in a multidisciplinary project in which we investigate how modern artificial intelligence can support GLAM institutions (galleries, libraries, archives, and museums) in cataloguing and curating their rapidly expanding digital assets. As a case study, we shall work with non-photorealistic depictions of musical instruments in the artistic domain.</p><p>The structure of this paper is as follows. First, we motivate and contextualize our case study of musical instruments from within the scholarly framework of music iconography and computer vision, but also from the more pragmatic context of the research project from which this focus has emerged. We go on to describe the construction and characteristics of an annotated benchmark dataset, the MINERVA dataset, that will be released together with this paper, through which we hope to stimulate further research in this area. Using this benchmark data, we stress-test the available technology for the identification and detection of objects in images and discuss the current limitations of systems. To illustrate the broader relevance of our approach, we apply the trained benchmark system &lsquo;in the wild&rsquo;, on unseen and out-of-sample heritage data, followed by a quantitative and qualitative evaluation of the results. Finally, we identify what seem to be the most relevant directions for future research.</p><h2 id=motivation>Motivation</h2><h2 id=music-iconography>Music iconography</h2><p>The present paper must be understood against the wider scholarly background of music iconography, a Humanities field of inquiry with a rich, interdisciplinary history in its own right.<a class=footnote-ref href=#buckley1998> [buckley1998] </a>concisely defined music iconography as a field being &ldquo;concerned with the study of the visual representation of musical topics. Its primary materials include portraits of performers and composers, illustrations of instruments, occasions of music-making, and the use of musical imagery for purposes of metaphorical or allegorical allusion&rdquo;. Because of this wide range of topics, at the intersection of art history and musicology<a class=footnote-ref href=#baldassarre2007> [baldassarre2007] </a><a class=footnote-ref href=#baldassarre2008>[baldassarre2008] </a>, the field takes pride of its interdisciplinarity.</p><p>Music iconography deliberately adopts a &ldquo;methodological plurality&rdquo;<a class=footnote-ref href=#baldassarre2007> [baldassarre2007] </a>which is increasingly complemented with digital approaches. A major achievement in this respect has been the establishment (in 1971) and continued expansion and curation of an international digital inventory for musical iconography, the <em>Répertoire International d&rsquo;Iconographie Musicale</em> (RIDIM). Now publicly available as an online web resource (<a href=https://ridim.org/>https://ridim.org/</a>), RIDIM functions as a reference image database, designed to facilitate the efficient yet powerful description and discovery of music-related art works<a class=footnote-ref href=#green2013> [green2013] </a>. The need for such an international inventory has been acknowledged as early as 1929 and its significant scope facilitates the international study of music-related phenomena and their depiction across the visual arts.</p><p>Music iconography has an important tradition of focused studies targeting the deep, interpretive analysis of individual artworks or small collections of them. Such hermeneutic case studies have the advantage of depth, but understandably lack a more panoramic perspective on the phenomena of interest and, for instance, diachronic or synchronic trends and shifts therein. The large-scale, &ldquo;serial&rdquo; study of musical instruments as depicted across the visual arts remains a desideratum in the field and has the potential of bringing a macroscopic perspective to historical developments. In the present paper, we explore the feasibility of applying methods from present-day computer vision, in an attempt to scale up current approaches. The primary motivation of this endeavour is that digital music iconography – or &ldquo;Distant&rdquo; music iconography, in an analogy to similar developments in literary studies<a class=footnote-ref href=#wevers2020> [wevers2020] </a><a class=footnote-ref href=#arnold2019>[arnold2019] </a>– in principle has much to gain from such methods, at least if they are carefully applied and in continuous interaction with experts in the domain. Our focal point is the automated identification and detection of individual musical instruments in unrestricted, digitized materials from the realm of the visual arts.</p><p>This scholarly initiative is embedded in the collaborative research project INSIGHT (Intelligent Neural Systems as InteGrated Heritage Tools), which aims to stimulate the application of Artificial Intelligence to the rapidly expanding digital collections of a selection of federal museum clusters in Belgium.<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> One important, transcommunal aspect to Belgium&rsquo;s cultural history relates to music and musical history, with the invention of the saxophone by Adolphe Sax as an iconic example. An additional factor is the presence of the Musical Instruments Museum in the capital (Brussels) that contributed significantly to international research projects in this area (and which is a partner in the INSIGHT project). This contextualization, finally, is also important to understand our specific choice for the topic of musical instruments, as a representative and worthwhile case study on the application of modern machine learning technology in digital heritage studies.</p><h2 id=computer-vision>Computer vision</h2><p>The methodology for the present paper largely derives from machine learning and more specifically computer vision, a field concerned with computational algorithms that can mimic the perceptual abilities of humans and their capacity to construct high-level interpretations from raw visual stimuli<a class=footnote-ref href=#ballard1982> [ballard1982] </a>. In the past decade, this field has gone through a remarkable renaissance, following the emergence of powerful learning techniques based on so-called neural networks. In particular the advent of &ldquo;convolutional&rdquo; networks<a class=footnote-ref href=#lecun2015> [lecun2015] </a>has led to dramatic advances in the state of the art for a number of standard applications, including image classification (&ldquo;Is this an image of a cat or a dog?&rdquo;) and object detection (&ldquo;Draw a bounding box around any cats in this image&rdquo;). For some of these tasks, modern computer systems have even been shown to rival the performance of humans<a class=footnote-ref href=#russakovsky2015> [russakovsky2015] </a>. In spite of the impressive advances in recent computer vision research, it is generally acknowledged that the state of the art is still confronted with a number of major, as yet unsolved, challenges. In this section we highlight four concrete issues that are especially pressing, given the focus of this paper on image collections in the artistic domain. These challenges motivate our work from the point of view of computer vision, rather than art history.</p><h2 id=photo-realism>Photo-realism</h2><p>One major hurdle is that computer vision nowadays strongly gravitates towards so-called photo-realistic material, i.e. digitized or born-digital versions of photographs that do not actively attempt to distort the reality they depict. The best example in this respect is the influential ImageNet dataset<a class=footnote-ref href=#russakovsky2015> [russakovsky2015] </a>, that offers highly realistic photographic renderings of everyday concepts drawn from WordNet&rsquo;s lexical database. While some more recent heritage collections of course abound in such photo-realistic material (e.g. advertisements in historic newspapers), traditional photography does not take us further back in time than the nineteenth century<a class=footnote-ref href=#hertzmann2018> [hertzmann2018] </a>. Additionally, the Humanities study many other visual arts that prioritize much less photorealistic representation and focus even on completely &lsquo;fictional&rsquo; renderings of (potentially imagined or historical) realities. While there has been some encouraging and worthwhile prior work into the application of computer vision to non-photorealistic depictions, this work is generally more scattered and the results (understandably) less advanced than those reported for the photorealistic domain. Inspiring recent studies in this area include<a class=footnote-ref href=#crowley2014> [crowley2014] </a><a class=footnote-ref href=#van2015>[van2015] </a><a class=footnote-ref href=#seguin2018>[seguin2018] </a><a class=footnote-ref href=#bell2019>[bell2019] </a>.</p><h2 id=data-scarcity>Data scarcity</h2><p>It is a well-known limitation that convolutional neural networks require large amounts of manually annotated example data (or training data) in order to perform well. To address this issue, the community has released several public datasets over the years<a class=footnote-ref href=#xiang2014> [xiang2014] </a><a class=footnote-ref href=#russakovsky2015>[russakovsky2015] </a><a class=footnote-ref href=#mensink2014>[mensink2014] </a><a class=footnote-ref href=#strezoski2017>[strezoski2017] </a><a class=footnote-ref href=#lin2014>[lin2014] </a>which has allowed the successful training of a large set of neural architectures<a class=footnote-ref href=#he2016> [he2016] </a><a class=footnote-ref href=#szegedy2015>[szegedy2015] </a><a class=footnote-ref href=#simonyan2014>[simonyan2014] </a>. However, the nature of the images included in these datasets is mostly photo-realistic, also because such images are relatively straightforward to obtain and annotate. These image collections are very different in terms of texture, content and availability from the sort of data that can nowadays be found in the digital heritage domain.</p><p>Computer vision researchers interested in the artistic domain have attempted to alleviate the relative dearth of training data by either releasing domain-specific datasets<a class=footnote-ref href=#mensink2014> [mensink2014] </a><a class=footnote-ref href=#strezoski2017>[strezoski2017] </a>or through the application of transfer learning<a class=footnote-ref href=#sabatelli2018> [sabatelli2018] </a>, a machine learning paradigm which allows the application of neural networks to domains where training data is scarce. For image classification, for instance, these efforts have indeed greatly contributed to overall feasibility of applying computer vision outside the photo-realistic domain<a class=footnote-ref href=#sabatelli2018> [sabatelli2018] </a>. Both approaches, however, have limitations when it comes to the complementary task of object detection. Popular datasets such as the Rijksmuseum collection<a class=footnote-ref href=#mensink2014> [mensink2014] </a>or the more recent OmniArt dataset<a class=footnote-ref href=#strezoski2017> [strezoski2017] </a>do not come with the metadata required for object-detection problems.</p><p>With this work, we take one step forward in addressing these limitations. Firstly, the MINERVA dataset that we present below, specifically tackles the problem of object detection within the broader heritage domain of the visual arts, introducing a novel benchmark for researchers working at the intersection of computer vision and art history. Secondly, we present a number of baseline results on the newly introduced dataset. The results are reported for a representative set of common architectures, which were pre-trained on photo-realistic images. This allows us to investigate to what extent these methods can be reused when tested on artistic images.</p><h2 id=irrelevant-training-categories>Irrelevant training categories</h2><p>Previous studies have demonstrated the feasibility of &ldquo;pretraining&rdquo;: with this approach, networks are first trained on (large) photorealistic collections (i.e. the source domain) and then applied downstream (or further fine-tuned) on an out-of-sample target domain, that has much less annotated data available. While generally useful, this approach is still confronted with the problem that the annotation labels or categories attested in the source domain are often of little interest within the target domain (i.e. art history, in the present case). The popular Pascal-VOC dataset<a class=footnote-ref href=#everingham2010> [everingham2010] </a>, for instance, tackles the detection of 20 classes, out of which more than a third constitute different kinds of transportation systems, such as trains, boats, motorcycles and cars. Naturally, these means of transportation are very unlikely to be represented in artworks that date back to the premodern period. The more complex MS-COCO dataset<a class=footnote-ref href=#lin2014> [lin2014] </a>presents similar problems: even though the amount of classes increases to 80, most of the objects which should be detected are again unlikely to be represented within historical works of art, since they correspond to objects which have only been relatively recently invented such as &ldquo;microwave&rdquo;, &ldquo;cell-phone&rdquo;, &ldquo;tv-monitor&rdquo;, &ldquo;laptop&rdquo;, or &ldquo;remote&rdquo;, and the like. This poses a serious constraint when it comes to the use of pre-trained object-detectors for artistic collections. As with most supervised learning algorithms, models trained on these collections will only perform well on the sort of data on which they have been explicitly trained. To illustrate this model bias, we report some (nonsensical) detections in the first row of images presented in<a href=#figure01>Figure 1a</a>.</p><h2 id=robustness-of-the-models>Robustness of the models</h2><p>Popular object detectors such as YOLO<a class=footnote-ref href=#redmon2018> [redmon2018] </a>and Fast R-CNN<a class=footnote-ref href=#ren2017> [ren2017] </a>have been designed to perform well on the above-mentioned photo-realistic datasets. However, the variance of the samples denoting a specific class within these datasets is usually much smaller when compared to that in artistic collections. As an example, we refer to a number of images representing the person class within the Pascal-VOC dataset: we can observe from the two leftmost images of the bottom row of<a href=#figure01>Figure 1</a>that the representation of a &lsquo;person&rsquo; is overall relatively unambiguous and hardly distorted. As a result, the person class is usually easily detected by e.g. the YOLO architecture. However, we can see that this task already becomes harder when a person has to be detected within a painting (potentially with a highly distorted representation of the humans in the scene). As shown by the two rightmost images of the bottom row (<a href=#figure01>Figure 1b</a>), a YOLO-V3 model does not see most of the persons represented in the paintings and misclassifies them as non-human beings (e.g. &ldquo;bear&rdquo;).</p><figure><img loading=lazy alt src sizes="(max-width: 768px) 100vw, 80vw" srcset="500w,
800w, w" class=landscape><figcaption><p>Examples showing the limitations that occur when a standard object detector trained on photo-realistic images is tested in the domain of the visual arts. Figure 1a: Four anecdotal examples showing that the &ldquo;person” class is usually reasonably detected by the YOLO architecture, although other, non-sensical detections frequently occur. Figure 1b: the two images on the left show that the variation in depiction of people is limited in photorealistic material, in comparison to the artistic representations of people (two examples to the right).</p></figcaption></figure><p>All examples in<a href=#figure01>Figure 1</a>come from a pretrained YOLO-V3 model<a class=footnote-ref href=#redmon2018> [redmon2018] </a>which has been originally trained on the COCO dataset<a class=footnote-ref href=#lin2014> [lin2014] </a>and then tested on artworks coming from<a class=footnote-ref href=#mensink2014> [mensink2014] </a>and<a class=footnote-ref href=#strezoski2017> [strezoski2017] </a>. The images presented in the first row illustrate that the network is biased towards making detections which are very unlikely to appear in premodern depictions. These detections correspond to the identification of objects such as &ldquo;suitcase&rdquo;, &ldquo;umbrella&rdquo; or &ldquo;frisbee&rdquo; and &ldquo;banana&rdquo;. The two last images presented in the second row show that standard models fail to properly recognize a simple class such as person. In fact, they fully fail in detecting most of the persons that are present in the artworks due to these representations being highly different from the persons that are present in the Pascal-VOC dataset (first two images of the second row).</p><h2 id=minerva-dataset-description>MINERVA: dataset description</h2><p>In this section, we describe MINERVA, the annotated dataset in the field of object detection that is presented in this work. This novel benchmark dataset will be released jointly with this paper.<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup> The main task under scrutiny here is the detection of musical instruments in non-photorealistic, unrestricted image collections from the artistic domain. We have named the dataset with the acronym MINERVA, which stands for &lsquo;Musical INstrumEnts Represented in the Visual Arts&rsquo;, after the Roman goddess of the arts (amongst many other things).</p><h2 id=data-sources>Data Sources</h2><p>The base data for our annotation effort was assembled in a series of &lsquo;concentric&rsquo; collection campaigns, where we started from smaller, but high-quality datasets and gradually expanded into larger, albeit less well curated data sources.</p><p>RIDIM: We harvested a collection of high-quality images from the RIDIM database, in those cases where the database entries provided an unambiguous hyperlink to a publicly accessible image. These records were already assigned MIMO codes by a community of domain experts, which provided important support to our in-house annotators (especially during the first experimental rounds of annotations).RMFAB/RMAH: We expanded on the core RIDIM data by including (midrange resolution) images from the digital collections of two federal museums in Brussels: the RMFAB (Royal Museums of Fine Arts of Belgium, Brussels) and the RMAH (Royal Museums of Art and History, Brussels). These images were selected on the basis of previous annotations that suggested they included depictions of musical instruments, although no more specific labels (e.g. MIMO codes) were available for these records at this stage. Copyrighted artworks could not be included for obvious reasons (copyright lasts for 70 years from the death of the creator under Belgian intellectual law).Flickr: To scale up our annotation efforts, finally, we collected a larger dataset of images from the well-known image hosting service &lsquo;Flickr&rsquo; (<a href=https://www.flickr.com>www.flickr.com</a>). We harvested all images from a community-curated collection of depictions of musical instruments in the visual arts pre-dating 1800.<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup> This third campaign yielded much more data than the former two, but these were more noisy and contained a variety of false positives that had to be manually deleted during the annotation phase.</p><p>Our collection efforts were inclusive, and the resulting dataset should be considered as &ldquo;unrestricted&rdquo;, covering a variety of periods, genres and materials (although it was not feasible to include more precise metadata about these aspects in the dataset). Note that, exactly because of this highly mixed data origin, the distribution in MINERVA does not give a faithful representation of any kind of historic reality: music iconography gives a highly colored perspective on &ldquo;popular&rdquo; instruments in art history and some instruments may not often have been depicted, even though they were popular at the time. Likewise, other instruments are likely to be over-represented in iconography.</p><h2 id=vocabulary>Vocabulary</h2><p>To increase the interoperability of the dataset, individual instruments have been unambiguously identified using their MIMO codes. The MIMO (Musical Instrument Museums Online) initiative is an international consortium, well known for its online database of musical instruments, aggregating data and metadata from multiple heritage institutions<a class=footnote-ref href=#dolan2017> [dolan2017] </a>.<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup> An important contribution is their development of a uniform metadata documentation standard for the field, including a (multilingual) vocabulary to identify musical instruments in an interoperable manner. The MIMO ontology is hierarchical, meaning that each individual leaf node in their concept tree (e.g. &lsquo;viola&rsquo;) is a hyponym of a wider instrument category (e.g. &lsquo;viola&rsquo; ∈ &lsquo;string instruments&rsquo;).<a href=#table01>Table 1</a>shows examples of annotation labels from this ontology. Our dataset provides a spreadsheet that allows for the easy mapping of individual instruments to their instrument category. Below, we shall report experiments for the more fine-grained and less granular, hypernym versions of the categorization task.<br>Examples of annotation labels from the MIMO ontology (not all were encountered in MINERVA). <strong>Instrument hypernym</strong> <strong>Stringed instruments</strong> <strong>Wind instruments</strong> <strong>Percussion instruments</strong> <strong>Keyboard instruments</strong> <strong>Electronic instruments</strong> <em>Example instruments</em> Lute, psaltery, fiddle, viola da gamba, citternTransverse flute, end-blown trumpet, horn, shawm, bagpipeTambourine, cylindrical drum, frame drum, friction drum, bellPianoforte, virginal, portative organ, harpsichord, clavichordElectric guitar, synthesizer, theremin, vocoder, mellotron</p><h2 id=annotation-process>Annotation process</h2><p>Using the conventional method of rectangular bounding boxes, we have manually annotated 16,142 musical instruments (of which 172 unique) in a collection of 11,765 images, within the open-source<a href=https://cytomine.be>Cytomine</a>software environment<a class=footnote-ref href=#marée2016> [marée2016] </a>. Often multiple instruments appeared within the same images and bounding boxes were therefore allowed to overlap. Example annotations and a screenshot of the annotation environment are presented in<a href=#figure02>Figure 2</a>.</p><p>The dataset contains artistic objects from diverse periods and of various types, ranging from paintings, sculptures, drawings, to decorative arts, manuscript illuminations and stained-glass windows. Thus, they involve a daunting diversity of media, techniques and modes. Whereas in some cases the images were straightforward to annotate (e.g. an image representing a bell in full frame), several obstacles occurred on a recurrent basis. These obstacles can be linked to three parameters:</p><p>Representation: A challenging aspect was the variety of artistic depiction modes represented in the dataset, ranging from photo-realistic renderings to heavily stylized depictions from specific art-historical movements (e.g. impressionism, pointillism, fauvism, cubism, &mldr;) (<a href=#figure03>Figure 3a</a>). Additionally, visibility could be low due to a proportionally small instrument depiction or the profusion of details (<a href=#figure03>Figure 3b</a>). In some instances, the state of the depicted object and its medium made the detection of the instrument difficult, e.g. a damaged medieval tympanon (<a href=#figure03>Figure 3b</a>).Quality: Other, more pragmatic issues arose from the images themselves. Occasionally, the quality of the images was too low to be able to detect the instruments (e.g. low resolution or compression defects) (<a href=#figure03>Figure 3c</a>). A great deal of the images did not meet international quality standards for heritage reproduction photography (uniform and neutral environment and lighting, frontal point of view), which implies that the instruments were even more difficult to detect.Boxes: The use of a rectangular shape for the bounding boxes sometimes has limitations and implied a certain lack of precision, e.g. in the case of a diagonally positioned flute, or in the case of overlapping instruments (<a href=#figure03>Figure 3d</a>). For some instruments which consist of several parts, e.g. a violin and its bow, only the main part (the violin) was annotated.</p><figure><img loading=lazy alt src sizes="(max-width: 768px) 100vw, 80vw" srcset="500w,
800w, w" class=landscape><figcaption><p>Illustration of the annotation interface in Cytomine<a class=footnote-ref href=#marée2016> [marée2016] </a>.</p></figcaption></figure><figure><img loading=lazy alt src sizes="(max-width: 768px) 100vw, 80vw" srcset="500w,
800w, w" class=landscape><figcaption><p>Examples of difficulties encountered when annotating images.</p></figcaption></figure><h2 id=characteristics>Characteristics</h2><p>An important share of the annotations which we collected were singletons, i.e. instruments that were only encountered once or twice. Although we release the full dataset, we shall from now on only consider instruments that occurred at least three times that allow for a conventional machine learning setup (with non-overlapping train, validation and test sets, that include at least one instance of each label). Whereas the full MIMO vocabulary covers over 2,000 vocabulary terms for individual instruments, only a fraction of these were attested in the 4,183 images which we use below (overview in<a href=#table01>Table 1</a>). Note that this table shows a considerable drop in the original number of images that we annotated, because we only included images that (a) actually contained an instrument and (b) images depicting instruments that occurred at least thrice.</p><p>93 different instrument categories appear at least thrice in the dataset. A visualization of the heavily skewed distribution of the different instruments can be seen in<a href=#figure04>Figure 4</a>, where each instrument is represented together with its corresponding MIMO code (between parentheses). This distribution exposes two core aspects of this dataset (but also of music iconography in general): (i) its strong Western-European bias, which has been historically acknowledged, and which scholars are actively trying to correct nowadays, but which is a slow process; (ii) the &lsquo;heavy-tail&rsquo; distribution associated with cultural data in general; i.e. only a fraction of instruments, such as the lute, harp and violin, are depicted with a high frequency, the rest occurs much more sparsely.</p><figure><img loading=lazy alt src sizes="(max-width: 768px) 100vw, 80vw" srcset="500w,
800w, w" class=landscape><figcaption><p>Distribution of the instrument types in the full MINERVA dataset.</p></figcaption></figure><h2 id=versions-and-splits>Versions and splits</h2><p>The label imbalance described in the previous paragraph is a significant issue for machine learning methods. We therefore experiment with the data in five versions (that are available from the repository) that correspond to object detection tasks of varying complexity. We start by exploring whether it is possible to just detect the presence of an instrument in the different artworks, without the additional need of also predicting the class of the detected instrument. We refer to this benchmark as single-instrument object detection. We then move to three more challenging tasks in which we also aim at correctly classifying the content of the detected bounding boxes. We include data for this detection task for the top-5, the top-10 and top-20 most frequently occurring instruments, a customary practice in the field. Finally, we also repeat this task for all images, but with the &ldquo;hypernym&rdquo; labels of the instrument categories (see<a href=#figure05>Figure 5</a>).</p><p>Each version of the dataset comes with its own training, development and testing splits, where we offer the guarantee that at least one of the instrument classes in the task is represented in each of the splits. Additionally, the splits are stratified so that the class distribution is approximately the same in each split. The number of images per split in each version is summarized in<a href=#table02>Table 2</a>. The hypernym version of the dataset is not reported in this table as it shares the same images and splits as the single-instrument version (they both contain all instruments). We used a standard implementation<a class=footnote-ref href=#pedregosa2011> [pedregosa2011] </a>for a randomized and shuffled split at the level of images and the following, approximate proportions: 1/2 train, 1/4 dev, and 1/4 test. Images may contain multiple instruments, so that the actual number of instruments (as opposed to images) may vary relatively strongly across splits.<br>Image and instruments distributions of the training, development and test sets for the four different benchmarks presented in this paper (single instrument, top-5 instruments, top-10 instruments and top-20 instruments). <strong>Training-set</strong> <strong>Dev-set</strong> <strong>Test-set</strong> <strong>Total</strong> ImagInstImagInstImagInstImagInstSingle inst18574243113722881189210241838633Top-5 inst9521589540852724117322163614Top-10 inst122721476801127898150628054780Top-20 inst1471291586015431047183833786296<br><figure><img loading=lazy alt src sizes="(max-width: 768px) 100vw, 80vw" srcset="500w,
800w, w" class=landscape><figcaption><p>Distribution of the 5 hypernym categories over the three splits in the MINERVA dataset.</p></figcaption></figure></p><h2 id=benchmark-experiments>Benchmark experiments</h2><h2 id=classification>Classification</h2><p>In the first benchmark experiment, we start by investigating whether convolutional neural networks are able to correctly classify the different instruments that are present in the dataset. That means that we focus on the image classification task and postpone the task of object detection to the next section. To this end, we have extracted the various patches delineated by the bounding boxes in the detection dataset as stand-alone instances. Note, however, that patches from the same images always ended in the same split, to avoid information leakage across the splits. Example patches are shown in<a href=#figure06>Figure 6</a>.</p><figure><img loading=lazy alt src sizes="(max-width: 768px) 100vw, 80vw" srcset="500w,
800w, w" class=landscape><figcaption><p>Examples of the patches delineated by the bounding boxes, extracted from MINERVA images for the classification experiment.</p></figcaption></figure><p>Next, we tackled this task as a standard machine-learning classification problem for which we applied a representative selection of established neural network architectures. All of these networks were pretrained on the Rijksmuseum dataset<a class=footnote-ref href=#mensink2014> [mensink2014] </a>, for which the weights are publicly available<a class=footnote-ref href=#sabatelli2018> [sabatelli2018] </a>. The tested architectures are: VGG19<a class=footnote-ref href=#simonyan2014> [simonyan2014] </a>, Inception-V3<a class=footnote-ref href=#szegedy2015> [szegedy2015] </a>and ResNet<a class=footnote-ref href=#he2016> [he2016] </a>. This approach is motivated by previous work<a class=footnote-ref href=#sabatelli2018> [sabatelli2018] </a>which shows that when it comes to the classification images from the domain of cultural heritage, popular neural architectures which have been trained on the large Rijksmuseum collection, can outperform the same kind of architectures that are pre-trained on ImageNet only. In order to maximize the final classification performance, all network parameters get fine-tuned, using the Adam optimizer<a class=footnote-ref href=#kingma2014> [kingma2014] </a>and minimizing the conventional categorical cross-entropy loss function over mini-batches of 32 samples. Additionally, we applied 3 different learning rates: 0.001, 0.0001, 0.00001. In order to handle the skewed distribution of the classes, we experimented with models including and excluding oversampling. The training regime is interrupted as soon the validation loss does not decrease for five epochs in a row.</p><p>In<a href=#table01>Table 2</a>and<a href=#table03>Table 3</a>we report the results in terms of Accuracy and F1-score for the MINERVA test sets. For the individual instruments, we do so for four versions of the dataset of increasing complexity: the top-5 instruments, top-10 instruments, top-20 instruments and the entire dataset. Analogously we report the scores for a classification experiment where the object detector is trained on the instrument hypernyms as class labels.<br>Classification results on the MINERVA test set for the three architectures (best results in bold).Top-5 instTop-10 instTop-20 instAll instHypernymsCNNAcc.F1Acc.F1Acc.F1Acc.F1Acc.F1R-Net68.7164.1052.8541.5530.738.4526.362.0872.2652.66V3 <strong>73.66</strong> <strong>70.29</strong> <strong>55.51</strong> <strong>44.77</strong> <strong>36.51</strong> <strong>19.06</strong> <strong>27.02</strong> <strong>6.67</strong> <strong>75.80</strong> <strong>57.03</strong> V1948.3335.9237.5215.2233.419.8720.171.7266.4140.35Confusion matrix for the classification experiment with ResNet on the MINERVA test set (the top-10 most frequently occurring instruments). <strong>Predicted label / Gold label</strong> <strong>Bagpipe</strong> <strong>E-b trumpet</strong> <strong>Harp</strong> <strong>Horn</strong> <strong>Lute</strong> <strong>Lyre</strong> <strong>Por. organ</strong> <strong>Rebec</strong> <strong>Shawm</strong> <strong>Violin</strong> <strong>Bagpipe</strong> <strong>31</strong> 01068120717 <strong>E-b trumpet</strong> 4 <strong>72</strong> 192141313821 <strong>Harp</strong> 82 <strong>227</strong> 11031101019 <strong>Horn</strong> 7514 <strong>9</strong> 16912514 <strong>Lute</strong> 610176 <strong>199</strong> 651542 <strong>Lyre</strong> 3019113 <strong>5</strong> 20311 <strong>Por. organ</strong> 3010100 <strong>57</strong> 014 <strong>Rebec</strong> 52140904 <strong>7</strong> 123 <strong>Shawm</strong> 41125211246 <strong>40</strong> 13 <strong>Violin</strong> 6122943547116 <strong>202</strong></p><h2 id=detection>Detection</h2><p>For the second benchmark experiment we report the results that we have obtained on the four of the five detection benchmarks introduced in the previous section. The way the different instruments are distributed in their respective test sets is visually represented in the first image of each row of<a href=#figure07>Figure 7</a>. For our experiments, we use the popular YOLO-V3<a class=footnote-ref href=#redmon2018> [redmon2018] </a>architecture which we fully fine-tune during training. To explore the benefits that transfer learning could bring to the artistic domain, we initialize the network with the weights that are obtained after training the model on the MS-COCO dataset<a class=footnote-ref href=#lin2014> [lin2014] </a>. The network gets then trained either with the Adam optimizer<a class=footnote-ref href=#kingma2014> [kingma2014] </a>or RMSprop<sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup> over mini-batches of 8 images.</p><p>To assess the performance of the neural network, we follow the same evaluation protocol that characterizes object detection problems in CV<a class=footnote-ref href=#lin2014> [lin2014] </a>. Each detected bounding box is compared to the bounding box which has been annotated on the Cytomine platform. We only consider bounding boxes for which the confidence level is ≥ 0.05, following the protocol established in<a class=footnote-ref href=#everingham2010> [everingham2010] </a>. We then compute the &ldquo;Intersection over Union&rdquo; (IoU) for measuring how much the detected bounding-boxes differ from the ground-truth ones. To assess whether a prediction can be considered as a &ldquo;true positive&rdquo; or a &ldquo;false positive&rdquo;, we define two, increasingly restrictive metrics: first, IoU ≥ 10 and, secondly, IoU ≥ 50. This approach is again inspired by<a class=footnote-ref href=#gonthier2018> [gonthier2018] </a>, where the authors report additional results with an IoU ≥ 10 on their IconArt dataset.<a href=#table05>Table 5</a>lists precision, recall and average precision (AP) scores for each detected class of each data version and<a href=#figure07>Figure 7</a>visually shows the number of true and false positive predictions in all cases. Examples of correct detections are shown in<a href=#figure08>Figure 8</a>.<br>A quantitative analysis of the results obtained on the four localization benchmarks introduced in this work. To distinguish different benchmarks in the table we separate them by a double line. We report the precision, recall and average-precision scores for each detected class. <strong>Instrument ≥ IoU</strong> <strong>Precision</strong> <strong>Recall</strong> <strong>AP</strong> Single-instrument ≥10 Single-instrument ≥ 500.630.470.420.310.350.22Stringed-Instruments ≥ 10Stringed-Instruments ≥ 500.650.530.360.290.280.20Wind-Instruments ≥ 10Wind-Instruments ≥ 500.430.320.070.050.040.02Percussion-Instruments ≥ 10Percussion-Instruments ≥ 500.320.210.040.030.020.01Keyboard-Instruments ≥ 10Keyboard-Instruments ≥ 500.610.450.110.080.070.04Electronic-Instruments ≥ 10Electronic-Instruments ≥ 50&mdash;&mdash;Harp ≥ 10Harp ≥ 500.680.600.620.540.550.46Lute ≥ 10Lute ≥ 500.570.470.430.350.360.26Violin ≥ 10Violin ≥ 500.370.260.220.160.120.07Shawm ≥ 10Shawm ≥ 500.130.080.040.020.010.00End-blown trumpet ≥ 10End-blown trumpet ≥ 500.280.240.040.030.010.01Harp ≥ 10Harp ≥ 500.620.560.560.510.460.39Lute ≥ 10Lute ≥ 500.550.470.420.360.330.25Violin ≥ 10Violin ≥ 500.260.200.190.140.060.04Shawm ≥ 10Shawm ≥ 500.170.170.030.010.000.00End-blown trumpet ≥ 10End-blown trumpet ≥ 500.670.170.020.030.010.00Bagpipe ≥ 10Bagpipe ≥ 50000000Portative-Organ ≥ 10Portative-Organ ≥ 500.240.240.130.130.060.06Horn ≥ 10Horn ≥ 50000000Rebec ≥ 10Rebec ≥ 50&mdash;&mdash;Lyre ≥ 10Lyre ≥ 50&mdash;&mdash;-<br><figure><img loading=lazy alt src sizes="(max-width: 768px) 100vw, 80vw" srcset="500w,
800w, w" class=landscape><figcaption><p>A visual representation of how many instruments should be detected in the testing sets of the four MINERVA benchmarks that are introduced in this paper (first plot of each row). The second and third plots represent the true and false detections that we have obtained with a fully fine-tuned YOLO network. Results are computed with respect to an IoU ≥ 10 and an IoU ≥ 50.</p></figcaption></figure></p><figure><img loading=lazy alt src sizes="(max-width: 768px) 100vw, 80vw" srcset="500w,
800w, w" class=landscape><figcaption><p>Sample visualizations of the detections obtained on the MINERVA test set for a fully fine-tuned YOLO architecture. The first three rows report the detection of any kind of instrument within the images (single-instrument task), while the last three rows also report the correct classification of the detected bounding boxes.</p></figcaption></figure><h2 id=additional-experiments>Additional experiments</h2><p>As an additional stress-test, we have applied a trained object detector to two external data sets, in order to assess how valid and performant our approach is when applied &ldquo;in the wild&rdquo;. We have considered two out-of-sample datasets:</p><ul><li>RMFAB/RMAH: 428 out-of-sample images from the digital assets of both museum collections that are not included in the annotated material (and which are thus not included the train and validation material of the applied detector), because the available metadata did not explicitly specify that they contained depictions of musical instruments. (This collection cannot be shared due to copyright restrictions.)</li><li>IconArt: a generic collection of 6,528 artistic images, collected from the community-curated platform <em>WikiArt: Visual Art Encyclopedia</em> (<a href=https://www.wikiart.org/>https://www.wikiart.org/</a>). The IconArt subcollection was previously redistributed by<a class=footnote-ref href=#gonthier2018> [gonthier2018] </a>:<a href=https://wsoda.telecom-paristech.fr/downloads/dataset/>https://wsoda.telecom-paristech.fr/downloads/dataset/</a>.</li></ul><p>Note that both external datasets differ in crucial aspects:RMFAB/RMAHcan be considered &ldquo;out-of-sample&rdquo;, but &ldquo;in-collection&rdquo;, in the sense that these images derive from the same digital collections as many of the images represented in MINERVA. Additionally, we can expect extremely low detection rates for this dataset, because the presence of musical instruments will already have been flagged in a large majority of cases by the museum&rsquo;s staff. Thus, the application ofRMFAB/RMAHshould be viewed as a rather conservative stress test or sanity check, mainly checking for images that might have been missed by annotators in the past. The IconArt dataset is &ldquo;out-of-sample&rdquo; and &ldquo;out-of-collection&rdquo;, in the sense that these images derive from a variety of other sources. It is therefore fully unrestricted, and this test can be considered a curiosity-driven validation of the method &ldquo;in the wild&rdquo;. Importantly, IconArt was not collected with specific attention for musical instruments, so here too, we can anticipate a rather low detection rate (since many works of art simply do not feature any instruments). For all these reasons, we only evaluate the results on these external datasets in terms of precision (as recall is much less meaningful in this context).</p><p>Following these differences, we have applied the single-instrument detector to theRMFAB/RMAHdata and the hypernym detector to IconArt. Keeping an eye on the feasibility of the manual inspection, we have limited the number of instances returned by only allowing detections with a confidence score ≥ 0.20 (which is a rather generous threshold). Next, the results have then been evaluated in terms of precision, i.e. the number of returned image regions that actually represent musical instruments. The results are presented in<a href=#table06>Table 6</a>.<a href=#figure10>Figure 10</a>showcases a number of cherry-picked successful examples of detections from the out-of-collection IconArt images.<br>Quantitative evaluation of the method on two out-of-sample datasets in terms of precision, restricted to detections with a confidence score ≥ 0.20. <strong>Collection</strong> <strong>Total images</strong> <strong>Detections</strong> <strong>True positives</strong> <strong>RMFAB/RMAH</strong> 4281626 <strong>IconArt</strong> 652811842</p><h2 id=discussion>Discussion</h2><h2 id=skewed-results>Skewed results</h2><p>First and foremost, we can observe that the scores obtained across all benchmarks are generally much lower than those reported for other datasets in computer vision (outside of the strict artistic domain). This drop in performance was to be expected and can be attributed to both the smaller size of the training data and the higher variance in the representation spectrum of musical instruments (across periods, materials, modes and, artists). Secondly, one can observe large fluctuations in the identifiability and detectability of individual instrument categories across both tasks. Not all of the fluctuations are easy to account for.</p><p>We first consider the classification results. The confusion matrix reported in<a href=#table04>Table 4</a>clearly shows that the classes representing the top-4 of instruments (harp, lute, violin, and portative organ) can be learned rather successfully, but that the performance rapidly breaks down for instrument categories at lower frequency ranks. Thus, while the accuracies for the top-5 experiments are relatively satisfying, especially in terms of accuracy (V3: <em>acc=73.66; F1=70.29</em> ), the performance rapidly degrades for the more difficult setups. The results for the &ldquo;all&rdquo; classification experiment, where every instrument category is included no matter its frequency, are nothing less than dramatic (V3: <em>acc=27.02; F1=6.67</em> ) and call for in-depth further research. The significant divergence between accuracy scores and F1 scores demonstrate that class imbalance is thus another aspect in which MINERVA presents a more challenging benchmark than its photorealistic counterparts.</p><p>The skewness of the class distribution in MINERVA is representative of the long-tail distribution that we commonly encounter in cultural data. This imbalance is somewhat alleviated in the hypernym setup, where the labels are of course much better distributed over a much smaller number of classes ( <em>n=5</em> ). The general feasibility of this specific task is demonstrated by the encouraging scores that can be reported for the Inception-V3 architecture on this task ( <em>acc=75.80; F1=57.03</em> ). Note, additionally, that the &ldquo;Electronic instruments&rdquo; hypernym is included for completeness in this task, although the label is very infrequent and inevitably pulls down the (macro-averaged) F1-score in this respect. Overall, we notice than the Inception-V3 architecture yields the highest performance on average for the classification task.</p><p>Similar trends can be observed for the musical instrument detection task. First of all, we should emphasize the encouraging scores for the &ldquo;single-instrument&rdquo; detection task that simply aims to detect musical instruments (no matter their type). Here, a relatively high precision score is obtained ( <em>prec=0.63</em> for <em>IoU ≥ 10</em> ), which seems on par with comparable object categories for modern photo-realistic collections<a class=footnote-ref href=#ren2017> [ren2017] </a>. Thus, this algorithm might not be fully apt at retrieving every single instrument from an unseen collection, but when it detects an instrument, we can be relatively sure that the detection deserves further inspection by a domain expert. Equally heartening scores are evident for most of the instrument hypernyms (with the notable exception of the under-represented &ldquo;Electronic instruments&rdquo; hypernym). While these detection tasks are of course relatively coarse, this observation nevertheless entails that this sort of detection technology can already find useful applications in the field (see below).</p><p>When making our way down the frequency list in<a href=#table05>Table 5</a>, we again observe how the results break down dramatically for less common instrument categories. The fact that an over-represented category like harps can be reasonably well detected ( <em>AP(IoU ≥ 10)=0.55; AP(IoU ≥ 50)=0.46</em> ), should not lead the attention away from the fact that a state of the art object detector, such as YOLO, fails miserably at detecting a number of iconographically highly salient instruments, such as lyres and end-blown trumpets. At this stage, it is unclear whether this is caused by mere class imbalance or by the higher variance in the iconographic depiction of specific instruments. Bagpipes, for instance, occur frequently across images in MINERVA but might display much more depiction variance than, for instance, a harp.</p><h2 id=saliency-maps>Saliency maps</h2><p>The results from the previous question call into question which visual properties the neural networks find useful to exploit in the identification of instruments. Importantly, the characteristic features exploited by a machine learning algorithm need not coincide with the properties that are judged most relevant by human experts and the comparison of both types of relevance judgements is worthwhile. In this section, we therefore perform model criticism or &ldquo;network introspection&rdquo; on the basis of the so-called &ldquo;saliency maps&rdquo; that can be extracted from a trained model<a class=footnote-ref href=#boyarski2017> [boyarski2017] </a>. These saliency maps make visible to which regions in the original image the network paid most attention to, before arriving at its final classification decision. All examples discussed below come from the experiments on the hypernym dataset for the VGG19 network.<a href=#figure09>Figure 9</a>shows a series of manually selected, insightful examples, including the original image (as inputted into the network after preprocessing), as well as the saliency map obtained for it. We limit these examples to the representative hypernyms &lsquo;Stringed instruments&rsquo; and &lsquo;Wind instruments.&rsquo;</p><p>The maps in<a href=#figure09>Figure 9</a>vividly illustrate that the network focuses on two broad types of regions: properties of the instruments itself (which was expected) but also the immediate context of the instruments, and more specifically the way they are operated, handled or presented by people, c.q. musicians. The characteristics of the salient regions in the examples in<a href=#figure09>Figure 9</a>could be described as:</p><p>Stringed instruments:</p><p>(a) Focus on the neck of the stringed instrument, as well as the characteristic presence of tuning pins at the end of the neck;(b) Sensitive to the presence of stretched fingers in an unnatural position;(c) Typical conic shape of a lyre, with outward pointing ends connected by a bridge;</p><p>Wind instruments:</p><p>(d) Symmetric presence of tone holes in the areophone;(e) Elongated, cylindric shape of the main body of the areophone with wider end;(f) Mirrored placement of fingers and hands (close to one another).</p><p>These characteristics strongly suggest that the way an instrument is handled (i.e. its immediate iconographic neighborhood) is potentially of equal importance as the shape of the actual instrument, an insight that we will further expand on below.</p><figure><img loading=lazy alt src sizes="(max-width: 768px) 100vw, 80vw" srcset="500w,
800w, w" class=landscape><figcaption><p>Saliency maps for several stringed (subfigures (a) to (c)) and wind (subfigures (d) to (f)) instruments.</p></figcaption></figure><h2 id=error-analysis-false-positives>Error analysis: false positives</h2><p>In this section, we offer a qualitative discussion of the false positives from the out-of-sample tests reported in the previous section, i.e. instances where the detectors erroneously thought to have detected an instrument. This eagerness is a known problem of object detectors: a system that is trained to recognize &ldquo;sheep&rdquo; will be inclined to see &ldquo;sheep&rdquo; everywhere. Anecdotally, people have noted how misleading contextual cues can indeed be a confounding factor in image analysis. One blog post for instances noted how a major image labeling service tagged photographs of green fields with the &ldquo;sheep&rdquo; label, although no sheep whatsoever were present in the images.<sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup> Eagerness-to-detect or over-association is therefore a clear first shortcoming of this method when applied in the wild, mainly because it was only trained in images that actually contain musical instruments. Interestingly, the false positives come in clusters that shed an interesting light on this issue. Below we list a representative number of error clusters:</p><figure><img loading=lazy alt src sizes="(max-width: 768px) 100vw, 80vw" srcset="500w,
800w, w" class=landscape><figcaption><p>Examples of successful detections in IconArt for &ldquo;stringed instruments”.</p></figcaption></figure><figure><img loading=lazy alt src sizes="(max-width: 768px) 100vw, 80vw" srcset="500w,
800w, w" class=landscape><figcaption><p>Anecdotal examples of false positive detection, divided in 7 interpretive clusters (numbered a-g).</p></figcaption></figure><p>The above categorization illustrates that the false positives are rather insightful, mainly because the absence of an instrument highlights the contextual clues that are at work. Of particular relevance is the observation that the iconography surrounding children closely resembles that of instruments. This seems related to the intimate and caring body language of both the caretakers and musicians in such compositions. The immediate iconographic neighborhood of children clearly reminds the detector of the delicacy and reverence with which instruments are portrayed and presented in historical artworks. This delicacy and intimacy in body language can be specifically related to the foregrounding of fingers, the prominent portrayal of which invariably triggers the detector, also in the absence of children. Some of these phenomena invite closer inspection by domain experts in music iconography and suggest that serial or panoramic analyses are a worthwhile endeavour in this field, also from the point of view of more hermeneutically oriented scholars.</p><h2 id=conclusions-and-future-research>Conclusions and future research</h2><p>In this paper, we have introduced MINERVA, to our knowledge the first sizable benchmark dataset for the identification and detection of individual musical instruments in unrestricted, digitized images from the realm of the visual arts. Our benchmark experiments have highlighted the feasibility of a number of tasks but also, and perhaps primarily, the significant challenges that state-of-the-art machine learning systems are still confronted with on this data, such as the &ldquo;long-tail&rdquo; of the instruments&rsquo; distribution and the staggering variance in depiction across the images in the dataset. We therefore hope that this work will inspire new (and much-needed) research in this area. At the end of this paper, we wish to formulate some advice and concerns in this respect.</p><p>One evident direction from future research is more advanced transfer learning, where algorithms make more efficient use of the wealth of photorealistic data that is provided, for instance, by MIMO<a class=footnote-ref href=#dolan2017> [dolan2017] </a>. The main issue with the MIMO data in this respect is that the bulk of these photographs are context-free (i.e. the instruments are photographed in isolation, against a white or neutral background), which is almost never the case in the artistic domain. Preliminary research demonstrated that this a major hurdle to established pretraining scenarios. Cascaded approaches, where instruments are detected first and only classified in a second stage might be a promising avenue here.</p><p>One crucial final remark is that AI has an amply attested tendency not only to be sensitive to biases in the input data but also to amplify them<a class=footnote-ref href=#zou2018> [zou2018] </a>. Whereas the computational methods presented here have the potential to scale up dramatically the scope of current research in music iconography, it also comes with ideological dangers. The technology could further strengthen the bias on specific canonical regions and periods in art history and lead the attention even further away from artistic and iconographic cultures that are already in specific need of reappraisal. The community will therefore have to think carefully about bias correction and mitigation. Collecting training data in a diverse and inclusive manner, with ample attention for resource-lower cultures should be a key strategy in future data collection campaigns.</p><h2 id=acknowledgements>Acknowledgements</h2><p>We wish to thank Remy Vandaele for the help with Cytomine and for the fruitful discussions related to computer vision and object detection. Special thanks go out to our annotators and other (former) colleagues in the museums involved: Cedric Feys, Odile Keromnes, Lies Van De Cappelle and Els Angenon. Our gratitude also goes out to Rodolphe Bailly for his support and advice regarding MIMO. Finally, we wish to credit our former project member dr. Ellen van Keer with the original idea of applying object detection to musical instruments. This project is generously funded by the Belgian Federal Research Agency BELSPO under the BRAIN-be program.</p><ul><li id=arnold2019>Arnold, T., and Tilton, L., “Distant viewing: analyzing large visual corpora.” _Digital Scholarship in the Humanities_ , 34 (2019), i3-i16.</li><li id=baldassarre2007>Baldassarre, A. “Quo vadis music iconography? The Repertoire International d'Iconographie Musicale as a case study” _Fontes Artis Musicae_ , 54 (2007), 440-452.</li><li id=baldassarre2008>Baldassarre, A. “Music Iconography: What is it all about? Some remarks and considerations with a selected bibliography” _Ictus: Periódico do Programa de Pós-Graduação em Música da UFBA_ , 9 (2008), 55-95.</li><li id=ballard1982>Ballard, D. H., and Christopher M. Brown, C. M. _Computer Vision_ , Upper Saddle River (1982).</li><li id=bell2019>Bell, P., and Impett, L. “Ikonographie und Interaktion. Computergestützte Analyse von Posen in Bildern der Heilsgeschichte” _Das Mittelalter_ , 24 (2019): 31–53.</li><li id=boyarski2017>Mariusz Bojarski, Anna Choromanska, Krzysztof Choromanski, Bernhard Firner, Larry J. Ackel, Urs Muller, Philip Yeres, Karol Zieba, “VisualBackProp: Efficient Visualization of CNNs for Autonomous Driving” _Proceedings of the 2018 IEEE International Conference on Robotics and Automation (ICRA)_ , 2018, 4701-4708. DOI: 10.1109/ICRA.2018.8461053.</li><li id=buckley1998>Buckley, A. “Music Iconography and the Semiotics of Visual Representation” _Music in Art_ , 23 (1998), 5-10.</li><li id=crowley2014>Crowley, E., and Zisserman, A. “The State of the Art: Object Retrieval in Paintings using Discriminative Regions” In Valstar, M., French, A., and Pridmore, T. (eds), _Proceedings of the British Machine Vision Conference_ , Nottingham (2014), s.p.</li><li id=dolan2017>Dolan, E. I. “Review: MIMO: Musical Instrument Museums Online” _Journal of the American Musicological Society_ , 70 (2017): 555-565.</li><li id=everingham2010>Everingham, M., Van Gool, L., Williams, C. K., Winn, J., and Zisserman, A. “The Pascal visual object classes (VOC) challenge” In _International journal of computer vision_ , 88(2) (2010): 303–338.</li><li id=gonthier2018>Gonthier, N., Gousseau, Y., Ladjal, S. and Bonfait, O. “Weakly supervised object detection in artworks” In _Proceedings of the European Conference on Computer Vision (ECCV)_ (2018): 692–709.</li><li id=green2013>Green, A., and Ferguson, S. “RIDIM: Cataloguing music iconography since 1971” _Fontes Artis Musicae_ , 60 (2013), 1-8.</li><li id=he2016>He, K., Zhang, X., Ren, S., and Sun, J. “Deep residual learning for image recognition.” In _Proceedings of the IEEE conference on computer vision and pattern recognition_ , pages 770–778, 2010.</li><li id=hertzmann2018>Hertzmann, A. “Can Computers Create Art?” Arts, 7 (2018) doi:10.3390/arts7020018.</li><li id=hockey2004>Hockey, S. “A History of Humanities Computing.” In S. Schreibman, R. Siemens, and J. Unsworth (eds.), _A Companion to Digital Humanities_ , Oxford (2004), pp. 3–19.</li><li id=huang2017>Huang, G., Zhuang, L., Van Der Maaten, L., and Weinberger, K. “Densely connected convolutional networks” In _Proceedings of the IEEE conference on computer vision and pattern recognition_ (2017), pp. 4700– 4708.</li><li id=kingma2014>Kingma, D. P., and Ba, J. “A method for stochastic optimization” _arXiv preprint arXiv:1412.6980_ , 2014.</li><li id=lecun2015>LeCun, J., Bengio, Y., and Hinton, G., “Deep Learning” _Nature_ , 521 (2015): 436–444.</li><li id=lin2014>Lin T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollar, P and Zitnick, C. L. “Microsoft COCO: Common objects in context” In _European conference on computer vision_ , pages 740–755. Springer, 2014.</li><li id=marée2016>Marée, R., Rollus, L. Stévens, B., Hoyoux, R., Louppe, G., Vandaele, R., Begon, J., Kainz, P., Geurts, P., and Wehenkel “Collaborative analysis of multi-gigapixel imaging data using Cytomine” _Bioinformatics_ , 32 (2016): 1395–1401.</li><li id=mensink2014>Mensink, T. and Van Gemert, J. “The Rijksmuseum challenge: Museum-centered visual recognition” In _Proceedings of International Conference on Multimedia Retrieval_ , page 451. ACM, 2014.</li><li id=pedregosa2011>Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R. and Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., and Duchesnay, E. “Scikit-learn: Machine Learning in Python” _Journal of Machine Learning Research_ , 12 (2011): 2825-2830.</li><li id=redmon2018>Redmon, J. and Farhadi, A. “Yolov3: An incremental improvement” _arXiv preprint arXiv:1804.02767_ , 2018.</li><li id=ren2017>S. Ren, K. He, R. Girshick, and J. Sun, “Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks” _IEEE Transactions on Pattern Analysis and Machine Intelligence_ , 39 (2017), 1137-1149.</li><li id=russakovsky2015>Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, K., Khosla, A., Bernstein, M. et al. “Imagenet large scale visual recognition challenge” _International journal of computer vision_ , 115(3) (2015), 211–252.</li><li id=sabatelli2018>Sabatelli, M., Kestemont, M., Daelemans, W. and Geurts, P. “Deep transfer learning for art classification problems” In _Proceedings of the European Conference on Computer Vision (ECCV)_ , pages 631–646, 2018.</li><li id=schmidhuber2015>, Schmidhuber, J. “Deep Learning in Neural Networks: An Overview” _Neural Networks_ , 61 (2015), 85-117.</li><li id=seguin2018>Seguin, B. “The Replica Project: Building a visual search engine for art historians” XRDS: Crossroads, _The ACM Magazine for Students - Computers and Art_ , 24 (2018), 24-29.</li><li id=simonyan2014>Simonyan, K. and Zisserman, A. Very deep convolutional networks for large-scale image recognition. _arXiv preprint arXiv:1409.1556_ , 2014.</li><li id=strezoski2017>Strezoski, G. and Worring, M. “Omniart: multi-task deep learning for artistic data analysis” _arXiv preprint arXiv:1708.00684_ , 2017.</li><li id=szegedy2015>Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, S., Erhan, D., Vanhoucke, V., and Rabinovich, A. “Going deeper with convolutions” In _Proceedings of the IEEE conference on computer vision and pattern recognition_ (2015), pp. 1–9.</li><li id=van2015>Van Noord, N., Hendriks, E., and Postma, E., “Toward Discovery of the Artist's Style: Learning to recognize artists by their artworks” _IEEE Signal Processing Magazine_ , 32 (2015), 46-54.</li><li id=wevers2020>Wevers M., and Smits, T. “The visual digital turn: Using neural networks to study historical images” _Digital Scholarship in the Humanities_ , 35 (2020), 194–207.</li><li id=xiang2014>Xiang, Y., Mottaghi, R., and Savarese, S. “Beyond pascal: A benchmark for 3d object detection in the wild” In _IEEE Winter Conference on Applications of Computer Vision_ , pages 75–82. IEEE, 2014.</li><li id=zou2018>Zou, J., and Schiebinger, L. “AI can be sexist and racist — it's time to make it fair” _Nature_ , 559 (2018): 324-326.</li></ul><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>See<a href=https://hosting.uantwerpen.be/insight/>https://hosting.uantwerpen.be/insight/</a>. This project is generously funded by the Belgian Federal Research Agency BELSPO under the BRAIN-be program.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>All code used in this paper is publicly available from this repository:<a href=https://github.com/paintception/MINeRVA>https://github.com/paintception/MINeRVA</a>. Likewise, the MINERVA dataset can be obtained from this DOI on Zenodo: 10.5281/zenodo.3732580.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p><a href="https://web.archive.org/save/https://www.flickr.com/groups/1991907@N24/?rb=1">https://web.archive.org/save/https://www.flickr.com/groups/1991907@N24/?rb=1</a>&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p><a href=https://web.archive.org/save/https://www.mimo-international.com/MIMO/>https://web.archive.org/save/https://www.mimo-international.com/MIMO/</a>&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5><p>There is no officially published reference for RMSprop, but scholars commonly refer to this lecture from Geoffrey Hinton and colleagues:<a href=https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf>https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf</a>&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:6><p><a href=https://web.archive.org/save/https://aiweirdness.com/post/171451900302/do-neural-nets-dream-of-electric-sheep>https://web.archive.org/save/https://aiweirdness.com/post/171451900302/do-neural-nets-dream-of-electric-sheep</a>## Bibliography&#160;<a href=#fnref:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div></div></article></main><footer><nav aria-label="footer links"><ul><li><a class=highlight-focus href=/dhqwords/tags/>Tags</a></li><li><a class=highlight-focus href=/dhqwords/about/>About</a></li><li><a class=highlight-focus href=/dhqwords/categories/>Keywords</a></li></ul></nav><div class=icons><a class="license highlight-focus" rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons Attribution No-Derivatives 4.0 International License" src=/dhqwords/img/logos/license.svg width=120 height=42></a></div></footer></body></html>