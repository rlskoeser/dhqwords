<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=theme-color content="#3D206C"><meta http-equiv=x-ua-compatible content="IE=edge"><link rel=apple-touch-icon sizes=180x180 href=/img/favicon/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/img/favicon/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/img/favicon/favicon-16x16.png><link rel=icon type=image/png sizes=384x384 href=/img/favicon/android-chrome-384x384.png><link rel=icon type=image/png sizes=192x192 href=/img/favicon/android-chrome-192x192.png><link rel=icon type=image/png sizes=150x150 href=/img/favicon/mstile-150x150.png><link rel="shortcut icon" href=/favicon.ico><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/img/favicon/safari-pinned-tab.svg color=#3d206c><link rel=schema.dc href=http://purl.org/DC/elements/1.0/><meta name=citation_public_url content="https://rlskoeser.github.io/dhqwords/vol/15/1/000506/"><meta name=citation_title content="Comparative K-Pop Choreography Analysis through Deep-Learning Pose Estimation across a Large Video Corpus"><meta name=citation_date content="2021/03"><meta name=citation_author content="Peter Broadwell"><meta name=citation_author content="Timothy R. Tangherlini"><meta name=citation_abstract content="1. Introduction The ability to derive accurate information about human body poses and movements from arbitrary still images and videos introduces considerable new opportunities for digital humanities scholarship, especially in the realm of dance choreography analysis. Most such inquiries have previously occurred within visual media studies and among what might be considered &amp;amp;ldquo;DH-adjacent&amp;amp;rdquo; communities of dance and performance, with scholars using motion-capture systems to record the movements of small numbers of live dancers in controlled environments."><meta name=citation_journal_title content="DHQwords"><meta name=citation_issue content="15.1"><meta name=citation_publisher content="Center for Digital Humanities, Princeton University"><meta name=DC.rights content="http://creativecommons.org/licenses/by/4.0/"><meta name=author content="Peter Broadwell, Timothy R. Tangherlini"><meta name=generator content="Center for Digital Humanities, Princeton University"><meta name=dcterms.created content="2021-03"><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-regular.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-italic.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-regular.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-300.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-700.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-300italic.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-500italic.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-700italic.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-500.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/Source_Sans_Pro/SourceSans3VF-Roman.otf.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/Source_Sans_Pro/SourceSans3VF-Italic.otf.woff2 crossorigin><title>Comparative K-Pop Choreography Analysis through Deep-Learning Pose Estimation across a Large Video Corpus</title><meta name=description content="DHQwords Issue 15.1, March 2021. an experiment republishing DHQ articles with Hugo & Startwords theme. "><meta property="og:title" content="Comparative K-Pop Choreography Analysis through Deep-Learning Pose Estimation across a Large Video Corpus"><meta property="og:description" content="1. Introduction The ability to derive accurate information about human body poses and movements from arbitrary still images and videos introduces considerable new opportunities for digital humanities scholarship, especially in the realm of dance choreography analysis. Most such inquiries have previously occurred within visual media studies and among what might be considered &ldquo;DH-adjacent&rdquo; communities of dance and performance, with scholars using motion-capture systems to record the movements of small numbers of live dancers in controlled environments."><meta property="og:type" content="article"><meta property="og:url" content="https://rlskoeser.github.io/dhqwords/vol/15/1/000506/"><meta property="og:image" content="https://rlskoeser.github.io/dhqwords/social.png"><meta property="article:section" content="vol"><meta property="article:published_time" content="2021-03-05T00:00:00+00:00"><meta property="article:modified_time" content="2021-03-05T00:00:00+00:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://rlskoeser.github.io/dhqwords/social.png"><meta name=twitter:title content="Comparative K-Pop Choreography Analysis through Deep-Learning Pose Estimation across a Large Video Corpus"><meta name=twitter:description content="1. Introduction The ability to derive accurate information about human body poses and movements from arbitrary still images and videos introduces considerable new opportunities for digital humanities scholarship, especially in the realm of dance choreography analysis. Most such inquiries have previously occurred within visual media studies and among what might be considered &ldquo;DH-adjacent&rdquo; communities of dance and performance, with scholars using motion-capture systems to record the movements of small numbers of live dancers in controlled environments."><link rel=stylesheet href=/dhqwords/style.css><link rel=stylesheet href=/dhqwords/print.css media=print><script src=/dhqwords/js/polyfills.js></script><script defer src=/dhqwords/js/bundle.js></script></head><body class=article><header><nav class=main aria-label=main><ul><li class=home><a href=/dhqwords/>dhq</a></li><li class=issues><a href=/dhqwords/vol/>volumes</a></li></ul></nav></header><main><article><div class=grid><header><p class=number><a href=/dhqwords/vol/15/1/>Issue 15.1</a></p><p class=theme>AudioVisual Data in DH</p><h1>Comparative K-Pop Choreography Analysis through Deep-Learning Pose Estimation across a Large Video Corpus</h1><p><ul class=authors><li><address>Peter Broadwell</address></li><li><address>Timothy R. Tangherlini</address></li></ul></p><p><time class=pubdate datetime=2021-03>March 2021</time></p><ul class="categories tags"></ul><ul class=tags></ul><p class=formats></p></header><section class=print-only><a class=first-page-header href=/ aria-label=Startwords><img alt=Startwords src=/pdf-logotype.svg></a>
<a class=page-header href=/ aria-label=Startwords><img alt=Startwords src=/pdf-logo.svg></a>
<a href=http://doi.org/ rel=alternate class=page-doi>doi:</a></section><div class=text-container><h2 id=1-introduction>1. Introduction</h2><p>The ability to derive accurate information about human body poses and movements from arbitrary still images and videos introduces considerable new opportunities for digital humanities scholarship, especially in the realm of dance choreography analysis. Most such inquiries have previously occurred within visual media studies and among what might be considered &ldquo;DH-adjacent&rdquo; communities of dance and performance, with scholars using motion-capture systems to record the movements of small numbers of live dancers in controlled environments. The advent in the past few years of powerful deep learning-based models capable of accurately estimating poses directly from digital images and video footage greatly expands the scope and variety of questions researchers can pursue. We consider the particularly exciting prospect of being able to conduct studies of massive amounts of recorded choreography as another facet of the emergent practice of &ldquo;distant viewing&rdquo; of visual materials — a development that is itself analogous to the foundational digital humanities practice of &ldquo;distant reading&rdquo; of large collections of texts <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> <sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>.</p><p>Deep learning-based approaches tend to be faster and more accurate than prior computer vision methods for estimating human poses in standard visual-spectrum single-camera images and videos <sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>. The new methods also rival the accuracy of dedicated motion-capture systems and far exceed their potential scope given the physical demands of dedicated motion capture, raising the prospect of applying these deep learning methods to large recorded corpora <sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup>. We present the initial stages of such an inquiry, focusing on a domain that is an excellent match for the capabilities of deep learning-based pose estimation: K-pop dance choreography.</p><h2 id=2-why-k-pop>2. Why K-pop?</h2><p>K-pop gained considerable traction in the South Korean domestic entertainment market in the aftermath of the financial downturn that rocked the South Korean economy in the late 1990s <sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup> <sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup>. The pop music genre began to dominate the regular and virtual airwaves in tandem with the rise in online social networks, video and music sharing platforms, and the broader cultural phenomenon of the Korean Wave (Hallyu), a wave that gained its initial impetus with the immense popularity of Korean television dramas throughout East Asia <sup id=fnref:7><a href=#fn:7 class=footnote-ref role=doc-noteref>7</a></sup>. It was into this well-primed social media environment that K-pop was launched, and the genre quickly evolved to include several distinguishing features, including: (i) an emphasis on individual songs (as opposed to larger &ldquo;albums&rdquo;) promoted via music videos; (ii) the development of individual &ldquo;idols&rdquo; and largely single-sex musical/dance groups; (iii) a coherent musical style based largely on non-antagonistic Europop and American hip-hop styles; (iv) a heavily produced visual style that emphasized costumes, sets, dramatic lighting, and a kinetic shot vocabulary; and (v) tightly choreographed, frequently energetic, dance.</p><p>The global ubiquity of online video sharing and streaming services, which accelerated with the launch of YouTube in 2005, helped make K-pop an international phenomenon, with videos attracting many millions of views and solidifying fan bases throughout the world. Because of the potential for significant financial gain, the K-pop industry quickly began to attract substantial funding from the private sector and public agencies eager to promote South Korean cultural products globally <sup id=fnref:8><a href=#fn:8 class=footnote-ref role=doc-noteref>8</a></sup>. As the genre developed and the music market pivoted almost entirely away from album-based sales to video-singles and ad-based revenue, the industry began to internationalize. Consequently, it is not uncommon for producers, videographers, choreographers, music composers, lyricists, musicians, and even the idols and K-pop group members themselves to come from countries other than Korea. This internationalization has resulted in a remarkably productive collaborative environment with creative input coming from people with diverse musical, choreographic and visual backgrounds and traditions. In turn, this creative melting pot feeds a productive tension between the expectations of the broad consensus of what constitutes &ldquo;K-pop&rdquo; developed over the past decade by the industry, performers and their fans on the one hand, and the individualistic creative desires of the various individuals contributing to the production of new K-pop videos on the other. While aspects of K-pop production <sup id=fnref:9><a href=#fn:9 class=footnote-ref role=doc-noteref>9</a></sup>, economics <sup id=fnref:10><a href=#fn:10 class=footnote-ref role=doc-noteref>10</a></sup> <sup id=fnref:11><a href=#fn:11 class=footnote-ref role=doc-noteref>11</a></sup>, musical collaboration <sup id=fnref:12><a href=#fn:12 class=footnote-ref role=doc-noteref>12</a></sup>, fandom and international reception <sup id=fnref:13><a href=#fn:13 class=footnote-ref role=doc-noteref>13</a></sup> <sup id=fnref:14><a href=#fn:14 class=footnote-ref role=doc-noteref>14</a></sup> <sup id=fnref:15><a href=#fn:15 class=footnote-ref role=doc-noteref>15</a></sup> <sup id=fnref1:7><a href=#fn:7 class=footnote-ref role=doc-noteref>7</a></sup> <sup id=fnref:16><a href=#fn:16 class=footnote-ref role=doc-noteref>16</a></sup> as well as broader considerations of K-pop in the contexts of gender <sup id=fnref:17><a href=#fn:17 class=footnote-ref role=doc-noteref>17</a></sup> <sup id=fnref:18><a href=#fn:18 class=footnote-ref role=doc-noteref>18</a></sup> <sup id=fnref:19><a href=#fn:19 class=footnote-ref role=doc-noteref>19</a></sup> <sup id=fnref1:10><a href=#fn:10 class=footnote-ref role=doc-noteref>10</a></sup>, political philosophies <sup id=fnref:20><a href=#fn:20 class=footnote-ref role=doc-noteref>20</a></sup>, body aesthetics <sup id=fnref:21><a href=#fn:21 class=footnote-ref role=doc-noteref>21</a></sup> <sup id=fnref2:10><a href=#fn:10 class=footnote-ref role=doc-noteref>10</a></sup>, and hybridity and cultural appropriation <sup id=fnref:22><a href=#fn:22 class=footnote-ref role=doc-noteref>22</a></sup> <sup id=fnref:23><a href=#fn:23 class=footnote-ref role=doc-noteref>23</a></sup> have received considerable scholarly attention, far less attention has been paid to the kinesthetic dimensions of the genre <sup id=fnref:24><a href=#fn:24 class=footnote-ref role=doc-noteref>24</a></sup>. In particular, a typology of K-pop dance movements and considerations of the overall choreography of K-pop have not been subjected to rigorous analysis, possibly because of the overwhelming size of the ever-growing K-pop corpus.</p><p>K-pop dance is marked by the integration of a broad range of popular dance styles, most notably American hip hop genres including b-boying (breakdancing), popping and locking, and other street dance styles; Indian popular dance genres such as bhangra; and borrowings from other coordinated dance traditions such as American cheer and stepping. While not all K-pop videos are dominated by dance, or can even be considered &ldquo;dance forward,&rdquo; those that are tend to include either an individual solo dancer, or highly coordinated, often same-sex, groups featuring 4–9 dancers with break-out solo dances, occasionally set against much larger coordinated ensemble dances. Psy&rsquo;s satirical &ldquo;Gangnam Style&rdquo; music video provides an excellent sampler of the different types of dances that characterize the genre <sup id=fnref:25><a href=#fn:25 class=footnote-ref role=doc-noteref>25</a></sup>. Ironically, the video far exceeded the international popularity of any previous K-pop song while parodying the genre&rsquo;s conventions along with the superficial lifestyles of Seoul&rsquo;s nouveau riche.</p><p>In the &ldquo;official&rdquo; music video for a K-pop song, dances are often interspersed with narrative video scenes, and presented in a fragmentary form. Such fragmentary dance visualizations are challenging for automated analysis. Fortunately, many groups also release &ldquo;dance practice&rdquo; videos that present the entire dance choreography for the song, supplementing renditions of the choreography in concert and in &ldquo;comeback&rdquo; (new release) performances on the Korean networks&rsquo; weekly live music television broadcasts. These sources allow fans to learn the dance moves and, ultimately, to record their own &ldquo;dance cover&rdquo; of a song. As a result, there is a considerable and growing corpus of variant forms of entire dances, shorter dance sequences, and dance moves that, taken together, represent an intriguing opportunity to explore aspects of K-pop dance reproduction, stability and variation, including considerations of borrowing from other periods, genres, styles or artists; inter-artist influence; and incremental change in dance moves and sequences.</p><p>K-pop dance videos provide excellent material for developing approaches to understanding and describing dance moves and sequences in a consistent manner at scale. The growing corpus offers a unique opportunity to develop pose- and movement-oriented analytical techniques and data models that in many ways parallel previous research with text corpora: producing, for example, a kinesthetic search engine that would allow dance poses, moves, or larger sequences to act as the search input, as opposed to text descriptors, and return time-stamped results from the broader corpus. Such a search engine would, in turn, facilitate the study of dance evolution, influence, borrowing, and innovation across not only K-pop but, if scaled to include other dance traditions, potentially across many different dance and movement domains, from those mentioned above to other popular Korean music genres such as trot (트로트), and even martial arts and folk dances. We limit the corpus considered in this study to K-pop music videos produced in Korea or by Korean management groups and production houses between 2004 and 2020, resulting in a full-size corpus of over 10,300 videos, primarily sourced from YouTube. Our analytical case studies draw from a subcorpus of approximately 220 official choreography demonstration/dance practice videos posted to YouTube since 2012.</p><h2 id=3-relatedfoundational-computational-approaches-motion-capture>3. Related/foundational computational approaches: motion capture</h2><p>The primary contribution of deep learning-based pose estimation to the K-pop research envisioned here, and to similarly AV-oriented digital humanities agendas, derives from its speed and accuracy when detecting and estimating the poses of potentially unlimited numbers of human figures from images and video captured &ldquo;in the wild&rdquo; with a single visual-light camera. Earlier, non deep-learning approaches to this type of pose estimation tended to perform less well overall in the same way and for the same reasons that deep learning-based approaches to automated image analysis tasks such as semantic segmentation, captioning, and object detection and masking decisively surpassed previous computer vision approaches <sup id=fnref1:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>.</p><p>It is important to note that motion capture-based pose detection methods have been — and remain — capable of capturing pose and movement data at greater levels of accuracy and comprehensiveness than deep-learning approaches. For example, these methods typically record positions in three dimensions natively, rather than inferring 3-D positions (if done at all) from single-camera 2-D images, as in the case of most deep learning methods. Motion capture, though, tends to work only with a small number of dancers (often just one), imaged live in controlled conditions using dedicated hardware and software systems. Such &ldquo;mocap rigs&rdquo; have over the years included wearable wireless (or wired) tracking devices, setups in which one or more cameras detect reference markers worn on the body and the face, and &ldquo;markerless&rdquo; systems that combine a visual-light camera with infrared laser ranging sensors to build a 3-D map of the objects in front of them. The second of these technologies drove the proliferation of performance capture-based characters in popular films of the early 2000s, and the first and third were the signature innovations of the Nintendo Wii and the Microsoft Kinect interactive gaming systems, introduced in 2006 and 2010 respectively <sup id=fnref:26><a href=#fn:26 class=footnote-ref role=doc-noteref>26</a></sup> <sup id=fnref:27><a href=#fn:27 class=footnote-ref role=doc-noteref>27</a></sup>. Despite the broad adoption of such methods and their potential benefits, traditional dance motion capture assumes that one has access not only to the equipment, but also to dancers capable of reproducing the desired moves, sequences and complete choreography.</p><p>The partial equivalence of deep learning pose estimation output to motion-capture data means that researchers using deep learning-based techniques can derive inspiration and potentially even analytical techniques from prior motion capture-based dance studies. A rich lineage of motion-capture inquiries exists, with many studies exploring how to record, analyze, and communicate the nuances of full-body choreography — as opposed to previous systems for notating foot movements — that prompted Rudolf Laban and his colleagues to develop Labanotation in the 1920s, to be followed by Benesh Movement Notation in the 1940s as well as several other schemes <sup id=fnref:28><a href=#fn:28 class=footnote-ref role=doc-noteref>28</a></sup>. Yet technology-assisted dance analysis efforts to date have been typically somewhat narrow in scope, often focusing on just a single dancer, and primarily intended to contribute to dance pedagogy <sup id=fnref:29><a href=#fn:29 class=footnote-ref role=doc-noteref>29</a></sup>, close analysis of the micro-scale nuances of a specific dance or movement <sup id=fnref:30><a href=#fn:30 class=footnote-ref role=doc-noteref>30</a></sup>, or oriented towards dance entertainment systems and video games.</p><p>One noteworthy motion-capture dance study, highly relevant to the present discussion, was a large-scale, multi-year project at the Korean Electronics and Telecommunication Research Institute (ETRI), which also focused on K-pop dance <sup id=fnref:31><a href=#fn:31 class=footnote-ref role=doc-noteref>31</a></sup>. From 2014 to 2017, the ETRI researchers used a Kinect-type system to generate motion-capture recordings of professional dancers re-enacting choreography from a large set of popular K-pop numbers. They subsequently developed techniques for characterizing and comparing the recorded poses, eventually assembling a large database of K-pop dance poses. Potential uses of the comparison techniques and database as envisioned in the project documentation included helping to adjudicate choreography copyright disputes and serving as source data for a mocap-based K-pop &ldquo;dance karaoke&rdquo; platform or a similarly featured K-pop dance pedagogy system <sup id=fnref1:31><a href=#fn:31 class=footnote-ref role=doc-noteref>31</a></sup>.<sup id=fnref:32><a href=#fn:32 class=footnote-ref role=doc-noteref>32</a></sup> The project&rsquo;s methods for pose characterization are broadly similar to the distance matrix-based approach used in the present study <sup id=fnref:33><a href=#fn:33 class=footnote-ref role=doc-noteref>33</a></sup>. The ETRI researchers&rsquo; technique for comparing short dance moves uses an extended version of a previous study&rsquo;s metric based on &ldquo;dynamic time-warping&rdquo; analysis <sup id=fnref:34><a href=#fn:34 class=footnote-ref role=doc-noteref>34</a></sup>. This method, which involves comparing the frames of a given motion to a labeled &ldquo;reference&rdquo; version of the motion, is not applicable to the present study, which endeavors to extract descriptors of K-pop poses and gestures in a largely unsupervised manner (i.e., directly from video sources). Nevertheless, it may prove relevant to future expansions of this work.</p><h2 id=4-getting-a-leg-up-with-deep-learning>4. Getting a leg up with deep learning</h2><p>The deep learning methodologies underpinning recent advances in pose estimation are fundamentally the same as those that drove the earlier breakthroughs in object detection and segmentation for still images. In brief: large sets of potentially meaningful image features, often derived by applying certain filters, overlays, and &ldquo;convolutions&rdquo; to the images, are fed to an interlinked system of data structures (&ldquo;neurons&rdquo;), which repeatedly applies fairly straightforward mathematical calculations to &ldquo;learn&rdquo; which features are the most effective at helping the entire network discern between different labels for the input images, e.g., &ldquo;dog,&rdquo; &ldquo;cat,&rdquo; &ldquo;arm,&rdquo; &ldquo;leg.&rdquo; Large quantities of such labeled data are needed to train the models. Although this input can be derived from existing digital resources, massive amounts of novel manual efforts, often obtained through low-wage or entirely uncompensated labor, are needed to train state-of-the-art models, (e.g., Google&rsquo;s reCAPTCHA service) <sup id=fnref:35><a href=#fn:35 class=footnote-ref role=doc-noteref>35</a></sup>. The resulting models tend to perform quite well at tasks related to isolating, identifying and describing the objects they have been trained to detect. For pose estimation, the first of these tasks often consists of deciding which &ldquo;regions of interest&rdquo; on an image (established by dividing the input image into a grid of regions of varying sizes and dimensions and then applying an &ldquo;ROI&rdquo; evaluation sub-model to each) seems likely to contain a human figure. After finding these ROIs, the pose estimation model is employed to identify and localize discrete portions of the detected figure.</p><p>Many pose estimation models operate by collapsing the detected probabilistic &ldquo;field&rdquo; for, say, an arm into discrete keypoints (wrist, elbow, shoulder) until eventually a full pose &ldquo;skeleton&rdquo; of keypoints and connecting linkages is obtained. This approach was used for the first major open-source deep learning-based pose estimation project, OpenPose <sup id=fnref:36><a href=#fn:36 class=footnote-ref role=doc-noteref>36</a></sup>. Deep learning-based face detection methods also follow a similar process, and it is not difficult to see how the pose comparison approaches described below, predicated as they are on the notion of a pose &ldquo;fingerprint,&rdquo; resemble some facial recognition/matching techniques. It is also not hard to envision the mass surveillance applications to which both methods lend themselves <sup id=fnref:37><a href=#fn:37 class=footnote-ref role=doc-noteref>37</a></sup> <sup id=fnref:38><a href=#fn:38 class=footnote-ref role=doc-noteref>38</a></sup>. Importantly, deep learning-based pose detection has grown to encompass a much wider variety of use cases beyond its most obvious applications in security and retail surveillance and driver-assist technology. Examples include the development of &ldquo;in-bed pose estimation&rdquo; for monitoring of hospital patients using low-cost cameras <sup id=fnref:39><a href=#fn:39 class=footnote-ref role=doc-noteref>39</a></sup>, as well as the DeepLabCut software, which facilitates the training of non-human pose estimation models to aid observational studies of a virtually limitless range of animals <sup id=fnref:40><a href=#fn:40 class=footnote-ref role=doc-noteref>40</a></sup>.</p><figure><img loading=lazy alt src=/dhqwords/vol/15/1/000506/resources/images/figure01.png sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/15/1/000506/resources/images/figure01_hud9a9adbfe206accc149929f12fc47cfd_59465_500x0_resize_box_3.png 500w,
/dhqwords/vol/15/1/000506/resources/images/figure01_hud9a9adbfe206accc149929f12fc47cfd_59465_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000506/resources/images/figure01.png 300w" class=portrait><figcaption><p>The COCO “Common Objects in Context” pose keypoint set. The numbering of the keypoints may vary between software implementations. The numbers here apply to the other figures in this paper.</p></figcaption></figure><p>When providing output coordinates for detected figures, human pose estimation models usually adhere to a standard set of keypoints, such as the 17 keypoints of the COCO (Common Objects in Context) library <sup id=fnref:41><a href=#fn:41 class=footnote-ref role=doc-noteref>41</a></sup>. This particular set truncates the figure&rsquo;s arms at the wrists and the feet at the ankles, which is not ideal for choreographic analysis nor for many other potential applications (<a href=#figure01>Figure 1</a>). Accordingly, developers have developed expanded body keypoint standards (such as BODY_25) or, in the case of OpenPose, simply superimposed other models for detecting hand and face landmarks <sup id=fnref1:36><a href=#fn:36 class=footnote-ref role=doc-noteref>36</a></sup>. The resulting figures can have as many as 130 keypoints, though one motivation for using smaller keypoint sets is that detecting and subsequently comparing more keypoints usually requires more processing time, storage and power. The &ldquo;model zoo&rdquo; provided by the developers of a given pose estimation package likewise typically includes a range of tuned models, each prioritizing or deprioritizing speed, accuracy, the number of output keypoints, model size, and resource requirements based upon its expected use. For example, a faster but lower-resolution model might be deployed in a smartphone application offering real-time body tracking. A slower, &ldquo;heavier,&rdquo; higher-resolution model could be the right fit for a well-resourced digital humanities research team seeking to resolve fine details of inter-frame movement within a set of pre-recorded dance videos, especially if the team has ample time to run the pose estimation software on the videos.</p><figure><img loading=lazy alt src=/dhqwords/vol/15/1/000506/resources/images/figure02.png sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/15/1/000506/resources/images/figure02_hua18c5615139cd673e0ae30b6b3cfddd9_2136877_500x0_resize_box_3.png 500w,
/dhqwords/vol/15/1/000506/resources/images/figure02_hua18c5615139cd673e0ae30b6b3cfddd9_2136877_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000506/resources/images/figure02_hua18c5615139cd673e0ae30b6b3cfddd9_2136877_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000506/resources/images/figure02.png 1374w" class=landscape><figcaption><p>A visualization of the full DensePose output: body masks (including hair and clothing), segmented body parts with contours, and keypoint skeletons.</p></figcaption></figure><p>Other pose estimation models actually project the detected probabilistic body part fields onto a 3-D model of the body, so that instead of a simplified skeleton, the output consists of a much larger set of points, regions and connectors that define a full 3-D body surface, similar to how a fishing net thrown over a person would form a body-shaped &ldquo;mesh&rdquo; (<a href=#figure02>Figure 2</a>). This mesh output option is available from Facebook Research&rsquo;s DensePose project <sup id=fnref:42><a href=#fn:42 class=footnote-ref role=doc-noteref>42</a></sup>; it gives researchers access to a much less reductionist model of the body (including full hands and feet) which is certainly appealing in some applications. One practical concern is that the resulting per-frame body meshes for a single video can use an enormous amount of storage space if not properly compressed.</p><p>Another practical consideration is that even the most heavyweight, non time-constrained pose estimation models may not perform well with imagery that is visually distorted, shot from oblique angles, poorly lit, or involves figures whose features are obscured by costumes and walls, or are simply truncated by the frame. That the previous list resembles a primer on music-video cinematography should suggest one reason why most of the present study&rsquo;s initial investigations used supplemental &ldquo;dance practice&rdquo; videos or fan-produced &ldquo;cover&rdquo; dance videos rather than the original music videos. At the root of these problems is the very limited ability of most deep learning models to extrapolate beyond their training data, so if a pose-estimation model is primarily trained via labeled, segmented images of people in brightly lit environs with visible faces performing mundane activities like walking or standing, its ability to resolve, for example, figures wearing masks or swinging their arms vigorously above their heads is likely to be quite limited.</p><p>Most pose estimation projects at present aspire to excel at figure detection and pose estimation during the first &ldquo;pass&rdquo; across an image, which has obvious relevance to core time- and resource-constrained applications such as real-time pedestrian detection systems for automobiles. This emphasis on first pass methods means that, in general, alternative methods involving multi-pass processing and smoothing tend to receive less attention, despite their potential to improve model performance in ways that would be especially beneficial to digital humanities researchers working with recorded media. The PoseFix package, for example, managed to achieve state-of-the-art accuracy simply by applying its statistical &ldquo;pose refinement&rdquo; model to the output of other methods <sup id=fnref:43><a href=#fn:43 class=footnote-ref role=doc-noteref>43</a></sup>. Such corrective calculations can be as straightforward as setting limits on how distant a figure&rsquo;s head can possibly be from the shoulders in a non-catastrophic scenario. Similarly promising but heretofore backgrounded efforts involve expanding models to incorporate the causal implications of a figure&rsquo;s previous position (and its future position, if known) to its current one <sup id=fnref:44><a href=#fn:44 class=footnote-ref role=doc-noteref>44</a></sup>. Developers acknowledge the importance of tracking multiple poses across frames — especially so when figures may pass each other in the same shot — but this is often, and perhaps erroneously, relegated to a &ldquo;post-processing&rdquo; step, something to be considered after the actual pose estimation has been done <sup id=fnref:45><a href=#fn:45 class=footnote-ref role=doc-noteref>45</a></sup>.</p><p>The accuracy of pose estimation models continues to improve as more varied training sets and clever algorithms are developed. Recent advances in the speed and accuracy of three-dimensional object detection from single-camera sources promise to offer researchers an even greater wealth of information about figures recorded on video and their relationship to their environment <sup id=fnref:46><a href=#fn:46 class=footnote-ref role=doc-noteref>46</a></sup>. Furthermore, software platforms and cloud services continue to emerge that make it much easier to provision and configure the software &ldquo;stack&rdquo; and computing resources necessary to run these models (for example, by providing access to cloud-based graphical processing units, or GPUs, which greatly accelerate deep learning tasks). As a consequence of this rapid pace of development, the decision whether to run pose estimation on raw music videos or on their accompanying dance demonstration videos is already more a question of focusing exclusively on dance choreography versus also examining computationally the many other types of performative uses of the human pose that appear in music videos.</p><h2 id=5-getting-down-to-the-features-analytical-methods>5. Getting down to the features: analytical methods</h2><p>This section describes the fundamental approaches to analyzing deep learning pose estimation output that we have applied to data from K-pop dance videos in the early phases of the research agenda outlined above, and also outlines some of the more elaborate techniques we may pursue in future work. These methods primarily concern pose characterization and comparison and the exploratory and interpretive affordances they offer when applied to a large number of videos. We also outline potential approaches to pose clustering and time-series analysis of movement and synchronization.</p><h2 id=pose-representation-comparison-and-correction>Pose representation, comparison and correction</h2><p>The raw output of deep learning-based posed estimation software is not fundamentally different from motion-capture data, so many of our techniques may have appeared in prior mocap-based analyses. Because of the reliance of these earlier studies on closed-source systems and the lack of technical details in their associated publications, such implementation-level aspects are difficult to ascertain fully. In any case, our methods by no means encompass the available techniques. Yet seeking comprehensiveness would undercut the central message of this paper: that the ability to run deep learning-based pose detection at scale across large video corpora empowers researchers to pose new questions and to develop methods for addressing them that probably have never been used before — at the very least, not in the domain of choreography analysis. One need only consider the history of computational text studies as an analogue: methods of linguistic examination and structural analysis certainly existed in the pre-digital era, but the advent of digital texts, and particularly the availability of massive quantities of digital texts, prompted an explosion of computational methods, especially at the level of large-scale, &ldquo;distant&rdquo; reading: topic models (LDA), semantic embeddings, named entity detection, network analysis to name but a few. Choreographic analysis has the potential to follow a similar trajectory.</p><p>Despite the relatively porous boundaries of K-pop vis-a-vis other forms of Korea-based popular music and the paucity of meaningful descriptive metadata for YouTube videos, in previous work we showed that it is possible to discover thousands of official K-pop music videos on YouTube by querying the videos uploaded to channels run by K-pop production companies identified via online knowledge bases (Wikidata, MusicBrainz) <sup id=fnref:47><a href=#fn:47 class=footnote-ref role=doc-noteref>47</a></sup>. For reasons discussed above, we supplemented this list with a smaller number of official dance practice, demonstration and &ldquo;dance cover&rdquo; videos to facilitate development and evaluation of our choreographic pose analysis techniques.</p><p>We obtained the pose estimation data used in the case studies below by processing videos with software from the Open PifPaf project <sup id=fnref:48><a href=#fn:48 class=footnote-ref role=doc-noteref>48</a></sup>, which we used due to its accuracy and relative ease of setup. The 17-keypoint COCO pose output data has modest storage requirements — an average of 7 megabytes of uncompressed data for a 4-minute video — and is fairly straightforward to process as CSV or JSON (<a href=#figure03>Figure 3</a>). We also ran pose estimation on the entire video corpus using a DensePose model, a process that took several weeks on a dedicated multiple-GPU system.<sup id=fnref:49><a href=#fn:49 class=footnote-ref role=doc-noteref>49</a></sup> The full DensePose &ldquo;mesh&rdquo; output, which is appealing because it allows for the calculation of additional features such as hands and feet positions and even clothing, can require 30 gigabytes or more of storage per 4-minute video (highly reactive to the number of figures in most shots), and often is neither straightforward to compress nor to interpret.</p><figure><img loading=lazy alt src=/dhqwords/vol/15/1/000506/resources/images/figure03.png sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/15/1/000506/resources/images/figure03_hu04dc3e61ca323d372b0280c6f0d5f1eb_731522_500x0_resize_box_3.png 500w,
/dhqwords/vol/15/1/000506/resources/images/figure03_hu04dc3e61ca323d372b0280c6f0d5f1eb_731522_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000506/resources/images/figure03_hu04dc3e61ca323d372b0280c6f0d5f1eb_731522_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000506/resources/images/figure03.png 1326w" class=landscape><figcaption><p>Keypoint pose estimation output as tabular data (top) and JSON (bottom). Mesh data consists of nested arrays and is not immediately comprehensible outside of visualizations.</p></figcaption></figure><p>At the fundamental level, an estimated pose is defined by its labeled keypoints and the extrapolated linkages between them, expressed as x,y coordinates on the visual plane of the screen (with a third spatial coordinate, z, if depth is also estimated). One obvious method of quantifying the difference between, for example, two 17-keypoint COCO poses is simply to sum the distance between the two instances of each keypoint using a metric such as Euclidean distance. This metric also can be a proxy for the amount of motion between two poses if they derive from the same figure at adjacent time intervals. These methods may suffice in a controlled, single-dancer motion capture studio environment, but generalizing pose characterization and comparison to any &ldquo;in the wild&rdquo; video footage requires more sophisticated approaches. Simply summing raw paired keypoint distances can be an incredibly inaccurate measure when, for example, we wish to compare poses in two different contexts, such as when the figures being compared are viewed at varying distances from the camera, or belong to different-sized people.</p><p>Solutions typically require devising a different &ldquo;feature set&rdquo; to describe the pose numerically, which in turn calls for different inter-pose comparison techniques. One rudimentary approach is to consider only the angle of certain linkages, such as in the arms or legs, because their angles are not affected by changes in scale. This approach, however, discards all potentially useful information about the positions of unconnected keypoints. Another, more promising class of solutions involves representing the pose not as a set of keypoints and the skeletal linkages between them, but rather as a &ldquo;distance matrix&rdquo; of the distances from each keypoint to every other keypoint (<a href=#figure04>Figure 4</a>). Comparing two such pose representations to quantify similarity or movement can then be accomplished by applying statistical tests designed to measure the degree of correlation between two matrices — a process that ignores differences in scale between the two poses. For our initial studies, we used the Mantel test, which provides a measure both of the strength of the correlation between the input matrices (this correlation can be expressed as a number between 0 and 1) and the computed probability that this correlation is due to random fluctuations in the data <sup id=fnref:50><a href=#fn:50 class=footnote-ref role=doc-noteref>50</a></sup>.</p><figure><img loading=lazy alt src=/dhqwords/vol/15/1/000506/resources/images/figure04.png sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/15/1/000506/resources/images/figure04_hu7d54c4769707f9eae9ca7eae54a2c0e8_767064_500x0_resize_box_3.png 500w,
/dhqwords/vol/15/1/000506/resources/images/figure04_hu7d54c4769707f9eae9ca7eae54a2c0e8_767064_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000506/resources/images/figure04_hu7d54c4769707f9eae9ca7eae54a2c0e8_767064_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000506/resources/images/figure04.png 1210w" class=landscape><figcaption><p>A source image with keypoint overlay, a plot of the detected keypoints plotted separately (center), and the corresponding normalized inter-keypoint distance matrix (right). In the distance matrix, pairs of points that are close together are represented by dark squares, while those that are far apart receive light squares.</p></figcaption></figure><p>Another class of enhanced pose representation and comparison methods considers only whether keypoints are closest to each other. In the simplest form of such an &ldquo;adjacency&rdquo; matrix, the cell for [right elbow, right ear] would record a 1 if the right elbow is closer to the right ear than to any other keypoint, and a 0 otherwise. This approach obviously disregards much of the estimated pose data and sacrifices accuracy as a consequence, but retains the overall spatial organization of the pose and has the advantage of being quite fast, requiring few calculations to characterize a pose and to compare two poses to each other. Our implementation of this method expands it further by calculating the Delaunay triangulation around the detected keypoints <sup id=fnref:51><a href=#fn:51 class=footnote-ref role=doc-noteref>51</a></sup>. This technique provides an alternative set of keypoint linkages to the standard body skeleton model, one in which every keypoint is connected to at least three others, producing a set of triangles that covers the shape of the pose in a geometrically efficient manner (<a href=#figure05>Figure 5</a>). We then represent these connections via a graph Laplacian matrix, which quantifies both the adjacency and degree of each point <sup id=fnref:52><a href=#fn:52 class=footnote-ref role=doc-noteref>52</a></sup>. Although the distance matrix and graph Laplacian matrix for a pose have the same dimensions (17x17 for the COCO keypoints), because a graph Laplacian contains only positive or negative integers, comparing two poses represented in this manner involves simply subtracting one from the other and summing their differences — a very computationally &ldquo;lightweight&rdquo; operation compared to the Mantel test for the distance matrices. It is also worth noting, however, that a standard Delaunay triangulation discards the right/left labels of the keypoints, meaning that frames of a pose with the figure facing the camera would be scored as identical to the mirror-image of the same pose with the figure turned 180 degrees away from the camera — which may or may not be desirable.</p><figure><img loading=lazy alt src=/dhqwords/vol/15/1/000506/resources/images/figure05.png sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/15/1/000506/resources/images/figure05_hu443d36ed8b1ae972292b397ad2c94af6_972517_500x0_resize_box_3.png 500w,
/dhqwords/vol/15/1/000506/resources/images/figure05_hu443d36ed8b1ae972292b397ad2c94af6_972517_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000506/resources/images/figure05.png 1182w" class=landscape><figcaption><p>Delaunay triangulations of detected figure keypoints.</p></figcaption></figure><p>Pose comparison methods, including those described above, must accommodate the near-certainty of incomplete pose and keypoint data. Even sophisticated pose detection software can lose track of body landmarks and sometimes entire figures for multiple frames. Often, the software detects part of the pose, but its &ldquo;confidence&rdquo; value for a keypoint or the entire figure drops low enough that the data points are removed from the output to avoid spurious results. Especially problematic with single-camera &ldquo;in the wild&rdquo; videos are cases in which even a human observer could only speculate as to the true coordinates of a keypoint, such as when a limb or facial landmark is obscured from view. Most pose comparison methods, including the matrix-based methods used here, do not easily accommodate missing data values. Our software therefore falls back on both spatial and temporal interpolation to fill in missing keypoints. The relatively high frame rates of modern video assists in the latter: the position of a missing keypoint often can be placed somewhere between its last and next known position. Failing that, spatial interpolation often allows us to place with high confidence, for example, an eye that is obscured by a hat brim between its adjacent ear and nose. A last-resort option is to position a missing keypoint at the center of the pose. Both the distance matrix technique and the Delaunay triangulation-based graph Laplacian approach, due to their addition of extra linkages to the base keypoint set, are still able to produce usable results when undefined values are replaced with such a default.</p><p>A final obstacle is that most pose detection packages do not attempt to track the figures in a scene, simply numbering the figures in a shot via an arbitrary ordering, e.g., left-to-right, or sorted by the size of their bounding box, regardless of identity. This practice leads to discrepancies in the movement data when, for example, one figure changes places with another. To ameliorate these errors, we also run our corrected pose detection output through the AlphaPose project&rsquo;s &ldquo;PoseFlow&rdquo; software, which attempts to generate a single movement track for each distinct figure throughout the duration of the video <sup id=fnref:53><a href=#fn:53 class=footnote-ref role=doc-noteref>53</a></sup>. Even when PoseFlow fails to &ldquo;keep track&rdquo; of a dancer, our current choreography analysis methods benefit from considering each figure only within the frame of reference of its own body landmarks, excluding lateral and backwards/forwards movement relative to the camera and other dancers, and from the fact that we generally examine the simultaneous movements of multiple dancers in aggregate rather than individually. Therefore the failures of the pose tracking software to resolve correctly the trajectories of two overlapping poses generally do not introduce significant discrepancies into our analytical results.</p><h2 id=pose-clustering-sequence-detection-and-distant-movement-characterization>Pose clustering, sequence detection, and &ldquo;distant&rdquo; movement characterization</h2><p>Having chosen a set of methods that can represent a pose and quantify the similarity and difference between two poses, such that the degree of difference between two poses from the same dancer can serve as a reasonable proxy for motion, it is then possible to pursue a variety of more aggregate, &ldquo;distant&rdquo; analyses that consider groupings and sequences of poses over time. The case studies below describe the techniques that we find particularly promising and relevant for the study of K-pop choreography and its influences, but they are by no means an exhaustive set. Ultimately, we believe that a macroscopic approach that includes a series of these analytical techniques will support researchers as they move toward the &ldquo;thick&rdquo; analysis of dance at scale <sup id=fnref:54><a href=#fn:54 class=footnote-ref role=doc-noteref>54</a></sup> <sup id=fnref:55><a href=#fn:55 class=footnote-ref role=doc-noteref>55</a></sup>.</p><p>The computational considerations germane to these analyses involve at their fundamental levels many standard details of statistical and numerical computation. These include sampling (e.g., whether to consider every pose in every frame), or whether a subset of poses (chosen randomly or by weeding out repetitions) can suffice to produce significant results at much lower computational cost in time and resources. Of further concern are methods for smoothing and interpolation, already discussed above regarding pose correction: how best to reduce the influence of transient data errors on the results without disregarding legitimate phenomena, and to &ldquo;fill in&rdquo; missing values with a suitable degree of confidence. At the higher levels are issues such as the choice of algorithm for clustering or time-series comparison and their various hyperparameters (e.g., how many clusters we expect to find in the data set).</p><p>The example analyses below illuminate how the foundational pose characterization and comparison methods described in the previous section build progressively towards one of our primary long-term goals: isolating a typography not only of K-pop poses but of gestures and ultimately dance &ldquo;moves.&rdquo; Although this work is ongoing, the examples below indicate the planned trajectory: using pose similarity and clustering methods to identify &ldquo;key&rdquo; poses that occur frequently with minor variations; the sequences of key poses and interstitial movements that recur frequently across the time series of one or multiple videos thus identify themselves as significant &ldquo;moves&rdquo; within the dance vocabulary of K-pop.</p><h2 id=6-case-studies-with-k-pop>6. Case studies with K-pop</h2><p>The following case studies highlight some of the potential applications of the pose characterization and comparison techniques described above. Specifically, the first of these examples involves applying pose similarity calculations in a variety of ways to a single-dancer video performance: comparing time-adjacent poses to establish the movement profile of the dance, and comparing poses across the duration of the dance either exhaustively or selectively (via clustering) to highlight repeated poses and pose sequences. The second example applies many of the same techniques to a video of multi-person choreography, with the addition of inter-dancer comparison of poses and motion to detect and quantify the degree of synchronized posing and movement present across the video. The third study calculates per-video and aggregate values of each previously discussed metric for a test corpus of 20 K-pop choreography videos, divided into equal halves according to the gender of the performing group.</p><h2 id=solo-dance-repeated-pose-sequence-discovery>Solo dance: repeated pose sequence discovery</h2><p>For a single-person video, our goal was to use the pose comparison methods described above to detect when a dance video is repeating certain poses and motions. Being able to find repeated poses illustrates the suitability of these techniques to building a kinesthetic database of poses that can be searched via a &ldquo;query&rdquo; pose to find similar poses. Given this, detecting repeated motions can be as straightforward as noticing when a &ldquo;query&rdquo; sequence of poses matches a reference sequence of poses within some similarity threshold and across some time window. As this example will show, groups of repeated motions tend to correspond to repeated formal sections and suggest the potential of performing automated computation-based formal analyses across a large dance video corpus.</p><p>For this case study, we used an instructional recording by dance cover specialist Lisa Rhee of Blackpink member Jennie Kim&rsquo;s debut single &ldquo;Solo,&rdquo; which was choreographed by the prolific New Zealand-based choreographer Kiel Tutin.<sup id=fnref:56><a href=#fn:56 class=footnote-ref role=doc-noteref>56</a></sup> One way of testing how computational pose comparisons can contribute to detecting dance repetition is via the brute-force expedient of comparing the pose in every frame of a single-dancer video to every other frame, and plotting the results on a correlation heatmap matrix (a type of visualization commonly used to explore patterns of repetition in time-series data). Such heatmaps tend to be difficult to read at first, but provide the initiated with a wealth of visual cues about patterns of correlation and similarity; if desired, these features also can be described numerically by applying established analytical techniques to the correlation matrix.</p><p><a href=#figure06>Figure 6</a> shows a correlation heatmap for the entire performance, which lasts just over 2 minutes and 43 seconds, captured in 4,079 frames (25 frames per second). The poses were represented as normalized distance matrices as explained above, and therefore the comparison between any two poses returns a Mantel correlation value from 0 (least similar) to 1 (most similar). On the heatmap, these similarity values are visualized via the color scale, with lighter colors indicating lower similarity. The x and y axes of the heatmap represent the progression of frames of the video, with the start time 0:00 at bottom left, so that the cell at position x, y represents a comparison of the pose at frame x to the pose at frame y. The matrix is therefore symmetric around the diagonal x=y axis, which naturally always has a similarity value of 1 and appears as a dark diagonal line dividing the heatmap into two right triangles.</p><figure><img loading=lazy alt src=/dhqwords/vol/15/1/000506/resources/images/figure06.png sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/15/1/000506/resources/images/figure06_hu02b2e24756714e5411f76ceae16e8875_549428_500x0_resize_box_3.png 500w,
/dhqwords/vol/15/1/000506/resources/images/figure06_hu02b2e24756714e5411f76ceae16e8875_549428_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000506/resources/images/figure06.png 919w" class=landscape><figcaption><p>Time-series correlation heatmap of Lisa Rhee’s dance cover of Jennie’s “Solo." Darker colors indicate higher degrees of pose similarity. The repeating dance “chorus" sections at 0:38–0:60 and 1:38–2:00 are highlighted.</p></figcaption></figure><p>On a time-series correlation heatmap, repeated sections typically appear as dark lines running parallel to the central diagonal x=y axis, and such a feature is indeed visible here (highlighted in <a href=#figure06>Figure 6</a>). Closer inspection reveals that it does in fact indicate a very close repetition of the choreography originally seen at 0:38 to 0:60 almost exactly a minute later, at 1:38 to 2:00. Not surprisingly, the repeated choreography corresponds exactly to the appearances of the song&rsquo;s chorus, hinting at the likely interpretive payoffs of a truly multimodal audiovisual analysis.</p><p>Exhaustively comparing every pose to every other pose across an entire video, let alone multiple videos, quickly becomes a prohibitively cumbersome computational task. A more scalable method is to apply similarity-based clustering to all or to a representative sample of the poses in one or more videos, and to identify when poses in the same similarity cluster, or &ldquo;family,&rdquo; reoccur, and moreover when members of clusters reoccur in sequence — a major step towards identifying both formal sections within a choreographic plan, and also smaller, segmentable dance sections (i.e., moves).</p><p>Clustering necessarily sacrifices some precision, and requires judicious selection of parameters to produce a useful partitioning of the entire pose space. For this example, we applied the OPTICS hierarchical density-based clustering algorithm <sup id=fnref:57><a href=#fn:57 class=footnote-ref role=doc-noteref>57</a></sup>. Many other types of clustering algorithms may be applied fruitfully to this task, but we found OPTICS suited to the exploratory nature of our analysis because it does not require one to specify how many clusters are to be found in advance, but rather builds clusters based on a user-supplied minimum number of members per cluster. This hyperparameter can be set to a value with some intuitive justification; we chose to use the number of frames per second in the video recording (approximately 25, in this case), reasoning that a pose that is held for longer than one second, or that appears cumulatively for at least this length of time, may be significant and should be considered for cluster membership by the algorithm.</p><figure><img loading=lazy alt src=/dhqwords/vol/15/1/000506/resources/images/figure07.png sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/15/1/000506/resources/images/figure07_hu40d88c0df07f2603fa85e087671e3343_89798_500x0_resize_box_3.png 500w,
/dhqwords/vol/15/1/000506/resources/images/figure07_hu40d88c0df07f2603fa85e087671e3343_89798_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000506/resources/images/figure07.png 1024w" class=landscape><figcaption><p>The representative “key" poses of the 10 pose clusters found by the OPTICS algorithm when it is configured to find a minimum of 25 poses per cluster. Each key pose was constructed by taking the average keypoint locations of all members of a cluster.</p></figcaption></figure><p>The clustering analysis for this video found ten groups of poses using the settings discussed above. To aid in visualization and comparison, we computed a representative &ldquo;centroid&rdquo; pose for each cluster by averaging the relative keypoint positions of each pose in the cluster (<a href=#figure07>Figure 7</a>). The OPTICS algorithm leaves a potentially large number of samples unassigned to any cluster, so we elected to assign each of these to the cluster with a representative centroid pose that was most similar to the unassigned pose. Visualizing the occurrences of these cluster groups on a timeline (<a href=#figure08>Figure 8</a>) makes it possible to detect repeated segments of choreography. The chorus sections identified in the pose correlation heatmap above, as well as the bridge section leading to them, are easily discernible. Key pose #9, with the upraised right arm, is particularly recognizable as a signature pose of the song, even though the COCO 17-keypoint set is unable to resolve the raised index finger that evokes the song&rsquo;s title.</p><figure><img loading=lazy alt src=/dhqwords/vol/15/1/000506/resources/images/figure08.png sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/15/1/000506/resources/images/figure08_huce963c91d885ccef44abaa31446175b9_72806_500x0_resize_box_3.png 500w,
/dhqwords/vol/15/1/000506/resources/images/figure08_huce963c91d885ccef44abaa31446175b9_72806_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000506/resources/images/figure08.png 1024w" class=landscape><figcaption><p>The pose distribution heatmap of the key poses from Figure 7 throughout the duration of the song, with the two appearances of the chorus outlined in red and the two “bridge" sections outlined in blue. Both section types repeat their previous choreography when they reoccur, which is apparent in the similar patterns of their sections on the heatmap. Occurrences of poses that the OPTICS clustering algorithm explicitly assigned to one of the 10 groupings are represented by blocks with a darker shade, while the unmatched poses that we subsequently assigned to their “nearest neighbor" key pose are in a lighter shade.</p></figcaption></figure><p>Thinking more broadly while looking more closely, it might also be appealing to be able to search through a much larger corpus of K-pop videos for occurrences of pose #9, as well as others from the directly mimetic gestures that populate the first 15 seconds of the choreography. Several of these are well enshrined in K-pop iconography such as, for example, the two-handed heart-shaped pose at 0:04, which is quickly and dramatically split in two at the six-second mark — an obvious nonverbal declaration that this is a breakup song — while others are relative newcomers, such as the sharp dismissal of items (messages, in this case) from a smartphone screen at 0:10 to 0:12.</p><p><a href=#figure09>Figure 9</a> visualizes the analytical methods described above via accompanying graphics as well as overlays of an excerpt of the video, with displays of summary values and a progress indicator (the moving red vertical line) superimposed on the time-series components. Viewing the changes in the distance matrix visualized this way alongside the actual poses imparts a more intuitive understanding of how it is used to generate similarity and movement values. The general resemblance of the distance matrix-based movement and the graph Laplacian-based movement series is apparent here, as is the graph Laplacian series&rsquo; comparatively “noisier” representation of the amount of inter-frame movement.</p><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://player.vimeo.com/video/486935721 style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 title="Deep learning-based choreography analysis of an instructional recording by dance cover specialist Lisa Rhee of Blackpink member Jennie’s debut single Solo." webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe></div><h2 id=group-dance-synchronized-movement-detection>Group dance: synchronized movement detection</h2><p>Multi-person performances are by far the most common type in K-pop, just as multiple-member idol groups are much more numerous than solo performers. Even solo songs are likely to incorporate backup dancers in live performance as well as in the official music video. Reflecting the eclectic combination of genres that are incorporated into K-pop, the choreography for an idol group single may feature several different types of dance sections, including solo interludes or <em>pas de deux</em> by the group&rsquo;s primary dancer(s) as well as gestural punctuations during a showcase of the group&rsquo;s main rapper. Yet the signature so-called &ldquo;point&rdquo; dance segments, which are most often staged with the group performing the same moves while facing towards the camera, tend to receive the most attention, to the extent that mechanistic mass synchronization is arguably the most salient feature of K-pop choreography in the global public imagination. This is largely by design; the didactic nature of these sections&rsquo; staging, in addition to serving other functions within a music video&rsquo;s narrative, furthers the choreography&rsquo;s main contribution to the carefully crafted viral appeal of K-pop singles, which is that fans and sometimes the general public in Korea and occasionally even the global population (as in the case of Psy&rsquo;s &ldquo;Gangnam Style&rdquo;) derive social capital from knowing and being able to re-enact these moves.</p><p>Group synchronization is straightforward to detect and to quantify using the pose comparison methods described above, through the simple expedient of observing the computed difference between each pair of figures in a single frame (note that the number of comparisons required per frame is the answer to the well-known &ldquo;handshake&rdquo; problem: n*(n - 1)/2 for n figures), then examining the mean and standard deviation of these per-frame values over time. We would expect that the mean degree of similarity would increase while the standard deviation would decrease during episodes of synchronization, which is exactly what we see when this technique is applied to the official &ldquo;dance practice&rdquo; video for the boy group BTS&rsquo;s single &ldquo;Fire,&rdquo; choreographed (as are several of BTS&rsquo;s other hits) by the American choreographer Keone Madrid.<sup id=fnref:58><a href=#fn:58 class=footnote-ref role=doc-noteref>58</a></sup></p><p>As visualized in <a href=#figure10>Figure 10</a>, the mean intra-frame pose similarity increases markedly and the standard deviation ranges shrink around 0:10 as the frenetic, popping and locking-influenced group dance begins and the song launches into the introductory hook section, with its accompanying EDM &ldquo;hoover&rdquo; synth effects. Note that this is also the first dance choreography that appears in the official music video for &ldquo;Fire,&rdquo; following its introductory narrative imagery, reinforcing the importance of synchronized group dance sections to the multimedia appeal of K-pop singles. Pose synchronization levels remain high for much of the choreography, with the highest sustained values occurring during the returns of the main hook at 1:07 and 2:04, and again leading to the coda at 2:43. These sections, and particularly the first two, share several moves and poses, although it is somewhat indicative of BTS&rsquo;s unorthodox approach to K-pop conventions that the choreography is generally more varied among these recapitulations than might be expected.</p><figure><img loading=lazy alt src=/dhqwords/vol/15/1/000506/resources/images/figure09.png sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/15/1/000506/resources/images/figure09_huf884b1b5f89d6955f64eb1b132a920c3_189135_500x0_resize_box_3.png 500w,
/dhqwords/vol/15/1/000506/resources/images/figure09_huf884b1b5f89d6955f64eb1b132a920c3_189135_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000506/resources/images/figure09_huf884b1b5f89d6955f64eb1b132a920c3_189135_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000506/resources/images/figure09.png 1211w" class=landscape><figcaption><p>Mean (blue line) intra-frame pose similarity values with the standard deviation plotted above and below (orange and green dotted lines) for each frame of BTS’s official dance practice video for “Fire.” Similarity values were smoothed by computing the moving average of a one-second sliding window around each time value. Sections of high similarity (> 90%) indicate dancing with synchronized movements and poses among the group members.</p></figcaption></figure><figure><img loading=lazy alt src=/dhqwords/vol/15/1/000506/resources/images/figure10.png sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/15/1/000506/resources/images/figure10_huc2ec71360a8ecf34673a23e63a11f8e3_235900_500x0_resize_box_3.png 500w,
/dhqwords/vol/15/1/000506/resources/images/figure10_huc2ec71360a8ecf34673a23e63a11f8e3_235900_800x0_resize_box_3.png 800w,/dhqwords/vol/15/1/000506/resources/images/figure10_huc2ec71360a8ecf34673a23e63a11f8e3_235900_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/1/000506/resources/images/figure10.png 1211w" class=landscape><figcaption><p>Mean (blue line) movement values averaged across all dancers for adjacent frames with the standard deviation plotted above and below (orange and green dotted lines). The movement values were smoothed by computing the moving average of a one-second sliding window around each time value.</p></figcaption></figure><p><a href=#figure12>Figure 12</a> animates the time-series analyses from <a href=#figure10>Figure 10</a> and <a href=#figure11>Figure 11</a> with progress indicators alongside playback of the dance practice video, further accompanied by a visualization of the average inter-frame movement values for each body keypoint computed across all figures in the frames, as well as a time-series heatmap and visualization of the most prominent key poses detected in the video. The &ldquo;individual pose movement&rdquo; timeline superimposes the inter-frame movement values for each of the detected dancers, highlighting some details that the averaging visualization (<a href=#figure11>Figure 11</a>) elides. Note that the analysis terminates prior to the conclusion of the dance practice video itself; the arrival of dozens of backup dancers at 3:00 makes it difficult to compare the conclusion section to what came before.</p><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://player.vimeo.com/video/486941563 style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 title="Deep learning-based choreography analysis of the dance practice video for the boy group BTS's single Fire." webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe></div><h2 id=distant--movement-analysis-early-steps>“Distant” movement analysis: early steps</h2><p>There is a temptation to apply the techniques for pose characterization and synchronization detection described above in an evaluative manner, aggregating numbers to support the conclusion that idol group A is &ldquo;more synchronized&rdquo; than group B, or that group A employs a greater variety of dance poses than their presumably less talented and less dedicated fellows. Our intention, however, is to employ such &ldquo;distant&rdquo; analyses to provide material for a more in-depth consideration of the influences and factors shaping the parameters of creativity and production in K-pop dance. Just as establishing a typology of K-pop dance poses and gestures will aid in identifying influences from other genres, pose and motion analysis can help to investigate some of the cultural and artistic practices being enacted (or subverted) through K-pop dance performances.</p><p>As an illustration of the interpretive potential of &ldquo;distant&rdquo; aggregate analyses of choreographic corpora, we inspect the movement-based signatures of one of the most foregrounded structural factors in K-pop, namely the gender divide that results in the vast majority of idols being segregated into groups consisting solely of young men or young women. As mentioned above, a great deal of K-pop scholarship investigates the degree to which the performances, fashion, makeup, comportment, marketing and reception of K-pop idols either conforms to or seeks to blur notions of gender roles and modes of masculinity and femininity, in Korea or internationally. A computationally driven inquiry into dance choreography potentially can contribute much to this discussion. Our initial case study involved selecting an equal number of K-pop dance practice videos from boy and girl idol groups over a limited time period and examining how the aggregate data produced from the time-series analyses described above can be used to answer, and more importantly, to raise questions about the role of gender in K-pop performance.</p><p>Our selection method involved surveying the available dance practice videos from boy and girl idol groups from the recent past and excluding videos with aspects that would complicate automated pose detection-based choreography analysis, such as camera angles that obscure dancers or studio mirrors and costumes that might confuse the pose-detection software. Of the remaining videos, we sought, albeit informally, to select a range of group sizes and song types that would be generally representative of the population of recent official dance practice videos. For this initial study, our data set consisted of the 20 videos (10 each from girl and boy groups) from 2017 to the present listed in Table 1. We ran each video through the pose detection, correction, tracking, interpolation, smoothing, movement and synchronization analyses discussed above, producing the summary statistics for each video presented in the table.</p><p>Also present in Table 1 as well as Table 2 is an additional statistic that we were interested in investigating: the degree and significance of the correlation between a group&rsquo;s averaged inter-frame movement time series and its average intra-frame pose similarity time series, as visualized in <a href=#figure11>Figure 11</a> and <a href=#figure10>Figure 10</a>, respectively. One could reasonably expect these to be inversely correlated, i.e., a group&rsquo;s inter-pose similarity might become more difficult to maintain when its members are all moving quickly. Following this line of reasoning, we sought here to investigate the intuitive hypothesis that a reduced negative correlation between movement and synchronization might indicate what musicians and dancers (and others) refer to as a &ldquo;tight&rdquo; performance, i.e., one that maintains group cohesion amidst increased difficulty. To assign this notion a quantitative metric, we computed the Pearson correlation coefficient between the movement and pose similarity time series for each video. Under our hypothesis, a &ldquo;tight&rdquo; performance would have a higher correlation value than others. This inquiry is an early exemplar of our aspirations to introduce more discerning analytical criteria into our analytical methods than the overtly reductive quantification of synchronization and movement, as even a casual overview of a few K-pop choreography videos will reveal that there is a lot more going on than rote synchronization: different sub-groupings of the members may perform different moves simultaneously, or enact the same movements in cascading sequence over time, among many other patterns.<br>Per-video statistics for 20 choreography videos, 10 each from girl and boy groups. The average per-dancer movement (calculated every 1/6 of a second to allow straightforward comparison between videos recorded at 24, 30, and 60 frames per second) and the average intra-frame similarity for all dancers are presented with standard deviation values, which help to indicate whether the movement and similarity values are generally consistent or vary widely across the video. BOY GROUPS <strong>Video release date</strong> <strong>Frames per sec</strong> <strong>Group members</strong> <strong>Avg movement per 1/6 sec per dancer</strong> <strong>Avg intra-frame pose similarity</strong> <strong>Pct of video with frame pose sim >.9</strong> <strong>Movement: similarity correlation (Pearson)</strong> TXT - &ldquo;Can&rsquo;t We Just Leave the Monster Alive?&rdquo; 2020-04-12 29.97 5 5.31 +/- 4.01 0.93 +/- 0.09 72% -0.29 TXT - &ldquo;Angel or Devil&rdquo; 2019-12-01 29.97 5 4.85 +/- 3.68 0.93 +/- 0.08 71% -0.32 BTS - &ldquo;DNA&rdquo; 2017-09-24 29.97 7 4.94 +/- 3.58 0.9 +/- 0.11 55% -0.14 BTS - &ldquo;Idol&rdquo; 2018-09-02 59.94 7 5.66 +/- 3.95 0.92 +/- 0.08 73% -0.15 X1 - &ldquo;Flash&rdquo; 2019-09-04 29.97 10 4.74 +/- 3.78 0.88 +/- 0.14 56% -0.37 Cravity - &ldquo;Break All the Rules&rdquo; 2020-04-20 29.97 9 5.71 +/- 3.43 0.93 +/- 0.07 72% -0.28 Dongkiz - &ldquo;Lupin&rdquo; 2020-03-20 23.98 5 3.94 +/- 2.67 0.94 +/- 0.08 78% -0.3 EXO - &ldquo;Electric Kiss&rdquo; 2018-01-12 29.97 8 5.74 +/- 3.74 0.93 +/- 0.07 77% -0.26 SF9 - &ldquo;Good Guy&rdquo; 2020-01-09 23.98 9 4.35 +/- 3.37 0.92 +/- 0.09 70% -0.35 Stray Kids - &ldquo;Levanter&rdquo; 2019-12-11 23.98 8 4.66 +/- 3.76 0.86 +/- 0.18 44% -0.32 GIRL GROUPS AOA - &ldquo;Excuse Me&rdquo; 2017-01-10 23.98 7 2.59 +/- 1.99 0.96 +/- 0.04 87% -0.11 Blackpink - &ldquo;As If It&rsquo;s Your Last&rdquo; 2017-06-24 23.98 4 5.9 +/- 3.97 0.96 +/- 0.05 68% -0.04 EXID - &ldquo;I Love You&rdquo; 2018-11-26 29.97 5 2.6 +/- 2.37 0.96 +/- 0.06 88% -0.08 GFriend - &ldquo;Crossroads&rdquo; 2020-02-05 59.94 6 6.88 +/- 7.83 0.95 +/- 0.06 81% -0.31 Itzy - &ldquo;Wannabe&rdquo; 2020-03-11 60 5 7.22 +/- 7.85 0.95 +/- 0.08 80% -0.6 Momoland - &ldquo;I&rsquo;m So Hot&rdquo; 2019-03-24 29.97 7 4.04 +/- 2.97 0.95 +/- 0.08 82% -0.7 Oh My Girl - &ldquo;Bungee (Fall In Love)&rdquo; 2019-08-13 29.97 7 3.97 +/- 2.64 0.91 +/- 0.11 74% -0.28 Red Velvet - &ldquo;Umpah Umpah&rdquo; 2019-08-29 29.97 5 2.98 +/- 1.48 0.96 +/- 0.04 91% -0.1 Twice - &ldquo;Dance the Night Away&rdquo; 2018-07-10 30 9 5.88 +/- 3.82 0.94 +/- 0.07 79% -0.18 Twice - &ldquo;Knock Knock&rdquo; 2017-02-25 29.97 9 3.31 +/- 2.59 0.95 +/- 0.06 77% 0.08 Aggregate statistics for the 20 choreography videos from Table 1. To facilitate comparison between the &ldquo;girl group&rdquo; and &ldquo;boy group&rdquo; categories, the movement time series and intra-frame pose similarity time series for the 10 videos in each category were concatenated together. The distributions of the movement and pose similarity values then were compared to each other via a Welch&rsquo;s <em>t</em> -test, with the resulting <em>t</em> -statistic and two-tailed <em>p</em> -value (the probability that the differences between the two are due to chance) provided in the table. The correlation values between the movement and similarity time series for each category&rsquo;s meta-video also was measured by calculating Pearson&rsquo;s correlation coefficient (Pearson&rsquo;s <em>r</em> ). <strong>COMBINED VIDEO STATISTICS</strong> Mean movement per 1/6 sec - boy groups 5.01 +/- 3.68 Welch&rsquo;s <em>t</em> -test - boys movement vs. girls movement ( <em>t</em> ) 9.76 Mean movement per 1/6 sec - girl groups 4.5 +/- 4.61 Welch&rsquo;s <em>t</em> -test - boys movement vs. girls movement ( <em>p</em> ) 1.77E-22 Mean intra-frame pose similarity - boy groups 0.92 +/- 0.09 Welch&rsquo;s <em>t</em> -test - boys similarity vs. girls similarity ( <em>t</em> ) -33.1 Mean intra-frame pose similarity - girl groups 0.95 +/- 0.06 Welch&rsquo;s <em>t</em> -test - boys similarity vs. girls similarity ( <em>p</em> ) 7.26E-234 Movement:similarity correlation - boy groups (Pearson&rsquo;s <em>r</em> ) -0.24 Movement:similarity correlation - girl groups (Pearson&rsquo;s <em>r</em> ) -0.28<br>A perusal of the statistics in Table 1 gives the impression that despite considerable variability among videos, the girl group performances in our sample feature a greater amount of intra-frame pose similarity, while the boy groups tend to exhibit a higher degree of overall movement. The difference in pose similarity is especially evident from the fraction of each video in which the intra-frame similarity (calculated using the Mantel matrix correlation test described above) is above 90%. The aggregate statistics in Table 2, derived by concatenating all of each category&rsquo;s videos together at a sample rate of 1/6th of a second (the smallest common subdivision among videos recorded at 24, 30 or 60 frames per second, so that videos with higher frame rates do not have disproportionate effect on the results) and applying a Welch&rsquo;s unequal variances <em>t</em> -test to compare the two resulting &ldquo;meta-videos,&rdquo; confirm the high statistical significance of these gender-based disparities. Given the small number of videos in this sample relative to the thousands of choreographed K-pop numbers, however, it is entirely possible that these differences are solely an artifact of our video selection process. Even so, it is difficult to resist the impulse to divine the influence of gender-based cultural notions of activity/passivity, conformance/individuality, extroversion/introversion, as well as physical differences, upon the numbers. More productive and less statistically questionable humanistic insights, however, may be derived by examining the exceptions to and outliers of these trends, as the following example briefly demonstrates.</p><p>The analysis of the correlation between movement and synchronization via the Pearson correlation test described above produced intriguing but not entirely conclusive results. As with the other statistics, it would benefit from a larger sample size and from controlling for song tempo (e.g., in average beats per minute) as a numerical proxy for song style. Yet also in a similar manner to the other comparisons, there is potentially even more to learn by investigating the outliers rather than merely the aggregated statistics. In this case, the dance performance of the girl group Twice&rsquo;s single &ldquo;Knock Knock&rdquo; is the only video in either category with a positive correlation between movement and synchronization. Watching the video closely reveals that this correlation (or more to the point, the lack of a negative correlation) is due to the choreography&rsquo;s fairly unorthodox alternation of sustained tableaux featuring multiple sub-divisions of the group&rsquo;s members in contrasting poses (low similarity, low movement) with more standard synchronized dance sections (high similarity, higher movement). The use in this performance of a choreographic technique that differs considerably in kind and degree from the rest serves to highlight the diverse range of aesthetic &ldquo;concepts&rdquo; that K-pop groups and their artistic collaborators deploy to differentiate one release from another. The further study of such outliers promises to lead to a more in-depth understanding of the role of dance in the K-pop culture industry. And returning to the question of whether it might be possible to quantify the &ldquo;tightness&rdquo; of a performance, this early experiment suggests that the metric also would need to take into account the speed of the movements (i.e., motion over time), assigning more significance to faster motions.</p><p>As the discussion above indicates, a sizable set of other &ldquo;distant&rdquo; analyses remains to be explored, in addition to expanding the size of the analyzed video corpus. Further metrics might include more summational measures, such as calculating an entire dance&rsquo;s average pose-based entropy (roughly indicating the &ldquo;diversity&rdquo; of poses) or its degree of movement autocorrelation (whether classes of similar poses, or sections featuring high-velocity movement, tend to appear in quick succession, or else tend to alternate regularly with contrasting materials), or quantifying the &ldquo;fluidity&rdquo; or &ldquo;jerkiness&rdquo; of movements individually or collectively. The brief analyses presented thus far illustrate some of the potential of these methods.</p><h2 id=7-conclusions-and-next-steps>7. Conclusions and next steps</h2><p>Our application of pose detection techniques and methods for comparison of the resulting matrices offers a visually rich approach to understanding dance similarity both within single K-pop videos and across multiple videos. While we are still quite far from being able to select a dance move or dance sequence and find all other instances of it in the ever-expanding K-pop video corpus, our experiments have shown that this is no longer a distant hope. The development of increasingly accurate pose detection algorithms coupled to relatively fast algorithms for calculating distances and, thus, similarities across detected poses allows us to identify a clear path forward for dance move and sequence comparison. Already, our work has produced intriguing discoveries, such as the relative infrequency of absolute group coordination in K-pop videos despite the general consensus that this is a key feature of K-pop dance performances.</p><p>These initial experiments are a foundational beginning to the complex problem of devising methods for the consistent discovery of dance moves and dance patterns at scale. There is significant room for development, and these preliminary engagements with the complex material of K-pop music videos suggest several productive avenues for future inquiry. The nascent field of &ldquo;big dance&rdquo; studies is already becoming more active: recently, the Google Arts & Culture group has engaged in high-profile collaborations with the choreographer Wayne McGregor with the goal of devising a motion-based search engine for a large, curated corpus of dance recordings. Their recurrent neural network-based next-pose generation project at the very least indicates one direction in which this type of research can be developed.<sup id=fnref:59><a href=#fn:59 class=footnote-ref role=doc-noteref>59</a></sup> More relevant to the present study is the &ldquo;Living Archive,&rdquo; an experimental UMAP embedding visualization web app, which is an excellent example of a big-data choreography analysis project, albeit one focused on a single choreographer&rsquo;s work.<sup id=fnref:60><a href=#fn:60 class=footnote-ref role=doc-noteref>60</a></sup> It is worth noting that the latter project is not more sophisticated technically than our pilot experiments, which thus suggests that relatively small DH research groups such as ours (approximately four people) are able to move the needle considerably without the massive resources of an internet giant.</p><p>While our work has focused exclusively on K-pop videos, the encouraging results suggest that these methods can help us address a series of complex questions not only in this corpus, but across many dance traditions that have been filmed. These questions include the discovery and documentation of stylistic similarity across large dance corpora. Given the ability to align videos with dates of production, these discoveries of similarities and, equally importantly, variations in similar dances could help with the characterization of large-scale dance trends over both time and, in the case of geographically linked traditions such as West Coast Hip Hop or K-pop, space. Through the application of neighborhood detection and clustering algorithms, it should also be possible to use the similarity of dance sequences as part of a classification system, allowing for the computational characterization of sub-genres <sup id=fnref:61><a href=#fn:61 class=footnote-ref role=doc-noteref>61</a></sup>. Intriguingly, given the ability of pose estimation to label body parts automatically, it may be possible to devise a typology of poses and, with an analysis of sequencing, dance moves, leading to an understanding of the vocabulary of moves and sequences in any video or group of videos. Such a computationally derived morphology of K-pop dance could, in turn, lead to investigations of influence, homage and borrowing (inadvertent or intentional) from other domains such as hip hop, Bollywood, Latin dance, martial arts and many other traditions, including traditional Korean dance genres such as p&rsquo;ungmul performance.</p><p>In short, our approach confirms the need for a macroscopic approach to the complex domain of popular dance. In addition to comparing poses, motions, moves, and sequences across thousands of videos, researchers are eager to analyze these dances at many different scales, from the broad domain of an entire genre such as K-pop all the way down to the individual performances of a specific dancer at a specific time. We believe these preliminary investigations and refinements of various techniques, given the productive results derived from their application, provide a clear roadmap for the further development of these methods and will help us answer open questions regarding dance development not only for K-pop but for many other genres as well.</p><h2 id=acknowledgements-and-notes>Acknowledgements and notes</h2><p>We are grateful to Dr. Paul Chaikin for suggesting the Delaunay triangulation approach to graph Laplacian pose characterization and comparison. Dr. Francesca Albrezzi and the students of the Winter 2019 Digital Humanities capstone seminar at UCLA inspired us to expand the range of research questions and corpora treated in this paper. We also thank the contributors to the K-Pop Database site (dbkpop.com) for maintaining a detailed listing of available K-pop dance practice videos on YouTube.</p><p>Source code for the examples presented in the text is available at <a href=https://github.com/broadwell/choreo_k>https://github.com/broadwell/choreo_k</a></p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>Arnold, Taylor, and Lauren Tilton. “Distant Viewing: Analyzing Large Visual Corpora.” <em>Digital Scholarship in the Humanities</em> , March 16, 2019. <a href=https://doi.org/10.1093/digitalsh/fqz013>https://doi.org/10.1093/digitalsh/fqz013</a>.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>Wevers, Melvin, and Thomas Smits. “The Visual Digital Turn: Using Neural Networks to Study Historical Images.” <em>Digital Scholarship in the Humanities</em> , January 18, 2019. <a href=https://doi.org/10.1093/llc/fqy085>https://doi.org/10.1093/llc/fqy085</a>.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>Elhayek, A., E. de Aguiar, A. Jain, J. Thompson, L. Pishchulin, M. Andriluka, C. Bregler, B. Schiele, and C. Theobalt. “MARCOnI — ConvNet-Based MARker-Less Motion Capture in Outdoor and Indoor Scenes.” <em>IEEE</em> <em>Transactions on Pattern Analysis and Machine Intelligence</em> 39.3 (March 1, 2017): 501–14. <a href=https://doi.org/10.1109/TPAMI.2016.2557779>https://doi.org/10.1109/TPAMI.2016.2557779</a>.&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p>Blok, Dylan, Jacob Pettigrew, Thecla Schiphorst, and Herbert H. Tsang. “Human Pose Detection Through Searching in 3D Database With 2D Extracted Skeletons.” In <em>2018 IEEE Symposium Series on Computational Intelligence (SSCI)</em> , 470–76. IEEE, Bangalore, India (2018). <a href=https://doi.org/10.1109/SSCI.2018.8628776>https://doi.org/10.1109/SSCI.2018.8628776</a>.&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5><p>Kim, Eun Mee and Jiwon Ryoo. “South Korean Culture Goes Global: K-Pop and the Korean Wave.” <em>Korean Social Science Journal</em> 34.1 (2007): 117-152.&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:6><p>Choi, JungBong, and Roald Maliangkay. <em>K-Pop–The International Rise of the Korean Music Industry</em> . Routledge (2014).&#160;<a href=#fnref:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:7><p>Lee, Sangjoon, and Abé Markus Nornes. <em>Hallyu 2.0: The Korean Wave in the Age of Social Media</em> . University of Michigan Press, Ann Arbor (2015). <a href=https://doi.org/10.3998/mpub.7651262>https://doi.org/10.3998/mpub.7651262</a>.&#160;<a href=#fnref:7 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:7 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:8><p>Lie, John. “What Is the K in K-Pop? South Korean Popular Music, the Culture Industry, and National Identity.” <em>Korea Observer</em> 43.3 (2012): 339–363.&#160;<a href=#fnref:8 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:9><p>Unger, Michael A. “The Aporia of Presentation: Deconstructing the Genre of K-Pop Girl Group Music Videos in South Korea.” <em>Journal of Popular Music Studies</em> 27.1 (2015): 25–47.&#160;<a href=#fnref:9 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:10><p>Oh, Chuyun. “Queering Spectatorship in K-Pop: The Androgynous Male Dancing Body and Western Female Fandom.” <em>The Journal of Fandom Studies</em> 3.1 (2015): 59–78.&#160;<a href=#fnref:10 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:10 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref2:10 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:11><p>Messerlin, Patrick A., and Wonkyu Shin. “The Success of K-Pop: How Big and Why So Fast?” <em>Asian Journal of Social Science</em> 45.4–5 (2017): 409–439.&#160;<a href=#fnref:11 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:12><p>Kim, Jeong Weon. “행보와 동행:≪ 월간 윤종신≫ 의 매체와 협업에 관한 고찰.” 대중음악 15 (2015): 45–73.&#160;<a href=#fnref:12 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:13><p>Han, Benjamin. “K-Pop in Latin America: Transcultural Fandom and Digital Mediation.” <em>International Journal of Communication (19328036)</em> 11 (2017).&#160;<a href=#fnref:13 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:14><p>Jang, Won Ho, and Jung Eun Song. “The Influences of K-Pop Fandom on Increasing Cultural Contact: With the Case of Philippine Kpop Convention, Inc.” 지역사회학 18 (2017): 29–56.&#160;<a href=#fnref:14 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:15><p>Epstein, Stephen. “From South Korea to the Southern Hemisphere: K-Pop below the Equator.” <em>Journal of World Popular Music</em> 3.2 (2016): 197–223.&#160;<a href=#fnref:15 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:16><p>Otmazgin, Nissim, and Irina Lyan. “Hallyu across the Desert: K-Pop Fandom in Israel and Palestine.” <em>Cross-Currents: East Asian History and Culture Review</em> 3.1 (2014): 32–55.&#160;<a href=#fnref:16 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:17><p>Laurie, Timothy N. “Toward a Gendered Aesthetics of K-Pop.” In Chapman, Ian, and Henry Johnson (eds), <em>Global Glam and Popular Music: Style and Spectacle from the 1970s to the 2000s</em> , 1st ed., Routledge (2016).&#160;<a href=#fnref:17 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:18><p>Manietta, Joseph Bazil. <em>Transnational Masculinities: The Distributive Performativity of Gender in Korean Boy Bands</em> . University of Colorado Boulder (2015).&#160;<a href=#fnref:18 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:19><p>Ota, Kendall. “Soft Masculinity and Gender Bending in Kpop Idol Boy Bands.” In <em>Cal Poly Pomona Lectures</em> (2015). <a href=http://hdl.handle.net/10211.3/138192>http://hdl.handle.net/10211.3/138192</a>.&#160;<a href=#fnref:19 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:20><p>Kim, Gooyong. “Between Hybridity and Hegemony in K-Pop&rsquo;s Global Popularity: A Case of Girls&rsquo; Generation&rsquo;s American Debut.” <em>International Journal of Communication (19328036)</em> 11 (2017).&#160;<a href=#fnref:20 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:21><p>Elfving-Hwang, Joanna. “K-Pop Idols, Artificial Beauty and Affective Fan Relationships in South Korea.” In <em>Routledge Handbook of Celebrity Studies</em> , Routledge (2018), pp. 190–201.&#160;<a href=#fnref:21 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:22><p>Jin, Dal Yong, and Woongjae Ryoo. “Critical Interpretation of Hybrid K-Pop: The Global-Local Paradigm of English Mixing in Lyrics.” <em>Popular Music and Society</em> 37.2 (2014): 113–131.&#160;<a href=#fnref:22 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:23><p>Oh, Chuyun. “Performing Post-Racial Asianness: K-Pop&rsquo;s Appropriation of Hip-Hop Culture.” In <em>Congress on Research in Dance</em> , Cambridge University Press (2014), pp. 121–125.&#160;<a href=#fnref:23 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:24><p>Saeji, Cedarbough. “Cosmopolitan Strivings and Racialization: The Foreign Dancing Body in Korean Popular Music Videos.” In <em>Korean Screen Cultures: Interrogating Cinema, TV, Music and Online Games</em> , edited by David Jackson and Colette Balmain, Peter Lang Publishers, Oxford (2016), pp. 257–92.&#160;<a href=#fnref:24 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:25><p>Howard, Keith. “Politics, Parodies, and the Paradox of Psy&rsquo;s &lsquo;Gangnam Style.&rsquo;” <em>Romanian Journal of Sociological Studies</em> , 1 (2015): 13–29.&#160;<a href=#fnref:25 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:26><p>Reilly, Kara. <em>Theatre, Performance and Analogue Technology: Historical Interfaces and Intermedialities</em> . Palgrave Studies in Performance and Technology. Palgrave MacMillan, Basingstoke (2013).&#160;<a href=#fnref:26 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:27><p>Sutil, Nicolás Salazar. <em>Motion and Representation: The Language of Human Movement</em> . MIT Press, Cambridge, Massachusetts (2015).&#160;<a href=#fnref:27 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:28><p>Watts, Victoria. “Benesh Movement Notation and Labanotation: From Inception to Establishment (1919–1977).” <em>Dance Chronicle</em> 38.3 (2015): 275–304.&#160;<a href=#fnref:28 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:29><p>Rizzo, Anna, Katerina El Raheb, and Sarah Whatley. “WhoLoDance: Whole-Body Interaction Learning For Dance Education.” In <em>Proceedings of the Workshop on Cultural Informatics</em> (2018) Vol. 2235: 41–50. November 3, 2018. <a href=https://doi.org/10.5281/ZENODO.1478033>https://doi.org/10.5281/ZENODO.1478033</a>.&#160;<a href=#fnref:29 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:30><p>Dong, Ran, Dongsheng Cai, and Nobuyoshi Asai. “Dance Motion Analysis and Editing Using Hilbert-Huang Transform.” In <em>ACM SIGGRAPH 2017 Talks</em> , 75:1–75:2. SIGGRAPH &lsquo;17. New York, NY, USA : ACM (2017). <a href=https://doi.org/10.1145/3084363.3085023>https://doi.org/10.1145/3084363.3085023</a>.&#160;<a href=#fnref:30 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:31><p>Kim, Dohyung. “생체역학적용 K-POP 댄스 안무 검색 및 자세 정확성 분석 기술 개발 (The Development of the Choreography Retrieval System from the K-POP Dance Database Including Biomechanical Information and the Analysis Technology of the Correctness of a Dance Posture).” Electronics and Telecommunications Research Institute, Daejeon, Korea (2017). <a href="http://www.ndsl.kr/ndsl/search/detail/report/reportSearchResultDetail.do?cn=TRKO201700003705">http://www.ndsl.kr/ndsl/search/detail/report/reportSearchResultDetail.do?cn=TRKO201700003705</a>.&#160;<a href=#fnref:31 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:31 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:32><p>Such systems have yet to appear, though similar motion mimicry-based dance games exist for the now-discontinued Microsoft Kinect and Nintendo Wii consoles. The dance move data for specific K-pop singles remains for sale on an online portal associated with the project, <a href=http://shop2.mocapkpop.cafe24.com>http://shop2.mocapkpop.cafe24.com</a>.&#160;<a href=#fnref:32 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:33><p>Kim, Yeonho, and Daijin Kim. “Real-Time Dance Evaluation by Markerless Human Pose Estimation.” <em>Multimedia Tools and Applications</em> 77.23 (December 2018): 31199–31220. <a href=https://doi.org/10.1007/s11042-018-6068-4>https://doi.org/10.1007/s11042-018-6068-4</a>.&#160;<a href=#fnref:33 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:34><p>Raptis, Michalis, Darko Kirovski, and Hugues Hoppe. “Real-Time Classification of Dance Gestures from Skeleton Animation.” In <em>Proceedings of the 2011 ACM SIGGRAPH/Eurographics Symposium on Computer Animation - SCA &lsquo;11</em> , 147. ACM Press, Vancouver, British Columbia (2011). <a href=https://doi.org/10.1145/2019406.2019426>https://doi.org/10.1145/2019406.2019426</a>.&#160;<a href=#fnref:34 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:35><p>Hara, Kotaro, Abi Adams, Kristy Milland, Saiph Savage, Chris Callison-Burch, and Jeffrey Bigham. “A Data-Driven Analysis of Workers&rsquo; Earnings on Amazon Mechanical Turk.” <em>ArXiv:1712.05796 [Cs]</em> , December 28, 2017. <a href=http://arxiv.org/abs/1712.05796>http://arxiv.org/abs/1712.05796</a>.&#160;<a href=#fnref:35 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:36><p>Cao, Zhe, Gines Hidalgo, Tomas Simon, Shih-En Wei, and Yaser Sheikh. “OpenPose: Realtime Multi-Person 2D Pose Estimation Using Part Affinity Fields.” <em>CoRR</em> abs/1812.08008 (2018). <a href=http://arxiv.org/abs/1812.08008>http://arxiv.org/abs/1812.08008</a>.&#160;<a href=#fnref:36 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:36 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:37><p>S.V. Aruna Kumar, Ehsan Yaghoubi, Abhijit Das, B.S. Harish and Hugo Proença. “The P-DESTRE: A Fully Annotated Dataset for Pedestrian Detection, Tracking, Re-Identification and Search from Aerial Devices” <em>arXiv:2004.02782</em> , 2020. <a href=http://arxiv.org/abs/2004.02782>http://arxiv.org/abs/2004.02782</a>.&#160;<a href=#fnref:37 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:38><p>Byeon, Yeong-Hyeon, Sung-Bum Pan, Sang-Man Moh, and Keun-Chang Kwak. “A Surveillance System Using CNN for Face Recognition with Object, Human and Face Detection.” In Kuinam J. Kim and Nikolai Joukov (eds), <em>Information Science and Applications (ICISA) 2016</em> 376:975–84. Lecture Notes in Electrical Engineering. Springer Singapore, Singapore (2016). <a href=https://doi.org/10.1007/978-981-10-0557-2_93>https://doi.org/10.1007/978-981-10-0557-2_93</a>.&#160;<a href=#fnref:38 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:39><p>Liu, Shuangjun, Yu Yin, and Sarah Ostadabbas. “In-Bed Pose Estimation: Deep Learning With Shallow Dataset.” <em>IEEE</em> <em>Journal of Translational Engineering in Health and Medicine</em> 7 (2019): 1–12. <a href=https://doi.org/10.1109/JTEHM.2019.2892970>https://doi.org/10.1109/JTEHM.2019.2892970</a>.&#160;<a href=#fnref:39 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:40><p>Mathis, Alexander, Pranav Mamidanna, Kevin M. Cury, Taiga Abe, Venkatesh N. Murthy, Mackenzie Weygandt Mathis, and Matthias Bethge. “DeepLabCut: Markerless Pose Estimation of User-Defined Body Parts with Deep Learning.” <em>Nature Neuroscience</em> 21.9 (September 2018): 1281–89. <a href=https://doi.org/10.1038/s41593-018-0209-y>https://doi.org/10.1038/s41593-018-0209-y</a>.&#160;<a href=#fnref:40 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:41><p>Lin, Tsung-Yi, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollár. “Microsoft COCO: Common Objects in Context.” <em>ArXiv:1405.0312 [Cs]</em> , February 20, 2015. <a href=http://arxiv.org/abs/1405.0312>http://arxiv.org/abs/1405.0312</a>.&#160;<a href=#fnref:41 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:42><p>Güler, Riza Alp, Natalia Neverova, and Iasonas Kokkinos. “DensePose: Dense Human Pose Estimation In The Wild.” <em>CoRR</em> abs/1802.00434 (2018). <a href=http://arxiv.org/abs/1802.00434>http://arxiv.org/abs/1802.00434</a>.&#160;<a href=#fnref:42 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:43><p>Moon, Gyeongsik, Ju Yong Chang, and Kyoung Mu Lee. “PoseFix: Model-Agnostic General Human Pose Refinement Network.” <em>CoRR</em> abs/1812.03595 (2018). <a href=http://arxiv.org/abs/1812.03595>http://arxiv.org/abs/1812.03595</a>.&#160;<a href=#fnref:43 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:44><p>Pavllo, Dario, Christoph Feichtenhofer, David Grangier, and Michael Auli. “3D Human Pose Estimation in Video with Temporal Convolutions and Semi-Supervised Training.” <em>ArXiv:1811.11742 [Cs]</em> , March 29, 2019. <a href=http://arxiv.org/abs/1811.11742>http://arxiv.org/abs/1811.11742</a>.&#160;<a href=#fnref:44 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:45><p>Andriluka, Mykhaylo, Umar Iqbal, Anton Milan, Eldar Insafutdinov, Leonid Pishchulin, Juergen Gall, and Bernt Schiele. “PoseTrack: A Benchmark for Human Pose Estimation and Tracking.” In <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2018)</em> , IEEE, Salt Lake City, UT, USA (2018) p. 5167. <a href=https://doi.org/10.1109/CVPR.2018.00542>https://doi.org/10.1109/CVPR.2018.00542</a>.&#160;<a href=#fnref:45 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:46><p>Ahmadyan, Adel, and Tingbo Hou. “Real-Time 3D Object Detection on Mobile Devices with MediaPipe.” <em>Google AI Blog</em> (blog). Accessed March 11, 2020. <a href=https://ai.googleblog.com/2020/03/real-time-3d-object-detection-on-mobile.html>https://ai.googleblog.com/2020/03/real-time-3d-object-detection-on-mobile.html</a>.&#160;<a href=#fnref:46 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:47><p>Broadwell, Peter, Timothy Tangherlini, and Hyun Kyong Hannah Chang. “Online Knowledge Bases and Cultural Technology: Analyzing Production Networks in Korean Popular Music.” In Jieh Hsiang (ed.), <em>Digital Humanities: Between Past, Present, and Future</em> . NTU Press, Taipei (2016), pp. 369– 94.&#160;<a href=#fnref:47 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:48><p>Kreiss, Sven, Lorenzo Bertoni, and Alexandre Alahi. “PifPaf: Composite Fields for Human Pose Estimation.” <em>CoRR</em> abs/1903.06593 (2019). <a href=http://arxiv.org/abs/1903.06593>http://arxiv.org/abs/1903.06593</a>.&#160;<a href=#fnref:48 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:49><p>Computational image analysis techniques proceed at speeds much closer to real-time human viewing than, say, computational text analysis tasks.&#160;<a href=#fnref:49 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:50><p>Giraldo, Ramón, William Caballero, and Jesús Camacho-Tamayo. “Mantel Test for Spatial Functional Data: An Application to Infiltration Curves.” <em>AStA Advances in Statistical Analysis</em> 102.1 (January 2018): 21–39. <a href=https://doi.org/10.1007/s10182-016-0280-1>https://doi.org/10.1007/s10182-016-0280-1</a>.&#160;<a href=#fnref:50 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:51><p>Delaunay, Boris. “Sur La Sphère Vide.” <em>Bulletin de l&rsquo;Académie Des Sciences de l&rsquo;URSS, Classe Des Sciences Mathématiques et Naturelles</em> 6 (1934): 793–800.&#160;<a href=#fnref:51 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:52><p>Chung, Fan R. K. <em>Spectral Graph Theory</em> . Regional Conference Series in Mathematics 92. Published for the Conference Board of the mathematical sciences by the American Mathematical Society, Providence, R.I. (1997).&#160;<a href=#fnref:52 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:53><p>Xiu, Yuliang, Jiefeng Li, Haoyu Wang, Yinghong Fang, and Cewu Lu. “Pose Flow: Efficient Online Pose Tracking.” <em>ArXiv:1802.00977 [Cs]</em> , July 2, 2018. <a href=http://arxiv.org/abs/1802.00977>http://arxiv.org/abs/1802.00977</a>.&#160;<a href=#fnref:53 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:54><p>Geertz, Clifford. <em>Thick</em> <em>Description: Toward an</em> <em>Interpretive Theory of Culture</em> . Basic Books, New York (1973), pp. 3–30.&#160;<a href=#fnref:54 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:55><p>Börner, Katy. “Plug-and-Play Macroscopes.” <em>Communications of the ACM</em> 54.3 (2011): 60–69.&#160;<a href=#fnref:55 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:56><p><a href="https://www.youtube.com/watch?v=Zu3hBEZ0RvA">https://www.youtube.com/watch?v=Zu3hBEZ0RvA</a>&#160;<a href=#fnref:56 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:57><p>Ankerst, Mihael, Markus M. Breunig, Hans-Peter Kriegel, and Jörg Sander. “OPTICS: Ordering Points to Identify the Clustering Structure.” <em>ACM SIGMOD Record</em> 28.2 (June 1, 1999): 49–60. <a href=https://doi.org/10.1145/304181.304187>https://doi.org/10.1145/304181.304187</a>.&#160;<a href=#fnref:57 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:58><p><a href="https://www.youtube.com/watch?v=sWuYspuN6U8">https://www.youtube.com/watch?v=sWuYspuN6U8</a>&#160;<a href=#fnref:58 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:59><p><a href=https://artsandculture.google.com/story/1AUBpanMqZxTiQ>https://artsandculture.google.com/story/1AUBpanMqZxTiQ</a>&#160;<a href=#fnref:59 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:60><p><a href=https://artsexperiments.withgoogle.com/living-archive/>https://artsexperiments.withgoogle.com/living-archive/</a>&#160;<a href=#fnref:60 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:61><p>Abello, James, Peter Broadwell, and Timothy R. Tangherlini. “Computational Folkloristics.” <em>Communications of the ACM</em> 55.7 (2012): 60–70.&#160;<a href=#fnref:61 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div></div></article></main><footer><nav aria-label="footer links"><ul><li><a class=highlight-focus href=/dhqwords/tags/>Tags</a></li><li><a class=highlight-focus href=/dhqwords/about/>About</a></li><li><a class=highlight-focus href=/dhqwords/categories/>Keywords</a></li></ul></nav><div class=icons><a class="license highlight-focus" rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons Attribution No-Derivatives 4.0 International License" src=/dhqwords/img/logos/license.svg width=120 height=42></a></div></footer></body></html>