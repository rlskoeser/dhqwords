<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://gohugo.io/" version="0.116.0">Hugo</generator><link href="https://startwords.cdh.princeton.edu/vol/15/2/" rel="alternate" type="text/html" title="html"/><link href="https://startwords.cdh.princeton.edu/vol/15/2/index.xml" rel="alternate" type="application/rss+xml" title="rss"/><link href="https://startwords.cdh.princeton.edu/vol/15/2/atom.xml" rel="self" type="application/atom+xml" title="Atom"/><updated>2023-08-04T17:14:09+00:00</updated><rights>This work is licensed under a Creative Commons Attribution-NoDerivatives 4.0 International License.</rights><id>https://startwords.cdh.princeton.edu/vol/15/2/</id><entry><title type="html">Automatic Identification of Types of Alterations in Historical Manuscripts</title><link href="https://startwords.cdh.princeton.edu/vol/15/2/000553/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://startwords.cdh.princeton.edu/vol/15/2/000553/</id><author><name>David Lassner</name></author><author><name>Anne Baillot</name></author><author><name>Sergej Dogadov</name></author><author><name>Klaus-Robert Müller</name></author><author><name>Shinichi Nakajima</name></author><published>2021-11-15T00:00:00+00:00</published><updated>2023-08-04T13:01:02-04:00</updated><content type="html"><![CDATA[<h2 id="editorial-prolegomenon">Editorial prolegomenon</h2>
<p>Classical scholarly editing has a long-standing tradition in distinguishing between different types of editions<a class="footnote-ref" href="#witkowski1924"> [witkowski1924] </a>. The characteristics of specific edition forms usually align with the intended readership, but they also take into account a bibliographic history that tends to differentiate more and more along time according to linguistic areas. In the German-speaking area, historical-critical editions that comprise an extensive historical-critical apparatus are often distinguished - with a clear hierarchical difference - from so-called study editions<a class="footnote-ref" href="#plachta2006"> [plachta2006] </a>. The common denominator between these two types of editions is that they aim to offer a “reliable text” as a central component<a class="footnote-ref" href="#plachta2006"> [plachta2006] </a>. In contrast to these types of editions, it is also possible to publish a reproduction of the manuscript image (facsimile edition). Plachta points out, however, that a facsimile edition is no substitute for the above two types of editions<a class="footnote-ref" href="#plachta2006"> [plachta2006] </a>.</p>
<p>Another way of differentiating between types of editions is to compare the intention in the text construction, which corresponds to the philosophy according to which the anglo-saxon area has mainly structured their approach. According to Andrews, “the old methods that have their root in classical philology” strive to assemble the “ideal” text, while the “new philology” seeks to find the “real” text<a class="footnote-ref" href="#andrews2013"> [andrews2013] </a>. In this conception, the ideal text tries to approach the author&rsquo;s intention, while the real text seeks to emulate the existing sources.</p>
<p>The type of edition an editor goes for is often defined by economic factors in printed editions, while in digital editions, this limitation can be obsolete in terms of the amount of pages available, or located on a different level (for instance due to the price of specific, cost-expensive technologies). More generally, in digital scholarly editions, differentiation characteristics can be renegotiated. As Andrews states, there are hardly any technical limitations in digital editions with regard to the size of the apparatus, and the number and resolution of facsimiles provided<a class="footnote-ref" href="#andrews2013"> [andrews2013] </a>.</p>
<p>This is not the only specificity that distinguishes digital from print editions. They also are machine-readable. With digital editions being available in digital formats, computers can not only handle repetitive tasks in the creation of the edition<a class="footnote-ref" href="#andrews2013"> [andrews2013] </a>, they can also be used to perform tasks that use the edition as source material. The most obvious example for this type of use is the full-text search, but the machine-readable form also allows the creation of a multitude of statistics and customed visualizations with very little effort<a class="footnote-ref" href="#ralle2016"> [ralle2016] </a>. Furthermore, Ralle emphasizes that the digitization of editions and scholarly editing in general allow to pay special attention to the processual aspects of the edition<a class="footnote-ref" href="#ralle2016"> [ralle2016] </a>. An edition can be extended or enriched after it has been initially published and does not need to befinishedat a specific moment in time. A digital edition can be modified dynamically, for instance like the Carl-Maria-von-Weber-Gesamtausgabe with a front page field calledWhat happened today?that connects to all instances of the current date in the corpus and highlights them - a content that changes from day to day and offers a different approach to the corpus than the traditional keyword search. Also, user interaction can be funneled back into the edition, for example when subsequent publications that are based on the edition are listed there. Interaction in and of itself can also be included: the search behaviour of users can be analyzed for better future suggestions or the edition can be enriched by third-party data. Every user of a digital edition, whether computer or human, is thus potentially able to engage in one form of editorial participation or the other<a class="footnote-ref" href="#schlitz2014"> [schlitz2014] </a><a class="footnote-ref" href="#siemens2012"> [siemens2012] </a><a class="footnote-ref" href="#shillingsburg2013"> [shillingsburg2013] </a>.</p>
<p>These special features of digital editions allow for paleography<a class="footnote-ref" href="#baillotschnopf2015"> [baillotschnopf2015] </a>to reach out to research questions hence unexplored in the Humanities due to the lack of tools and corpora allowing an automatic evaluation of alteration phenomena. It enables for instance to thoroughly reconstruct the history of a document by considering physical traces of alterations, meaning any smaller or larger text modifications on the manuscript, performed either by the author himself or herself or by others (see<a href="#methods">Methods</a>). This approach provides insights into the way in which authors, editors and other contributors work together, hence impacting our understanding of text genesis as a collaborative process.</p>
<p>In order to achieve substantial results in this field of research, fast and well-structured access to the document variants is required. Digital editions presenting the manuscript alterations allow to focus on diplomatic transcription or facsimile, as opposed to print editions where the focus is on a single copy text, itself usually optimized for readability. Examples of digital editions representing the document history include<a href="http://faustedition.net">faustedition.net</a><a class="footnote-ref" href="#goethe2017"> [goethe2017] </a>,<a href="http://bovary.fr">bovary.fr</a><a class="footnote-ref" href="#leclerc2009"> [leclerc2009] </a>,<a href="http://beckettarchive.org">beckettarchive.org</a><a class="footnote-ref" href="#beckett2011"> [beckett2011] </a>, and the edition that provided the background for the methodology we propose here: the digital scholarly edition “Letters and texts. Intellectual Berlin around 1800,” <a href="http://berliner-intellektuelle.eu/">berliner-intellektuelle.eu</a><a class="footnote-ref" href="#baillot"> [baillot] </a>, <em>BI</em> in the following.</p>
<h2 id="introduction">Introduction</h2>
<p><em>Letters and Texts. Intellectual Berlin around 1800</em> is a digital scholarly edition of manuscripts by men and women writers of the late 18th and early 19th century. The connection these writers have to the intellectual networks in the Prussian capital city are either direct (authors living and writing in Berlin) or indirect (editorial or epistolar relationship with Berlin-based intellectuals). The originality of this digital scholarly edition is that it is neither author-centered nor genre-based, but presents different types of selected manuscripts that shed light on the intellectual activity of Berlin at the turn of the 18th to the 19th century. This editorial choice is presented at length in<a class="footnote-ref" href="#baillot2014"> [baillot2014] </a>, where light is also shed on the uniqueness of the Prussian Capital City in the context of the period. While correspondences play a key role in the edition, they are considered as a part of the circulation of ideas that is at the core of the project, so that letters, and more generally egodocuments, are complemented by drafts of either literary works (among which two major romantic texts), scholarly writings (one dissertation) or administrative documents (related to the development of the Berlin University). A first editorial phase (2010-2016) allowed to publish manuscripts that cover different thematic areas and historical phases of the development of intellectual Berlin. They were selected based mainly on their scholarly relevance and on their accessibility (the publication policy of archives holding manuscripts having a major impact on their integration to a digital edition that displays a facsimile like this one does). Four main topics have emerged in this first phase: French, e.g. Huguenot Culture, Berlin University, Literary Romanticism and Women Writers. Depending on the topics, the letters published were complemented by other types of texts that document the circulation of ideas and of literary and scholarly works in the late 18th and early 19th century.</p>
<p>The edition can be browsed by theme, by author, by period, by holding institution, or by date. The single document can be displayed on one or two columns presenting at the user’s choice a facsimile of the current manuscript page, a diplomatic transcription, a reading version, the metadata, the entities occurring on the page and the XML file corresponding to the document. In this first development phase, 248 documents and 17 authors were encoded and presented in <em>BI</em> . In<a href="#figure01">Figure 1</a>, a quantitative summary of the BI corpus is given, which consists of introductory figures for the whole corpus in terms of size, temporal span and number of alterations and detailed information about individual authors.</p>




























<figure ><img loading="lazy" alt="" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Quantitative summary of the BI corpus. Subfigure (a) shows the number of documents grouped by each author. Adelbert Chamisso holds the largest share of documents. In (b), the actual number of characters by author shows a slightly different picture, instead of Chamisso, J. Euler has the largest portion. In Subfigure (c), the alterations in terms ofnumber of changed charactersbetween different versions of the documents are depicted for each author. A large number of total characters of an author does not necessarily mean a lot of alterations. Although Euler encompasses the largest number of characters there are close to no alterations in the documents he authored. In (d), total counts for the whole BI corpus are stated to show the extent of the annotational effort. In (e), the temporal distribution of the creation of the documents is shown for the whole corpus (top) and for each individual author (bottom). In this subfigure ((e), bottom), the number of documents created in each year is encoded in the intensity of the color.
        </p>
    </figcaption>
</figure>
<p>A major novelty about the BI edition is that it combines genetic edition and entity annotation in order to gain insight in intellectual networks, on the actual editing process of manuscripts (of literary and scholarly works) and on the discourse about this editing process (letters – most letters are interestingly also partly transformed into literary works in their own right and subject to editing). The genetic encoding gives precise information regarding deletions and additions in the manuscript text. The BI encoding guidelines make extensive use of the following specific sections of the TEI (P5)<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> guidelines additionally to the standard structure (Chapter 1-4): Manuscript Description (Chapter 10,<a href="https://www.tei-c.org/release/doc/tei-p5-doc/en/html/MS.html">https://www.tei-c.org/release/doc/tei-p5-doc/en/html/MS.html</a>), Representation of Primary Sources (Chapter 11,<a href="https://www.tei-c.org/release/doc/tei-p5-doc/en/html/PH.html">https://www.tei-c.org/release/doc/tei-p5-doc/en/html/PH.html</a>) and Names, Dates, People, and Places (Chapter 13,<a href="https://www.tei-c.org/release/doc/tei-p5-doc/en/html/ND.html">https://www.tei-c.org/release/doc/tei-p5-doc/en/html/ND.html</a>), which offers the possibility to markup text alterations with tags such asaddanddel.</p>
<p>As already mentioned, the BI Edition features annotations on the genesis of the documents (genetic edition), which, being a digital edition, are machine-readable. The core question we will address in the following is therefore whether machine learning models<a class="footnote-ref" href="#wainwright2008"> [wainwright2008] </a><a class="footnote-ref" href="#rasmussen2006"> [rasmussen2006] </a><a class="footnote-ref" href="#nakajima2019"> [nakajima2019] </a>that analyze the alterations within the documents can be used to gain new insights into author, editor, and archivist practices, as well as practices of the intellectual societies in the document’s creation time. The investigation of this question is only made possible by the meticulous (digital) annotation of the historical documents that provides previously unavailable enrichments and perspectives on the sources.</p>
<p>From the perspective of edition theory Ehrmann stresses that the importance of analyzing the alterations in manuscripts for literary studies and scholarly editing lies not only in the fact that they allow an insight into the author&rsquo;s writing process in the case of author-made changes, but also in the fact that they help identify the respective contribution in the case of co-authorships<a class="footnote-ref" href="#ehrmann2016"> [ehrmann2016] </a>. The first question that arises when examining every alteration is the question of the underlying reason, be it for a minor correction of mistakes or a wide-ranging content-related alteration. This leads to the question of the originator of the alteration and, as Ehrmann stresses, whether the alteration is wanted by the author<a class="footnote-ref" href="#ehrmann2016"> [ehrmann2016] </a>. In the specific case of an edition of correspondence, the intended readership is bound to change dramatically in the aftermath of publication. A letter that was originally written to a friend is made public to a large readership and in the process of preparation, the editor applies alterations to the original letter, most of the time with a correction phase on the original manuscript itself. This is the case for many manuscripts in the BI edition, commented on as follows by the editors:</p>
<blockquote>
<p>One characteristic of letters is that you generally are not the first one to read them when you discover them in an archive. Not only have they been addressed to a person or a group of persons in the first place [..], many of the letters we at least are working on have already been edited in the last centuries. But not in extenso, no: they have been abridged, overwritten, corrected according to the expectation of the audience in the time that they were edited.<br>
<a class="footnote-ref" href="#baillotbusch2015"> [baillotbusch2015] </a></p>
</blockquote>
<p>Moreover, the novel machine learning method (alterLDA) presented here also offers new opportunities for many other areas of automated analysis of variants of sources, especially within the Digital Humanities. AlterLDA is based on the topic model latent Dirichlet allocation (LDA). “Topic Modeling has proven immensely popular in Digital Humanities” <a class="footnote-ref" href="#schoch2017"> [schoch2017] </a>. LDA is particularly popular in the DH because it is suitable for explorative text analysis. With the automated compilation of word lists by LDA, new topics can be identified in large text corpora whose existence was previously unknown. In this context, it almost always forms the first analysis step on text data, but it can in fact also be used for non-textual data<a class="footnote-ref" href="#jelodar2019"> [jelodar2019] </a><a class="footnote-ref" href="#liu2016"> [liu2016] </a>. In addition to LDA, which provides the identification of the overall relevant topics of the corpus to be examined and the specific topics of the individual documents of the corpus, alterLDA is particularly concerned with the variants of the documents. The starting conditions for this work are as follows: from the point of view of edition theory, the question of document variants is of major importance, and this has not yet been sufficiently investigated with Distant Reading methods. From a methodological point of view, there is a very widespread Topic Model (LDA), which is already recognized and accepted practice in the Digital Humanities. In this paper we therefore close the gap by adapting LDA in order to model document variants.</p>
<p>The processual aspects of text genesis in the sources underlying the edition are thus highlighted and supported by the processual aspects of the edition itself. If a document in its past has already been prepared for publication by an editor, then his or her notes in the TEI-XML are annotated in the same way as when the editors of the BI Edition leave notes: with the <note>-tag.</p>
<p>Parts may be deemed inappropriate for publishing to a broader readership at a certain place and time due to their political or religious context, or for revealing private information about a person or a group.</p>
<p>The application on the BI corpus is particularly interesting because the latter consists mainly of letters, which, especially around 1800 in Germany, exhibit a strong tension between public and private sphere. The framework presented here includes four methods that range from basic, well established, rule-based methods to a specialized, novel machine learning method (alterLDA) that was developed for exactly this purpose. From a methodological point of view, this is a challenge for all disciplines involved, conceived as a scenario optimized so that all sides benefit from each other. Finally, the newly introduced method is also applied to discover alteration candidates in the documents that are not yet altered. These findings led to, and hopefully will continue to fuel, interesting discussions on parts of the edition that were unnoticed thus far.</p>
<p>Due to machine readability, documents in digital editions can be modified by computer programs in such an algorithmic way that they are transformed into something else. This transformation is described by Stephen Ramsay in the concept of Algorithmic Criticism:</p>
<p>Any reading of a text that is not a recapitulation of that text relies on a heuristic of radical transformation. The critic who endeavors to put forth areadingputs forth not the text, but a new text in which the data has been paraphrased, elaborated, selected, truncated, and transduced<a class="footnote-ref" href="#ramsay2011"> [ramsay2011] </a>.</p>
<p>The methods to which Ramsay refers here, e.g. tf-idf normalization, are mostly deterministic methods. In this work, however, a probabilistic method is used to transform the documents and their variants, which uses previously collected data and relates the observations to it. Thus, these statistical transformations resemble human reading more than purely deterministic approaches and therefore foster the methodological concept of the “radical transformation” described within the Algorithmic Criticism towards a more general criticsm that includes non-explicit algorithms.</p>
<p>This transformation in an algorithmic way is a very standard technique (e.g. counting co-appearances of speakers in scenes of a play). However in recent years Machine Learning models are being used more broadly for e.g. Named Entity Recognition<a class="footnote-ref" href="#dalen-oskam2016"> [dalen-oskam2016] </a><a class="footnote-ref" href="#jannidis2015"> [jannidis2015] </a>. The patterns that are used to identify entities are not stated explicitly by a programmer but are learned from the data at training time. In general, Machine Learning methods would then be methodologically less strict than classical explicit algorithmic transformations and therefore be possibly also more human-like. However, when giving up this explicitness there has to be a more rigorous evaluation of the machine’s output. For many applications, like the one presented in this work, we therefore rely on the evaluation by machine learning and humanities scholars, who can employ methods for interpreting and explaining machine learning models<a class="footnote-ref" href="#samek2019"> [samek2019] </a>.</p>
<h2 id="methods">Methods</h2>
<p>In this section, the machine learning methods for identifying the reason for a given alteration are presented, by first introducing the general data analysis pipeline. Then, we specify precise definitions of the most relevant concepts for alterations. After specifying the preprocessing steps, the novel alterLDA model is introduced. It is designed to analyze the most interesting, yet most complex types of alterations. Before the methodologies of each step are explained in further detail, the definitions for the most important and most frequently used terms are given here.</p>
<p>Given an arbitrary version of a document, we define an alteration to be a local group of added and/or deleted symbols that is performed by the author of an alteration. Basically, any symbol appearing in the document could be regarded as a single addition, but the state of the manuscript at the time of the investigation often makes it impossible to identify beyond doubt which groups of symbols belong to a particular writing session. The same problem exists with deletions: Was the sentence completed first, or did the author pause in the middle and correct something before completing the sentence? In BI, additions and deletions are considered as such when they clearly stand out, for example when they are crossed out or written to the margin. Sometimes co-occurring additions and deletions are also referred to as replacements. The alteration may range from a single character to whole passages of the document and can even be a local group with non-altered symbols in between. An alteration author is a single person or institution that alters the document, possibly the primary author him or herself. The alteration author has an alteration reason for which he or she decides to alter the document. This is a very specific reason, for examplethe alteration author thinks that a particular word is spelled differentlyora real person which is referred to in the document may not want to be recognized by the readers, so this part is censored.</p>
<p>Each alteration has a formal and content-related portion with varying emphasis. For example, if the author of an alteration changes the spelling of a single word this would not change the meaning of the document in most cases. On the contrary, adding multiple sentences to a document may change the content of the document significantly. Of course, whether an alteration is rather positioned on the form side or on the content side of the axis depends on the point of view of the recipient. Hence, the proposed method takes into account the formal changes of the document as well as the content-related changes. Smaller alterations tend to have a rather formal aspect, where longer alterations almost always are content-related.</p>
<p>The set of alterations can be broken down into different categories with respect to their alteration reason. One group of alterations is (1) the group of paratexts, for example archival notes, such as numberings or dates, or stamps and seals of the library or archival institution. Another group of alterations is (2) corrections of mistakes which consists in spelling alterations, grammatical changes and other corrections. (3) The third group contains stylistic alterations, for example replacing a token with its synonym or rearranging the word order. Of course, changing the word order is sometimes more than just a stylistic change, but one could e.g. begin a sentence withEs bedarf daher [..]as well as withDaher bedarf es [..]with very similar intentions. The last group of alterations which we call (4) content-related alterations incorporate alterations that either add new information to the document or suppress information that was present in the document before.</p>




























<figure ><img loading="lazy" alt="" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Flow chart of machine learning pipeline with four example alterations. The stream of documents is analysed in four steps that identify different reasons of alterations as depicted in the panel at the top. In the panel at the bottom, the details of the individal alterations are presented. Each alteration has a unique appearance and unique characteristics, like the type of ink and the way in which it fits into the surrounding script. The presented preview of the facsimiles are shown in greater detail in<a href="#figure09">Figure 9</a>,<a href="#figure10">Figure 10</a>,<a href="#figure11">Figure 11</a>and<a href="#figure12">Figure 12</a>
        </p>
    </figcaption>
</figure>
<p><a href="#figure02">Figure 2</a>illustrates how the identification method works. All alterations are put into the analysis pipeline, and after the initial distinction between author alterations and non-author alterations, the four tests for different types are performed on each alteration. As an example, there are four alterations depicted in the illustration that are fed into the model. A detailed explanation for an identification of the three non-content-related types of modifications is given in the appendix. By elimination of all other possible categories, the remaining alterations are of the content-related category. There are still a variety of reasons in this category worthwhile to identify. Rather than the general category we aim for providing a distinct reason for each alteration. The fourth alteration which is marked in red is a longer deletion and a detailed facsimile is shown in<a href="#figure02">Figure 2</a>. It is performed with a pencil which is different from the primary ink of the letter. It deals with the author’s sickness and with the sickness of the author’s mother. The extent of the alteration already indicates that this is not a correction of a mistake and since the part that is deleted is not replaced by anything else, it can be assumed that this alteration changes the amount of information provided. It is thus to be classified as a content-related alteration. At this point it is still to be identified for which specific reason the document has been altered. With alterLDA, the alteration is assigned to one of a set of candidate reasons as a final step, in this case <em>Sickness</em> -reason.</p>
<h2 id="related-work">Related Work</h2>
<p>We convey a generative topic model, that is based on Latent Dirichlet Allocation<a class="footnote-ref" href="#blei2003"> [blei2003] </a>and that is able to take into account the structural information of alterations. LDA is a widely used topic model that extends Latent Semantic Indexing<a class="footnote-ref" href="#deerwester1990"> [deerwester1990] </a>which is capable of assigning a distribution of topics to a document instead of only a single topic. LDA takes advantage of the fact that a text is organized in documents. This structural information is the reason for LDA to function. Based on this structure of documents, LDA can learn which words tend to co-occur and thus have a relation. Words that often occur with each other form a topic. In this context, a topic is merely a distribution of word frequencies.</p>




























<figure ><img loading="lazy" alt="" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Graphical representation of the LDA model. The plate notation visualizes the generative process of a probabilistic model by following the directions of the arrows. Givenαandη, one initially drawsβ, a distribution over words for each topic andθ, a topic distribution for each document. Then, for each token within a document, one draws a topic assignment and only then (becausewhas input arrows from both,βandz) one can drawwfrom the topic inβ, that was assigned inz.
        </p>
    </figcaption>
</figure>
<p>In<a href="#figure03">Figure 3</a>, the LDA model is shown in plate notation. An overview over the used symbols is given in the appendix. The plate notation shows the graphical representation of the LDA model. An open circle denotes a model variable and a shaded circle denotes an observed variable. Symbols without circle denote a hyper parameter. A rectangle indicates repetitions of the included variables. In this model,βrepresents the topic histograms,θrepresents the topic mixture for each document, z represents the topic assignment for each token position and w denotes the token itself. LDA has no notion of the order of words within a document, which is referred to in the literature as abag-of-wordsfor each document.</p>
<p>There exists a wide range of topic models that customize LDA by taking into account additional structural information. To replace the bag-of-words approach by introducing structural information about the word order is a major field of LDA research<a class="footnote-ref" href="#rosen-zvi2004"> [rosen-zvi2004] </a><a class="footnote-ref" href="#gruber2007"> [gruber2007] </a><a class="footnote-ref" href="#wallach2006"> [wallach2006] </a>. In addition, there is a broad research community that addresses the recognition and arrangement of hierarchies of topics<a class="footnote-ref" href="#blei2010"> [blei2010] </a><a class="footnote-ref" href="#paisley2015"> [paisley2015] </a>. LDA has also been modified to work with graph-structured documents<a class="footnote-ref" href="#xuan2015"> [xuan2015] </a>. However, we are not aware of any literature that shows how to model alteration reasons in a corpus of natural language. Therefore, this paper is an important contribution to close this gap, i.e. to provide the literary scholarly community with a novel method and to open up another field of application for the machine learning community.</p>
<h2 id="alteration-latent-dirichlet-allocation">Alteration Latent Dirichlet Allocation</h2>
<p>In<a href="#figure04">Figure 4</a>, the alterLDA model is described in plate notation. The upper part is standard LDA whereas the lower right part contains the newly introduced variables to model alterations.</p>
<p>In standard LDA, the observed variable (the input) is just the words within each document. In the alterLDA setting, the additional structural information about the alteration of each word is provided as input. With that, the alterLDA model tries to infer the tendency for each topic to be an alteration topic.</p>




























<figure ><img loading="lazy" alt="" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Plate notation of the new alterLDA generative model. Newly introduced is the lower branch with variablesc,γ, andξthat deal with alterations. There existMdocuments withNmtokens each, Also, there existKtopics and for each topic, there exist a tendency for it to be a reason for alteration (γ).
        </p>
    </figcaption>
</figure>
<p>The generative model detects reasons by taking into account all text, inside and outside the alterations. From alterations that were gone through manually, we expect to see alteration suggestions that mainly relate to the privacy of a person, political or religious topics may appear as well. In order to make the model description as clear as possible, we try to keep the mathematical formulations to a minimum. Therefore, we only include an explanation of the symbols used (see appendix), a graphical representation and the derivation of how the model can be algorithmically captured using a Collapsed Gibbs Sampler. Similar to LDA, in alterLDA there exists no feasible algorithm to compute the posterior distribution of the latent variables. Instead, approximate methods need to be applied to find a solution in reasonable time.</p>
<p>A Collapsed Gibbs Sampler is one of the possible approaches to find an approximate solution to the objective. Generally, a Gibbs Sampler iteratively samples the configuration of a specific latent variable based on the current configuration of all other model variables. An introductory tutorial on Gibbs Sampling LDA is presented by<a class="footnote-ref" href="#carpenter2010"> [carpenter2010] </a>. This algorithm can also be understood as an instance of a Markov Chain, a constrained iterative probabilistic model itself, where the current state only depends on the previous. From this perspective, the stationary of the Markov Chain represents the solution of the Gibbs Sampler. The source code of our implementation of the Collapsed Gibbs Sampler for the alterLDA model is publicly available.<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> In the appendix, derivation of the Collapsed Gibbs Sampler for the alterLDA model is given.</p>
<h2 id="results">Results</h2>
<p>In this section, we present three experiment settings which mainly differ in the splitting between training and test data. As shown in<a href="#figure05">Figure 5</a>, three settings are chosen, S1 as a straightforward explorative demonstration, S3 to comply with the methodological standards of data splitting for the performance report, as well as S2 for offering additional explorative results specific for this data set. We will first present the evaluation results that investigate the performance of alterLDA on the given data set and afterwards present explorative results that will be reconciled with expert knowledge. Apart from these experiments on the BI data set, the first experiments were performed on synthetic data, some results from these experiments are listed in the appendix.</p>




























<figure ><img loading="lazy" alt="" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Visualization of the data splitting setup for all settings. For each experiment a different data setting is used. The different Settings are shown in the leftmost column (S1, S2, S3). Within each setting, the row at the bottom depicts the final setting of the data. Setting 1 (only one row) does not require test sets, Setting 2 (only one row) aims at finding alteration candidates in texts with no alterations. For Setting 3, the process of creating the setting is depicted in multiple rows. First, only documents that contain alterations are chosen. Then, each individual document is shuffled and split into a training and a test part.
        </p>
    </figcaption>
</figure>
<h2 id="performance-evaluation">Performance Evaluation</h2>
<p>In this experiment, in which alterLDA is applied to the entire training data, it is to be determined whether the model in principle delivers plausible results. It will be verified whetherγfinds a meaningful topic composition that represents sensitive topics. This means that alteration topics may be a convolution of private and maybe political and religious matters.</p>
<p>With alterLDA, various parameters must be set which influence the outcome. These are the same parameters as for LDA: Number of topics and the Dirichlet prior for the topic distributionηand the topic mixtureα. There is also another parameter, the Dirichlet prior for the alteration tendencyξ. The default value for a Dirichlet prior is 1, but it can take any value greater than 0. The smaller the value, the more the variable tends to be focussed on single values, the larger the value, the more different values are considered. Using the topic mixture as an example, a smallαwould mean that LDA is looking for a solution where each document consists of only a few topics, a largeαfinds a solution with mixtures of many topics. AlterLDA is initialized in this setting withα= (.1, .1, …),η= (.1, .1, …) andξ= (.05, .05, …) as well as K=10. We chooseξsmall in this setting to create a sparseγso that alterLDA only learns one alteration topic.</p>
<p>The resulting topic learned in this naive approach as alteration-sensitive is visualized as a word cloud in<a href="#figure06">Figure 6</a>. It is very difficult to put a single label on thistopic.The most probable words are strongly influenced by global word frequencies, the strongest four words describe it:Sie Ich Brief schreiben.This does not come as a surprise since the corpus consists mainly of letters. However, it is also possible to find terms from any subject area that was suspected in advance of being altered:Sicknessterms are for exampleOperation,Bett,fürchten,Financialterms are e.g.Geschäft,Geldand regardingLove Storythere is for exampleliebandschön.</p>
<p>Beforehand, we assumed to also find political, religious topics but these do not appear in the naive setting whereas diverse private topics do occur.</p>




























<figure ><img loading="lazy" alt="" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>The strongest words of the topic that has a high alteration tendency after training alterLDA in the naive setting (Setting 1). The stronger the word, the larger the font. Strongest words are very general words on the topic of letters, words from the Financial, Sickness and Love Story are also weakly present.
        </p>
    </figcaption>
</figure>
<p>As visualized in<a href="#figure05">Figure 5</a>, the BI corpus consists of documents with alterations and documents without alterations. If we want to measure the performance of alterLDA in predicting the tendency for alteration, documents with alterations are much more helpful.</p>
<p>In Setting 3, we only use documents with alterations to produce the training and test set. We split every document individually into training and test set after shuffling to increase the chance that alterations are present in both sets<a class="footnote-ref" href="#muller2001"> [muller2001] </a>. After training, we use the topic mixtureθof the corresponding document.</p>
<p>In this setting, alterLDA is initialized withα= (1, 1, …),η= (1, 1, …) andξ= (1, 1, …) as well asK= 20. We explicitly chose the Dirichlet priors all equal to 1 as this can be considered the default. To allow for more topic diversity, we chose the number of topics a little bit higher than in the naive setting. This parameter combination will be used throughout the rest of the paper.</p>
<p>The performances on the total test set as well as for each individual author are shown in<a href="#table01">Table 1</a>. The performance varies considerably across different authors where D. Tieck, L. Tieck and Ad. Chamisso work well above chance level, the performance for Hoffmann and especially H. Finckenstein is weaker. In case of H. Finckenstein, this may be due to the fact that in the corpus there is only a single letter. For E.T.A. Hoffmann, there is also only one document in the corpus, but it presents two specialties. It is considerably longer than most documents in the corpus: it is not a letter, but the novella <em>Der Sandmann</em> . The larger size and the differing properties due to the genre seem to trade off to a slightly better performance than in the case of H. Finckenstein. We thus argue that the performance of alterLDA depends on the size of the training set and on the homogeneity of the documents.</p>
<p>The results of this setting are not meant produce new domain insights as it only aims at reproducing the alteration tendencies of already altered documents. However, screening performance difference across viewpoints such as <em>authors</em> still reveals properties of the underlying data set.<br>
Test set performance for documents with alterations. The test set is grouped by author and two performance measures are given. Balanced Accuracy and Area under Receiver Operating Characteristic. In both cases, the sklearn implementation is used.<a href="http://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metricsversion0.20.0">http://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics version 0.20.0</a>.GroupingBalanced AccuracyArea under ROC <em>Adelbert von Chamisso</em> .60.57 <em>Henriette v. Finckenstein</em> .38.07 <em>Immanuel v. Fichte</em> .49.64 <em>E.T.A. Hoffmann</em> .5.53 <em>Dorothea Tieck</em> .61.65 <em>Ludwig Tieck</em> .69.65 <em>Total</em>  <em>.67</em>  <em>.66</em></p>
<h2 id="explorative-analysis">Explorative Analysis</h2>
<p>In the explorative experiment (S2), the corpus is divided into two parts: On the one hand, all documents that contain changes and, on the other hand, all documents that do not contain any changes. The aim of the experiment is to train the model on the part of the corpus that contains changes and then let the model suggest which parts of the unchanged corpus may be changed in a similar way. There may be different reasons why some documents contain alterations and others do not. Assuming that all documents were reviewed by the same person and that person was also so diligent that he or she did not overlook a single passage, then alterLDA should at best-case scenario not propose an additional passage to be altered. We assume in this experiment that either not all documents have been reviewed for the same criteria or that relevant positions have been overlooked.<br>
The table shows the number of suggested alterations for different authors/editors. Only documents that were not truly altered are included. The number of suggested alterations represents the number of positions in documents that the method suggests to alter. Euler and Buch mainly wrote in French, which influenced the prediction a lot for these authors. Chamisso wrote in German although his mother tongue is French. Therefore there may be many minor mistakes that are suggested to be altered. Dorothea Tieck wrote about her mother&rsquo;s sickness which the method recognized as a sensible topic and therefore as a reason for alteration.AuthorSuggested alterations <em>Immanuel Hermann von Fichte</em> 3 <em>Karl August Varnhagen von Ense</em> 16 <em>Friedrich Wilhelm Neumann</em> 54 <em>Helmina von Chézy</em> 59 <em>Adelheid Reinbold</em> 73 <em>Henriette Herz</em> 76 <em>Friedrich von Schuckmann</em> 118 <em>Antonie von Chamisso</em> 258 <em>Friedrich Wilken</em> 340 <em>Ludwig Tieck</em> 389 <em>August Boeckh</em> 540 <em>Adolf Friedrich von Buch</em> 907 <em>Dorothea Tieck</em> 929 <em>Adelbert von Chamisso</em> 1075 <em>Jean Albert Euler</em> 2558<br>
In<a href="#table02">Table 2</a>the counts of the positions in documents with no alterations that have been suggested by the method are displayed for each author/editor. The rows in the table are sorted by the total amount of suggested alterations. Interestingly, the authors with many suggested alterations are not necessarily the ones that have a large share of total tokens of the corpus (see<a href="#figure01">Figure 1</a>, (b) and (c)). In the case of Euler and von Buch, this is due to the fact that their documents are mostly in French, whereas the alterLDA model in this case is primarily trained on German texts. For Boeckh, this is mainly due to the fact that the corpus encompasses only a few yet long documents and consequently there are not many documents present in the test set. Of course, there are other reasons for each author&rsquo;s ratio of corpus portion and number of suggested alterations. A person that altered all positions in the training set also diligently edited all documents in the test set and simply did not find any position that should be altered for the same reason: That the method did not find the respective amount in the test set can either mean that it was not able to find the right positions or that there were none.</p>
<p>For further analysis, we will ignore the texts by J. A. Euler and A. F. Buch and focus on the other four authors for which alterLDA suggested most alterations. As said, the texts by J. A. Euler and A. F. Buch were mainly written in French which influenced the number of suggested alterations. In<a href="#table03">Table 3</a>, the most common words that were suggested to be altered for individual authors are listed. For all authors except A. Boeckh, the majority of words seem to relate to the overalllettertopic, however for D. Tieck the keywordKrankheitSicknessappears. For Ad. Chamisso, words like e.g.schönandbegehrencan be observed that may relate to the topicLove Story. In the case of L. Tieck, a distinct convoluted alteration topic is not immediately conceivable. In the case of A. Boeckh, the topic of the documents in the corpus are mostly academia-related.<br>
Most common words that were suggested to be altered in texts from individual authors.Author25 most common suggested alteration words (descending order) <em>Dorothea Tieck</em> Sie, Brief, schreiben, Ich, schön, gewiß, denken, einig, lesen, all, gleichen, Düsseldorf, Agnes, Krankheit, Freund, kennen, erhalten, Arbeit, Dresden, halten, weiß, Die, Leben, Berlin, Lüttichau <em>Adelbert v. Chamisso</em> Brief, schreiben, Ich, de, Die, weiß, kennen, all, wissen, schön, 4, Freund, denken, Sie, gleichen, halten, 3, neu, ton, erhalten, begehren, bleiben, einig, lesen, Sache <em>Ludwig Tieck</em> Sie, Freund, Ich, Geist, Brief, Tieck, Dresden, Von, Ihr, umarmen, Juli, halten, sogleich, erleben, Die, schwach, schweigen, sprechen, Mich, Herrn, Vergnügen, fordern, Masse, gleichen, eintreten <em>August Boeck</em> Mitglied, Seminar, Sie, Prämie, Fichte, erhalten, Arbeit, 1813, 2, Verfasser, 1812, Übung, hiesig, 4, Fähigkeit, welch, außerordentlich, Nummer, zahlen, Prüfung, Wernike, Anstalt, anfangen, Gedicht, Studiosus<br>
For a better understanding of alteration suggestions, a closer look into the individual authors is provided. D. Tieck’s documents reveal a sequence of letters that she wrote to F. Uechtritz in the years between 1831 and 1840. In the letters she repeatedly mentions her mother’s sickness until her death in February 1837. Later, in March 1837, the father of F. Uechtritz passed away as well, D. Tieck writes about this in Letter 28.</p>
<p>By manually reviewing this series of letters, the editors of the BI edition agreed in many cases with the classification of the alterLDA model that the sickness of D. Tieck’s mother plays a role for the alteration tendencies of the documents. In some cases, however, human experts and the model disagreed about the reason for alteration. The following excerpt from letter 12 is identified by our proposed method as a stylistic alteration.</p>
<blockquote>
<p>Meine arme Mutter<br>
leidet schon seit längerer Zeit an Unter=<br>
leibsbeschwerden, der Arzt sagt es seyen<br>
Verhärtungen und Anschwellungen der Drü=<br>
sen, sie hat schon seit längerer Zeit viel zu leiden,braucht schon<br>
seit 3 Monathen,trinkt seit 4 Wochen hier<br>
Karlsbad, undallesbis jetzt ohne den<br>
mindesten Erfolg.<br>
<a href="#baillot">( <em>BI</em> , Dorothea Tieck to v. Uechtritz, Letter 12, p. 2)</a>And one can argue that this is actually a stylistic alteration, because the information about the mother’s sickness is preserved after the alteration. However, the detail that her mother has pelvic complaints is suppressed in the second version - this discrepancy in detail can be decisive for classification as a content-related alteration.</p>
</blockquote>
<p>In<a href="#figure07">Figure 7</a>, the number of suggested alterations (for the test set) and the number of actual alterations (for the training set) are displayed for each document by Ad. Chamisso. Most of the letters are addressed to L. de La Foye (Ad. Chamisso’s best friend), some are addressed to Antonie von Chamisso (Ad. Chamisso’s wife). For each of the addressees, the letters are ordered by date. There are two letters (letter 10 and 11) which stand out significantly with regard to the number of alterations, letter 10 actually encompasses a large number of alterations, whereas letter 11 is part of the test set and thus does not have any (content-related) alterations. The alterLDA model suggests an almost equally high number of alterations for letter 11, presumably because it consists of topics that the alterLDA model estimates to be altered accordingly - this shows that the alterLDA model captures subtle changes of topics by the same author.</p>




























<figure ><img loading="lazy" alt="" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>The letters are divided into documents containing content-related alterations (white background) and documents without content-related alterations (purple background). The alterLDA model is trained on the white part and predicts possible alterations on the purple part, so the blue line shows the number of real alterations and suggested alterations, depending on the background. Left of the dotted separator, we find letters addressed to L. de La Foye, on the right side letters addressed to An. Chamisso. There are two consecutive outliers with significantly higher numbers of alteration words. One is part of the training set, one is part of the test set, the temporal proximity may indicate a content-related proximity that the model was able to capture.
        </p>
    </figcaption>
</figure>
<p>In<a href="#figure08">Figure 8</a>, the correspondence from L. Tieck to F. Raumer that is depicted ranges from years 1815 to 1840. The left panel showing the number of letters that were sent during that year grouped by whether they contain content-related alterations. The right panel shows the number of tokens that were altered (blue) and the number of tokens that alterLDA suggests to be altered (orange).</p>
<p>Just comparing the blue bars of the two panels reveals that despite the fact that in 1836 there was only one letter written, there occurs the third-highest number of altered tokens. By examining the letter, it turns out that Tieck wrote about his financial problems and his plans to sell his book collection to the Count Yorck von Wartenburg.<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup></p>
<p>Referring back to<a href="#table03">Table 3</a>, Financial terms are not present in the most common alteration suggestions for L. Tieck. This could indicate that the person editing L. Tieck’s letters did not miss parts that refer to this financial struggle.</p>
<p>When also considering the suggestions by alterLDA (the orange bars of the panel on the right), one letter from 1838 draws the most attention just by the sheer number of suggested alterations. In this letter, L. Tieck refers to disputes between the Catholic Church and the Prussian state at that time.<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup> By arguing about this political controversy, L. Tieck chooses his wording in such a way that alterLDA suggests alterations. This could indicate that across the training corpus there might be the same tendency to alter parts of the documents that deal with a political controversy. The fact that alterLDA highlights a document containing a mixture of political and religious topics supports the hypothesis that the alterations in the BI corpus do not only consist of privacy matters, but also involve a wider political dimension. This result confirms and gives a novel dimension to the assertion that letters as a text genre evolve, especially in the German context of the 1800s, at the interface between private and public matters. In that sense, the role played by alterations aiming at balancing private and public dimensions is central and needs to be further delved into. AlterLDA provides a systematic approach to this major issue in literary studies.</p>




























<figure ><img loading="lazy" alt="" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Left panel shows the timeline with counts of letters from Ludwig Tieck to Friedrich von Raumer. The right panel shows the number of tokens that were altered (blue) and the number of suggested tokens (orange). The comparison between the number of letters and the number of alterations for each year shows that there are times where the letters were altered more (e.g. 1836). The letters with suggested alterations deal with financial, political and religuous topics.
        </p>
    </figcaption>
</figure>
<h2 id="conclusion">Conclusion</h2>
<p>This paper presented a general framework for analyzing alterations in historical documents, ranging from simple error corrections to stylistic changes and even to content-related alterations. In addition to established methods such as regular expressions, string distances and vector space comparison, a new probabilistic model for the classification of reasons for alterations has been introduced (alterLDA).</p>
<p>This work contributes to the understanding of text genesis, as it provides insight into the layers of changes in documents. It also offers a quantitative way of evaluating which topics are at what times prone to be altered and are therefore sensitive.</p>
<p>From a machine learning point of view, the BI data set posed special challenges because, on the one hand, the data set is very small and, on the other hand, it comprises several languages, which also differ greatly from the ones used today. Nonetheless, alterLDA was able to confidently find alterations on unseen, labelled data. Exploratively, the method was able to find characteristics on unseen, unlabeled data that in many cases match the expert analysis. The method hence proves to be useful to draw the human reader&rsquo;s attention to specific parts of the large corpus that may otherwise be unnoticed, and by doing so serves as an example of how a machine learning method may assist a scholar as a collaborating reader and a potential collaborating editor.</p>
<p>With regard to the editorial issues first presented here, it is to be noted that the machine-readability of the BI edition makes it possible to serve problem-specific, individualised editions tailored to the research question of a reader/scholar. This project showcases how machine learning methods radically transform the way in which scholars engage historical documents, by taking advantage of the quality of deeply-annotated data: editorial and machine learning expertise can be brought together to explore in depth Humanities research questions.</p>
<p>This research benefited greatly from expert knowledge on the corpus as well as from novel ML methods that were designed for this corpus. This collaboration therefore presents important guidelines for practical and methodological steps that will help other projects to enrich their digital editions with automated annotation or ML-guided corpus exploration.</p>
<p>To achieve further progress, highly interdisciplinary research is mandatory where novel ML models are conceived, and domain knowledge from literary studies is interacting with statistical inference.</p>
<h2 id="appendix">Appendix</h2>
<h2 id="non-content-related-alteration-processing">Non-content-related Alteration Processing</h2>
<p>To identify the non-content-related alteration categories, existing methods are used, however, for the identification of the specific content related reasons on the right, the novel alterLDA method is used. As the main contribution of this work lays in introducing the new alterLDA model, the description of the non-content-related alterations. In this section, the description of the established methods is shortly summarized, whereas the description of alterLDA is given more space in the forthcoming subsections.</p>
<h2 id="paratexts">Paratexts</h2>
<p>In<a href="#figure02">Figure 2</a>, the excerpt of the facsimile marked as archival note (orange) has the number 6 written in the top right corner of the sheet, this detail is shown in<a href="#figure09">Figure 9</a>. The corresponding xml transcription is the following:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-xml" data-lang="xml"><span style="display:flex;"><span><span style="color:#f92672">&lt;note</span> <span style="color:#a6e22e">type=</span><span style="color:#e6db74">&#34;foliation&#34;</span> <span style="color:#a6e22e">place=</span><span style="color:#e6db74">&#34;margin-right inline&#34;</span> <span style="color:#a6e22e">hand=</span><span style="color:#e6db74">&#34;#pencil_1&#34;</span><span style="color:#f92672">&gt;</span>6<span style="color:#f92672">&lt;/note&gt;</span>
</span></span></code></pre></div><p>As the header reveals, this pencil numbering has been performed by an archivist:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-xml" data-lang="xml"><span style="display:flex;"><span><span style="color:#f92672">&lt;handNote</span> <span style="color:#a6e22e">xml:id=</span><span style="color:#e6db74">&#34;pencil_1&#34;</span> <span style="color:#a6e22e">scope=</span><span style="color:#e6db74">&#34;minor&#34;</span> <span style="color:#a6e22e">medium=</span><span style="color:#e6db74">&#34;pencil&#34;</span> <span style="color:#a6e22e">scribe=</span><span style="color:#e6db74">&#34;archivist&#34;</span><span style="color:#f92672">&gt;</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-xml" data-lang="xml"><span style="display:flex;"><span><span style="color:#f92672">&lt;seg</span> <span style="color:#a6e22e">xml:xml:lang=</span><span style="color:#e6db74">&#34;de&#34;</span><span style="color:#f92672">&gt;</span>Hand eines Archivars, in Bleistift.<span style="color:#f92672">&lt;/seg&gt;</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-xml" data-lang="xml"><span style="display:flex;"><span><span style="color:#f92672">&lt;seg</span> <span style="color:#a6e22e">xml:xml:lang=</span><span style="color:#e6db74">&#34;en&#34;</span><span style="color:#f92672">&gt;</span>Hand of an archivist, in pencil.<span style="color:#f92672">&lt;/seg&gt;</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-xml" data-lang="xml"><span style="display:flex;"><span><span style="color:#f92672">&lt;seg</span> <span style="color:#a6e22e">xml:xml:lang=</span><span style="color:#e6db74">&#34;fr&#34;</span><span style="color:#f92672">&gt;</span>Main d&#39;un archiviste, crayon de papier.<span style="color:#f92672">&lt;/seg&gt;</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-xml" data-lang="xml"><span style="display:flex;"><span><span style="color:#f92672">&lt;/handNote&gt;</span>
</span></span></code></pre></div><p>For the second pencil note in Figure 9 there is no scribe annotated although it also contains nothing but a numbering:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-xml" data-lang="xml"><span style="display:flex;"><span><span style="color:#f92672">&lt;note</span> <span style="color:#a6e22e">type=</span><span style="color:#e6db74">&#34;foliation&#34;</span> <span style="color:#a6e22e">place=</span><span style="color:#e6db74">&#34;align(center)&#34;</span> <span style="color:#a6e22e">hand=</span><span style="color:#e6db74">&#34;#pencil_2&#34;</span><span style="color:#f92672">&gt;</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-xml" data-lang="xml"><span style="display:flex;"><span><span style="color:#f92672">&lt;hi</span> <span style="color:#a6e22e">rend=</span><span style="color:#e6db74">&#34;underline&#34;</span><span style="color:#f92672">&gt;</span>99<span style="color:#f92672">&lt;/hi&gt;</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-xml" data-lang="xml"><span style="display:flex;"><span><span style="color:#f92672">&lt;/note&gt;</span>
</span></span></code></pre></div><p>The additions that have been performed by a different hand than the primary author and that contain numberings or dates, we consider to be archivists notes. Such archivist’s or editor’s additions can be identified with a very basic set of rules.</p>




























<figure ><img loading="lazy" alt="" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Archival note. BI, Adelbert von Chamisso to Louis de La Foye. Nachlass 239, Blatt 6. Staatsbibliothek Berlin / Manuscripts section. Reuse subject to prior approval by Staatsbibliothek Berlin. Published in: Letter from Adelbert von Chamisso to Louis de La Foye (fragment) (without place, 26 june 1804). Ed. by Anna Busch, Sabine Seifert. Prepared by Janine Katins. In collaboration with Sabine Seifert, Sophia Zeil. In: “Letters and texts: Intellectual Berlin around 1800.” Ed. by Anne Baillot. Berlin: Humboldt-Universität zu Berlin.<a href="http://www.berliner-intellektuelle.eu/manuscript?Brief005ChamissoandeLaFoye">http://www.berliner-intellektuelle.eu/manuscript?Brief005ChamissoandeLaFoye</a>. Last modified: 27 April 2015.
        </p>
    </figcaption>
</figure>
<h2 id="corrections">Corrections</h2>




























<figure ><img loading="lazy" alt="" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Correction of mistake ofwurdetowürde.BI, Adelbert von Chamisso to Louis de La Foye. Nachlass 239, Blatt 85. Staatsbibliothek Berlin / Manuscripts section. Reuse subject to prior approval by Staatsbibliothek Berlin. Published in: <em>Letter from Adelbert von Chamisso to Louis de La Foye</em> (Geneva, at the beginning of 1812). Ed. by Anna Busch, Sabine Seifert. Prepared by Lena Ebert. In: “Letters and texts: Intellectual Berlin around 1800.” Ed. by Anne Baillot. Berlin: Humboldt-Universität zu Berlin.<a href="http://www.berliner-intellektuelle.eu/manuscript?Brief047ChamissoandeLaFoye">http://www.berliner-intellektuelle.eu/manuscript?Brief047ChamissoandeLaFoye</a>. Last modified: 27 April 2015.
        </p>
    </figcaption>
</figure>
<p>The alteration marked in green replaces a single character of a word, for which the corresponding part of the facsimile is shown in<a href="#figure10">Figure 10</a>. [..]Geschichte wuürde lang und schal ausfallen[..] BI, Adelbert von Chamisso to Louis de La Foye. Letter 47, p. 1 This alteration is a correction of a mistake and conceptually, it is worth noting that the words before and after the alteration are very similar. This characteristic will be exploited for the identification of corrections. Identifying corrections is a considerably more difficult task because the corrected version does not necessarily have to be correct from what we know today. The fact that the alteration author corrected the text only means that he or she thought that his or her version is correct. We thus cannot rely on comparing the second version of the text with what an automatic spell checker would the first version correct to. Instead, we divided the problem even further into spelling alterations and grammatical alterations. For identifying spelling mistakes, the tokens of both versions are fuzzy-string matched against the common dictionary of lemmas. If both tokens match closely to the same lemma according to the Levenshtein distance, the two tokens are considered two different spellings of the same word. Fuzzy string matching of multiple tokens against a large vocabulary can be costly in terms of computing time and memory. For a larger data set an adjustment to this approach may be necessary. However, this approach gave better results than simply comparing Levenshtein distance of the tokens of both versions with each other, due to smaller tokens that are very similar but mean different things (e.g.hateandfatehave Levenshtein distance of 1 but have a very different meaning.) For identifying grammatical alterations, we assume that the forms of the tokens in the sentence change and probably punctuations are added or deleted, but the set of lemmas is preserved for the most part. Hence, if the forms or the part of speech of the tokens in the span change but the set of lemmas do not, this alteration is a grammatical correction.</p>
<h2 id="stylistic-alterations">Stylistic Alterations</h2>
<p>[..] DaherEs bedarf esdaher hier eines [..] BI, About the notion of philosophy PP. by Immanuel Hermann von Fichte, p. 14<br>
For identifying stylistic alterations, we assume that all corrections and paratexts are already labelled according to the described method. Thus, there are only stylistic alterations and moral censorships left to be labelled. In our understanding, a stylistic alteration preserves the meaning of the text by only changing the way it is posed which includes rearranging of words, the use of synonyms and rephrasing. In recent years, a method gained a lot of attention that strives to find a vector-space representation of words that capture its meaning. Words or sentences projected to this space reveal a high similarity (for example cosine-similarity) if they have the same meaning. We introduce a threshold and consider all alterations for which the vector-space embedding of the text before and after the alteration reveal a smaller distance to be a stylistic alteration.</p>
<p>The alteration marked in blue (<a href="#figure02">Figure 2</a>) which is shown in higher resolution in<a href="#figure11">Figure 11</a>, reorders the words at the beginning of a sentence without changing the meaning.</p>




























<figure ><img loading="lazy" alt="" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Stylistic alteration ofDaher bedarf estoEs bedarf daher.BI, About the notion of philosophy PP. by Immanuel Hermann von Fichte, p. 14
        </p>
    </figcaption>
</figure>
<p>For completeness, we also provide the individual facsimile of the content related alteration example in<a href="#figure12">Figure 12</a>.</p>




























<figure ><img loading="lazy" alt="" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Content-related alteration. Nachlass Uechtritz. Oberlausitzische Bibliothek der Wissenschaften Görlitz. Reuse subject to prior approval by Oberlausitzische Bibliothek der Wissenschaften Görlitz. Letter from Dorothea Tieck to Friedrich von Uechtritz (Dresden, 10 April 1835). Ed. by Sophia Zeil. Published in: “Letters and texts: Intellectual Berlin around 1800.” Ed. by Anne Baillot. Berlin: Humboldt-Universität zu Berlin.<a href="http://www.berliner-intellektuelle.eu/manuscript?Brief16DorotheaTieckanUechtritz">http://www.berliner-intellektuelle.eu/manuscript?Brief16DorotheaTieckanUechtritz</a>. Last modified: 24 January 2015.
        </p>
    </figcaption>
</figure>
<p>Used SymbolsIdentifierMeaningTypeDimensionalityVNumber of unique tokens in the dictionaryIntWNumber of tokens in the corpusIntMNumber of documentsIntNmNumber of tokens in document mIntKNumber of topicsIntαConcentration ofθHyper parameterKηConcentration ofβHyper parameterVξConcentration ofγHyper parameter2βTopic-term variableDirichletK x VθDocument-topic variableDirichletM x KγTopic-alteration-tendency variableDirichletK x 2zToken-topic variableCategoricalW x KwTokensObserved (Categorical)W x VcAlterationObserved (Categorical)W X 2</p>
<h2 id="collapsed-gibbs-sampler-of-the-alterlda-model">Collapsed Gibbs Sampler of the AlterLDA Model</h2>
<p>For the Collapsed Gibbs Sampler of the alterLDA model it is shown how to derive the posterior for the topic assignment at a current position, given the current configuration. First, the joint probability of the whole model is given before showing how to compute the topic assignment based on count statistics.The joint probability of the model is given by$$\begin{align*} p(\mathbf{w}, c, z, \gamma, \beta, \theta~|~\alpha,\eta, \xi) =&amp; p(c~|~z) \cdot p(\mathbf{w}~|~z,\beta) \cdot p(\gamma~|~\xi) \cdot p(\beta~|~\eta)\cdot p(z~|~\theta)\cdot p(\theta~|~\alpha)\ =&amp; \prod^{M,N}\text{cat}(c~|~c,\gamma) \times \prod^{M,N}\text{cat}(\textbf{w}~|~z,\beta)\times\prod^{M,N}\text{cat}(z~|~\theta)\ &amp;\times\prod^{M}\text{dir}(\theta~|~\alpha)\times\prod^{K}\text{dir}(\gamma~|~\xi)\times\prod^{K}\text{dir}(\beta~|~\eta)\ \end{align*}$$</p>
<p>We introduce a counter variablecwhich can be indexed in four dimensions, the current topic (k), the current document (m), the current alteration mode (a) and the current token (w).$$ c_{k,m,a,\mathbf{w}} = \sum_{n=1}^N \mathbf{I}( z_{m,n}=k \quad&amp;\quad w_{m,n} = \mathbf{w} \quad&amp;\quad c_{m,n} = a ) $$</p>
<p>In this setting, the desired computation is the probability of a topic assignment at a specific position given a current configuration of all other topic assignments. This probability can be formalized by$$\begin{align*} p(z_{m,n}~|~z_{-<em>{(m,n)}},\mathbf{w},c,\alpha,\eta,\xi)&amp;\propto~p(z</em>{m,n},z_{-_{(m,n)}},\mathbf{w},c~|~\alpha,\eta,\xi) \end{align*}$$</p>
<p>Adopting Equation 16 from the Carpenter paper, this probability can be written by marginalizingθ,βandγfrom the joint probability.$$\begin{align*} p(z_{m,n},z_{-<em>{(m,n)}},\mathbf{w},c~|~\alpha,\eta,\xi) = &amp; \int\int\int~p(\mathbf{w},c,z,\gamma,\beta, \theta~|~\alpha,\eta,\xi)d\theta~d\beta~d\gamma\ = &amp; \underbrace{\int~p(\theta~|~\alpha)\cdot~p(z~|~\theta)d\theta}<em>A\ &amp; \times \underbrace{\int~p(\mathbf{w}~|~z,\beta)\cdot~p(\beta~|~\eta)d\beta}<em>B\ &amp; \times \int~p(c~|~z,\gamma)\cdot~p(\gamma~|~\xi)d\gamma\ = &amp; \prod</em>{k=1}^K\int~p(\gamma_k~|~\xi)\prod</em>{m=1,n=1}^{M,N_m}p(c~|~\gamma</em>{\text{argmax}(z_{m,n})})d\gamma_k \times A \times B \end{align*}$$</p>
<p>A and B are substituted here because their derivation is identical to the one in Carpenter et al. Analogue to Equation 27 of Carpenter et al., after inserting the definitions of the Dirichlet distribution the result is proportional to three factors.$$\begin{align*} \propto (\mathbf{c}^-<em>{z</em>{m,n},<em>,</em>,<em>}+\alpha_{z_{m,n}})\left( \frac{ \mathbf{c}^-<em>{z</em>{m,n},</em>,<em>,\mathbf{w}<em>{m,n}}+\eta</em>{w_{m,n}} }{ \mathbf{c}^-<em>{z</em>{m,n},</em>,<em>,</em>}+\sum_v^V\eta_{v} } \right)\left( \frac{ \mathbf{c}^-<em>{z</em>{m,n},<em>,c_{m,n},</em>}+\xi_{w_{m,n}} }{ \mathbf{c}^-<em>{z</em>{m,n},<em>,</em>,<em>}+\sum_i^2\xi_{i} } \right) \end{align</em>}$$where⋅-denotes the counter disregarding the current positionm,n.</p>
<h2 id="results-on-synthetic-data">Results on Synthetic Data</h2>
<p>A big advantage of generative models is that they can be used to generate new data. The generated documents themselves may not be too interesting in the case of topic models, but they can be used to evaluate the functionality of the model. To do this, first, the variables of the model are initialized, and documents are generated. Now the variables are initialized again and based on the previously generated documents the old variable configurations are reconstructed. The performance of the inference can be measured by the accuracy of the reconstruction.</p>
<p>In<a href="#figure13">Figure 13</a>, the results of such an evaluation over 324 different experiment runs is shown, within the sparsity of the hyper-parameters as well as the number of tokens were varied. Each cell shows the mean reconstruction of c over two runs with a given set of parameter choices. The brighter the color of the cell, the better the reconstruction. Overall, a smaller alpha yields better results, independent of the size of the data set and the choice of the other concentration factors. Interestingly, for fewer documents andα=0.1,a smaller concentration factorηperforms better, wheras for either a larger number of documents or a largerα=1.0,a largerηis to be preferred. The explanation for this result is that with more documents, the exact proportions of the topics can be inferred more accurately, wheras for fewer documents, there is the chance of getting a few (sparse) topics right.</p>




























<figure ><img loading="lazy" alt="" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Grid search result for training accuracy ofc^parameter on synthetic data. Even with a small total number of tokens, the accuracy can be very high. Interestingly, the accuracy depends strongly on the sparsity ofα,ξ, andη.
        </p>
    </figcaption>
</figure>
<ul>
<li id="andrews2013">Andrews, T., 2013. “The third way: philology and critical edition in the digital age.”  _Variants_ , pp. 61-76.
</li>
<li id="baillot2014">Baillot, A. & Busch, A., 2014. “ “Berliner Intellektuelle um 1800” als Programm. Über Potential und Grenzen digitalen Edierens.”  _Romantik Digital_ , 1 9.
</li>
<li id="baillotbusch2015">Baillot, A. & Busch, A., 2015. “Editing for man and machine: The digital edition Letters and texts. Intellectual Berlin around 1800 as an example.”  _Variants_ , Volume 13.
</li>
<li id="baillot">Baillot, A., ed., n.d. _Letters and texts._  _Intellectual Berlin around 1800._ s.l., Berlin: Humboldt-Universität zu Berlin.
</li>
<li id="baillotschnopf2015">Baillot, A. & Schnöpf, M., 2015. “Von wissenschaftlichen Editionen als interoperable Projekte, oder: Was können eigentlich digitale Editionen?.”  _Historische Mitteilungen der Ranke-Gesellschat_ , Volume Beiheft 91, pp. 139-156.
</li>
<li id="beckett2011">Beckett, S., 2011-2018. _Digital Manuscript Project. A digital genetic edition_ , Brussels: University Press Antwerp.
</li>
<li id="bishop2006">Bishop, C. M., 2006. _Pattern Recognition and Machine Learning (Information Science and Statistics)_ . s.l.: Springer Science+Business Media, LLC.
</li>
<li id="blei2010">Blei, D. M., Griffiths, T. L. & Jordan, M. I., 2010. “The Nested Chinese Restaurant Process and Bayesian Nonparametric Inference of Topic Hierarchies.”  _J. ACM_ , Volume 57, pp. 1-30.
</li>
<li id="blei2003">Blei, D., Ng, A. & Jordan, M., 2003. Latent “Dirichlet allocation.”  _JMLR._ 
</li>
<li id="bojanowski2017">Bojanowski, P., Grave, E., Joulin, A. & Mikolov, T., 2017. “Enriching Word Vectors with Subword Information.”  _Transactions of the Association for Computational Linguistics_ , Volume 5, pp. 135-146.
</li>
<li id="carpenter2010">Carpenter, B., 2010. “Integrating out multinomial parameters in latent Dirichlet allocation and naive Bayes for collapsed Gibbs sampling.”  _Rapport Technique_ , Volume 4, p. 464.
</li>
<li id="dalen-oskam2016">Dalen-Oskam, K., 2016. “Corpus-based approaches to Names in Literature.” In: C. Hoigh, ed. _The Oxford Handbook of Names and Naming_ . Oxford: Oxford University Press.
</li>
<li id="deerwester1990">Deerwester, S. et al., 1990. “Indexing by latent semantic analysis.”  _Journal of the American society for information science_ , Volume 41, p. 391.
</li>
<li id="ehrmann2016">Ehrmann, D., 2016. “Textrevision — - Werkrevision. Produktion und Überarbeitung im Wechsel von Autoren, Herausgebern und Schreibern.”  _Editio_ , Volume 30.
</li>
<li id="goethe2017">Goethe, J. W., 2017. _Faust. Historisch-kritische Edition._ Frankfurt am Main / Weimar / Würzburg: s.n.
</li>
<li id="gruber2007">Gruber, A., Rosen-Zvi, M. & Weiss, Y., 2007. _Hidden Topic Markov Models_ . s.l., JMLR.
</li>
<li id="jannidis2015">Jannidis, F. et al., 2015. _Automatische Erkennung von Figuren in deutschsprachigen Romanen_ . Graz: ADHO.
</li>
<li id="jelodar2019">Jelodar, H. et al., 2019. “Latent Dirichlet allocation (LDA) and topic modeling: models, applications, a survey.”  _Multimedia Tools and Applications_ , 78(11), pp. 15169-15211.
</li>
<li id="leclerc2009">Leclerc, Y., ed., 2009. _Les Manuscrits de Madame Bovary_ . Rouen, s.n.
</li>
<li id="liu2016">Liu, L. et al., 2016. “An overview of topic modeling and its current applications in bioinformatics.”  _SpringerPlus_ , Volume 5.
</li>
<li id="muller2001">Müller, K.-R.et al., 2001. “An introduction to kernel-based learning algorithms.”  _IEEE Transactions on Neural Networks_ , 12(2), pp. 181-201.
</li>
<li id="nakajima2019">Nakajima, S., Kazuho, W. & Masashi, S., 2019. _Variational BAyesian Learning Theory_ . s.l.: Cambridge University Press.
</li>
<li id="paisley2015">Paisley, J., Wang, C., Blei, D. M. & Jordan, M. I., 2015. “Nested hierarchical Dirichlet processes.”  _IEEE transactions on pattern analysis and machine intelligence_ , Volume 37.
</li>
<li id="plachta2006">Plachta, B., 2006. _Editionswissenschaft: eine Einführung in Methode und Praxis der Edition neuerer Texte_ . 2nd ed. Stuttgart: Reclam.
</li>
<li id="ralle2016">Ralle, I. H., 2016. “Maschinenlesbar — - menschenlesbar. Über die grundlegende Ausrichtung der Edition.”  _Editio_ , Volume 30.
</li>
<li id="ramsay2011">Ramsay, S., 2011. _Reading Machines: Toward and Algorithmic Criticism_ . s.l.:University of Illinois Press.
</li>
<li id="rasmussen2006">Rasmussen, C. E. & Williams, C. K. I., 2006. _Gaussian Processes for Machine Learning_ . s.l.: MIT Press.
</li>
<li id="rosen-zvi2004">Rosen-Zvi, M., Griffiths, T., Steyvers, M. & Smyth, P., 2004. _The author-topic model for authors and documents_ . s.l., s.n., pp. 487-494.
</li>
<li id="samek2019">Samek, W. et al. eds., 2019. _Explainable AI: Interpreting, Explaining and Visualizing Deep Learning_ . s.l.: Lecture Notes in Artificial Intelligence. Volume 11700.
</li>
<li id="schoch2017">Schöch, C., 2017. “Topic Modeling Genre: An Exploration of French Classical and Enlightenment Drama.”  _Digital Humanities Quarterly_ , 11(2).
</li>
<li id="schlitz2014">Schlitz, S., 2014. “Digital Texts, Metadata, and the Multitude. New Directions in Participatory Editing.”  _Variants_ , Volume 11, pp. 71–-89.
</li>
<li id="schmidt2016">Schmidt, D., 2016. “Using standoff properties for marking-up historical documents in the humanities.”  _Information Technology: Human Computation_ , Volume 58, pp. 63-69.
</li>
<li id="shillingsburg2013">Shillingsburg, P., 2013. “Development Principles for Virtual Archives and Editions.”  _Variants_ , pp. 61-76.
</li>
<li id="siemens2012">Siemens, R. et al., 2012. “Toward modeling the social edition: An approach to understanding the electronic scholarly edition in the context of new and emerging social media*.”  _Literary and Linguistic Computing_ , Volume 27, pp. 445-461.
</li>
<li id="wainwright2008">Wainwright, M. J. & Jordan, M. I., 2008. _Graphical Models, Exponential Families, and Variational Inference_ . s.l.: now publishers.
</li>
<li id="wallach2006">Wallach, H., 2006. _Topic Modeling: Beyond Bag-of-words_ . New York, ACM.
</li>
<li id="witkowski1924">Witkowski, G., 1924. _Textkritik und Editionstechnik neuerer Schriftwerke_ . Leipzig: Haessel.
</li>
<li id="xuan2015">Xuan, J., Lu, J., Zhang, G. & Luo, X., 2015. “Topic model for graph mining.”  _IEEE transactions on cybernetics_ , Volume 45.
</li>
</ul>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>The encoding guidelines can be found at<a href="berliner-intellektuelle.eu/encoding-guidelines.pdf">berliner-intellektuelle.eu/encoding-guidelines.pdf</a>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p><a href="gitlab.tubit.tu-berlin.de/david.lassner/shipping_alterLDA">gitlab.tubit.tu-berlin.de/david.lassner/shipping_alterLDA</a>&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>“[..] Tieck plante aus finanzieller Bedrängnis heraus den Verkauf seiner Bibliothek an den Grafen Yorck von Wartenburg[..]”  <em>BI</em> , comment by Johanna Preusse in letter from Ludwig Tieck to Friedrich von Raumer (Dresden, 11. November 1836)&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>“Kontext der von Tieck angedeuteten Vorgänge waren Machtstreitigkeiten zwischen der katholischen Kirche und dem preußischen Staat. [..]”  <em>BI</em> , comment by Johanna Preusse in letter from Ludwig Tieck to Friedrich von Raumer (Dresden, 27. März 1838)## Bibliography&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry><entry><title type="html">Beyond the Word: Immersion, Art, and Theory in Environmental and Digital Humanities Prototyping</title><link href="https://startwords.cdh.princeton.edu/vol/15/2/000557/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://startwords.cdh.princeton.edu/vol/15/2/000557/</id><author><name>Hanna Musiol</name></author><published>2021-08-14T00:00:00+00:00</published><updated>2023-08-04T13:01:02-04:00</updated><content type="html"><![CDATA[<h2 id="heading"></h2>
<blockquote>
<p>I . . . approach the computer as a theatre machine.<br>
—Nancy Mauro-Flude (2016)</p>
</blockquote>
<blockquote>
<p>. . . text mining . . . usually begins with The Word. We extract The Word; we count The Word; we stem The Word to its root; we parse The Word; we name The Word; we disambiguate The Word; we collocate The Word; we count The Word again; we apply an algorithm that allows us to reconstruct the world of The Word as one we can visualize as a list, as a line graph, as a histogram in small multiples, or on big screens. We use the view this new world provides us to interpret The Word.<br>
—Tanya Clement (2016, 534)</p>
</blockquote>
<blockquote>
<p>The surface of the body is a thinking, feeling surface. . . I cannot stop touching the speech of the body.<br>
—Erin Manning (2007, 9)</p>
</blockquote>
<h2 id="beyond-the-word-immersion-art-and-theory-in-environmental-and-digital-humanities-prototyping">“Beyond the Word:” Immersion, Art, and Theory in Environmental and Digital Humanities Prototyping</h2>




























<figure ><img loading="lazy" alt="The picture shows a room with large windows and plants." src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Amanda Ackerman and Dan Richert, <em>Unknown Giants</em> , Kunsthall Trondheim (2017). Photo by Aage A. Mikalsen, Kunsthall Trondheim. Used with permission.
        </p>
    </figcaption>
</figure>
<p>Neither language, nor poetry, Amanda Ackerman (<a href="#ackerman2014">2014, n. pag.</a>) reminds us, can be “be entirely, and only, human.” Unlike Digital Humanities (DH) theorists who mainly explore the entanglement of human and computer languages, Ackerman and her collaborator, Dan Richert, the American transmedia poets and artists behind the 2017 <em>Unknown Giants</em> poetry installation, produce poems together with algorithms, humans, and nonhuman <em>living</em> organisms such as plants (Figure 1). Ackerman and Richert, the poetic duo, have long experimented with electronic, biosensing poetry, exploring, for instance, words as well as the ability of plants, measured through technological mediation, to respond to poems being read to them by humans. Thanks to their unique sensitivity, called “capacitance,” plants interact with electric impulses that human bodies, and sound, specifically, generate (Richert in Ackerman 2014). In their poetry installation exhibited as part of <em>Et Nytt Vi / A New We</em> <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> (2017) at Kunsthall Trondheim (KT) in Norway, Ackerman and Richter (2017) reintroduced these unlikely collaborators and poetry co-designers, fellow human primates as well as magnolia, picae, and eucalyptus (Figure 1). In this installation, the plants’ response to human proximity was recorded using volatile organic compound (VOC) sensors measuring terpenes levels. The exploratory, creative, and protective response of plants<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> — terpenes shield plants from insects — was translated into textual-poetic phrases in English, displayed on a TV, and timestamped and saved for future use. The result of this machine–human–plant encounter was a situational poetic immersive text,<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup> a techno-somatic archive of intimacy, fragility, and danger.<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup> Digital technology operated in their work as “a Form of Art” [<a href="#kordjakpiotrowska2013">Antonisz, quoted in Kordjak-Piotrowska 2013</a>] but also as “an art of living” <a class="footnote-ref" href="#tsing2017"> [tsing2017] </a>, binding different poets together — artists, this author, students, magnolia, and algorithms, helping different poetic stakeholders symbiote and create.</p>
<p>Ackerman and Richter’s expansive, multi-agential biosensing <em>Unknown Giants</em> poetry has yet to be routinely taught in American literature or DH studies survey courses, but such multimodal, interactive digital-sensorial literature will certainly arrive there eventually. At the moment, however, few literary studies and theory classes engage in training in and interrogation of critical, poetic, and speculative affordances and uses of digital technology as a vehicle for non- or postprint, new media literature and critical theory — paradoxically so, since multimodal cli-fi or electronic, biosensing, or electronic theatre, and AI-generated literature are not new, and calls for critical and comparative media interrogations of their different modalities and affordances, of entangled histories of “expressive languages” and their “aesthetic variables” at the time of print literature’s waning dominance, are more than a decade old<a class="footnote-ref" href="#fusco2003"> [fusco2003] </a><a class="footnote-ref" href="#hayles2013"> [hayles2013] </a><a class="footnote-ref" href="#manovich2001"> [manovich2001] </a><a class="footnote-ref" href="#raley2009"> [raley2009] </a><a class="footnote-ref" href="#simanowski2011"> [simanowski2011] </a><a class="footnote-ref" href="#thurston2013"> [thurston2013] </a><a class="footnote-ref" href="#zylinska2020"> [zylinska2020] </a>. In that context, Warren Sack’s (<a href="#sack2019">2019</a>) call for an institutional reclamation of <em>liberal arts</em> [my emphasis] as the foundation of “software arts” and a rethinking of DH’s marginalization of aesthetics, specifically, sounds as belated as it is urgent. Critical digital bio art and media studies practitioners<a class="footnote-ref" href="#manovich2001"> [manovich2001] </a><a class="footnote-ref" href="#mauroflude2016"> [mauroflude2016] </a><a class="footnote-ref" href="#mauroflude2019"> [mauroflude2019] </a><a class="footnote-ref" href="#ackerman2017"> [ackerman2017] </a><a class="footnote-ref" href="#lee2017"> [lee2017] </a><a class="footnote-ref" href="#noble2019"> [noble2019] </a>model such reclamations and often challenge the “monstrosity of [institutional] monocultures” <a class="footnote-ref" href="#swanson2017"> [swanson2017] </a>. Literature and literate arts programs, especially at the undergraduate levels,<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup> can learn from them how to restore the focus on aesthetic practice and digital humanities ethics within the institutional setting.</p>
<p>“Beyond the Word” is an empirical and experimental case study, exploring such practices and pedagogies in the work undertaken by the author, her collaborators, student participants, and partner institutions between 2017 and 2018. The article details our efforts to prototype immersive art-centered D&amp;EH instruction that emphasizes nondeterministic, creative, and reflective approaches to digital tools, which involved designing new syllabi and coordinating and teaching courses at Norwegian University of Science Technology (NTNU); developing public programming curricula — public talks, screenings, workshops, and co-curated immersive exhibits events — for undergraduate students in the Literature, Cultural Studies, and Teacher Training programs at NTNU, Trondheim-based migrant community members, and the general public; and, collaborating closely with several city and university partners: KT, Trondheim Kommune / The Trondheim Municipality (TK), and NTNU ARTEC. Redesigned literature courses discussed below engage thematic and practical considerations in DH &amp; EH — transmodal creative forms, preservation, archives, speculation, and collaboration and ethics. This article traces their entanglements with the word and textuality — but also moves <em>beyond</em> the word and its “capture and measurement” <a class="footnote-ref" href="#mauroflude2019"> [mauroflude2019] </a>— with art, digital apparati, and bodies used as narrative, speculative, immersive, and performative instruments. Ultimately, “Beyond the Word” reflects on what can happen when digital <em>and</em> environmental humanities come into close contact, if not <em>touch</em> , in the contemporary literature and theory classroom, and, often, when the art(ist) is literally present,<sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup> to reveal the poetic, speculative, performative, and ethical contingencies of digital tools.</p>




























<figure ><img loading="lazy" alt="Pink flower on patch of green grass with pink background." src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Rosemary Lee, <em>Symbiotic Sound</em> (2017). Blurring the boundaries between artifice and “nature,” aesthetics, biology and technology. Photo by Aage A. Mikalsen / Kunsthall Trondheim. Used with permission.
        </p>
    </figcaption>
</figure>
<h2 id="background-a-deh-landscape">Background: A D&amp;EH Landscape</h2>
<p>Despite the limited presence of art-centric D&amp;EH literature curricula, techno-organicist preoccupations with aesthetics and environments, with the living and mechanistic, are not absent from DH and EH projects and scholarship. They are common in, for instance, virtual worlds preservation work, alternate reality game (ARG) collaborations, and the examination of the deterioration and disappearance in technology and ecology in an era of the Anthropocene<a class="footnote-ref" href="#kraus2019"> [kraus2019] </a><a class="footnote-ref" href="#mauroflude2019"> [mauroflude2019] </a><a class="footnote-ref" href="#mcdonough2010"> [mcdonough2010] </a><a class="footnote-ref" href="#nowviskie2015"> [nowviskie2015] </a><a class="footnote-ref" href="#nowviskie2018"> [nowviskie2018] </a><a class="footnote-ref" href="#nowviskie2019"> [nowviskie2019] </a><a class="footnote-ref" href="#simanowski2011"> [simanowski2011] </a>. Ursula Heise (<a href="#heise2002">2002</a>) and Margret Linley (<a href="#linley2016">2016, 410–37</a>) theorize how nature metaphors shape digital architectures and discourses,<sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup> while others address the social, gendered, racialized, and colonial thinking that undergirds our understanding of nature <em>and</em> digital environments.<sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup> Importantly, EH, like critical media studies committed to the entangled materialist critique <em>and</em> aesthetic analysis, is explicit about seeing “nature” and “technology” as ideological and historical formations and as living, biophysical, aesthetic, organic and nonorganic, or <em>machinistic</em> objects<a class="footnote-ref" href="#laboratory2019"> [laboratory2019] </a><a class="footnote-ref" href="#tsing2012"> [tsing2012] </a><a class="footnote-ref" href="#tsing2017"> [tsing2017] </a><a class="footnote-ref" href="#haraway2008"> [haraway2008] </a><a class="footnote-ref" href="#haraway2016"> [haraway2016] </a><a class="footnote-ref" href="#nowviskie2018"> [nowviskie2018] </a>.<sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup> DH’s institutionalized focus on “precision-defined models of scaling” <a class="footnote-ref" href="#tsing2012"> [tsing2012] </a>and its “messy institutional realities” <a class="footnote-ref" href="#hunter2019"> [hunter2019] </a>, on the other hand, often preempts a similarly complex understanding of its aesthetics and politics. This leads some to worry that instrumentally deployed “digital methods,” algorithmic criticism among them, may “ero[de] our most unique facility in the humanities,” such as “the aptitude for fine-grained and careful interpretive observation” <a class="footnote-ref" href="#nowviskie2019"> [nowviskie2019] </a>,<sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup> its capacity for “contemplation” <a class="footnote-ref" href="#mauroflude2019"> [mauroflude2019] </a>,<sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup> and, in particular, its engagement with aesthetics and performance.</p>
<p>In Norway, as worldwide, as ecological and digital crises intensify, digital and environmental concerns do seep into the university curricula and research labs, often under the rubric of “sustainability” and “digital transformation” research foci.<sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup> However, DH and EH, and DH and art, are still segregated into different university and external funding, certification, and instruction schemes, and the national commitment to the extraction economy makes funding and sustaining <em>critical</em> liberal arts-based D&amp;EH initiatives challenging.<sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup> Locally, NTNU, the well-funded and largest research university in Norway,<sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup> is an institution with technological and sustainability foci but minimal contemporary critical theory, DH, or digital/media literacy instructional tradition as of 2019. Moreover, and paradoxically, NTNU offers minimal DH infrastructural and practical tech and humanities project-development support.<sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup> Thus, when Safiya Umoja Noble (<a href="#noble2019">2019, 27–28</a>) invokes Audrey Lorde’s, Roopika Risam’s, or Kent Oto’s work, or when Miriam Posner (<a href="#posner2016">2016, 41</a>) contends that “scholarly expertise in critical race theory, feminist and queer theory, and other interrogations of structures of power” is “the most complicated, challenging <em>computing</em> problem [my emphasis]” of DH, locally, the absence of a critical humanistic and DH curriculum prevents students and junior DH practitioners from even seeing computing and critical theory as interrelated, let alone from seeing computing dilemmas <em>through</em> a critical theory lens. In that landscape, and in contrast, new media, art, design and EH initiatives in the region that extend beyond the university are often sites of unapologetic experimentation with critical theory <em>and</em> liberal and “software arts” <a class="footnote-ref" href="#sack2019"> [sack2019] </a><a class="footnote-ref" href="#musiol2020"> [musiol2020] </a><a class="footnote-ref" href="#musiol2021"> [musiol2021] </a><a class="footnote-ref" href="#sack2019"> [sack2019] </a>. Trondheim’s own cultural infrastructure — with at least five contemporary art institutions explicitly dedicated to contemporary and, thus, digital/electronic art<sup id="fnref:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup> — provides a robust collaboration ecosystem, unparalleled, perhaps, by most standards for a city of under 200,000 residents. Institutions such as KT, with an official mandate and resources to engage with environmental and mixmedia art and the public, can easily expand the university classroom and open possibilities for interactions with new forms of digital and biosensing storytelling, theorizing, experimentation, and reflection.</p>
<p>Together with our collaborators, we assumed that literary studies students and scholars in training, familiar with interrogating and <em>playing</em> with aesthetics, with literary techniques and technologies, would welcome explorations of the performative and poetic approaches to digital narrative tools. Thus, when preparing our courses, we relied heavily on the external art networks that support such work. The first elective course, “Literature, EH, and <em>The Arts of Living on a Damaged Planet</em> (LEHALDP),” <sup id="fnref:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup> was developed in close partnership with KT’s <em>A New We</em> (2017) transspecies and transmedia storytelling exhibit co-curated by from the Laboratory for Aesthetics and Ecology (2019), the NTNU ARTEC, the NTNU Academic Guest Network / NTNU for Refugees, and the resettlement/integration unit of TK (the Trondheim Municipality). Our advanced theory course, “Theoretical Approaches to Literary Studies: A Toolbox for Literary Analysis (TALS),” also relied on these partnerships, if only for shorter D&amp;EH modules (in 2017 and 2018). Both initiatives promoted inclusive teaching and transmodal D&amp;EH scholarship, with a focus on environmental digital literature and art practices, and performative, creative, and reflective uses of digital tools. Since both courses were imagined as university D&amp;EH initiatives as well as community-engagement resources, they required a rethinking of what and where the classroom is and what it does — and, also, who is included and welcomed to be part of it. Creating an inclusive commons, then, as an immersive space for observation, performance, storytelling, experimentation, and reflection, was a practical and ethical precondition, one requiring intense preparation and multiple stakeholders.<sup id="fnref:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup> The extensive urban digital art infrastructure helped us grapple with this conundrum. Although the NTNU literature program (ISL/HF) was an official pedagogical base, with its traditional classroom, the library, and reading lists, most of our work happened at and with KT<sup id="fnref:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup> rather than in the university classrooms and its digital or media labs. KT, which transformed into a human–machine– “multispecies salon” <a class="footnote-ref" href="#kirksey2014"> [kirksey2014] </a>(Figures 1–2) with its immersive storytelling and installations co-curated by Ida Bencke and Dea Antonsen from the Laboratory for Aesthetics and Ecology (<a href="#laboratory2019">2019</a>), was integrated into our weekly assignments, gallery visits, public writing and creative digital storytelling and DH workshops, as a context for and <em>objects</em> of our work (Figures 1–4).</p>
<p>There were also several other reasons, beside those outlined above, for a close collaboration with KT. All classes at NTNU and all external programing (at KT or other partner institutions, such as the Falstad Human Rights Center) were open to students and to Trondheim’s permanent and temporary or transient residents, free of charge and without bureaucratic barriers.<sup id="fnref:20"><a href="#fn:20" class="footnote-ref" role="doc-noteref">20</a></sup> However, access to NTNU’s digital environments (hardware and software, DH training, tech support, library services, and even basic university student discussion platforms), while free in Norway, reinforces the digital divide, as it is restricted to registered students in ways that purely physical access to the classrooms and libraries in Norway and most of Europe never is.<sup id="fnref:21"><a href="#fn:21" class="footnote-ref" role="doc-noteref">21</a></sup> KT, on the other hand, literally allowed <em>all</em> interested participants — registered students, invited guests, residents, (im)migrants, asylum seekers, one-time lurkers, and other participants of various abilities — to play with, perform, and observe nonconventional use of digital technology in its environmental art installations. Finally, the immersive exhibition setup encouraged the use of multiple senses and did not privilege ocular, or monolingual or textual proficiency. This was important for some community members whose mother tongue was neither English nor Norwegian, and to neurodiverse participants with different narrative preferences, and digital, or physical access needs.</p>
<h2 id="collaborative-course-design">Collaborative Course Design</h2>
<h2 id="the-public-classroom">The Public Classroom</h2>
<p>Prior exposure to and learning from critical internet practitioners<a class="footnote-ref" href="#mauroflude2016"> [mauroflude2016] </a><a class="footnote-ref" href="#kraus2019"> [kraus2019] </a>at local and national conferences<sup id="fnref:22"><a href="#fn:22" class="footnote-ref" role="doc-noteref">22</a></sup> was instrumental and emboldened us — the art-literature-new media collaborators based at NTNU, within the municipality, and at local art institutions — to seize the opportunity to experiment with D&amp;EH to incorporate digital, immersive, bio-, and electronic art into the undergraduate literature classrooms in lieu of more instrumentalist DH training.<sup id="fnref:23"><a href="#fn:23" class="footnote-ref" role="doc-noteref">23</a></sup> We consulted EH and DH scholars, artists, curators, and practitioners on the syllabi and public programming in order to spur critical reflection and creative work using transmodal art. Some of our collaborators — Krista Caballero, a US-based mixmedia artist, exhibiting at the time at KT; Sissel Bergh, a Trondheim-based mixmedia artist mapping the South Sámi culture in the region; Marco Armiero, an environmental historian and the founder of the Toxic Bios digital archive at the Swedish Royal Institute of Technology (KTH); Henry Mainsah, an Oslo-based digital media scholar, designer, and speculative prototyper; Lisa Dush, a US-based new media and rhetorics scholar; and Carl Faurby, a curator and educator at KT — co-designed or guest-taught portions of our courses. Drawing on Brennan’s (2016) public DH work, together with the KT team, Carl Faurby and Helena Holmberg, we also collaborated on linking our literary and theory studies curriculum to public digital environmental art programming and film screenings. Crucial for the inclusive format of the courses was the practical support of Adria Sharman, the official of TK, who successfully advocated for a 75% reduction in textbook pricing for refugee academics; the NTNU Humanities Faculty’s small pedagogical grant, which covered transportation and D&amp;EH workshop costs; and the decision of KT’s then director, Helena Holmberg, to ensure fee-free entrance to KT on all days to all participants, regardless of their immigration or student status.</p>
<h2 id="pedagogical-toolbox">Pedagogical Toolbox</h2>
<p>Aside from required literary studies skill, we wanted to foreground the aesthetic and reflective potential of digital tools. Each week, we paired print literary and theoretical texts and specific interactive installations at KT with EH and DH keywords (symbiosis, collaboration, and postprint immersive literature; species extinction and conservation and digital preservation and archives; biosocial toxicity, digital waste, and ethics, etc.) and experimented with performative, immersion, prototyping, and theory-making workshops. We felt that experimentation and prototyping can help us “reappraise . . . the utilitarian design” of DH<a class="footnote-ref" href="#mauroflude2016"> [mauroflude2016] </a><a class="footnote-ref" href="#mauroflude2019"> [mauroflude2019] </a>, and we aimed to combine “the speculative inventiveness of design” and “the critical interpretation of the humanities to imagine what might be accomplished with digital tools <em>that don’t yet exist</em> ” <a class="footnote-ref" href="#burdick2015"> [burdick2015] </a>. In the process, we had to draw from diverse critical pedagogy and disciplinary traditions — public humanities, postcolonial studies, DH, EH, design studio and art pedagogy, and critical media/internet studies — and developed a series of interconnected reiterative activities, some deriving from the traditional print-based literary studies and others from multimodal writing and the design pedagogy toolbox<a class="footnote-ref" href="#brennan2016"> [brennan2016] </a><a class="footnote-ref" href="#mauroflude2017"> [mauroflude2017] </a><a class="footnote-ref" href="#musiol2021"> [musiol2021] </a>. The multi-genre and multimodal activities enabled a recognition of neglected forms of nonprint post/decolonial storytelling and meaning-making and speculations.<sup id="fnref:24"><a href="#fn:24" class="footnote-ref" role="doc-noteref">24</a></sup> The aim was to build on <em>and</em> enrich our own and our students’ DH and literary studies training — involving critical reading, watching, deep listening, aesthetic and cultural analysis, curriculum design, and so forth — with performative, immersive, <em>enacting</em> co-creation activities and collaborative methods common to critical making, performance studies, and speculative design.</p>
<h2 id="embodied-contemplation--immersion-as-reflection">“Embodied Contemplation” / Immersion as Reflection</h2>
<p>Importantly, our activities and experiences were not simply hands-on but, often, words-off, other-senses-on, with a focus on reflection and immersion, an approach that warrants its own entry. Against better advice, we centered on continuous meta-reflections on the learning processes and specific encounters with texts, artifacts, and living organisms, foregrounding epistemological ruminations on analog and digital technology and D&amp;EH art and literature in each class, exhibit visit, or workshop activity. In that sense, we reversed the order of Hayles and Pressman’s (<a href="#hayles2013">2013</a>) dictum “making, critique,” by beginning, counterintuitively, with theory and critical reflection. Ryan Cordell (<a href="#cordell2016">2016, 460</a>) warns against such “meta-discussions . . . [that] too often preclude engagement with its projects and theoretical engagements.” He suggests that students, unlike administrators and professionals, are not invested in the debates about the field or the disciplines (460–61) and implies that these conversations should follow, not preempt, the examination of case studies. We had heeded his advice in earlier practical workshops, and it had served us well. That time, however, we wanted to experiment with foregrounding theory, ethics, and reflection as missing “foundations of [D&amp;EH] design” <a class="footnote-ref" href="#zunger2018"> [zunger2018] </a>and to address the absence of critical theory in our institutional setting, specifically. We knew that we could take advantage of our unique immersive environment to generate instantaneous “critical affect,” that is, to make bodies <em>feel</em> critically, as well as to <em>think</em> critically. To this end, we used immersive multisensory environments at KT to engage with biodigital, physical, and political “touch” of multimodal storytelling, in spaces literally oversaturated with competing human and nonhuman bio and digital stimuli, enhanced by interactive digital tools such as algorithms, VOC terpene sensors, digital screens, the ambisonic sound system, and more<a class="footnote-ref" href="#manning2007"> [manning2007] </a>(Figures 1–4).<sup id="fnref:25"><a href="#fn:25" class="footnote-ref" role="doc-noteref">25</a></sup></p>
<p>The corporeal-reflective impact of this immersive method, engendering multisensory, machine-mediated somatic interactions and “unleashing affect” as an epistemic and critical tool, exceeded our expectations<a class="footnote-ref" href="#holmes2018"> [holmes2018] </a>. It resonated strongly with participants, challenging many assumptions about what students/participants want, know, or prefer. “[C]omputational technologies do not only reveal new insights about postdigital culture,” observes Nancy Mauro-Flude (<a href="#mauroflude2019">2019, 219</a>), they “also transform propensities for <em>embodied</em> contemplation, a subject at the heart of humanities scholarship [emphasis mine],” and, in our experiences, immersive art installations facilitated precisely such corporal humanistic reflections. Initially, our literature and humanities students, mostly newcomers to EH and DH, and to contemporary art, were more comfortable with the dominant role that ocular sensations, words and reading specifically, play in the process of learning and interpretation. As Cordell (<a href="#cordell2016">2016</a>) predicts, they were not troubled by disciplinary limits that a literature class may impose. However, as they encountered environmental print theory and fiction, alongside experiencing postprint work and immersive ecological art praxes spatially and in other, nontextual, somatic ways, their very understanding of possible modes of embodied storytelling and knowledge-making expanded, too. Temporarily taken aback by the force of sensory affects and disciplinary bordercrossings, participants soon found the techno-biologist poetic exchanges of affects and capacities between human bodies, machines, plants, animals, and archives both visceral and meaningful<a class="footnote-ref" href="#tsing2017"> [tsing2017] </a>. Many were literally and metaphorically <em>touched</em> by, for instance, how intertwined literature, digital art, and biology are, or how, for instance, digital technology “made plants speak to them” in <em>Unknown Giants</em> (Vegard Ruud, November 8, 2017<sup id="fnref:26"><a href="#fn:26" class="footnote-ref" role="doc-noteref">26</a></sup> ). Some also noted, with fascination, that digital technology became a literary prosthetic and translation platform, which, for instance, “ <em>serve[d] as a voice</em> for the plants, and g[ave plants] the ability to tell stories [my emphasis],” allowing us, humans, to hear them (Ida Tevik Haugen, October 31, 2017). Moreover, participants often focused on how digital tools enabled the “co-authoring” and co-archiving role of the audience, spurring further reflection. “The way the artists used the natural scent emitted by flowers in conjunction with technology in order to (re-)create poetry is in itself amazing,” wrote one participant of <em>Unknown Giants</em> (2017), “but the fact that the spectators also contribute to the process elevates the installation above the others” (Mats Øien, October 29, 2017). To Mats Øien, and others, the collaborative and performative character of <em>Unknown Giants</em> (2017), which “always show[s] unique and personalized output based on the observer(s) present,” demonstrates that “each individual is unique” but also “reminds us that humans are not the only kids on the block” (Mats Øien, October 29, 2017).</p>
<h2 id="archival-reimagining">Archival Reimagining</h2>
<p>Archival practices and preservation technologies have grave implications for the nonhuman “kids on the block,” for our understanding of nonhuman history, species extinction, and for imagining transspecies futures (Mats Øien, October 29, 2017).<sup id="fnref:27"><a href="#fn:27" class="footnote-ref" role="doc-noteref">27</a></sup> In previous courses, we had frequently exposed students to literary digital archives, their preservation missions, feminist or postcolonial ethical considerations, archiving and metadata curation methods, or exhibition techniques.<sup id="fnref:28"><a href="#fn:28" class="footnote-ref" role="doc-noteref">28</a></sup> Given our thematic interest in 2017 in EH, we wanted instead to explore how artists grapple with archival “heritage futures” of nonhuman extinct animal lore<a class="footnote-ref" href="#nowviskie2018"> [nowviskie2018] </a><a class="footnote-ref" href="#nowviskie2019"> [nowviskie2019] </a>.</p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://player.vimeo.com/video/238204874" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="A video installation by Krista Caballero with sound by Frank Ekeberg from Birding the Futures: Lab Series (2017) at Kunsthall Trondheim. Used with permission." webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div>
<p>To this end, we turned to one of two archival digital art projects exhibited at KT, Krista Caballero and Frank Ekeberg’s <em>Birding the Future: Lab Series</em> (2017), a mournful project on avian storytelling and species extinction (Figures 4–5).<sup id="fnref:29"><a href="#fn:29" class="footnote-ref" role="doc-noteref">29</a></sup> This archival installation used visual projections and an ambisonic speaker setup to spatialize extinct and nonextinct bird sounds (Figure 3) which enveloped visitors in an imagined, impossible sonic landscape of zebra finches — common lab birds of our era — and of the extinct Hawaiian Kauaʻi ʻōʻō honeyeater birds. The installation also activated multiple senses — sight, touch, hearing — immersing visitors in historical and speculative sounds within this archive and a transmedia elegy for the biodiversity, soundscape, and cultural lore lost. For instance, participants watched oversized, looped footage of distraught finches handled with lab instruments in an ornithological laboratory (Figures 3–4) while immersed in the archival and speculative soundscape. But while the video projection was consumed more passively, visitors also played with vintage stereoscopes, examining Krista Caballero’s composite cards and their different visual and tactile handling of human-avian stories (Figure 4).<sup id="fnref:30"><a href="#fn:30" class="footnote-ref" role="doc-noteref">30</a></sup> This transformed the gallery into a historic and imaginary, multisensory, dynamic, participatory archive<sup id="fnref:31"><a href="#fn:31" class="footnote-ref" role="doc-noteref">31</a></sup> eliciting different ways of hearing, feeling, reading about the transspecies entanglements (see Figures 4 and 5).</p>




























<figure ><img loading="lazy" alt="A person holds a stereoscope at an exhibit." src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Playing with stereoscopes and a stereograph from <em>Birding the Future: Lab Series</em> (2017) Lab Series, 2017. Photo by Krista Caballero / Kunsthall Trondheim. Used with permission.
        </p>
    </figcaption>
</figure>
<p>Like <em>Unknown Giants</em> (2017), <em>Birding the Future’s</em> (2017) archive depended on various digital and new and old media technology, such as hearing- and vision-enhancing analog and digital tools — video projectors; vintage stereoscopes; composite illustrations based on images taken with digital cameras and then processed with Adobe Pro, Photoshop, and Illustrator; as well as the vector-based amplitude panning algorithm; a multichannel audio (ambisonic) sound spatialization system; and a Raspberry Pis setup — to collapse the temporal distinctions between the irreversible past, the now of the exhibit, and the anticipated future. But their use of sound, visual, and computer technologies was not simply functional and prosthetic — enhancing human vision and hearing — but epistemological, speculative, performative, and “scriptive” <a class="footnote-ref" href="#bernstein2011"> [bernstein2011] </a>. To Bernstein (<a href="#bernstein2011">2011, 12, 69–91</a>), archival work is often a feat of forensic <em>and</em> performative imagination about how historical objects were and <em>might</em> have been used. If what no longer is can only be imagined and speculated about, the installation reminded us that imagination and performance are also indispensable, if neglected, <em>research</em> skills, and that technology can enable this complex understanding of research.</p>
<p>Such complex, multilayered, conflicting use of technology made students instantaneously aware of its obtrusive presence, of technologies’ histories, and of their speculative power in this archive (something we had been trying to expose in other DH classes, but with less success). Eirik Klakegg Thorsen wrote, for instance, that the installation offered “a great example of how technology can both be a distraction and the only possible way to really <em>imagine a historical moment</em> [my emphasis].” He continued:</p>
<blockquote>
<p>I thought that the stereoscope took some of the focus away from the extinct birds and drew the focus more over on the actual technology itself. If the speakers and video were . . . placed in a dark room where one could have focused solely on the sound and picture (or only sound), and tried to imagine the significance of why they [birds and their sounds] are gone, and not get distracted (by the first pair of stereoscopes I have touched since my childhood), I think the experience would have been even more powerful. But then, the artwork would have been much darker, without much hope, and the story would have been completely different. The digital sound and video of the birds show how digital technology may be the only possible way to archive some types [of] <em>historical moments</em> [my emphasis]. One could probably argue that it would be possible to paint a lifelike picture of the birds and maybe even describe a sound by the use of words and musical “notations,” but I would argue that, at least for the sound, it would not be possible to <em>do the imagination without the technology</em> . . . (Eirik Klakegg Thorsen, November 1, 2017; my emphasis)</p>
</blockquote>
<p>While DH often engages in more positivist forensic archival work, in this installation, vintage and new technology (stereoscopes and turn-of-the-century recordings) in particular operated as “scriptive” historical objects<a class="footnote-ref" href="#bernstein2011"> [bernstein2011] </a>, inviting audiences to touch and use them to imagine ways in which they had been used historically, replicating ways of seeing and hearing “nature” at the beginning of the previous and in our century. Just as our students did not initially consider that storytelling might take on transmedia forms, or that algorithms and magnolias can become poetic partners in crime, their initial understanding of history-making and archives derived from their trust in an objective and static textual record of human experience.<sup id="fnref:32"><a href="#fn:32" class="footnote-ref" role="doc-noteref">32</a></sup> Stereoscopic technology challenged such views; it produced a sense of archival multidimensionality and embodiment as it layered different genres and realms of touchable knowledge (science, cards, visual art, and transnational poetry) across time but also encouraged a performative, multisensory, digital-tools-mediated engagement with the archive (Figure 4).</p>
<p>Like <em>Unknown Giants</em> , <em>Birding the Future</em> activated an embodied historical, fleshy, archival sensorium; foregrounded multipurpose and multimodal affordances of digital and analog technology and art; and revealed the critical force of affect produced by immersive visual, sound, and digital tools. Moreover, participants also became attentive to the challenges of creating non- or trans-human stories archives, wondering how one can acknowledge the agency and “voice” of nonhuman “others” when these extend beyond human cognitive capacity. Moreover, this archival installation managed to raise with course participants the important questions in literary and D&amp;EH studies about access and presence in archives. Who belongs in a “heritage future” <a class="footnote-ref" href="#nowviskie2018"> [nowviskie2018] </a>? How can we preserve, display, care for nonhuman lore, without using violent practices and instruments of captivity (Figure 4)? We also wondered what constitutes ethical “collaboration,” storytelling, archival evidence, or historical “heritage” in that context. Ultimately, students and community residents came away from the installation with a different understanding of the role they can play in the archives and in the networks of ecological preservation, interpretation, and care.</p>
<h2 id="prototyping-theory-designs33">Prototyping Theory Designs<sup id="fnref:33"><a href="#fn:33" class="footnote-ref" role="doc-noteref">33</a></sup></h2>
<p>“Experiential prototyping,” according to Nancy Mauro-Flude (<a href="#mauroflude2017">2017</a>), is a “dynamic” and “performative” practice that sits “at the intersection of hands-on practice and critical making” (<a href="#mauroflude2017">167</a>). It is a creative community-building practice, and it allows us to examine the material, futuristic, and ethical consequences of technological design. During the concluding sections of our courses, we re-turned to reflection in participatory prototyping workshops, following models by Ackerman and Richert (<a href="#ackerman2017">2017</a>) in their biosensing poetry and art, Caballero and Ekeberg’s (<a href="#caballero2017">2017</a>) audiovisual speculative archive, and biotechnological blurring in Lee (<a href="#lee2017">2017</a>). Led by Henry Mainsah, we experimented with critical design prototyping to speculate about ideas and things “which do not yet exist” <a class="footnote-ref" href="#burdick2015"> [burdick2015] </a><a class="footnote-ref" href="#mauroflude2016"> [mauroflude2016] </a><a class="footnote-ref" href="#mauroflude2017"> [mauroflude2017] </a>. In order to achieve that, we borrowed elements of design studio pedagogy, especially the format of “design charrettes,” intensive and “scaffolded” activities designed to help with “giving material form to theoretical ideas, and developing and critiquing proposed solutions” <a class="footnote-ref" href="#howard2014"> [howard2014] </a>.</p>




























<figure ><img loading="lazy" alt="Over a dozen people sitting around a table in a work space." src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Prepping for provotyping with Henry Mainsah. Photo by Gulabuddin Sukhanwar. Used with permission.
        </p>
    </figcaption>
</figure>
<p>In his previous work, Mainsah had engaged in such critical digital design thinking and co-making, using in his pedagogical work with students, for instance, speculative Twitterbots, which aimed to disrupt debates about the environment and climate change denial on Twitter. He also incorporated WATCHA, a “fictive <em>disobedient</em> wearable object [my emphasis]” <a class="footnote-ref" href="#morrison2015"> [morrison2015] </a>, which plays with the quantified self, the internet-of-things, and self-surveillance culture. Using WATCHA, a “design fiction” artifact<a class="footnote-ref" href="#morrison2015"> [morrison2015] </a>that “tracks time not as we know it as a man-made construct, but as a feeling,” introduced students to speculative methods of research, helping them investigate “the relations between humans and technological products [in] their everyday use,” and reflect on different understandings <em>and</em> sensations of time (Henry Mainsah, personal communication, July 28, 2021). Similarly, his prototyping D&amp;EH workshops at NTNU aimed to explore the reflective and the speculative capacity, as well as the community-building potential of the prototyping process. Mainsah drew specifically from the work on “provotypes,” that is, prototypes that aim to make cultural claims, “interrupt people’s thinking,” and “astonish” or “disturb” them<a class="footnote-ref" href="#ruecker2015"> [ruecker2015] </a>.</p>
<p>In the first workshop, we worked with course themes, texts, and exhibition keywords (symbiosis, transspecies storytelling, extinction, toxicity, time, extraction, etc.). First, we gathered textual and physical materials on the subject. Its delightful list of D&amp;EH ephemera included a humanoid robot’s Norwegian citizenship test, initiated by Minh Chau Pham; a toxic “ <em>concept</em> Fake news [my emphasis],” which, Arnt Furunes argued, “embodies the <em>technological</em> manipulation of data, misrepresentation of fact, rhetoric and speculation, to make up falsehoods, twisted truths, monsters and ghosts out of voices [my emphasis]” ; Karoline Johansen’s pair of socks (!), called “Leftover yarn project – not waste” ; Øystein Bjørklund-Lassen’s picture of “El Pulpo Mecanico, a mutant-vehicle” ;<sup id="fnref:34"><a href="#fn:34" class="footnote-ref" role="doc-noteref">34</a></sup> and Nina Vitashenko’s “bag of CRISPRs” (November 16, 2017). Then, in small groups, we co-wrote short curatorial statements about them and sketched “rapid prototypes” of our inventions that “could raise awareness, stimulate discussion, or provoke debate about an important ecological issue.” <sup id="fnref:35"><a href="#fn:35" class="footnote-ref" role="doc-noteref">35</a></sup> Finally, we presented them to a wider audience and reflected on them on our blog.<sup id="fnref:36"><a href="#fn:36" class="footnote-ref" role="doc-noteref">36</a></sup> One of our digital toxicity “provotypes” <a class="footnote-ref" href="#ruecker2015"> [ruecker2015] </a>was an eerily familiar Fake News Cockatiel®©, described as “an amalgamation of . . . an internet-connected artificial-intelligence voice, reminiscent of Apple’s Siri and Amazon’s Alexa, in a plastic housing shaped like a cockatiel.”  “Its main function,” the design group representative explained, was “to trawl the net for fake news to recite to its owner. The device [was] voice operated, [with] redundant buttons and . . . a set of equally redundant spare buttons. The Fake News Cockatiel ®© [was] also fully ambulatory; it would creep around at night, seeking new vantage points from which to more effectively misinform . . .” (Arnt Furunes, November 9, 2017).</p>
<p>The second workshop, with a different group, focused explicitly on theory, and the task was to prototype futuristic theory devices, some eerily reminiscent of Slavoj Žižek’s “theory glasses,” that could instantly “generate a radical analysis of a cultural text.” <sup id="fnref:37"><a href="#fn:37" class="footnote-ref" role="doc-noteref">37</a></sup> In both, we followed Burdick’s (<a href="#burdick2015">2015, 15</a>) invitation “[t]o identify processes or methods specific to the challenge of designing Digital Humanities’ futures” and to blend “the generative — methods that look forward, asking what if? ” and “the reflective ones that reveal or critique what is. ” In Burdick’s (<a href="#burdick2015">2015, 15</a>) view, participants “can do both at the same time” when they draw “from two seemingly divergent conceptual domains, future visioning and critical theory.”</p>
<p>In both workshops, hackneyed tech buzzwords — rapid prototyping, innovation, invention, disruption — so cliché and common in DH, at NTNU and beyond, as well as ephemeral concepts, tools, and environmental concerns, were brought together and played with critically, tongue-in-cheek. As participants scrutinized and challenged them in their “provotypes” <a class="footnote-ref" href="#ruecker2015"> [ruecker2015] </a>they also reflected on the questions their speculative inventions invited or suppressed, the potential users they would gain or lose, and the troublesome issue of legal and commercial ownership. Again, the emphasis on experimentation and its consequences was key, “enabl[ing]” students to “unravel[] the tangled mess of ideology, narrative, and possibility” and to “reflect upon their learning, diagnose their design process, and map their impact as designers” <a class="footnote-ref" href="#ward2015"> [ward2015] </a>. The workshop evaluations supported Ward’s observations and included numerous meditations not simply on the prototyped object or concepts but on the importance of the process and the playful and grave ramifications of speculative designs.<sup id="fnref:38"><a href="#fn:38" class="footnote-ref" role="doc-noteref">38</a></sup> Participants also noted the expanded understanding of the impact of digital technology on their disciplinary and theoretical vocabulary, and on their interpretative practices. Foregrounding the speculative, poetic, critical, and philosophical potency of digital tools concretized for us and course participants the idea that ethics and theory are digital building blocks — and that they are the building blocks that are often missing. Ultimately, speculative prototyping, a workshop activity enabling creative, transmodal (sonic, somatic, tactile, kinetic, and textual) practices, became a framing metaphor for our pedagogical work within DH<a class="footnote-ref" href="#mauroflude2017"> [mauroflude2017] </a><a class="footnote-ref" href="#mauroflude2019"> [mauroflude2019] </a>, giving a name to our byzantine D&amp;EH teaching efforts and this article itself.</p>
<h2 id="reflection">Reflection</h2>
<p>In our context, institutionalized, “semi-normal[ized]” DH, unlike in critical internet or media studies, which celebrate new textual modalities, immersive, transmodal narration, and the poetics and theatre of the digital<a class="footnote-ref" href="#mauroflude2016"> [mauroflude2016] </a>, “semi-normal[ized]” DH in our institutional context often excludes emergent twentieth- and twenty-first-century digital art, poetry, and storytelling from the curricula<a class="footnote-ref" href="#ackerman2017"> [ackerman2017] </a><a class="footnote-ref" href="#underwood2019"> [underwood2019] </a>. Critical media art has much to teach word- and print-focused DH literary analytics, then, especially in terms of fostering noninstrumentalist experimentation with aesthetics and appreciation for “creative exuberance and innovative form” <a class="footnote-ref" href="#hayles2013"> [hayles2013] </a>. Moreover, Risam et al. (<a href="#risam2017">2017</a>) are right to point out the public mission obligation, especially at public-mission-oriented institutions, to attend to the digital and media literacy needs of undergraduate students. Institutional DH infrastructure, when available at all, is often built so as to neglect the needs of students whose career paths lie outside doctoral research and are instead often in cultural sectors, in public education classrooms, and local communities. DH’s focus on “logocentric practice[s]” <a class="footnote-ref" href="#clement2016"> [clement2016] </a>and on systematic and deliverable projects is, of course, warranted, needed, and should not be avoided, but the somatic, affective, creative, poetic, or speculative digital design practices that often escape “the precision-nested scales” of DH deserve attention and space, too<a class="footnote-ref" href="#tsing2012"> [tsing2012] </a>. And so does the emphasis on theory, as a design element. And on artistic practice and performative methods. Engaging in critical design processes and art-centered pedagogical activities re-centers on the theoretical, methodological, and ethical dilemmas of the humanities, including digital and environmental ones, of liberal arts, and makes these <em>matter</em> in an undergraduate classroom and in the community<a class="footnote-ref" href="#sack2019"> [sack2019] </a>.</p>
<p>Moreover, if theory and ethics are to shape design, we must make more room for them, if little of both is available at the undergraduate and pre-PhD levels elsewhere in our institutional setting in DH, humanities, more broadly, and in computer / information science training, alike. After all, scholars and academic and industry practitioners insist that critical race, disability studies, queer theory, or environmental justice praxis are foundational to digital literacy and are “computing” or design “problem[s]” themselves yet to be tackled<a class="footnote-ref" href="#posner2016"> [posner2016] </a><a class="footnote-ref" href="#hunter2019"> [hunter2019] </a><a class="footnote-ref" href="#noble2019"> [noble2019] </a><a class="footnote-ref" href="#mauroflude2016"> [mauroflude2016] </a><a class="footnote-ref" href="#mauroflude2019"> [mauroflude2019] </a>. Wachter-Boettcher (<a href="#wachter2017">2017</a>) writes explicitly of lethal “threats of toxic tech,” of “biased algorithms” and racist and “sexist apps,” and such “toxic tech” is seen as a direct human rights and social justice threat<a class="footnote-ref" href="#algorithmic2020"> [algorithmic2020] </a>. If, in practice, “safety and ethics” are “specialties, rather than the foundations of . . . design” for “[s]oftware engineers,” who still “believe they just need to learn to code, change the world, disrupt something,” without “the responsibility that comes with building things . . .” <a class="footnote-ref" href="#zunger2018"> [zunger2018] </a>, we should acknowledge that, locally, critical theory and ethics often are the scarcest and most frequently missing resources in DH<a class="footnote-ref" href="#hunter2019"> [hunter2019] </a><a class="footnote-ref" href="#posner2016"> [posner2016] </a>. If we are to explore the noninstrumentalist potential of D&amp;EH, we have to make it our central pedagogical focus. Agonizing over inaccessibility of DH infrastructure — hardware, software, DH tools training, library access, tech support availability, digital accessibility, and more — is a crucial consideration. But if we decide to explore and nurture “the speculative inventiveness of design” <a class="footnote-ref" href="#burdick2015"> [burdick2015] </a>, we cannot forget that critical theory instruction within DH matters, too, and that it is often absent — its absence a building block of toxic hardware that we build and the DH projects we fund.</p>
<p>Critical design and mixmedia art practices already tackle the human–machine interaction through the lens of critical theory and art, often literate and performance arts, and engage with ethics, reflection, social critique, and “the speech of the body,” as foundational, not peripheral, components of knowledge-making<a class="footnote-ref" href="#manning2007"> [manning2007] </a>. For us, Mauro-Flude’s (<a href="#mauroflude2019">2019</a>) digital practice-based theory of “performing the internet,” proved essential. In her view, nondeterministic, disobedient use of digital tools it requires shifting away from the notion of “use” of digital tools to the practice of, in our case, biodigital, speculative, immersive-reflective “performance.” Understanding digital tools as performative and sensorial instruments, or as Antonisz did, seeing technology, broadly, as an expressive “form of art,” we have realized, may be another aspect of digital literacy and theory we are not teaching, but we should<a class="footnote-ref" href="#kordjakpiotrowska2013"> [kordjakpiotrowska2013] </a>. If we open D&amp;EH pedagogy to embrace the ephemeral, immersive, performative, questioning uses of technology, we may be able to imagine and cocreate other, noncorporate models of digital “arts of living” and embodied and meditative bio-sociality<a class="footnote-ref" href="#tsing2017"> [tsing2017] </a><a class="footnote-ref" href="#mauroflude2017"> [mauroflude2017] </a>.</p>
<p>Our D&amp;EH initiatives depended on existing art and knowledge networks of collaboration, within and <em>outside</em> academia, to fulfill this utopian aspiration, while also addressing specific and practical institutional needs and gaps<a class="footnote-ref" href="#musiol2021"> [musiol2021] </a>. Therefore, using art modes of immersive inquiry within D&amp;EH knowledge- and theory-making might not work in other institutional contexts, and with different literature, art, critical theory, or DH instruction traditions; we certainly do not propose it as a replacement for a more instrumental DH curriculum. However, prototyping immersive D&amp;EH has opened unexpected portals for us, which itself may be instructive for educators and scholars interested in digital aesthetics and the noninstrumental potential of D&amp;EH. Working in the immersive environment at KT, for instance, brought debates about the place of ethics, theory, and transmedia aesthetics back into to the D&amp;EH literature classroom. It also helped students recognize that these conversations take place outside the university, too, and that they themselves can <em>play</em> an important part in them. Moreover, pushing “beyond the word” led us to explore digital technology and the human/nonhuman body as “thinking, feeling” interfaces, and <em>experience</em> viscerally, collectively and individually, the expressive affordances of D&amp;EH. This, in turn, emboldened us to further experiment with contemplative trans-human co-creation<a class="footnote-ref" href="#manning2007"> [manning2007] </a>. Ultimately, as dutiful students of critical and public pedagogy ourselves<a class="footnote-ref" href="#freire1993"> [freire1993] </a><a class="footnote-ref" href="#giroux1988"> [giroux1988] </a><a class="footnote-ref" href="#risam2019"> [risam2019] </a><a class="footnote-ref" href="#risam2017"> [risam2017] </a><a class="footnote-ref" href="#sandlin2010"> [sandlin2010] </a>, together with Caballero, Mainsah, Armiero, Faurby, Dush, KT, and student/participants, we nurtured the expanded classroom as a complex ecosystem: not only as a skills and content transit zone, but as an “enchanted” social space, a playful and a serious stage, a “multispecies salon,” a sensorial bio-cultural laboratory in which the creative practices of knowledge-making, care, archival research, collaboration, can be <em>felt</em> , rehearsed, performed, contemplated, and reimagined<a class="footnote-ref" href="#kraus2019"> [kraus2019] </a><a class="footnote-ref" href="#kirksey2014"> [kirksey2014] </a>.</p>
<h2 id="acknowledgements">Acknowledgements</h2>
<p>I thank Kari Kraus, Radhika Gajjala, and Nancy Mauro-Flude for modeling creative, critical DH at Futurescapes in 2016 and beyond; Kunsthall Trondheim (KT)’s Helena Holmberg, Carl Faurby, and Katrine Elise Pedersen, for their generous collaboration on D&amp;EH curriculum; Dan Richert, Amanda Ackerman, Krista Caballero, and Frank Ekeberg, for generously sharing their art production notes and reflections; Marco Armiero, Stephanie Le Manager, Kyle Whyte, and Elaine Gan, Anna Tsing, Heather Swanson, and Nils Bubandt, for inspiring the 2017 EH experiments and syllabus; Armiero, Sissel Bergh, Caballero, Henry Mainsah, and Lisa Dush, for their onsite workshops; the NTNU for Refugees / Academic Guest Network and Adria Sharman and the Trondheim Kommune, which supported inclusion of refugee/immigrant academics, and secured course book access; NTNU’s Humanities Faculty, for the pedagogical innovation grant that brought Henry Mainsah’s D&amp;EH prototyping workshops to campus; Henry Mainsah, for his work on the “povotyping” part of the article; NTNU ARTEC’s Andrew Perkis, for supporting transmedia humanities bordercrossings and public workshops; and most important, students Eirik Klakegg Thorsen, Arnt Skoge Furunes, Ida Tevik Haugen, Gulabuddin Sukhanwar, and Mats Øien, for sharing their work in this article, and, Leila Arghavani, Øystein Bjørklund-Lassen, Ingrid Hareland Bustad, Du Cheng, Eira Larsen, Arnt Skoge Furunes, Birna Ósk Gunnarsdottir, Leni Hansen, Rasha Hasan, Tuva Holthe-Berg, Synne Pedersen Kåsbøl, Karoline Johansen, Daniel André Johansen, Sunniva Kennedy, Viktor Okpe, Minh Chau Nguyen Pham, Sondre Sæterbø, Nina Vitashenkova, and other NTNU- and Trondheim-based writers, artists, and participants, for theorizing and prototyping D&amp;EH together.</p>
<ul>
<li id="ackerman2014">Ackerman, A. “Getting Plants to Write Poems.”  _Poetry Foundation_ , October 31, 2014,<a href="https://www.poetryfoundation.org/harriet/2014/10/getting-plants-to-write-poems">https://www.poetryfoundation.org/harriet/2014/10/getting-plants-to-write-poems</a>(accessed April 26, 2018).
</li>
<li id="ackerman2017">Ackerman, A. and Richert, D. _Unknown Giants_ [installation]. Kunsthall Trondheim (2017).
</li>
<li id="alaimo2010">Alaimo, S. _Bodily Natures: Science, Environment, and the Material Self_ . Indiana University Press, Bloomington (2010).
</li>
<li id="alaimo2007">Alaimo, S. and Hekman, S. J. (eds). _Material Feminisms_ . Indiana University Press, Bloomington (2007).
</li>
<li id="algorithmic2020">Algorithmic Justice League (2020),<a href="https://www.ajl.org/">https://www.ajl.org/</a>(accessed September 14, 2020).
</li>
<li id="armiero2017">Armiero, M. and De Angelis, M. “Anthropocene: Victims, Narrators, and Revolutionaries.”  _South Atlantic Quarterly_ , 116.2 (2017): 345–62.
</li>
<li id="bernstein2011">Bernstein, R. _Racial Innocence: Performing American Childhood from Slavery to Civil Rights_ . NYU Press, New York (2011).
</li>
<li id="biesenbach2010">Biesenbach, K. _Marina Abramović: The Artist Is Present_ . The Museum of Modern Art, New York (2010).
</li>
<li id="brennan2016">Brennan, S. A. “Public, First,” in Gold and Klein (2016), pp. 384–89.
</li>
<li id="bonsignore2016a">Bonsignore, E., Hansen, D., Kraus, K., Visconti, A. and Fraistat, A. _Roles People Play: Key Roles Designed to Promote Participation and Learning in Alternate Reality Games,_ in _Proceedings of the 2016 Annual Symposium on Computer-Human Interaction in Play_ , October 16, 2016, pp. 78–90.
</li>
<li id="bonsignore2016b">Bonsignore, E., Hansen, D., Pellicone, A., Ahn, J., Kraus, K., Shumway, S., Kaczmarek, K., Parkin, J., Cardon, J., Sheets, J., Holl-Jensen, C. and Koepfler, J. “Traversing Transmedia Together: Co-designing an Educational Alternate Reality Game for Teens, with Teens,” in _Proceedings of the 15th International Conference on Interaction Design and Children_ , June 21, 2016, pp. 11–24.
</li>
<li id="burdick2015">Burdick, A. “Meta! Meta! Meta! A Speculative Design Brief for the Digital Humanities,”  _Visible Language_ , 49.3 (2015): 12–33.
</li>
<li id="burdick2012">Burdick, A., Drucker, J., Lunenfeld, P., Pressner, T. and Schnapp, J. _Digital Humanities_ . MIT Press, Cambridge, MA (2012).
</li>
<li id="caballero2017">Caballero, K. and Ekeberg, F. _Birding the Future: Lab Series_ (2017).
</li>
<li id="chun2016">Chun, W. H. K., Grusin, R., Jagoda, P. and Raley, R. “The Dark Side of the Digital Humanities,” in Gold and Klein (2016), pp. 493–509.
</li>
<li id="clement2016">Clement, T. E. “The Ground Truth of DH Text Mining,” in Klein and Gold (2016), pp. 534–35.
</li>
<li id="cordell2016">Cordell, R. “How Not to Teach Digital Humanities,” in Klein and Gold (2016), pp. 459–74.
</li>
<li id="haraway2017"> _Donna Haraway: Storytelling for Earthly Survival_ [film]. Dir. F. Terranova. DVD (2017).
</li>
<li id="dunne2013">Dunne, A. and Raby, F. _Speculative Everything: Design, Fiction, and Social Dreaming_ MIT Press, Cambridge, MA and London (2013).
</li>
<li id="fiormonte2016">Fiormonte, D. “Toward a Cultural Critique of Digital Humanities,” in Klein and Gold (2016), pp. 438–58.
</li>
<li id="freire1993">Freire, P. _Pedagogy of the Oppressed_ . New York: Continuum (1993).
</li>
<li id="fusco2003">Fusco, C. and Dominguez, R. “On-Line Simulations / Real-Life Politics: A Discussion with Ricardo Dominguez on Staging Virtual Theatre.”  _TDR_ , 47.2 (Summer 2003): 151–62.
</li>
<li id="gardner2017">Gardner, L. “Can Design Thinking Redesign Higher Ed?,”  _Chronicle of Higher Education_ , 64 (September 10, 2017).
</li>
<li id="galey2010">Galey, A. and Ruecker, S. “How a Prototype Argues.”  _Literary and Linguistic Computing_ , 25.4 (2010): 405–24.
</li>
<li id="gil2015">Gil, A. “The User, the Learner and the Machines We Make” (2015),<a href="http://go-dh.github.io/mincomp/thoughts/2015/05/21/user-vs-learner/">http://go-dh.github.io/mincomp/thoughts/2015/05/21/user-vs-learner/</a>.
</li>
<li id="giroux1988">Giroux, H. _Teachers as Intellectuals: Toward a Critical Pedagogy of Learning_ Praeger (1988).
</li>
<li id="gold2012">Gold, M. K. (ed). _Debates in the Digital Humanities 2012_ . University of Minnesota Press, Minneapolis (2012).
</li>
<li id="gold2016">Gold, M. K. and Klein, L. F. (eds). _Debates in the Digital Humanities 2016_ . University of Minnesota Press, Minneapolis (2016).
</li>
<li id="gold2019">Gold, M. K. and Klein, L. F. _Debates in the Digital Humanities 2019_ . University of Minnesota Press, Minneapolis (2019).
</li>
<li id="haraway2008">Haraway, D. “Otherworldly Conversation, Terran Topics, Local Terms.” In S. Alaimo and S. J. Hekman (eds), _Material Feminisms, Indiana University Press_ , Bloomington (2008), pp. 157–188.
</li>
<li id="haraway2016">Haraway, D. _Staying with the Trouble: Making Kin in the Chthulucene_ . Durham, NC, Duke University Press (2016).
</li>
<li id="hayles2012">Hayles, N. K. _How We Think: Digital Media and Contemporary Technogenesis_ . University of Chicago Press, Chicago (2012).
</li>
<li id="hayles2013">Hayles, N. K. and Pressman, J. (eds). _Comparative Textual Media: Transforming the Humanities in the Postprint Era_ . University of Minnesota Press, Minneapolis (2013).
</li>
<li id="heise2002">Heise, U. K. “Unnatural Ecologies: The Metaphor of the Environment in Media Theory,”  _Configurations_ , 10 (2002): 149–68.<a href="http://muse.jhu.edu/journals/con/summary/v010/10.1heise.html">http://muse.jhu.edu/journals/con/summary/v010/10.1heise.html</a>.
</li>
<li id="heise2017">Heise, U., Christensen, J. and Niemann, M. (eds). _The Routledge Companion to the Environmental Humanities_ . Routledge, London (2017).
</li>
<li id="helden2017">Heldén, J. and Jonson, H. _Encyclopedia_ . Exhibited at Kunsthall Trondheim (2017).
</li>
<li id="holmes2018">Holmes, B. _Affectivist Manifesto: Artistic Critique in the 21st Century_ (2008),<a href="https://brianholmes.wordpress.com/2008/11/16/the-affectivist-manifesto/">https://brianholmes.wordpress.com/2008/11/16/the-affectivist-manifesto/</a>(accessed December 10, 2018).
</li>
<li id="howard2014">Howard, Z. and Somerville, M. M. “A Comparative Study of Two Design Charrettes: Implications for Codesign and Participatory Action Research,”  _CoDesign_ , 10 (2014): 46–62.
</li>
<li id="hunter2019">Hunter J. “The Digital Humanities and Critical Theory : An Institutional Cautionary Tale,” in Gold and Klein (2019), pp. 188-194.
</li>
<li id="kirksey2014">Kirksey, E. _The Multispecies Salon_ . Duke University Press, Durham, NC (2014).
</li>
<li id="kordjakpiotrowska2013">Kordjak-Piotrowska, J. (ed). _Antonisz: Technology for Me Is a Form of Art_ . Zacheta National Gallery, Warsaw, Krakow (2013).
</li>
<li id="kraus2019">Kraus, K. 2019. “The Care of Enchanted Things,” in Gold and Klein (2019), pp. –.
</li>
<li id="laboratory2019"> _Laboratory for Aesthetics and Ecology_ (2019).<a href="http://www.labae.org/">http://www.labae.org/</a>(accessed July 20, 2019).
</li>
<li id="lee2017">Lee, R. _Symbiotic Sound_ [installation]. Kunsthall Trondheim (2017).
</li>
<li id="linley2016">Linley, M. “Ecological Entanglement of DH,” in Gold and Klein (2016), pp. 410–37.
</li>
<li id="liu2012">Liu, A. “Where Is Cultural Criticism in the Digital Humanities?,” in _Gold and Klein_ (2012), pp. 490–509.
</li>
<li id="lowe2015">Lowe, L. _The Intimacies of Four Continents_ . Duke University Press, Durham, NC (2015).
</li>
<li id="manning2007">Manning, E. _Politics of Touch: Sense, Movement, Sovereignty_ . University of Minnesota Press, Minneapolis (2007).
</li>
<li id="manovich2001">Manovich, L. _The Language of New Media_ T. MIT Press, Boston (2001).
</li>
<li id="mauroflude2014">Mauro-Flude, N. “Occult Computing for Artists: An Introduction.”  _UnMagazine_ , 8.2 (2014): 1–12.
</li>
<li id="mauroflude2016">Mauro-Flude, N. “Divination: A Romantic Mutiny in a Maelstrom of Data [Performance-Lecture]” (2016).<a href="https://livestream.com/internetsociety/radnetworks/videos/141149701">https://livestream.com/internetsociety/radnetworks/videos/141149701</a>(accessed December 20, 2018).
</li>
<li id="mauroflude2017">Mauro-Flude, N. “Methodologies of Risk and Experimental Prototyping,” in C. C. Baker and K. Sicchio (eds), _Intersecting Art and Technology in Practice: Techne/Technique/Technology_ , Routledge, London (2017), pp. 169–81.
</li>
<li id="mauroflude2019">Mauro-Flude, N. “Performing the Internet: Post Internet Folklore,” in S. R. Wong, H. Li and M. Chou (eds), _Digital Humanities and Scholarly Research Trends in the Asia-Pacific_ (2019), IGI Global, Hershey, PA, pp. 200–27.
</li>
<li id="mcdonough2010">McDonough, J. P., Olendorf, R., Kirschenbaum, M., Kraus, K., Reside, D., Donahue, R., Phelps, A., Egert, C., Lowood, H. and Rojo, S. _Preserving Virtual Worlds Final Report_ . University of Illinois at Urbana-Champaign, Maryland Institute for Technology in the Humanities, Rochester Institute of Technology, Stanford University Libraries, and Library of Congress National Digital Information Infrastructure for Preservation Program. (2010).
</li>
<li id="mcpherson2012">McPherson, T. “Why Are the Digital Humanities So White? or Thinking the Histories of Race and Computation,” in Gold and Klein (2012),<a href="http://dhdebates.gc.cuny.edu/debates/text/29">http://dhdebates.gc.cuny.edu/debates/text/29</a>(accessed March 22, 2016).
</li>
<li id="mittell2019">Mittell, J. “Videographic Criticism as a Digital Humanities Method,” in Gold and Klein (2019), pp. 224–42.
</li>
<li id="morrison2015">Morrison, A, _WATCHA_ (2015).<a href="https://designresearch.no/news/watcha">https://designresearch.no/news/watcha</a>(accessed April 2, 2021).
</li>
<li id="musiol2018">Musiol, H. “Metaphors of Decryption: Designs, Poetics, Collaborations,” in R. S. Restrepo (ed), _Decrypting Power_ , Rowman and Littlefield, London (2018), pp. 157–78.
</li>
<li id="musiol2020">Musiol, H. “Toxicity, Speculation, and Rights: Political Imagination in Mixmedia, Literary, and Cinematic Futurescapes,” in S. Pinto and A. Schultheis Moore (eds), _Writing Beyond the State: Post-Sovereign Approaches to Human Rights and Literary Studies_ , Palgrave, London (2020), pp. 330–57.
</li>
<li id="musiol2021">Musiol, H. “ Spaces for Other Ways to Learn : Postcolonial Environmental Fiction, Media, and Pedagogy in the North of the Global North.” Special Issue, _MLA Teaching Postcolonial Environmental Literature and Media_ , C. Iheka (ed) (forthcoming 2021).
</li>
<li id="anewwe2017"> _A New We / Ett Nytt Vi_ . Kunsthall Trondheim, Norway (2017), co-curated by Helena Holmberg, Carl Faurby, and Laboratory for Aesthetics and Ecology.<a href="https://kunsthalltrondheim.no/en/utstillinger/et-nytt-vi">https://kunsthalltrondheim.no/en/utstillinger/et-nytt-vi</a>(accessed September 14, 2020).
</li>
<li id="noble2019">Noble, S. U. “Toward a Critical Black Digital Humanities,” in Gold and Klein (2019), pp. 27–35.
</li>
<li id="nowviskie2015">Nowviskie, B. “Digital Humanities in the Anthropocene.”  _Digital Scholarship in the Humanities_ , 30 (2015): 4–15.
</li>
<li id="nowviskie2018">Nowviskie, B. “Reconstitute the World: Machine Reading Archives of Mass Extinction” (June 12, 2018).<a href="http://nowviskie.org/2018/reconstitute-the-world/">http://nowviskie.org/2018/reconstitute-the-world/</a>(accessed November 22, 2016).
</li>
<li id="nowviskie2019">Nowviskie, B. “Capacity through Care,” in Gold and Klein (2019), pp. 424–26.
</li>
<li id="posner2016">Posner, M. “What Is Next: The Radical Unrealized Potential of Digital Humanities,” in Gold and Klein (2016), pp. 32–41.
</li>
<li id="posner2019">Posner, M. “Sample | Signal | Strobe: Haunting, Social Media, and Black Digitality |,” in Gold and Klein (2019), pp. 101-122.
</li>
<li id="raley2009">Raley, R. _Tactical Media_ . University of Minnesota Press, Minneapolis.
</li>
<li id="rappaport2008">Rappaport, J. “Beyond Participant Observation: Collaborative Ethnography as Theoretical Innovation.”  _Collaborative Anthropologies_ , 1 (2008): 1–31.
</li>
<li id="risam2016">Risam, R. “Navigating the Global Digital Humanities: Insights from Black Feminism,” in Gold and Klein (2016), pp. 359–67.
</li>
<li id="risam2019">Risam, R. _New Digital Worlds: Postcolonial Digital Humanities in Theory, Praxis, and Pedagogy_ . Northwestern University Press, Evanston (2019).
</li>
<li id="risam2017">Risam, R., Snow, J. and Edwards, S. “Building an Ethical Digital Humanities Community: Librarian, Faculty, and Student Collaboration.”  _Library Faculty Publications_ , 1 (2017),<a href="https://digitalcommons.salemstate.edu/library_facpub">https://digitalcommons.salemstate.edu/library_facpub</a>(accessed September 14, 2020).
</li>
<li id="ruecker2015">Ruecker, S. “A Brief Taxonomy of Prototypes for the Digital Humanities.”  _Scholarly and Research Communication_ , 6 (2015): 1–11.
</li>
<li id="sack2019">Sack, W. _The Software Arts_ . MIT Press, Boston (2019).
</li>
<li id="sandlin2010">Sandlin, J. A., Schultz, B. D., Burdick, J. (eds). _Handbook of Public Pedagogy: Education and Learning beyond Schooling_ . Routledge, New York (2010).
</li>
<li id="simanowski2011">Simanowski, R. _Digital Art and Meaning: Reading Kinetic Poetry, Text Machines, Mapping Art, and Interactive Installations_ . Electronic Mediations 35. University of Minnesota Press, Minneapolis (2011).
</li>
<li id="smith2012">Smith, N. D. “Design Charrette: A Vehicle for Consultation or Collaboration?” Paper presented at Participatory Innovation Conference (PIN-C), Melbourne, January 12–14, 2012.
</li>
<li id="stoler2010">Stoler, A. L. Along the Archival Grain: Epistemic Anxieties and Colonial Common Sense. Princeton University Press, Princeton, NJ (2010).
</li>
<li id="swanson2017">Swanson, H. A., Tsing, A. L., Bubandt, N. O. and Gan, E. “Introduction: Bodies Tumbled into Bodies.” In A. L. Tsing, H. A. Swanson, E. Gan and N. Bubandt (eds), _Arts of Living on a Damaged Planet: Ghosts and Monsters of the Anthropocene_ , University of Minnesota Press, Minneapolis (2017), pp. M1–M14.
</li>
<li id="thurston2013">Thurston, N. _Of the Subcontract: Or Principles of Poetic Right_ . Information as Material, London (2013).
</li>
<li id="tsing2012">Tsing, A. L. “On Nonscalability: The Living World Is Not Amenable to Precision-Nested Scales,”  _Common Knowledge_ , 18 (2012): 505–24.
</li>
<li id="tsing2017">Tsing, A. L., Swanson, H. A., Gan, E. and Bubandt, N. (eds). _Arts of Living on a Damaged Planet: Ghosts and Monsters of the Anthropocene_ . University of Minnesota Press, Minneapolis (2017).
</li>
<li id="vaidhyanathan2018">Vaidhyanathan, S. “Techno-Fundamentalism Can’t Save You, Mark Zuckerberg,”  _New Yorker_ , April 21, 2018,<a href="https://www.newyorker.com/tech/elements/techno-fundamentalism-cant-save-you-mark-zuckerberg">https://www.newyorker.com/tech/elements/techno-fundamentalism-cant-save-you-mark-zuckerberg</a>(accessed May 2, 2018).
</li>
<li id="wachter2017">Wachter-Boettcher, S. _Technically Wrong: Sexist Apps, Biased Algorithms, and Other Threats of Toxic Tech_ . Norton, New York (2017).
</li>
<li id="ward2015">Ward, M. “Rapid Prototyping Politics: Design and the De-material Turn.” In W. Jonas, S. Zerwas and K. von Anshelm (eds), _Transformation Design: Perspectives on a New Design Attitude_ , Birkhäuser, Basel, (2015), pp. 227–45.
</li>
<li id="whyte2016">Whyte, K. P. “Our Ancestors’ Dystopia Now: Indigenous Conservation and the Anthropocene,” in U. K. Heise, J. Christensen and M. Niemann (eds), _The Routledge Companion to the Environmental Humanities_ , Routledge, New York (2016), pp. 206–15.
</li>
<li id="underwood2019">Underwood, T. “Digital Humanities as a Semi- Normal Thing,” in Gold and Klein (2019), pp. 96-98.
</li>
<li id="vinsel2018">Vinsel, L. “Design Thinking Is a Boondoggle. Its Adherents Think It Will Save Higher Ed. They’re Delusional,”  _Chronicle of Higher Education_ , May 21, 2018.
</li>
<li id="zunger2018">Zunger, Y. “Computer Science Faces an Ethics Crisis. The Cambridge Analytica Scandal Proves It,”  _Boston Globe_ , March 22, 2018,<a href="https://www.bostonglobe.com/ideas/2018/03/22/computer-science-faces-ethics-crisis-the-cambridge-analytica-scandal-proves/IzaXxl2BsYBtwM4nxezgcP/story.html">https://www.bostonglobe.com/ideas/2018/03/22/computer-science-faces-ethics-crisis-the-cambridge-analytica-scandal-proves/IzaXxl2BsYBtwM4nxezgcP/story.html</a>(accessed June 22, 2018).
</li>
<li id="zylinska2020">Zylinska, J. _AI Art: Machine Visions and Warped Dreams_ . Open Humanities Press (2020).
</li>
</ul>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Hereafter cited as <em>A New We</em> .&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Ackerman argues that the threat-linked interaction was not intended as a focus on the work (personal communication, May 15, 2018). A plant’s protective response is nonetheless, at least partially, wired into its biophysical reactions (D. Richert, personal communication, June 20, 2018).&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Ackerman (2014) observed in her previous work with various plants and poetry that plants’ poetic responses are not dictated simply by biological determinism, as “each plant seemed to have” exhibited poetic preferences and subjectivity, “its own syntactical signature and . . . emphasis on certain cadences, words, or phrase recursions.”&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Ackerman deemphasizes the fear factor, yet it is nonetheless part of the plant’s somatic response.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>See Risam et al. (<a href="#risam2017">2017</a>).&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>This is a play on the title of the 2010 MoMA retrospective of the work by the performance art pioneer Marina Abramović<a class="footnote-ref" href="#biesenbach2010"> [biesenbach2010] </a>.&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>Linley (<a href="#linley2016">2016</a>) observes that digital realms are imagined and built as environments, natural landscapes, populated with viruses, worms, and other things being born, albeit digitally, replicating a particularly “cultural” understanding of nature and culture.&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>See Alaimo (<a href="#alaimo2010">2010</a>), Haraway (<a href="#haraway2008">2008</a>,<a href="#haraway2016">2016</a>), Mauro-Flude (<a href="#mauroflude2014">2014</a>,<a href="#mauroflude2016">2016</a>,<a href="#mauroflude2019">2019</a>), and Nowviskie (<a href="#nowviskie2019">2019</a>). Feminists and scholars of critical race, postcolonial, or disability studies have long exposed the effects of misogynistic, racist, classist, colonial, and ableist bias baked into the theoretical, methodological, pedagogical, and institutional infrastructure and archives<a class="footnote-ref" href="#fiormonte2016"> [fiormonte2016] </a><a class="footnote-ref" href="#noble2019"> [noble2019] </a><a class="footnote-ref" href="#posner2016"> [posner2016] </a><a class="footnote-ref" href="#posner2019"> [posner2019] </a><a class="footnote-ref" href="#wachter2017"> [wachter2017] </a>. Roopika Risam (<a href="#risam2016">2016</a>,<a href="#risam2019">2019</a>) documents the telling concentration of DH centers in former colonial metropoles. Some of the most obvious discriminatory aspects of DH projects and digital environments, their inaccessibility for users with the most common physical impairments, are pervasive. At a November 2019 NTNU DH event, <em>none</em> of the tested public university websites were designed with basic digital accessibility in mind. Other well-documented cases of racial, gender/sexuality-, linguistic-, and class-based exclusion derive from this geopolitical distribution of, and limited access to, DH laboratories, software, expertise, and funding<a class="footnote-ref" href="#fiormonte2016"> [fiormonte2016] </a>. The reasons for such a retreat from concerns about equity and accessibility in digital design, especially in Norway, are hard to ascribe. They may be tied to short-term funding models, or to lack of scrutiny of DH projects’ inaccessible architecture and research outcomes, but the absence of mandatory training in critical theory, media literacy, and digital accessibility across disciplines is also a contributing factor.&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>The impact of environmental digital art and EH is particularly important here, not simply thematically but methodically. EH foregrounds the need to engage with nontextual archives, local knowledge, and attends to diverse, often nonlogocentric forms and “genres of observation and storytelling” from across disciplines<a class="footnote-ref" href="#armiero2017"> [armiero2017] </a><a class="footnote-ref" href="#tsing2017"> [tsing2017] </a><a class="footnote-ref" href="#linley2016"> [linley2016] </a><a class="footnote-ref" href="#nowviskie2015"> [nowviskie2015] </a><a class="footnote-ref" href="#tsing2012"> [tsing2012] </a><a class="footnote-ref" href="#whyte2016"> [whyte2016] </a>.&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>This concern and “arguments against algorithmic visualization and analysis,” Nowviskie (<a href="#nowviskie2019">2019</a>) argues, “are not . . . fueled by nostalgic scholarly conservatism, but rather emerge across the political spectrum.”&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>Again, critical media studies can offer useful models here. See Fusco and Dominguez (<a href="#fusco2003">2003</a>), Mauro-Flude (<a href="#mauroflude2016">2016</a>,<a href="#mauroflude2019">2019</a>), and Mittell (<a href="#mittell2019">2019</a>).&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p>See the largest funding schemes at the Norwegian Research Council (NFR) at<a href="https://www.forskningsradet.no/en/call-for-proposals/">https://www.forskningsradet.no/en/call-for-proposals/</a>. Art projects are funded by a separate agency, Arts Council Norway, at<a href="https://www.kulturradet.no/english">https://www.kulturradet.no/english</a>.&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13">
<p>2019 is a watershed moment for EH in Norway, marking the official establishment of a four-year nation-wide Environmental Humanities Doctoral consortium program, called NoRS-EH. See<a href="https://prosjektbanken.forskningsradet.no/en/project/FORISS/299199?Kilde=FORISS&amp;distribution=Ar&amp;chart=bar&amp;calcType=funding&amp;Sprak=no&amp;sortBy=date&amp;sortOrder=desc&amp;resultCount=30&amp;offset=30&amp;Fag.3=Marin+teknologi">https://prosjektbanken.forskningsradet.no/en/project/FORISS/299199?Kilde=FORISS&amp;distribution=Ar&amp;chart=bar&amp;calcType=funding&amp;Sprak=no&amp;sortBy=date&amp;sortOrder=desc&amp;resultCount=30&amp;offset=30&amp;Fag.3=Marin+teknologi</a>.&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14">
<p>Its 2019 budget is 9.6 billion NOK. See<a href="https://www.ntnu.edu/facts">https://www.ntnu.edu/facts</a>.&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:15">
<p>The particular type of humanities collaboration in Norway with computer science in DH training and project maintenance is also reflection-worthy. Those who teach Python or GIS to humanists (mainly linguists and literary scholars) do not share the same commitment to or interest in cultural critique on which electronic arts, critical internet studies, or critical design practice are built, and their role in collaborative projects is often reduced to out- or insourcing of computational skills on a contingent, nonreciprocal basis. Moreover, at NTNU, purchases of hardware and software are always prioritized over investment in transdisciplinary critical <em>humanware</em> . Thus, <em>the way</em> in which powerful digital tools and platforms arrive in the humanities means also the stealth arrival of a particular form of collaborative practice, and promoting faith in the transparent and innately positive agency of technology. This, in turn, transforms the humanities toolbox but also displaces its few ethical methods practices in place that were laboriously fought for by artists, humanists, theorists, and designers since the middle of the previous century.&#160;<a href="#fnref:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:16">
<p>Among these are Kunsthall Trondheim (a contemporary art gallery/center), Rockheim (Norway’s national immersive museum of contemporary popular music), Trondheim Kunstmuseum (a fine-arts museum), Trondhjems Kunstforening (a contemporary art association and gallery in Trondheim), and Trøndelag Senter for Samtidskunst (the regional Center for Contemporary Art).&#160;<a href="#fnref:16" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:17">
<p>See the full LEHALDP syllabus at<a href="https://www.asle.org/teaching_resources/environmental-humanities-transmedia-syllabus/">https://www.asle.org/teaching_resources/environmental-humanities-transmedia-syllabus/</a>. The TALS syllabus has yet to be included in the full text repository.&#160;<a href="#fnref:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:18">
<p>For a detailed discussion of the class and the administrative challenges to inclusion work at NTNU, see Musiol (<a href="#musiol2021">forthcoming 2021</a>).&#160;<a href="#fnref:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:19">
<p>Kunsthalls (art halls) are contemporary art institutions in Europe with an explicit community-engagement mission.&#160;<a href="#fnref:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:20">
<p>Access barriers, especially for students with physical impairments, are always there. However, the fact that we could invite nondegree, unenrolled migrant participants to the initiative and reward them with an official NTNU certificate of attendance without fees or legal and bureaucratic barriers was a major achievement.&#160;<a href="#fnref:20" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:21">
<p>Limits placed on who can participate in digital environments at an otherwise very <em>public</em> public university confirms what DH scholars and digital rights activists have been saying for a long time: new technology distributions and copyright law reinscribe, not fix, social inequality.&#160;<a href="#fnref:21" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:22">
<p>Among the influential inspirations are a Futurescapes symposium in new media, technology, and the humanities, with exhibits and artist-led lecture sessions, including Kari Kraus’s ARG participatory design workshops and her digital enchantment and decay work (<a href="#kraus2019">2019</a>) and Nancy Mauro-Flude’s spectral-digital performance; and a subsequent Technology &amp; Emotions conference co-organized by NTNU ARTEC with the Oslo-based Polyteknisk Forening (a national engineering association) and an i/o lab bioart curatorial collective from Stavanger.&#160;<a href="#fnref:22" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:23">
<p>Note that as of 2018, some creative practices are now officially recognized as “artistic research,” a form of knowledge-making and an academic field in Norway.&#160;<a href="#fnref:23" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:24">
<p>For a discussion of new, transmodal tools and (as) postcolonial theory method in this and other NTNU initiatives, see Musiol (<a href="#musiol2020">2020</a>,<a href="#musiol2021">forthcoming 2021</a>).&#160;<a href="#fnref:24" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:25">
<p>Moreover, our participants represented diverse fields and came from different administrative levels and functions in the university, cultural heritages, and urban ecosystems: we worked with students, senior scholars, junior and veteran artists, curators, DH guest-speakers, university-unaffiliated migrants, and high-powered administrators of academic or cultural institutions.&#160;<a href="#fnref:25" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:26">
<p>All references to participants’ public writing / blog work are cited in text only and used with permission.&#160;<a href="#fnref:26" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:27">
<p>Ursula Le Guin, and other speculative fiction writers, is credited not only as a novelist but as an important environmental humanities thinker in Tsing et al. (<a href="#tsing2017">2017</a>) and in the documentary <em>Donna Haraway: Storytelling for Earth’s Survival</em> (2017).&#160;<a href="#fnref:27" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:28">
<p>Much radical recovery work aims to resist the destruction or concealment of cultural productions by colonial subjects, women, LGBTQI+ persons, or persons of color. See Bernstein (<a href="#bernstein2011">2011</a>), Lowe (<a href="#lower2015">2015</a>), Stoler (<a href="#stoler2010">2010</a>), and Jim Hubbard and Sarah Schulman in the ACT UP Oral History Project, for instance. This class acknowledged their work, but it engaged specifically with the absence of the nonhuman “voice” in cultural heritage archives.&#160;<a href="#fnref:28" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:29">
<p>For a discussion of another nonhuman species extinction archival installation by Heldén and Jonson (2017), see Musiol (<a href="#musiol2020">2020, 269–72</a>).&#160;<a href="#fnref:29" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:30">
<p>Each card displayed images of different bird species, and ornithological and cultural information about them — including poetry and other cultural lore associated with different migratory birds in the different regions through which they migrate.&#160;<a href="#fnref:30" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:31">
<p>Many digital archives are participatory and crowdsourced. However, here the participation, while collective, was also corporeal, intimate, and individual first.&#160;<a href="#fnref:31" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:32">
<p>Of course, our Norwegian students had different understandings of the objectivity of archives than did Indigenous Sámis or (im)migrant participants of our classes.&#160;<a href="#fnref:32" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:33">
<p>Henry Mainsah contributed to the drafting of this section and led the workshops described herein.&#160;<a href="#fnref:33" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:34">
<p>All commented on course blog on November 9, 2017.&#160;<a href="#fnref:34" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:35">
<p>Prototyping Environmental Humanities Brief handout.&#160;<a href="#fnref:35" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:36">
<p>Prototyping Environmental Humanities Brief handout.&#160;<a href="#fnref:36" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:37">
<p>Prototyping Digital Humanities Brief handout.&#160;<a href="#fnref:37" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:38">
<p>All were included in an anonymous DEH Discussion + Workshop Reflection assessment conducted between October 30 and 31, 2017; results are publicly available at NTNU.## Bibliography&#160;<a href="#fnref:38" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry><entry><title type="html">Hands-On Reading: An Experiment in Slow Digital Reading</title><link href="https://startwords.cdh.princeton.edu/vol/15/2/000558/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://startwords.cdh.princeton.edu/vol/15/2/000558/</id><author><name>Aditi Nafde</name></author><author><name>Matt Coneys Wainwright</name></author><author><name>Kate Court</name></author><author><name>Fiona Galston</name></author><author><name>James Cummings</name></author><author><name>Tiago Sousa Garcia</name></author><published>2021-07-17T00:00:00+00:00</published><updated>2023-08-04T13:01:02-04:00</updated><content type="html"><![CDATA[<h2 id="introduction-background-and-concept">Introduction: Background and Concept</h2>
<p>The initial concept of a digital reading and handwriting app stemmed from an interest in exploring the endurance and adaptability of handwriting in response to technological change. Before the invention of the printing press, writing a text by hand was the dominant means of producing a book. The scribes employed to undertake this task copied out texts slowly and carefully and, as a result, were usually their first readers. This task gave them an intimate and detailed understanding of the text. The invention of print changed the craft of book production. The mechanical processes of the press meant that printed books did not look like handwritten manuscripts although, in a close parallel to e-readers with page-flip features, printers tried to copy the handwritten book format as closely as they could. Nonetheless, scribes were aware of the differences between manuscripts and print, caused by the mechanics of the press, as were the patrons who commissioned manuscript copies, and scribes swiftly began to incorporate features of print into their manuscripts<a class="footnote-ref" href="#nafde2020"> [nafde2020] </a>. By the late sixteenth century, scribes were producing manuscripts that could have been mistaken for printed books. This change in scribal practice disrupts the linear narrative of supersession that often accompanies the invention of a new technology: that the new technology at first supplements and then eventually supplants the older technology (the German abbot, Johannes Trithemius, for example, wrote of his fear that print would eventually destroy manuscripts<a class="footnote-ref" href="#trithemius1494"> [trithemius1494] </a>). But this linear narrative does not take into account that the older technology also adapts and changes in response to the new. This is particularly the case with handwriting, which has adapted through several technological changes. The Hands-On Reading app leverages handwriting’s adaptability to explore the relationship between digital reading and digital writing.</p>
<p>The digital revolution has prompted a renewed interest in handwriting and its place in a multimedia world. In addition to the popular resurgence of calligraphy, there has been substantial experimentation in digital handwriting tools: from the integration of styluses with existing touchscreen technologies to the development of dedicated hardware and software solutions (e.g. Remarkable, Kaligo, Moleskine Smart Writing, the handwriting to text conversion tools built into some mobile phones and tablets). Unlike the majority of these commercial products, however, the development of Hands-On Reading was informed by a familiarity with medieval manuscript culture and, in particular, its blurring of the boundaries between scribe and reader. This allows us to better explore the relationship between writing and reading. </p>
<p>The project was also informed by an awareness of different modes of reading and the possibilities they offer. Since the late 1990s, a growing body of research has explored the varying ways in which readers approach printed and digital texts, as well as the broader impact of digital culture on the evolution of reading practices. For all their advantages, it is generally acknowledged that digital texts tend to encourage swift reading at the expense of contemplative analysis. Recent research has found that university students who access texts in digital form are less likely to take detailed notes or employ other study strategies<a class="footnote-ref" href="#schugar2011"> [schugar2011] </a>; they excel in identifying and recalling the information needed to answer concrete questions, yet they are outperformed by peers who read in print when it comes to abstract questions that require inferential reasoning<a class="footnote-ref" href="#kaufman2016"> [kaufman2016] </a>. These findings have prompted a renewed interest in the value ofactive reading,whereby readers engage in critical or evaluative thinking as they read a text, often simultaneously taking notes – a practice familiar to any medievalist who has encountered heavily annotated manuscripts or early printed books. Yet such scholars will also be familiar with another form of reading prevalent in the Middle Ages: the slow reading of scribes, who copied texts word by word from an exemplar, and were therefore constrained to read them at a measured and regular pace, with all the time this task afforded for unhurried reflection. We wanted to use the app to explore the potential value of this process of slow reading within a digital context, and to assess how it influenced readers’ perceptions of and responses to the digital text.</p>
<p>Development of the app brought together a team with significantly varied expertise. The project’s PI and RA shared research interests in medieval reading and handwriting practices, familiar with DH research tools though primarily from an end-user perspective. The involvement of researchers affiliated with Animating Text Newcastle University (ATNU), a collaboration between the Faculty of Humanities and Social Sciences and Newcastle University’s Digital Institute, brought specific expertise in the application of DH techniques to textual editing. Engagement with the broader public and creative practitioners such as professional calligraphers was central to the project, allowing investigation of contemporary trends in reading and writing and perceptions of how reading and writing practices might change in the future. The day-to-day development of the app was accomplished by research software engineers with extensive experience of both commercial and academic software development. This combination of perspectives was crucial to the conception of Hands-On Reading.</p>
<h2 id="initial-design-and-related-work">Initial Design and Related Work</h2>
<p>Digital reading has grown vastly with the introduction of platforms such as Google Books that are digitising material at such dizzying rates that “it would take a human an estimated twenty thousand years to read such a vast collection at the reasonable pace of two hundred words per minute, without interruptions for food or sleep” (<a href="#aurnhammer2019">Aurnhammer et al. (2019)</a>, referencing<a href="#aiden2013">Aiden and Michel (2013)</a>, refer to Google who have scanned “over 30 million books” ). One of the consequences of this growth is the tendency towards computer-aided distant reading whereby broad patterns are identified across large volumes of digitised texts by machine<a class="footnote-ref" href="#aurnhammer2019"> [aurnhammer2019] </a>. Our interest, on the other hand, is in exploring the possibilities offered by digital texts for critical or evaluative close reading. To better understand how users interact with digital texts and digital writing tools, we held a symposium to which we invited academics, creative practitioners (mostly professional calligraphers but also artists and designers), and digital specialists, i.e. groups of practitioners that read and write differently. Conversations with this range of users allowed us to draw more concrete conclusions about the kind of experience we hoped to provide through the app, as well as its potential value from a research perspective. One of the key outcomes of these initial discussions was the notion of slowing down digital reading to allow time for critical thinking and reflection. This notion is particularly timely given the rise in bite-sized information on social media platforms that results in increasingly fast-paced reading. One of the ways we wished to explore slowing down digital reading in designing the app was to connect reading to writing. While other DH studies explore the possibilities offered by digital texts for active reading – annotating while reading – we are interested in the possibilities offered by digital texts for using writing <em>as a means of</em> reading – reading as modelled by medieval scribes.</p>
<p>In order to encourage slow reading, we had to carefully consider the writing surface, the writing space, and the choice of pen tool in designing our app. A number of DH projects have attempted to expand the functionality of annotation tools, bringing together the applications of the pen with the possibilities offered by digital writing. One of the earliest examinations of computer-based active reading was Schilit et al.’s XLibris “active reading machine” <a class="footnote-ref" href="#schilit1998"> [schilit1998] </a>. XLibris used a “commercial high-resolution pen tablet display” that mimicked the “key affordances of paper” to allow readers to mark up a digital text by hand, thereby encouraging active reading. Various projects have attempted to develop this machine-based active reading further: PapierCraft experimented with hybrid machine-paper format to bring together the best of both paper and machine<a class="footnote-ref" href="#liao2008"> [liao2008] </a>; GatherReader sought to maintain the flow of reading while offering a range of solutions for annotation<a class="footnote-ref" href="#hinckley2012"> [hinckley2012] </a>. More recently, dedicated updates to e-reader devices explore the possibilities of active reading by replicating the look and affordances of paper (such as page flipping) while also offering additional features such as adjustable text size, quick definitions or translations, and the ability to easily share text; but annotation is often limited to underlining, highlighting, or typing notes. Commercially available e-paper tablets – such as the Remarkable and Sony’s DPT-RP1 – have allowed users to annotate and edit digital text by hand, with specialist screen and stylus technologies seeking to mimic the feel and tangible nuance of physical writing. While such developments are promising, they remain in their infancy and are targeted at specific business and creative audiences rather than at readers. Studies that examine machine-aided active reading have found there to be a significant difference between reading digitally and reading on paper which limits the reader’s ability to read actively<a class="footnote-ref" href="#sellen1997"> [sellen1997] </a><a class="footnote-ref" href="#adler1998"> [adler1998] </a><a class="footnote-ref" href="#ohara1997"> [ohara1997] </a>(these are conveniently summarised by<a href="#tashman2011">Tashman and Edwards (2011)</a>).</p>
<p>We were therefore aware that in creating an app to encourage slow reading the type of pen tool used would affect both reading and writing practices. Early research into digital writing undertaken by Microsoft found that “there is a rich set of deeply rooted behaviors that people exhibit when working with pen, paper, clippings, pages, and books” <a class="footnote-ref" href="#hinckley2010"> [hinckley2010] </a>, a concept which rang true in our initial discussions with creative practitioners. In exploring the use of digital tools to mimic physical writing, Hinckley et al. separatedpenfromtouch,finding that “the pen writes, touch manipulates, and the combination of pen + touch yields new tools.” Relatedly, Weibel et al. experimented with what they call “paper-digital tools” by using Livescribe digital pens to understand the practice of note taking and data collection practices, finding that the use of digital pens caused their study participants to modify their note taking practices<a class="footnote-ref" href="#weibel2012"> [weibel2012] </a>. This influenced the choice of pen tool for our app. Users should not need a specialist stylus but should be able to use any pen tool with which they are familiar in order to avoid introducing a new element to the study of the relationship between digital writing and digital reading. They should, however, have the option to select from various pen sizes and colours, with touch-screen input using either a finger or stylus, to allow them to colour code or size their writing as they wish. </p>
<p>We were also aware that writing surfaces affect the way the pen is used. Our early discussions with creative practitioners and potential end users brought to light the key differences between digital and physical surfaces. The calligraphers in particular noticed the lack of sensorial feedback when writing on a screen, a crucial difference between traditional and digital writing. Recent studies have made similar observations: that the digital writing surface has an immediate effect on the style of writing and, as a result, digital writing is “usually larger and sloppier” than its ink and paper counterpart<a class="footnote-ref" href="#agrawala2005"> [agrawala2005] </a>. Their solution was not to change the texture of the writing surface but to change the available writing space so DIZI (Digital Ink Zooming Interface) zooms in on the writing space to offset the inclination to increase letter size when writing digitally. Building on the idea of changing the available writing space, Yoon et al. experimented with creating extra space via their TextTearing interface, which allows the user to break up a page of text in order to annotate freely without interrupting its flow<a class="footnote-ref" href="#yoon2013"> [yoon2013] </a>. They concluded that their participants preferred TextTearing techniques to writing in “naturally occurring space[s]” on digital pages. But such dynamic pages that permit expanding margins and creating spaces between lines interrupts the natural flow of writing and disrupts reading, as Yoon et al. themselves observed.<a href="#weibel2012">Weibel et al. (2012)</a>conversely noted that paper allows spatial organisation of handwritten annotations in a way which digital tools do not. It was crucial for the development of the Hands-On Reading app therefore to strike a careful balance between such dynamism and the possible disruptions to the reading process it causes. Our primary objective was to facilitate interaction with text on the virtual page in a free and uninhibited way, offering an experience as close as possible to reading and writing by hand on paper.</p>




























<figure ><img loading="lazy" alt="" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Initial design of the application
        </p>
    </figcaption>
</figure>
<p>The medieval manuscript and early printed page are ideal models for the interaction between reading and writing. Where digital reading material such as the page-turning books available on e-readers or the articles published in e.g. <em>DHQ</em> provide little marginal space, medieval manuscripts and early printed books were often designed with large margins - and sometimes ruled with blank columns - for the very purpose of extensive annotation. To emulate the scribal process of slow reading, with pen in hand, the initial design of the app was based on a medieval manuscript page. The main page of the app displayed an image of a single vellum page that imitated a familiar non-digital writing surface. A blank vellum background was sourced from a fifteenth-century manuscript in Newcastle University Library Special Collections (Newcastle University Library, Mediaeval MS 4) onto which we provided only four lines of text alongside wide margins for annotations. To encourage interaction with the text, decorative elements such asdrop capinitials were limited to a simple outline that could be filled in. We used a blank decorativeHumanistic White Vine (Bianchi Girari)initial designed by Klaus-Peter Schäffel. The text was rendered semi-transparent so as to elicit tracing and, in order that the text was easily readable and yet calligraphic enough to suggest it was a template, we chose Schäffel’s 1480 Humanistica font. A modern translation of the prologue to Chaucer’s <em>Canterbury Tales</em> was selected for the text as it is unfamiliar enough to warrant annotation, yet not too unfamiliar in its language (being a modern translation) to be prohibitive or off-putting. </p>
<p>Explicit instructions were limited in order to avoid the risk of prescribing the nature of interactions, but users were given the opportunity to provide specific feedback on their experience of using the app. It was vital to find a way to gather data on user interactions that could be analysed as part of the broader Manuscripts after Print project, with the potential to pave the way for further refinement of the app through subsequent funding bids. The decision was made to ask users to register an account to facilitate data acquisition and to allow users to save their work. </p>
<h2 id="development">Development</h2>
<h2 id="phase-1">Phase 1</h2>
<p>Hands-On Reading runs as a web application.<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> It is optimised to be used with a tablet so that users can draw with a stylus, rather than the more awkward and less familiar experience of using their computer mouse. The development of a web application, rather than native phone application,<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> but with thefeelof a mobile app was identified as a practical goal within the timescale, and would allow compatibility with both iOS and Android. The technology stack included a client built using Angular 9 and a server built using Nest.js.<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup> Angular was chosen as an open-source front end framework: as it uses Typescript,<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup> it offers the security and ease of programming of a typed language. Nest.js is built on the popular Node.js server-side runtime environment, but also offers Typescript support, which made it easier for the developers to move between front and back-end programming. A benefit of using Node.js based frameworks is the volume of community support online and the wealth of open source libraries available.</p>




























<figure ><img loading="lazy" alt="" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Screenshot from Phase 1 of the development annotated with key features.
        </p>
    </figcaption>
</figure>
<p>The drawing functionality was provided by Fabric.js JavaScript framework, built upon HTML5 canvas.<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup> We chose Fabric.js as it is open-source and provides a variety of drawing and shape creation functionality, enabling us to build an interactive object model on top of a HTML canvas element. A reset function allowed users to remove all annotations on the canvas. Consistent with the historical use of different inks and sizes of script in manuscript rubrication and decoration, users could also change the stylus colour and width, with these options housed within a menu at the bottom of the screen (<a href="#figure02">Figure 2</a>). </p>
<p>A mongoDB document-based database was used to store feedback forms at this phase of the development.<sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup> mongoDB is schemaless and therefore could adapt to storing additional data in future versions, rather than requiring a more static and predetermined schema.<sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup> This was beneficial as we anticipated that the database content would evolve in phase 2 to store more varied data. </p>
<h2 id="testing">Testing</h2>
<p>Beta-testing of the app was conducted at a workshop in December 2019, twelve weeks after the initial symposium. Participants included a number of individuals who had previously collaborated with the Manuscripts after Print project, chiefly professional calligraphers and special collections librarians. The workshop was also attended by a number of academics, both with specialisms in medieval and early modern studies and from other disciplines. While participants were provided with iPads and styluses, they were also encouraged to access the app via their own devices, which included android tablets, phones and laptops, and to experiment both with and without annotation tools.</p>
<p>Following half an hour of testing, the floor was opened for general discussion. Participants were also invited to complete a feedback form, answering the following questions (which were specifically chosen to be as open-ended as possible):</p>
<p>Did you enjoy using the app?If you could enhance or improve any aspect of the app, what would it be?Did the app make you think differently about handwriting?Did the app change the way you read the text?Do you think the app would be of use to any specific user groups?What device did you use?What is your occupation?</p>
<p>While participants were broadly positive about the concept of the app, two main criticisms were raised:</p>
<p>A concern over the limited guidance offered to the user. Responses to question two included:the task is not clear;a little guidance&hellip;;no task instructions.While these comments were noted, they did not represent a major concern as one of the key objectives of the workshop was to see how users approached the app without instruction. Indeed, one participant noted, “[I’m] glad we didn’t get this information at the time.” Frustrations regarding functionality, especially the absence of zoom and undo features. Users who accessed the app on touchscreen devices mentioned that they instinctively expected compatibility with touchscreen gestures. The lack of a zoom function was noted particularly by phone users, while there was also discussion of whether an erase function was appropriate within an app that mirrored a manuscript page; several calligraphers suggested that a &ldquo;scrape to remove&rdquo; function would be more in keeping with the style.</p>
<p>Questions three and four revealed a notable disparity between testers of different professions. The responses of calligraphers generally indicated that the app had not encouraged them to think differently about handwriting or read the text in a different way; one admitted that this might be “because I’ve been thinking about [handwriting] for a while.” The experience of using the app with pen tools rather than a pen itself stopped calligraphers from reading the text carefully, with one commenting, “I began to read the words as I traced but because that experience was frustrating I stopped reading.” However, academics and librarians stated that using the app had made them consider the relationship between the reader and the text and had encouraged them to contemplate the text&rsquo;s material form. One wrote that “it made me think about handwriting and decoration, material and page design as being inseparable;” another wrote that “it made me think differently about the relationship between handwriting (as a reader) [and] text;”  “it made me focus more on how I read.”</p>
<p>Professional calligraphers generally did not consider the app to be a potential tool for teaching calligraphy, “unless it provided a starting point to lead someone to say I want to know more .” These responses confirmed the limitations of a generic touch screen/stylus combination for creating an experience akin to calligraphic writing with paper and pen; they also confirmed that the app could be used as a tool to encourage reading the digital text <em>by</em> writing, rather than a digital writing tool per se.</p>
<h2 id="phase-2">Phase 2</h2>
<p>Reflecting on these responses, it was clear that the functionality of the app limited the way the text was read. The largest barrier to reading appeared to be the gap between the expectations of users, the affordances of digital tools, and the difference between paper and digital interfaces. The app was therefore refined to address some of the issues raised by testers and to add new features that allow further reflection on the initial research questions.</p>
<p>The most pressing of these changes was the introduction of an erase button. We had initially rejected the addition of an eraser on the grounds that it would not reflect medieval annotation practices, opting instead to include a reset button which deleted all annotations. However, testers found this option frustrating and too distant from their experience of using digital tools. Part of these frustrations is that testers came to the digital interface with pre-set expectations of digital reading and writing, specifically around the features that are typically available to them on tablets. Therefore ascrape to erasefeature, which would remove a layer of annotation and slightly damage the vellum underneath it, in keeping with medieval annotation practices, would have likewise been incompatible with how digital devices are used today. While we were conscious that the app could not directly imitate the affordances of paper and pen, the digital eraser function is both closer to its physical counterpart and a common feature on digital annotation programmes and users are therefore likely to find it more intuitive than the other options. The eraser was not a pre-existing feature of Fabric.js and was implemented as an add-on, removing any drawing or imagery existing on the canvas wherever the userdraws. </p>
<p>At this stage we would also have liked to respond to feedback by introducing zooming, panning, and responsive sizing (automatic resizing of the app based on the user’s device). This is probably the most significant sacrifice necessitated by Fabric.js. To facilitate responsive resizing, zoom functionality, and associated panning across the screen, Fabric.js required the manuscript page to be set as a background image on the canvas. However, the implementation of the eraser meant that it removed all drawing on the canvas, not only the users&rsquo; additions, and therefore the manuscript image had to be added to the background HTML in a parent <code>&lt;div&gt;</code> rather than as part of the resizable canvas.<sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup> While this was a compromise, we were also conscious of the potential disruptions to the reading process created by digital features such as zoom, as described by<a href="#yoon2013">Yoon et al. (2013)</a>. Having observed our testers, we noted that the majority of them demonstrated familiarity with the features usually available on tablets, such as pinch to zoom gestures. Such paradigms are now entrenched enough that users expect their availability in all apps, finding their absence counterintuitive. Such gestures do not reflect the affordances of paper and it is these sorts of changing expectations that are driving the gap between physical and digital books. It will be key to a further redesign of the app to negotiate between user-expectations and the experience of focussed reading with a pen. </p>




























<figure ><img loading="lazy" alt="" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Colour picker introduced in Phase 1 and finalised in Phase 2.
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Left hand page
        </p>
    </figcaption>
</figure>
<p>Other changes to the app were implemented to facilitate its use, thereby allowing users to operate it more intuitively and focus on reading rather than the frustrations of unexpected functionality. To simplify pen colour choice, black, red, and blue were added to the colour picker as pre-set colours, as shown in<a href="#figure03">Figure 3</a>. We also added page turn functionality to encourage users to read beyond the first page and increased the amount of text available. Given that we had to abandon responsive functionality at this stage, the font size was carefully chosen to ensure it could be clearly read while leaving enough blank marginal space around the text to encourage annotation. The number of lines displayed on the screen was limited to four, conforming with the size of a standard iPad. The text was displayed overlaid on an image of a right and left facing manuscript page, as shown in<a href="#figure04">Figure 4</a>. Users move forward and backwards in the text with buttons on each side of the manuscript.</p>




























<figure ><img loading="lazy" alt="" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Phase 1 menu
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Phase 2 menu annotated with features
        </p>
    </figcaption>
</figure>
<p>Stylistically, we refined the appearance of the app to a coherent theme giving the feel of a native application. This was achieved using Bootstrap 4, an open source css toolkit, chosen for its mobile-first styles throughout the components offered.<sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup> We particularly made use of the button grouping and styling and layout assistance that Bootstrap offers.<a href="#figure05">Figures 5</a><a href="#figure06">and 6</a>show the evolution from the Phase 1 to Phase 2 menu with additional features. The styling makes the menu smaller, thereby ensuring that it does not encroach too heavily on the annotation space. Icons rather than text keeps button size to a minimum. So that it is not intrusive, we opted for a black background to match the black background against which most manuscript pages are photographed and introduced a green and blue colour scheme to pick up colours in the illustrated manuscript page. Finally, we included a pop-up window on opening the app which details instructions for its use. These instructions are kept to a minimum to avoid directing readers too precisely, allowing us to monitor the variety of annotations made by users. </p>




























<figure ><img loading="lazy" alt="" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Login page
        </p>
    </figcaption>
</figure>
<p>In order to encourage reading over a longer period of time and to enable us to gather further data on how the app is used, user accounts that allow page saving and resuming were introduced in this phase of redesign. Users are now able to register with their email and set up a password, which is saved in the database in an encrypted form using password hashing provided by bcrypt.<sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup> <a href="#figure07">Figure 7</a>shows the login page, including the option to show or hide the password. On logging into the app at a later date, users are presented with their last saved page to resume reading. Fabric.js made saving images in the database straightforward through the functionality to export the canvas as an SVG string suitable to be entered into a text-based database.<sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup> As users make and save additional changes to a page, that page is updated in the database. To reflect the additional data now being collected by the app, admin features were added, including the ability to view a table of users, the number of pages saved with date stamp, and the number of feedback forms submitted. These data can be downloaded in csv form for ease of analysis and interoperability. Each saved page can also be downloaded and viewed.</p>
<p>Further iterations of the app might include features which address our initial research questions more fully. For instance, a feature that captures the timings of the marks made on the page or video recordings of the page would give us a fuller understanding of the process of reading with a pen in hand. Automatic resizing for different devices would make the app more accessible to mobile phone as well as tablet users. </p>
<h2 id="conclusions-and-future-work">Conclusions and Future Work</h2>
<p>While Hands-On Reading demonstrates the importance of looking at the adaptability of old technology such as medieval manuscripts and early printed books in creating tools for digital reading and writing, there are several ways in which the app might be further developed to explore the possibilities of slow reading. Despite recent advances, the gap between current digital technology and its paper counterpart remains sufficiently broad enough to prompt the question of whether digital reading can directly replace the physical book. Readers approach the digital and the physical text with different expectations; their way of processing information in digital and non-digital platforms also differs. Thus, if medieval practices of reading with pen in hand cannot be translated directly from physical paper formats to current digital formats, then a broader question arises: can digital interfaces offer new ways of reading which surpass the possibilities of physical books?</p>
<p>Early twenty-first century predictions of the demise of print publishing have proven to be unfounded, with a strong revival in sales of printed books over the last decade. Some categories of books – such as genre fiction and academic publications – have flourished in the new medium, yet many others remain firmly rooted in the print market. That audiences prefer to access different kinds of books in specific formats does not lack historical precedent. Moreover, as the recent decline in e-reader sales indicates, many readers prefer to access digital texts via computers, smartphones and tablets rather than seeking new devices for reading. Though future technological developments may well allow us to shrink the gap between print and digital media, it seems entirely likely that the two modes of textual consumption will continue to coexist, albeit with the emergence of new forms of interaction between them.</p>
<p>However, one area where there seems to be considerable potential for permeability between paper and digital technologies is that of writing. The stylus is held and moved in the same way as a normal pen; even in its most basic form it affords a familiar experience, while advanced hardware and software tools are capable of emulating (if not fully replicating) the tactility and precision of writing with a pen on paper. Current touch screen and stylus technologies are not yet sufficiently advanced to teach practical calligraphy at anything more than a basic level, but the growing number of mass-market and specialist software solutions that facilitate “inking” indicates the potential for digital writing to complement and enhance existing digital tools and modes of consumption.</p>
<p>With readers increasingly accessing digital texts with a stylus in hand, this is an opportune moment to explore the possibilities offered by slow reading within a digital context. Rather than imitating paper and pens to encourage active reading in digital formats, the gap between paper, pens, and the current affordances of digital technology could itself be leveraged to offer new ways of reading. While physical books are not designed to allow a reader to copy over a text while reading, digital texts can easily be manipulated to explore this functionality, encouraging the interaction of text and pen for example by tracing words. Doing so permits the reader to engage with the text in an entirely new way: reading <em>by</em> writing.</p>
<p>Slowing the process of reading, and thereby allowing time for evaluative and critical thinking, means that the app could be adapted to a variety of audiences. The development process brought the question of the app’s target audience into sharper focus. The initial concept of the app emerged from a set of broad research questions rather than an attempt to fill a gap in the market, and throughout the design and trial phases we sought to explore potential applications in an open and non-prescriptive way. Feedback from testing workshops confirmed our assumptions about the limited value of Hands-On Reading as a tool for teaching calligraphy, but also raised various possible ways in which it might be adapted for other educational uses. The most promising of these suggestions – and the most consistent with our research goals – is further development of the app as a tool to explore and enhance reading by writing in primary, secondary, and higher education. The growing body of work on the impact of digital reading in this setting<a class="footnote-ref" href="#cull2011"> [cull2011] </a><a class="footnote-ref" href="#schugar2011"> [schugar2011] </a><a class="footnote-ref" href="#chen2013"> [chen2013] </a><a class="footnote-ref" href="#kaufman2016"> [kaufman2016] </a>is only the most recent phase of a longstanding tradition of scholarship on how students can and should read, stretching back to the twelfth-century <em>Metalogicon</em> of John of Salisbury. More recent research indicates that the use of handwriting apps with a tablet and stylus not only improves children’s ability to write with pen and paper but also aids in their general development, academic success, and emotional wellbeing (these studies are conveniently summarised in<a href="#klein2021">Klein et al. (2021)</a>). Existing apps intended for both school and home use – such as Kaligo, iTrace and Handwriting Without Tears – teach letter and number formation through a similar process of tracing and are designed to complement national curricula. But while several DH projects have studied the benefits of annotation on memory and understanding at primary and secondary level<a class="footnote-ref" href="#butler2019"> [butler2019] </a><a class="footnote-ref" href="#bonnetonbotte2020"> [bonnetonbotte2020] </a><a class="footnote-ref" href="#araujo2021"> [araujo2021] </a>and university-level study<a class="footnote-ref" href="#marshall1997"> [marshall1997] </a><a class="footnote-ref" href="#wolfe2000"> [wolfe2000] </a><a class="footnote-ref" href="#hong2012"> [hong2012] </a>, only relatively limited attention has been paid to the role that writing plays in facilitating the comprehension and interrogation of the digital text (on this subject see<a href="#fitzpatrick2013">Fitzpatrick (2013)</a>). Hands-On Reading’s unique digital medieval format could be leveraged to further aid the process of annotation and so information retention, and offer the opportunity to more deeply examine the concept of slow reading with pen in hand.</p>
<p>Finally, the development of Hands-On Reading has allowed us to reflect on our initial research questions: why does handwriting have an enduring relevance in a digital age, and can a more hands-on approach to reading lead to a deeper engagement with the text? Though it is clear that numerous factors have contributed to handwriting’s longevity, development and testing of the app has shed particular light on the role of handwriting in facilitating the critical and inferential evaluation of textual sources, whether in printed or digital form. As the digital revolution continues to profoundly change the way we consume texts – and broader social trends highlight the fundamental importance of interpretative analysis – there is a pressing need to equip contemporary readers with the tools to approach the digital text on a deeper, more reflective level. There is nothing new about the use of handwriting to promote more active forms of reading, but digitally replicating much older forms of reading while writing can allow us to make the most of these technological advances while simultaneously taking a slower approach to reading, with all the critical and evaluative benefits this brings. </p>
<ul>
<li id="ackerman2011">Ackerman, R. and M. Goldsmith. “Metacognitive regulation of text learning: On screen versus on paper.”  _Journal of Experimental Psychology: Applied,_ 17: 1 (2011). DOI:<a href="https://doi.apa.org/doiLanding?doi=10.1037%2Fa0022086">https://doi.apa.org/doiLanding?doi=10.1037%2Fa0022086</a>
</li>
<li id="adler1998">Adler, A., A. Gujar, B. L. Harrison, K. O'Hara, and A. Sellen. “A diary study of work-related reading: design implications for digital reading devices.”  _CHI 1998: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems_ (1998). DOI:<a href="https://doi.org/10.1145/274644.274679">https://doi.org/10.1145/274644.274679</a> 
</li>
<li id="agrawala2005">Agrawala, M. and M. Shilman. “DIZI: a digital ink zooming interface for document annotation.”  _Human- Computer Interaction-INTERACT 2005_ (2005). DOI:<a href="http://graphics.stanford.edu/papers/dizi/DIZI.3.pdf">http://graphics.stanford.edu/papers/dizi/DIZI.3.pdf</a> 
</li>
<li id="aiden2013">Aiden, E., and J. Michel. _Uncharted: Big Data as a Lens on Human Culture_ (Penguin, 2013).
</li>
<li id="annett2014">Annett, M., F. Anderson, W. F. Bischof, and A. Gupta. “The pen is mightier: understanding stylus behaviour while inking on tablets.”  _Proceedings of the 2014 Graphics Interface Conference_ (Canadian Information Processing Society: 2014), 193-200.
</li>
<li id="araujo2021">Araújo, S., M. Domingues, and T. Fernandes, (2021, May 7). “From hand to eye: A meta-analysis of the benefit from handwriting training in visual graph recognition.”  _OSF Preprints_ (2021). DOI:<a href="https://doi.org/10.31219/osf.io/quywt">https://doi.org/10.31219/osf.io/quywt</a>
</li>
<li id="aurnhammer2019">Aurnhammer, Christoph, Iris Cuppen, Inge van de Ven, Menno van Zaanen. “Manual Annotation of Unsupervised Models: Close and Distant Reading of Politics on Reddit.”  _Digital Humanities Quarterly_ 13: 3 (2019) DOI:<a href="http://www.digitalhumanities.org/dhq/vol/13/3/000431/000431.html">http://www.digitalhumanities.org/dhq/vol/13/3/000431/000431.html</a> 
</li>
<li id="baron2017">Baron, Naomi S. “Reading in a Digital Age.”  _Phi Delta Kappan_ 99: 2 (2017), 15-20. DOI:<a href="https://doi.org/10.1177/0031721717734184">https://doi.org/10.1177/0031721717734184</a>
</li>
<li id="bonnetonbotte2020">Bonneton-Botté, Nathalie, Sylvain Fleury, Nathalie Girard, Maëlys Le Magadou, Anthony Cherbonnier, Mickaël Renault, Eric Anquetil, Eric Jamet. “Can tablet apps support the learning of handwriting? An investigation of learning outcomes in kindergarten classroom.”  _Computers & Education_ 151 (2020). DOI:<a href="https://doi.org/10.1016/j.compedu.2020.103831">https://doi.org/10.1016/j.compedu.2020.103831</a>
</li>
<li id="butler2019">Butler, C., R. Pimenta, J. Tommerdahl, C. T. Fuchs and P. Caçola. “Using a handwriting app leads to improvement in manual dexterity in kindergarten children.”  _Research in Learning Technology_ 27 (2019). DOI:<a href="https://doi.org/10.25304/rlt.v27.2135">https://doi.org/10.25304/rlt.v27.2135</a>
</li>
<li id="chen2013">Chen et al. “Graduate student use of a multi-slate reading system.”  _CHI '13: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems_ (2013). DOI:<a href="https://doi.org/10.1145/2470654.2466237">https://doi.org/10.1145/2470654.2466237</a>
</li>
<li id="cull2011">Cull, B. W. “Reading revolutions: Online digital text and implications for reading in academe.”  _First Monday_ 16: 6 (2011). DOI:<a href="https://doi.org/10.5210/fm.v16i6.3340">https://doi.org/10.5210/fm.v16i6.3340</a>
</li>
<li id="fitzpatrick2013">Fitzpatrick, N. “Digital Reading: A Question of Prelectio?” In C. Fowley, C. English, and S. Thouësny (eds.), _Internet Research, Theory, and Practice: Perspectives from Ireland_ (Research-publishing.net, 2013) pp. 1-16. DOI:<a href="https://doi.org/10.14705/rpnet.2013.000084">https://doi.org/10.14705/rpnet.2013.000084</a>
</li>
<li id="hinckley2010">Hinckley, Ken, Koji Yatani, Michel Pahud, Nicole Coddington, Jenny Rodenhouse, Andy Wilson, Hrvoje Benko, Bill Buxton. “Pen + touch = new tools.”  _UIST 2010: Proceedings of the 23nd annual ACM symposium on User interface software and technology_ (2010). DOI:<a href="https://doi.org/10.1145/1866029.1866036">https://doi.org/10.1145/1866029.1866036</a>
</li>
<li id="hinckley2012">Hinckley, Ken, Xiaojun Bi, Michel Pahud, and Bill Buxton. “Informal Information Gathering Techniques for Active Reading.”  _CHI '12: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems_ (2012). DOI:<a href="http://dx.doi.org/10.1145/2207676.2208327">http://dx.doi.org/10.1145/2207676.2208327</a>
</li>
<li id="hong2012">Hong, Matthew, Anne Marie Piper, Nadir Weibel, Simon Olberding, James Hollan. “Microanalysis of active reading behavior to inform design of interactive desktop workspaces.”  _TS 2012: Proceedings of the 2012 ACM international conference on Interactive tabletops and surfaces_ (2012). DOI:<a href="https://doi.org/10.1145/2396636.2396670">https://doi.org/10.1145/2396636.2396670</a>
</li>
<li id="karvanidou2017">Karvanidou, Eleni, “Is Handwriting Relevant in the Digital Era?”  _Antistasis_ 7 (2017) 153-164.
</li>
<li id="kaufman2016">Kaufman, G. and M. Flanagan, “High-low split: Divergent cognitive construal levels triggered by digital and nondigital platforms.”  _Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems_ (2016). DOI:<a href="https://doi.org/10.1145/2858036.2858550">https://doi.org/10.1145/2858036.2858550</a>
</li>
<li id="klein2012">Klein, E., I. Montgomery and J. G. Zwicker. “Theory and Evidence for Pre-printing Development: A Scoping Review.”   _Journal of Occupational Therapy, Schools, & Early Intervention_ (2021). DOI: <a href="https://doi.org/10.1080/19411243.2021.1875388">https://doi.org/10.1080/19411243.2021.1875388</a>
</li>
<li id="liao2008">Liao, Chunyuan, François Guimbretière, Ken Hinckley, and Jim Hollan. “Papiercraft: A Gesture-based Command System for Interactive Paper.”  _ACM Transactions on Computer-Human Interaction._ 14:4 (2008). DOI:<a href="http://dx.doi.org/10.1145/1314683.1314686">http://dx.doi.org/10.1145/1314683.1314686</a>
</li>
<li id="marshall1997">Marshall, Catherine C. “Annotation: from paper books to the digital library.”  _DL 1997: Proceedings of the second ACM international conference on Digital libraries_ (1997) DOI:<a href="https://doi.org/10.1145/263690.263806">https://doi.org/10.1145/263690.263806</a>
</li>
<li id="nafde2020">Nafde, Aditi. “Replicating the Print Aesthetic in Manuscripts before c.1500.”  _Digital Philology_ 9:2 (2020) 120-144. DOI:<a href="https://doi.org/10.1353/dph.2020.0008">https://doi.org/10.1353/dph.2020.0008</a>
</li>
<li id="neef2011">Neef, Sonja, _Imprint and Trace: Handwriting in the Age of Technology_ (London: Reaktion, 2011).
</li>
<li id="ohara1997">O'Hara, K. and A. Sellen. “A comparison of reading paper and on-line documents.”  _CHI 1997: Proceedings of the ACM SIGCHI Conference on Human factors in computing systems_ (1997). DOI:<a href="https://doi.org/10.1145/258549.258787">https://doi.org/10.1145/258549.258787</a>
</li>
<li id="schilit1998">Schilit, Bill N., Gene Golovchinsky, and Morgan N. Price. “Beyond Paper: Supporting Active Reading with Free Form Digital Ink Annotations.”  _CHI 1998: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems_ (1998). DOI:<a href="http://dx.doi.org/10.1145/274644.274680">https://dx.doi.org/10.1145/274644.274680</a>
</li>
<li id="schugar2011">Schugar, J. T., H. Schugar, and C. Penny. “A nook or a book: Comparing college students’ reading comprehension level, critical reading, and study skills.”  _International Journal of Technology in Teaching and Learning_ 7:2 (2011). 174-192.
</li>
<li id="sellen1997">Sellen, A. and R. Harper. “Paper as an analytic resource for the design of new technologies.”  _CHI 1997: Proceedings of the ACM SIGCHI Conference on Human factors in computing systems_ (1997). DOI:<a href="https://doi.org/10.1145/258549.258780">https://doi.org/10.1145/258549.258780</a>
</li>
<li id="tashman2011">Tashman, Craig S. and W. Keith Edwards. “Active reading and its discontents: the situations, problems and ideas of readers.” CHI 2011: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (2011). DOI:<a href="https://doi.org/10.1145/1978942.1979376">https://doi.org/10.1145/1978942.1979376</a> 
</li>
<li id="trithemius1494">Trithemius, Johannes. _In Praise of Scribes: De Laude Scriptorum_ , ed. by Kaus Arnold, trans. by Roland Behrendt (Lawrence, Kansas: Coronado Press, 1974).
</li>
<li id="weibel2012">Weibel, Nadir, Adam Fouse, Colleen Emmenegger, Whitney Friedman, Edwin Hutchins, and James Hollan. “Digital Pen and Paper Practices in Observational Research.”  _CHI 2012: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems_ (2012). DOI:<a href="http://dx.doi.org/10.1145/2207676.2208590">http://dx.doi.org/10.1145/2207676.2208590</a>
</li>
<li id="wickberg2020">Wickberg, Adam. “New Materialism and the Intimacy of Post-digital Handwriting.”  _Trace_ 4 (2020). DOI:<a href="http://tracejournal.net/trace-issues/issue4/05-wickberg.html">http://tracejournal.net/trace-issues/issue4/05-wickberg.html</a>
</li>
<li id="wolfe2000">Wolfe, Joanna L. “Effects of annotations on student readers and writers.”  _DL 2000: Proceedings of the fifth ACM conference on Digital libraries_ (2000). DOI:<a href="https://doi.org/10.1145/336597.336620">https://doi.org/10.1145/336597.336620</a>  
</li>
<li id="yoon2013">Yoon, Dongwook, Nicholas Chen, and Francois Guimbretière. “TextTearing: opening white space for digital ink annotation.”  _Proceedings of the 26th annual ACM symposium on User interface software and technology (UIST 2013)_ (2013). DOI:<a href="http://dx.doi.org/10.1145/2501988.2502036">http://dx.doi.org/10.1145/2501988.2502036</a>
</li>
<li id="weijing2018">Weijing Yuan, Marlene Van Ballegooie and Jennifer L. Robertson. “Ebooks Versus Print Books: Format Preferences in an Academic Library.”  _Collection Management_ 43:1 (2018). DOI:<a href="https://doi.org/10.1080/01462679.2017.1365264">https://doi.org/10.1080/01462679.2017.1365264</a>
</li>
</ul>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>An application running on a server rather than installed on each user’s machine.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>i.e. apps available through app stores, usually on mobile devices, specific to either Apple iOS or Android.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Theclientis the front-end visual appearance and behaviour of a website, whereas theserveris the back-end of the application which handles user log in and communication with the database. Generally, the termsclientandfront-endare used interchangeably, as areserverandback-end.Angular 9 is a web application framework that helps to build the client. Nest.js is a back-end framework that does a similar role for the server.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Typescript is a programming language that builds on the language JavaScript.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>Fabric.js is a library of code available for programmers to integrate into their programs that offers drawing functionality. HTML5 is a text formatting mark-up language which dictates the arrangement of content on the webpage&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>mongoDB is a type of database used to store text-based data.&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>Some databases store data in tables with pre-determined fields and relationships between fields, e.g. a user will have an email and a related password. In contrast, in a schemaless database fields are not pre-determined and the database does not follow a strict schema, e.g. fields can be changed during development to add a username.&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>A <code>&lt;div&gt;</code> is a block of content within the site.&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>CSS is a language used to write descriptions for formatting HTML content. Bootstrap 4 is a library of code that can be integrated into a program that provides pre-set styling for common elements such as buttons.&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>A hashed password is a scrambled version of the original password that can be unscrambled using a secret key known only to the website. This means that if the scrambled version of the password is ever intercepted or accessed in the database by an unauthorized source it cannot be unscrambled. bcrypt is a library of code that offers this functionality.&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>An SVG (scalable vector graphic) is an image type where information can be stored as text characters using the XML markup language.## Bibliography&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry><entry><title type="html">Going Digital: Teaching Crevecoeur in the Twenty-First Century</title><link href="https://startwords.cdh.princeton.edu/vol/15/2/000552/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://startwords.cdh.princeton.edu/vol/15/2/000552/</id><author><name>Mary Mcaleer Balkun</name></author><author><name>Diana Hope Polley</name></author><published>2021-06-15T00:00:00+00:00</published><updated>2023-08-04T13:01:02-04:00</updated><content type="html"><![CDATA[<h1 id="heading"></h1>
<h2 id="heading-1"></h2>
<p>During a 2011 Northeast Modern Language Association (NeMLA) panel dedicated to the works of eighteenth-century French-American author J. Hector St. John de Crèvecoeur, panelists and attendees participated in an engaged discussion regarding the rewards and challenges of teaching Crèvecoeur to 21st-century college students. Among the biggest challenges discussed, audience members noted the surprising lack of available teaching editions of Crèvecoeur’s most celebrated work, <em>Letters from an American Farmer</em> . American literature anthologies, for example, typically include only small sections of <em>Letters</em> as a way to introduce students to the style and subject matter of Crèvecoeur’s text. Trade publishers such as Dover, Penguin Classics, and Oxford Classics have published complete editions of the text, but — with their lack of limited (if any) annotations and other supplementary aids for textual, historical, cultural, or critical interpretations — these editions are not ideal for classroom use and have not been published with educators and students in mind.<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> Given the literary and cultural significance yet textual complexity of Crèvecoeur’s narrative, panel attendants remarked, there was a decisive need for a teaching edition of <em>Letters</em> .</p>
<p>Since meeting as presenters on this NeMLA panel, we have teamed up to try to identify a workable solution to what we saw as a challenge to professors of early American literature: finding a way to allow faculty to more fully integrate Crèvecoeur’s <em>Letters</em> into the college classroom. This article will outline our pedagogical concerns with current print editions of Crèvecoeur’s work (and other early American texts); lessons learned from our early research on alternative digital critical models; our attempts as faculty to begin to develop and create a digital critical edition of <em>Letters</em> ; our experiences using this digital edition in the classroom; and the broader implications for the growth of digital editions of early American (and other) texts. Finally, we will discuss an initially unanticipated shift in our project: transferring the editorial responsibilities over to our students and teaching them to curate the text themselves as a way to help them become active readers, to deepen their engagement with material, to educate them about the editorial process, and to move them towards the professional.</p>
<h2 id="context">Context</h2>
<p>Crèvecoeur’s <em>Letters from an American Farmer</em> is a complex, comprehensive canonical American text studied across disciplines, including English, History, and Political Science. What makes the text so well suited for classroom discussion at all levels are the questions it raises about American identity and utopian idealism, as well as the rich and varied ways it does so. In asking that now famous question, “What then is the American, this new man?” , Crèvecoeur’s <em>Letters</em> became the first and most important text to openly examine early American identity, culture, politics, and social structures (Letter III). The narrative seeks to address all facets of American colonial life on the eve of the American Revolution. Caught between a utopian view of the colonies shaped by Enlightenment thinking and the growing chaos and conflict around him, the narrator simultaneously highlights both early idealism and disillusionment in the American project. In the process, he touches on fundamental issues that were prevalent in late eighteenth-century America and are still of critical significance today: immigration, commerce, the frontier, the environment, Indigenous populations, the transatlantic slave trade, and disparate class structures. In addition to its approach to important social and historical issues, the text — as an epistolary novel that skirts the boundaries between fiction, history, and autobiography — poses fundamental questions about literary form itself. <em>Letters</em> offers students, teachers, and scholars alike a critical lens through which to examine American history, politics, culture, literature, and language.</p>
<p>Despite the popularity of, the importance of, and the classroom opportunities provided by Crèvecoeur’s text, it is difficult for students to access and interpret for a number of reasons. The late-eighteenth-century language is challenging in its own right. The question of the text’s genre is also convoluted: <em>Letters</em> is an epistolary narrative comprised of twelve letters written by a fictional character, an Orange County New York farmer James, to a British subject named Mr. F.B., describing — as James says — “our American modes of farming, our manners, and peculiar customs” (Letter I). Although Crèvecoeur himself was French born and only immigrated to America in the late 1750s, he shares enough qualities with James — as a farmer, living in Orange County New York, struggling to navigate the tenuous socio-historical dynamics of rural life on the cusp of the American Revolution — that the text ultimately blurs the boundaries between autobiography, non-fictional prose, and the novel. The ambiguous genre often makes it difficult for students to ascertain the distance between the author and the narrator. This distance is further complicated by sudden narrative shifts between and even within letters, where, for example, notes or brief histories from James or even from another writer are occasionally inserted into the narrative. Finally, and most importantly, the narrative itself is engrained in the socio-historical background of the late Colonial period in America as well as the complex philosophy of the Enlightenment (and, arguably, the ultimate breakdown of that philosophy) and, as such, includes references to various eighteenth-century subjects — botany, farming, hunting, whaling, slavery, Indigenous culture, religion — that often elude twenty-first century readers.</p>
<p>To fill the gap left by anthologies that publish fragments of <em>Letters</em> — but fail to illustrate the complexities of genre, the development of the narrative, the narrator’s emerging story, as well as its radically shifting tone — and trade editions that publish the complete text — but lack the necessary detailed annotations and developed ancillary materials to make sense of the context — we set out to publish a print critical edition of the complete text, replete with those necessary supplementary reading materials. We reached out to a couple of publishers, such as Norton and Broadview, who specialize in critical editions particularly developed for and marketed towards the college classroom. We received interest in our initial inquiries and requests for more developed proposals, but ultimately the consensus seemed to be that — although a teaching edition of <em>Letters</em> was, indeed, both timely and valuable — given the trade editions currently available, the market could not accommodate another print edition.</p>
<h2 id="exploring-a-digital-edition--from-the-theoretical-to-the-practical">Exploring a Digital Edition — from the Theoretical to the Practical</h2>
<p>Because of the limited print publishing options and the growing rise in the digital market, we decided to shift course and research digital publishing possibilities. As we did, we quickly realized that, while more challenging, the digital might provide a better opportunity to achieve our primary goal: to make Crèvecoeur’s text not only available but also accessible to the twenty-first century student. Given the very intertextual nature of Crèvecoeur’s writing, a digital approach to his work began to make the most sense, particularly for today’s students. Not only are twenty-first century students woefully uninformed about much of the history and culture of early America but they are also not inclined to do research (even using an online dictionary) to look up information. Unfortunately, printed footnotes are not especially useful either, since most students tend to skip them: today’s students are used to having information at their fingertips — and lots of it. The elements of Crèvecoeur that students, especially non-English or History majors, often tend to find dull (the sections on botany and whaling, for instance) could now be made visual and relevant to, almost tactile for, the contemporary reader. In addition, as an open-source edition, this new format would speak to potential print edition market concerns, would expand open access to early American literature for students and other readers alike, would allow us to engage in the growing field of the digital humanities, and could eventually provide the opportunity to further develop our project to include other early American texts. In general, an online edition would allow us to make use of current developments in technology that would maximize the learning potential for readers and open up the textual world of early America to those outside academia.</p>
<p>While the digital option was compelling, it created a number of immediate challenges. The most self-evident obstacle was that, while both of us had various levels of expertise and experience utilizing technology in the classroom, neither of us at the time could claim to be “digital humanists.” In essence, we “fell into” the digital humanities. Rather than a conscious dive into the digital — its practical applications, its theoretical underpinnings, and the nuances of its scholarship — we were led there, essentially as a publishing refuge when we realized that the analog could no longer accommodate what we had initially hoped to produce. Not only were we not coders, but to cross over into the digital we would have to begin, largely as novices, to investigate whether there were any publishing platforms that could accommodate our Platonic ideal of this edition; and then we would have to consider the theoretical implications of this publishing paradigm shift, implications that we could not then even fully anticipate. Although this circuitous route toward the digital humanities is no doubt far more typical in academia, this alternate perspective — the attempt to penetrate the digital humanities after years of producing more traditional scholarship — is not readily reflected in DH scholarship. Matthew K. Gold and Laura F. Klein’s 2012, 2016, and 2019 <em>Debates in the Digital Humaities</em> , for example, each presents a near exhaustive approach to the “debates” in the digital humanities — defining, theorizing, critiquing, teaching, and envisioning the future of DH (in the 2012 edition, for example) — but by and large each edition still focuses on those who already understand the terms of the debate. In what follows, we hope to add to the rich discussion of the digital humanities by offering a slightly different perspective: a process that involved familiarizing ourselves with foundational digital humanities scholarship, exploring relevant digital platforms, and engaging with other digital humanities projects and pedagogies to begin to carve a space for our digital edition. In short, we hope to share our occasionally painful, frequently tortuous and challenging, but also deeply rewarding process of “going digital.”</p>
<p>The first step in switching our perspective from the print to the digital was confronting new theoretical questions, including critical and pedagogical issues that were not evident when our goal was simply to make a print critical edition of <em>Letters from an American Farmer</em> . For one thing, the existence of and increasing emphasis on digital texts over their print counterparts has raised, and continues to raise, questions about the very nature of scholarship, criticism, and textuality as well as how these elements intersect. In their “Introduction” to <em>Text Editing, Print and the Digital World</em> , Marilyn Deegan and Kathryn Sutherland provide an overview of some of these critical issues. For example, can the standards and methods used to create print texts also be considered valid when creating digital editions, and how does the nature of the text impact the way it is read and used? In short, they observe, “Our ideas of what constitutes a literary work are under revision: what factors determine its boundaries and shape, what we mean by text and what features define it” <a class="footnote-ref" href="#deegan2009"> [deegan2009] </a>. The questions that guided their 2009 volume in many ways remain at the heart of debates over digital editions today; we had to consider several of these questions as we began to investigate digital options.</p>
<p>The most important and immediate question we had to confront was the one about form: what exactly did we mean when we said we were compiling a “digital critical edition ” ? In <em>Digital Scholarly Editing: Theories, Models and Methods</em> , Elena Pierazzo, referencing the work of Edward Vanhoutte, observes that “a great theoretical effort has been deployed in order to distinguish digital archives from digital editions and documentary editions from critical editions” <a class="footnote-ref" href="#pierazzo2015"> [pierazzo2015] </a>. Our first task was to tease out these differences. We had in mind the Norton editions as a generally accepted and respected model of a print “critical edition” : a text with an approved version of the text; annotations that explain terms, concepts, and references to help guide the reader; documents that set the text in its historical and cultural context; scholarly articles that introduce the reader to the critical conversation taking place around the text over time; and a bibliography.</p>
<p>When translating this model of a critical edition to the digital, we generally subscribed to the type of ideal digital model Peter Shilingsburg outlines in “How Literary Works Exist: Convenient Scholarly Editions” :</p>
<blockquote>
<p>The scholarly electronic edition of the future…will be convenient: it will be as cheap as a paperback book, with a user-friendly interface…with bookmarks, highlighting, space for marginal notes, and the ability to annotate… In order to avoid the down sides of paperback books, the electronic edition must give accurate access to representations (images) of specific historical forms of the text and specific critical editions of the text and to the ancillary materials that contextualized the texts at the time of origin and the times of reception that we care about. It would be even more convenient if the accumulation of scholarship related to the work were also at hand.<a class="footnote-ref" href="#shillingsburg2009"> [shillingsburg2009] </a><br>
While we wanted to make all of these digital materials “convenient” and “at hand” for the reader<a class="footnote-ref" href="#shillingsburg2009"> [shillingsburg2009] </a>, we were also cognizant of the tension regarding the digital that Jessica Pressman and Lisa Swanstrom cogently articulate in “The Literary And/As the Digital Humanities,” specifically the tension between data/information and interpretation in DH. We wanted to utilize the digital to provide that critical information for readers (annotations, ancillary materials, accumulation of scholarship) but simultaneously prevent the digital from turning readers into passive recipients of knowledge. In order to ensure that these technological tools would be used to help readers become engaged participants, to make sure, in the words of Pressman and Swanstrom, “literary critics don’t take data at its word,” we wanted to incorporate interactive features, such as integrated reading questions and the and the capacity for commentary<a class="footnote-ref" href="#pressman2013"> [pressman2013] </a>.</p>
</blockquote>
<p>Most importantly, however, we agreed with Patrick Sahle’s definition in “What is a Scholarly Digital Edition?” that “a digitized edition is not a digital edition” <a class="footnote-ref" href="#sahle2016"> [sahle2016] </a>. We did not want to create a digitized facsimile of an analog edition. Rather, we wanted to take the work that digital archive projects such as <em>Project Gutenberg</em> had done and make these unedited digital texts not only available but <em>accessible</em> to students. Acknowledging that we would not be able to create that “ideal” edition of the future that Shillingsburg outlines, we aimed to approximate a version of it. Our initial plan was to create a digital edition of Crèvecoeur’s <em>Letters</em> that would include critical annotations as well as links to appropriate supplemental materials as hypertext; students would be able to move their cursor over or click on a word to locate definitions, reading questions, historical and cultural references, videos, and hyperlinks to relevant web sites. We wanted to create an interactive experience for the reader that included the kinds of dynamic features Sahle uses to define digital editions: “browsing paths,”  “real hyperlinks,” and “integrated technical tools” <a class="footnote-ref" href="#sahle2016"> [sahle2016] </a>.</p>
<p>While critics such as Sahle stop short of distinguishing between a <em>critical</em> and scholarly edition, our goal was to move beyond form and also to distinguish our digital critical edition of <em>Letters</em> from a <em>scholarly</em> edition that focused primarily on textual criticism. Dennis D. Moore has published several collections of Crèvecoeur’s writing that pay careful attention to textual sources and composition and are critical for academic scholarship. We hoped to take on the work that Kathryn Sutherland claims has been deemed less glamorous: the kind of textual annotation that is “most in favour with student readers, general readers and commercially minded publishers, to all of whom it is perceived as adding value” <a class="footnote-ref" href="#sutherland2009"> [sutherland2009] </a>. And perhaps this was one place where digital editions could start to differ from their analog counterparts: that they take seriously the place of textual annotations and the readers who value them. At the same time, we were concerned with maintaining the accuracy of the edition and demonstrating that this type of innovative and interactive digital edition could be as viable and reliable as the more traditional printed one we had hoped to create at the outset. This is why we were careful about the edition of <em>Letters</em> we chose to use and about creating that edition ourselves, not depending on another digital edition already in existence, although there are several. We ultimately chose the 1783 Thomas Davies London edition of <em>Letters</em> as the most authoritative, and transcribed the text from a digital facsimile of the original into a .doc file to ensure its accuracy. In addition, any minimal emendations that were made would be noted in the edition. The focus would be on editing the apparatus over the text, but at the same time we wanted to ensure textual precision.</p>
<p>Beyond the theoretical underpinnings, the challenges we faced were also quite practical: while we have both used technology in various forms for our teaching (such as learning management systems, presentation tools, group communication tools) and recognize its pedagogical potential in the classroom, neither one of us had extensive experience with digital publishing platforms. Furthermore, neither of us knew or was prepared to learn website-building languages such as HTML, CSS, or Javascript. Given our own technological limitations, we began to explore what types of digital critical editions had already been published online, either early American or other literary texts. This research would give us a general sense of the scope of similar existing and ongoing digital projects, specifically critical editions geared towards college-level classroom use; it would allow us to ascertain the technologies that might have been used to create such projects (as well as provide us with valuable contacts); and it would provide us with a better visual sense of the parameters for our project — the kind of architectural, stylistic, and pedagogical opportunities, as well as the boundaries such various technologies might afford us.</p>
<p>What surprised us initially was not how many but rather how few digital critical editions we were initially able to locate easily and readily online, and in particular how few online editions we found with interactive, digitized annotations and hyperlinks, new textual models constructed with the contemporary student in mind. Extensive and invaluable progress has clearly been made in recent decades in the digital humanities. Well-known digital archives, such as <em>Project Gutenberg</em> and the Internet Archive, have done immeasurable work digitizing texts and making them available online for teaching and scholarship purposes, but the purpose of these important projects has been primarily to archive these texts rather than to annotate or edit them. More recently, smaller non-profit projects have contributed critical work in this area. <em>Just Teach One</em> , sponsored by <em>Commonplace: The Journal of Early American Life</em> , is one example of a digital recovery project that is more specifically suited for the classroom, as it not only offers digital transcriptions of texts (in this case “neglected or forgotten” early American texts) but also annotates and edits them. While <em>Just Teach One</em> and projects like it provide editions with a basic apparatus, they are still not “digital critical editions” in the sense that they are digitized PDFs, which is to say that they do not offer those “integrated technical tools” Sahle characterizes as an essential part of such digital editions<a class="footnote-ref" href="#sahle2016"> [sahle2016] </a>.</p>
<p>Many other digital projects are working on that same important process of recovery and/or preservation but also offer various levels of dynamic integration with those critical “technical tools.” Other sites, such as <em>Digital Library of Medieval Manuscripts</em> and <em>American Transcendentalism Web</em> , as well as single-author archival projects, such as the <em>Willa Cather Archive</em> , <em>The Walt Whitman Archive</em> , <em>Jane Austen Manuscripts</em> , <em>Digital Thoreau</em> , and <em>The William Blake Archive</em> , are representative of the type of digital textual scholarship and digital reading experiences being created online. Each of these sites offers distinctive digital models with unique features particularly valuable for those starting out in the digital humanities. For example, <em>Internet Shakespeare</em> offers the ancillary material (an Introduction, Critical Survey, Bibliography, etc…) fundamental to a critical edition, various versions of each text, and the type of digitally-integrated annotations we planned to include. <em>The William Blake Archive</em> integrates multimedia to recreate exhibitions of Blake’s work. <em>Digital Thoreau</em> makes manifest a “fluid text” edition of the seven existing manuscript versions of <em>Walden</em> . While each of these sites, and the majority of those we examined, tend to be geared towards micro-literary communities, we did identify other interdisciplinary projects, such as <em>The Vault at Pfaff’s: An Archive of Art and Literature by the Bohemians of Antebellum New York</em> , sponsored by Lehigh University, which provide a cornucopia of primary and secondary sources, a digital arcade of hyperlinked sources, but which are focused on a very limited theme, in this case the fascinating 1860s New York City bohemian world of Charles Pfaff’s Manhattan beer cellar. Finally, we discovered some for-profit digital publishers, such as Touch Press, which work with publishers to create digital content; Touch Press&rsquo;s version of T.S. Eliot’s <em>The Wasteland</em> includes notes, various audio recordings, and critical and dramatic interpretations of the poem.</p>
<p>Each of these projects was valuable in helping us to conceive of possibilities within the framework of digital publishing; we did, however, learn two valuable, interrelated lessons in our initial investigation. First, the sites we found most readily tended to be geared to scholarly rather than critical editions of work. <em>Digital Thoreau</em> , for instance, is more thoroughly focused on what Kathryn Sutherland calls “the establishment of the text, its variants and transmission history” as opposed to the interdisciplinary, multimedia critical edition we hoped to create. As discussed, scholarly editions of Crèvecoeur’s <em>Letters of an American Farmer</em> currently exist, and we were not looking to create another. Rather we sought to create an open-source digital edition for the student for whom those unmediated, scholarly print editions were intimidating and difficult to penetrate. In short, although the sites we found were helpful in showcasing the array of options the digital might provide (integrated multimedia, interactive comment tools, hyperlinked annotations), the projects were far larger in scope (e.g. exhaustive single-author archives) and seemed framed with a different audience in mind.</p>
<p>The most important lesson we learned from this hands-on, primary research, however, was not what we found but rather how hard the material itself was to find. We were struck by the fact that these projects were not catalogued anywhere; instead, they seemed to float in separate silos rather than being gathered together in a single or even several classified repositories. To find a print critical edition, one can search on Amazon or ask a local book seller; to find a digital critical edition, one either needs to be a scholar in a specific literary field or have some similar entry point. There are online directories, such as the Omeka Directory and the Open Access Directory, which gather all types of digital sites and projects, but they do not specifically identify critical editions as such. By contrast, we had hoped to locate a repository of digital editions much in the same way one can go to <em>Project Gutenberg</em> to locate a collection of unedited electronic texts.</p>
<p>Thus, in addition to the challenges already named, we soon realized that if we were to move to the digital, one of our challenges would be to figure out how to make our open-source edition available to those beyond our own students. If well-resourced projects such as <em>Internet Shakespeare Editions or Jane Austen Manuscripts</em> were not immediately evident to us as professors of English and newly-inquiring digital humanists, how could we ensure that our edition would ultimately be generally available to students and other educators alike? We began to recognize that we would also need to navigate the important issue of a repository, a digital cooperative where readers could go to access the edition itself as well as other digital critical editions being done in the shadows. As a result, our search for an appropriate platform expanded to include one that could serve as a repository while also being readily available to others.</p>
<p>Initially, we had hoped to find an online press or nonprofit scholarly platform that was developing these types of digital critical editions and might be able to share information on emerging technologies appropriate to creating such an edition and potentially even hosting the Crèvecoeur edition we planned to produce. Although we failed to find that source in our early research, we discovered a promising publishing platform called “Scalar” when we attended a workshop entitled “Critical DH (Digital Humanities) Interventions in Scholarly Communications and Publishing” at the 2015 Modern Language Association (MLA) Convention in Vancouver, Canada. Scalar describes itself as “a free, open source authoring and publishing platform that’s designed to make it easy for authors to write long-form, born-digital scholarship online. Scalar enables users to assemble media from multiple sources and juxtapose them with their own writing in a variety of ways, with minimal technical expertise required” <a class="footnote-ref" href="#scalar"> [scalar] </a>. We had some basic concerns with the publishing platform, specifically that we would be constructing this edition using a technological infrastructure that was uniquely built by Scalar and housed by the Alliance for Networking Visual Culture. Once we designed the edition within the framework of Scalar’s platform, we would not be able to move it. Should its funding source fail or should we find a more appropriate platform, we would have to start from scratch. Given the 75,000-word length of <em>Letters</em> , this prospect was daunting.</p>
<p>That said, in all other respects Scalar was almost exactly the type of platform we were looking for. Like WordPress the program was user-friendly, but unlike WordPress, Scalar was designed by academics for the specific purpose of developing innovative digital online scholarship, and as such each new project was appropriately and reassuringly called a “book.” The open-source platform had partnered with various humanities centers that permitted its authors to integrate media more seamlessly, and its design allowed for a clean but flexible and varied display of interactive digital features — from basic notes to internal tags and paths to annotated audio or visual images to reader comments. Given that our initial research pointed to ongoing work in the digital humanities and, more specifically, work on digital critical editions that was still quite piecemeal and dispersed, Scalar seemed like an excellent option: the project would be housed on an academic platform whose mission was linked to scholarly work within the field of the digital humanities, and it offered us the technology we needed without the immediate requirement of technical expertise.</p>
<p>Before committing to Scalar, however, we spent some time looking at other platforms, such as Omeka, an open-source web publishing platform. Two major differences made us realize that Scalar was clearly the better choice. First, Scalar describes itself as a platform for those who want to publish “book-length works” <a class="footnote-ref" href="#scalar"> [scalar] </a>, and Omeka specializes in hosting “digital collections and…media-rich online exhibits” <a class="footnote-ref" href="#omeka"> [omeka] </a>; since we were focused on creating a book-length work, Scalar was ideal in this regard. Second, the free Omeka Classic version requires users to have an external server to host their material, while Scalar offers a server as part of their platform. In short, Scalar is self-contained and free, and for those who are beginning in the digital humanities, this makes it a more user-friendly option. Finally, however, and most importantly, we didn’t really find Scalar; Scalar in essence came to us. For two academics who “fell into” the digital humanities and had a specific project in mind, being able to engage with Scalar at an MLA workshop, hands-on, with other academics, and having access to Scalar representatives with whom we could keep in touch to ask specific technological and strategic questions, this platform was clearly the right place to start.</p>
<h2 id="creating-a-digital-edition">Creating a Digital Edition</h2>
<p>Once we had identified a suitable platform and began to build the edition, some strategic questions related to annotations and hyperlinks became apparent. The issue of hyperlink “stability” is, of course, one that is repeatedly discussed when developing online materials. For example, in “URLs Link Rot: Implications for Electronic Publishing,” a study of articles published by Emerald Publishers between 2008 and 2012, D. Vinay Kumar, B. T. Sampath Kumar, and D. R. Parameshwarappa “found that 48.53 per cent of URL citations were not accessible and the remaining 51.47 per cent of URL citations were still accessible” <a class="footnote-ref" href="#kumar2015"> [kumar2015] </a>. A related issue involves open-source accessibility. We had decided to use the <em>Oxford English Dictionary (OED)</em> as our source for annotated definitions; we not only wanted to offer contextually-appropriate eighteenth-century definitions but also to include the link to the original online source so that students could peruse the various definitions further as well as other information, such as the word’s pronunciation, etymology, and origin. The problem is that the <em>OED</em> requires an account, and we expressly wanted to avoid subscription-based sources and the need to sign in to retrieve annotation links. As a result, we chose the less ideal course: to construct our own definitions and provide a link to the online <em>Merriam-Webster Dictionary</em> for ancillary information. Similar challenges arose regarding historical, philosophical, scientific, and other related hyperlinks. The contextual relevancy, the scholarly value, the factual accuracy, and the stability of the link were of primary importance. More largely, however, we wanted to build a conscious plan about the types of sources we privileged. What open sources would count as scholarly enough? Would the History Channel, for example, be a source academic enough for college-level students? If so, would we want to try to use that source as consistently as possible? Would it be better to locate the most suitable source for any given textual reference, or might the edition then seem like a random hodge podge of hyperlinks without a specific purpose, targeted audience, and directed coherence? This issue was compounded by exactly what had drawn us to this project in the first place: a sufficient number of the references in <em>Letters from an American Farmer</em> were abstruse enough not to offer us a wealth of dynamic online reference options.</p>
<p>There were other challenges as well that involved not only strategy but time: what became immediately evident was that this digital project was going to be far more time consuming than our initial plan to construct a print edition. In addition to struggling through the ins-and-outs of Scalar’s architecture and learning the various facets of its technological possibilities (and limitations), every attempt to insert an annotation, include a hyperlink, or embed media involved several steps and a careful system of organization. Given that Scalar’s infrastructure is not (unlike WordPress, for example) based on hierarchical relationships but has, instead, what it calls a “flattened hierarchy” (where every “node” is treated as a “page” on the same level, and the editor can connect the pages as desired), each new page would need to be given a careful title and description that would allow us to connect it to the text as desired and, as importantly, be able to relocate it as needed later amongst hundreds and (given the length of this text) potentially thousands of other “pages.” And once a page was created and the annotation or embedded media image or hyperlink was constructed, the page would then need to be internally linked to the appropriate word or phrase in the text itself. The academic and pedagogical process was only a portion of the work; the technological work was also a challenge, and without the help of graduate assistants or the support of grants, the project was going to take more time than we had hoped. More importantly, the question of scalability — which is to say the ability eventually to create other early American teaching editions and to create a repository for such digital editions — would prove daunting without locating other digital humanities partnerships.<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup></p>
<p>For this reason, we decided to pilot a portion of the project. Before we determined if we were going to dedicate what might be — given our teaching and other research commitments — a year or more of editorial work to finish the whole edition, and before we engraved all twelve letters onto Scalar and became locked into that platform, we chose to complete only a strategic selection of the letters first while simultaneously presenting what we had completed at national conferences and beginning to use this portion of the project in the classroom to start to get feedback from the reader’s perspective. We therefore began working on the four most seminal letters in the text: Letter I, which establishes the narrative context and framework; Letter III, which offers the most salient line and thematic discussion in the text: “What then is the American, this new man?” ; Letter IX, which addresses the profligate lifestyle of plantation life in Charleston, South Carolina as well as an extended and powerful diatribe on the evils of slavery; and Letter XII, which presents our narrator and protagonist — once steeped in the idealism and reason of Enlightenment doctrine and confident that the American colonies were “the most perfect society now existing in the world” — now driven to despair by the chaos and danger of the impending American Revolution (Letter III).</p>
<p>Although we have made our Scalar digital edition of Crèvecoeur’s <em>Letters</em> public for the purposes of limited classroom use, conference presentations, and publications, the edition is very much a work in progress. For that reason, we attach the “live” link to the digital edition here —<a href="http://scalar.usc.edu/works/crevecoeur--letters-from-an-american-farmer/index">http://scalar.usc.edu/works/crevecoeur&ndash;letters-from-an-american-farmer/index</a>— with a certain reticence. In his essay “The Book, the E-text, and the Work Site, ” Paul Eggert helps to explain this reticence when he discusses the relatively “complete” nature of a print publication as a “unified piece of scholarship” — “every part of the volume enlightened by every other part, all of it seamlessly interdependent and unobjectionably cross-referenced, nothing said twice, all of it as near perfectly balanced as you could ever make it” — as opposed to the open-ended, never complete nature of the digital edition, in which errors can be made (or addressed) at any point<a class="footnote-ref" href="#eggert2009"> [eggert2009] </a>. For Eggert, the concern is primarily that such lack of closure “will prove an opportunity to drop [our] standards” <a class="footnote-ref" href="#eggert2009"> [eggert2009] </a>. One might equally point, however, to the anxiety with regards to digital publication of <em>never</em> attaining perfection or completion. We are still in the early stages of creating the edition (at minimum, we plan to have Letters I, III, IX, and XII annotated and replete with an Introduction, Emendations, and an extended Bibliography); however, we wanted to make the edition available for public commentary as a way to turn the project into a communal critical and pedagogical venture.</p>
<p>In many ways, our students have played a foundational role in this communal venture. Initially, our plan was simply to pilot the in-progress edition in our undergraduate and graduate classes in order to get a sense of how students engaged with our edition: to get feedback on their experience reading the online edition versus the traditional Norton print text and also on their experience navigating the various types of annotations and hyperlinks in the Scalar interface. As we began to contemplate how the edition might be made available and accessible for students, and as we began to pilot the edition in class, however, it became clear how students could use Scalar to become active participants in their own editorial work. As Brett Hirsch suggests in <em>Digital Humanities Pedagogy</em> , “the digital humanities is about learning <em>by</em> doing,” and as we ourselves learned by “going digital,” we wanted to explore how the digital might not only enhance the literary experience for our students but also enhance their own critical thinking experience as well (<a class="footnote-ref" href="#hirsch2012"> [hirsch2012] </a>, original emphasis). Thus, in what follows, we discuss the various ways in which we have integrated the project into the classroom: first by piloting our edition in the classroom and then increasingly asking our students to partake in textual ownership, by using Scalar to annotate their own versions of <em>Letters</em> and then ultimately by using the platform to curate their own texts and create their own digital editions using our edition as a model.</p>
<h2 id="using-scalar-in-the-classroom-piloting-the-edition--diana-h-polley">Using Scalar in the Classroom: Piloting the Edition — Diana H. Polley</h2>
<p>In my Early American Literature class — a 300-level undergraduate course that combined English majors with general education students — at Southern New Hampshire University (SNHU) during fall semester 2017, I had students read versions of Crèvecoeur’s <em>Letters from an American Farmer</em> in three different formats: portions of the text from the print Norton edition (which I had assigned for the class), portions of the text from our online edition of <em>Letters</em> that had been edited and annotated, and portions of the text from our online edition that had not yet been annotated. I asked for informal feedback, most importantly regarding whether they preferred reading the text online or in print; whether they preferred the online portions that had been annotated or left unannotated; and, more generally, whether they found the digital edition easy or difficult to navigate and whether the annotations and/or hyperlinks were helpful or intrusive.</p>
<p>Numerous studies have been done examining the effects of reading digitally versus in print, but the results have been far from conclusive. As Ferris Jabr observes in “The Reading Brain in the Digital Age,” most studies conducted before 1992</p>
<blockquote>
<p>concluded that people read slower, less accurately and less comprehensively on screens than on paper. Studies published since the early 1990s, however, have produced more inconsistent results…. And recent surveys suggest that although most people still prefer paper… attitudes are changing as tablets and e-reading technology improve and reading digital books for facts and fun becomes more common.<a class="footnote-ref" href="#jabr2013"> [jabr2013] </a><br>
Interestingly, I noticed that the preference for print over digital in my informal survey was clearly influenced by disciplinary major: it was my English majors who noted a preference for print over online texts, and I attribute this — given my personal knowledge of the English majors at SNHU — to their having grown up in a culture of print books; they tend to be the students who display in multiple ways (several of whom have tattoos of their favorite novels and poems, for example) a deep investment in the traditional culture of the humanities. Just as interesting, however, the majority of my general education students noted a specific preference for the digital and several mentioned the same reason: the prohibitive cost of print books versus the open access nature of digital books. Having required my students to buy the Norton anthology made them particularly aware, as one student said, that “books tend to be expensive.”</p>
</blockquote>
<p>Another critical question involved the issue of navigating the edition itself. Jabr points to an experience many readers have when trying to recall where they saw something in an analog text as an example of one of the advantages of print books: “Both anecdotally and in published studies, people report that when trying to locate a particular piece of written information they often remember where in the text it appeared.” In addition, he notes,</p>
<blockquote>
<p>An open paperback presents a reader with two clearly defined domains — the left and right pages — and a total of eight corners with which to orient oneself. A reader can focus on a single page of a paper book without losing sight of the whole text: one can see where the book begins and ends and where one page is in relation to those borders… …In contrast, most screens, e-readers, smartphones and tablets interfere with intuitive navigation of a text and inhibit people from mapping the journey in their minds.<a class="footnote-ref" href="#jabr2013"> [jabr2013] </a></p>
</blockquote>
<p>It was for this reason that, in our digital edition, we decided to keep each chapter of <em>Letters</em> as a single document, rather than creating separate pages as one would in a book or in trying to mimic an e-book. At its most basic, this allowed students to scroll ahead and see how long the text was and how much more they had to read, something which is not as effective when clicking ahead through numerous, individual pages. This kind of grounding in terms of text length is one way that reading the digital text can be made reminiscent of reading on paper. One can also still orient oneself visually, since for the reader the single “page” chapter can be similar to a book page; instead of “eight corners with which to orient oneself,” the reader “can focus on a single page of a paper book,” even if that page is quite long<a class="footnote-ref" href="#jabr2013"> [jabr2013] </a>. While this grounding certainly proved effective with my students, one issue became paramount: without standardized pagination, we had no easy way to locate text for communal discussion in class. Despite the fact that the digital allows for searchability and the textual annotations provide visual landmarks in the text, there were no quick indicators to reference. This experience verified the need for those critical paragraph indicators as a way to maintain a shared sense of orientation, and it became clear that moving forward with the edition we would need to add these visual landmarks.</p>
<p>Beyond this issue of visual landmarks, one of our main concerns about Scalar was the visible presence of annotations in the form of notecard symbols and highlighted words or phrases, which could prove to be a distraction, snagging the eye and interrupting the reading process. Despite this initial concern, students overwhelmingly found the annotations to be “very helpful.” They noted that the annotations helped them “read faster,” provided context and definitions, and allowed them to stay focused. What was interesting, however, was that while students unanimously found the note-type annotations (notes linked directly to the text on the page) beneficial, they were more split on their assessment of the videos and hyperlinks (those links that took them to external pages). Some noted the videos as particularly illuminating while others found them to be distracting, taking them away from the flow of the text; they generally appreciated the videos and hyperlinks and wanted them to be available, but some suggested the links be offered, unlike the annotations, at the end of a section rather than integrated in the middle of the text itself. Interestingly, it seemed that while many students were accustomed to reading texts that contained visible hyperlinks and other signs of authorial “interference,” picking and choosing what they want to look into as they go along and ignoring the rest, others felt more compelled to engage with each interactive feature, which ultimately diverted them from the flow of the narrative.</p>
<p>Piloting the edition in class provided a couple of key indicators: it confirmed that the work we were doing on the edition itself was pedagogically valuable, and it offered some important feedback on how to improve the reading experience. What also became clear, however, was that — as valuable as students found the annotations and hyperlinks that we had created for them — the digital benefits of Scalar meant that students could move beyond being just readers of the text. The flip side of the “unified piece of scholarship” Eggert notes in print publications is that, with the digital, students could now interact with the text with more ease and explore their roles not just as passive recipients of the text but as interactive interpreters, as curators of the text<a class="footnote-ref" href="#eggert2009"> [eggert2009] </a>.</p>
<h2 id="editing-the-text-students-create-a-communal-version-of-_letters_--mary-m-balkun">Editing the Text: Students Create a Communal Version of <em>Letters</em> — Mary M. Balkun</h2>
<p>It was this role of students as “interactive interpreters, as curators of the text,” that I chose to explore in my approach to the Crèvecoeur project in two graduate classes in early American literature. The first, in spring 2016, was “Hybridity/ies in Early America” ; the other, in spring 2017, was “From New Netherland to New York.” In part, I chose a project format that took advantage of the advanced status of these students, but I was also interested in the communal potential of digital texts and seeing how this might be experienced with different groups of students. With a digital text, readers can actively participate in the construction of information, either creating new material or adding to material that already exists. In some cases, readers can edit the existing text ( <em>Wikipedia</em> is probably the best known example of this), and the platform keeps track of edits so that the text becomes a living history of change and response; other texts allow for commentary alone, but the comments still become part of the user experience for readers. Using Scalar for our edition of <em>Letters from an American Farmer</em> meant we actually had multiple options available. New material could be added to an existing text; an existing text could be elaborated upon or commented on; and these elaborations could themselves be edited. The project as I envisioned it would engage students in the creation of their own, communal version of <em>Letters</em> , one that would span two different classes, and possibly more in the future.</p>
<p>Given the advanced skill level of the students, having them actually annotate the text seemed approproiate; however, I did not want students adding annotations directly into our working digital edition of <em>Letters</em> . Fortunately, one of the benefits of Scalar is that it is easy to create additional books from existing material. This meant I could create an independent edition of the text specifically for the students, which would avoid potential problems of having someone accidentally change or remove material from our original edition, or our having to eradicate material afterwards. At the same time, I told the students that at the end of the process I would review their annotations and those of the highest quality could be incorporated into our edition, giving credit in the acknowledgements for their contributions. Since Scalar assigns a number to each editor of a text, it would be easy enough to determine the creator of any of the annotations. Thus, the students were potentially participating in our professional project, doing “real world” work instead of work simply for a class, and possibly adding to their scholarly credentials. Having students do their own annotations also introduced them to digital humanities skills and a tool, Scalar, that could prove useful for their work later on, especially for those who planned to pursue an advanced degree.</p>
<p>For the first course in which I included the digital editing project, “Hybridity/ies in Early America,” I set parameters that would make the work manageable and keep the stakes relatively low, especially since it was my first attempt to have students use Scalar. Each of the students in the course was assigned a 500- to 750-word selection of one of the letters to annotate. I did not specify the exact number of annotations they had to create; instead, they were advised to be as thorough as possible without overwhelming the text with commentary. Because <em>Letters</em> was to be the final text in the assigned readings for the semester, we would read their annotated version instead of the print edition I would normally have assigned. Since each assigned segment would vary in terms of the number of annotations that made sense, the students were advised that their work would be assessed on the quality of the information provided and whether essential words, phrases, or concepts had been explained and illustrated. They were also advised to think of their audience as undergraduates as opposed to other graduate students as they made decisions about what to annotate and how to do so. Finally, in another form of communal engagement, I incorporated an in-class peer-review session, which took place the week before we were to start reading their annotated version of <em>Letters</em> . Students were paired off to review each other’s work and provide formal feedback: Were the annotations clear? Were they factually and mechanically correct? Did all the links work? Had anything important been left out? Students then had the subsequent week to make revisions.</p>
<p>In order to prepare the students for the project and give them as much time as possible to complete their annotations, I conducted a Scalar workshop during the second class meeting (we met once each week). During the workshop, each student was asked to annotate the same passage from <em>Letters</em> ; we then compared the results to see what each person had decided to annotate, why, and how, so they could see the results of individual editorial choices. In addition to the hands-on workshop, I posted directions for the basic Scalar functions — how to create a notecard, how to create a web link — in the online “shell” for the course (Seton Hall University uses Blackboard as its learning management system). I started each class session by asking how the project was going and regularly included that question in my weekly email updates to the class. Because they were all working on the same text and with the same digital tool, I found that the students were able to help one another much of the time. I would come to class and one of them would be explaining how to credit a source or how to edit a video with Scalar. Thus, in addition to learning the challenges of textual editing they were also learning to work collaboratively, which is not the norm for literature scholars. In addition to giving the students familiarity with one aspect of the editing process and a useful web tool, my goal was to get them to think outside the box, to use as many different resources as possible in their annotations, and to be creative. They could include any material that would enrich the experience of reading <em>Letters</em> for others, but they also had to think about how much they annotated, especially given the potentially crowded viewing field in Scalar, with its notecard symbols and colored hypertext links.</p>
<p>Besides showing the students how to use Scalar, I had an opportunity to provide them with a professional perspective on scholarly editing. I invited Tiffany Potter, who had edited the University of Toronto print edition of another text we were reading for the course, <em>Ponteach, or the Savages of America: A Tragedy</em> , to a Skype interview with the class, which she graciously accepted. The students were able to ask questions about her editing process, about the kinds of challenges she had faced, and about how she had dealt with some of the problems they were facing: how much to annotate, how much to take for granted, and how best to explain complicated matters in a brief space.</p>
<p>When the time finally arrived to read <em>Letters</em> for the course, the students were able to do two things: first, they were able to better understand certain aspects of the text as a result of the annotations and to comment on their efficacy; second, they were able to compare their reading of the annotated as opposed to the unannotated parts of <em>Letters</em> (since they had not annotated the text in its entirety), which led to a productive discussion about the value of annotations and what they added (or not) to the reading process. Having a personal stake in this particular version of the text gave the students an additional investment in the reading and discussion. The project evaluations (I administered a separate one from the regular course evaluation) were uniformly positive. Some students were frustrated with Scalar, as one would expect with any new technology tool, but overall they were able to comment intelligently and even passionately about what it meant to edit a text; most of them agreed that it was something they could see themselves doing again. All of them agreed that they would never look at an edited edition of a text in quite the same way again.</p>
<p>I incorporated a modified version of this project into a subsequent graduate course in spring 2017, “From New Netherland to New York.” The primary reason for the changes I made was that I had several students in the class who had taken the prior early American graduate class. In order to provide a different experience for them, in this later course students were given the option to work on one of three different digital projects, with the annotation of <em>Letters</em> being one of those. In this iteration I expanded the amount of text to be annotated. Whereas the first class had been asked to annotate only a 500- to 750-word section of <em>Letters</em> , each student in the spring 2017 course who opted to do the annotation project was given a full letter to edit. However, rather than having this second group of students work on a new, “clean” version of the text, I had them use the same version of <em>Letters</em> that the spring 2016 students had annotated, adding yet another dimension of collaboration. Their first directive was to review the existing annotations made by the students in the previous class and make sure they were factually correct and that all the links worked; they were also required to document any changes they made and to explain the reason for any changes using the Comment function in Scalar.</p>
<p>Giving students options can increase their investment in the work they are asked to do, and the <em>Letters</em> project in the spring 2017 course bore this out. The students who opted for this project were exceptionally committed to it and to the quality of the material they added; they were also critical (in the best sense) of the annotations that had already been added by students in the spring 2016 course. This kind of digital text project generates a complex reading community consisting of 1) those who have read the text without being involved in creating the annotations but who have benefitted from the work others have done; 2) those who have commented on the annotations, thereby influencing future editorial decisions; and 3) those who have actually annotated the text. This latter group can then be divided yet again into those who annotate the text at a particular moment in time (i.e. spring 2016 versus spring 2017). Since <em>Letters</em> was an edition in progress, students had a chance to think critically about what others had chosen to add to the text, change what was done previously, and then add to it themselves. They thus became curators of the text as opposed to simply readers, and, hopefully, readers with a different relationship to the material. Students involved in this type of project also gain a new awareness of audience, those for whom the annotations are being created, and have to ask of their work: What do those readers need to know? How much information is too much? How can the information best be conveyed? The result is a text that is interactive in the best possible sense. The annotations reflect what students themselves thought it would be useful to know as they and other students read, and it provides information for readers like themselves about terms, historical references, and cultural references. This graduate student edition remains a text that can be added to and updated by other classes, providing a communal experience for future students as well. Most importantly, being able to have students participate in the annotation process gives them a different level of engagement with the text, and it can ultimately — we hope — make them more active participants in a scholarly community. Finally, if their annotations are included in our “official” online edition of <em>Letters</em> , the students can be understood to be participating in an even larger community of readers and helping to “author” a text that is organic in the way it is shaped and develops over time.</p>
<h2 id="curating-the-text-students-create-a-digital-critical-edition--diana-h-polley">Curating the Text: Students Create a Digital Critical Edition — Diana H. Polley</h2>
<p>As a result of the various ways in which our students had both used the Crèvecoeur edition and participated in developing new digital content, I was encouraged to consider yet another type of digital exercise as a way to explore the possibilities of digital texts. As I was scheduled to teach an upper-level American Literature Seminar in spring 2018, I decided to use the opportunity to have my students engage in a semester-long project dedicated to constructing their own short digital critical editions using Scalar. I chose not to have students work on Crèvecoeur’s <em>Letters from an American Farmer</em> for several reasons. Most importantly, some students in my seminar had taken the Early American Literature course I had taught that previous fall when I piloted the Crèvecoeur digital edition. While the Crèvecoeur edition would prove to be a good model for them, I was concerned that if they were also asked to construct a digital edition of <em>Letters</em> , there would be a temptation to replicate elements of the edition we were producing; I wanted them to enjoy the freedom of a “ tabula rasa. ” In addition, my students were undergraduates, and I knew that the exercise itself would be particularly challenging. Given the difficulty of Crèvecoeur’s writing, I felt that choosing a more approachable early-American writer — both in form and theme — would be a better option. Therefore, I assigned them, instead, a choice of Benjamin Franklin essays. Over the course of fifteen weeks, my students were asked to incorporate their own selection of key Franklin essays into a digital edition that would include: digital annotations within the text (and related ancillary materials, such as maps, word clouds, videos, etc.); a Note on the Text and list of Emendations; an Introduction; and an Annotated Bibliography. What I realized was how quickly I had to adjust my expectations, not so much in terms of the digital but rather in terms of what my students understood about more traditional concepts associated with the print form (e.g. the critical edition and textual criticism). For example, while I was initially concerned about teaching my students the Scalar platform, I found they picked up the basics with ease. What they struggled with, however, was the very concept of what a critical edition <em>is</em> . In an age where raw, unedited literary texts are more readily available online, students seemed totally unfamiliar with the model of the critical edition and the editor’s role in textual production. Even more than the critical edition itself, students were utterly confused by the study of textual criticism. The notion that what they read is not automatically the “authoritative text,” that there may be versions that are more or less authentic, that an editor may have emended the text, and that as scholars their responsibility is to choose the most valid textual source: all of these ideas seemed foreign to them. When we began working on emendations, one student said: “Wait, I’m supposed to edit Benjamin Franklin? I can’t play God. I can’t do this.” After repeated discussions about textual criticism, I was relieved to hear this “aha!” moment: they were beginning to realize the import of what I was asking them to do.</p>
<p>Given my students’ confusion and their own uncertainty, asking them to be “start-to-finish” curators of a digital edition was particularly unnerving. The idea that my students would be using Yale’s Franklin Collection (what we had chosen as the starting point for the “authoritative text” ), and then — as undergraduates — emending, annotating, and digitally “publishing” that work, highlighted the concerns Eggert notes in comparing print and digital publishing forms and underscores the ease with which texts can be manipulated and endlessly altered online. That said, having my students create their own editions from beginning to end — creating a “Note on the Text,” emending that text and constructing an “Emendations” page, including an Introduction, providing annotations and hyperlinks, and offering an Annotated Bibliography — allowed them the chance not only to actively engage with and manipulate the text using the digital but also, ironically, to understand the traditional models associated with the print form. First, they were able to get a sense of the kind of detailed and pain-staking work that goes into textual criticism and, more philosophically, to ask about the ontological relationship between author and editor in the construction of the text. Second, they were able to locate that “state of prolonged anxiety” Eggert notes when trying to complete a print edition<a class="footnote-ref" href="#eggert2009"> [eggert2009] </a>. This project was not just another college exercise. Their work mattered. And I made clear that because Scalar was an “open source online and publishing platform” any edition they constructed for my class they could ultimately make public and even searchable for portfolios and jobs. They would be responsible for the work they produced, beyond the classroom. In many ways, therefore, this digital assignment gave students the opportunity to experience professionalism and the kind of high-stakes perfectionism that Eggert associates with print rather than digital publishing.<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup></p>
<h2 id="conclusion">Conclusion</h2>
<p>Our various experiences with digital critical editions have made us even more convinced of their value, whether for scholarship or for teaching or both. The Crèvecoeur edition has proven beneficial for precisely those reasons we set out to produce it: it would not only make <em>Letters from an American Farmer</em> available to students and a larger public audience, but it would, as importantly, make the text <em>accessible</em> to that audience. In the process, the digital medium would avoid current cost concerns from the publisher and the buyer associated with the print market and open up new ways for the reader to engage with the text and its context through interactive technological tools.</p>
<p>As our experience has revealed, however, there are several caveats when it comes to starting a digital edition. For one, an edition needs a repository, a stable place for the text to reside. While we have opted for Scalar, and have confidence that the tool will be functioning for some time to come, there are still risks that a non-profit platform like Scalar might lose its funding or that platform support may be limited. Given this inherent lack of stability, it is important to have a strategy for dealing with changes in tools, platforms, and applications. It also seems critical that, moving forward, more repositories are created for such editions. As discussed earlier, archives do exist, although they seem either to be reserved for a single author ( <em>The William Blake Archive</em> , for example) or the work of certain editors ( <em>Just Teach One</em> , for example). No doubt there are some clearinghouses that we simply could not locate. Ideally, however, any repository for such digital editions will be easily located and easily accessible by and for public readership, in much the way that archives for unedited texts currently are, such as <em>Project Gutenberg</em> or the <em>Full Text Archive</em> . To be effective, we would argue, such repositories should also — by nature — be open-source.</p>
<p>Of course, the most important lesson we have learned from our own experience creating a digital critical edition is the time it takes to engage in these projects (and, of course, this might account for their limited availability). Beyond helping to define and distinguish the medium, Sahle’s discussion of digitized vs digital editions helps explain the complexity involved in their creation; “browsing paths,”  “real hyperlinks,” and “integrated technical tools” all require — for their survival — the interdependence of other “real,” dynamic paths<a class="footnote-ref" href="#sahle2016"> [sahle2016] </a>. The nature of this interdependence means that the initial construction is time-consuming, as is its upkeep. While the original text may remain stable, the apparatus constructed does not and for this reason the result is a digital edition that is forever in flux.</p>
<p>This flux, what many critics note as its instability, is disconcerting; however, it can also be seen as a pedagogical and scholarly boon. The analog text is certainly more stable. Conversely, that stability comes at a price: it isn’t long before the analog edition becomes outdated, and what was the definitive version — with the most current theoretical essays and contextual material — is now out of step with contemporary and even cutting edge critical thought. The digital critical edition, because it can contain both past and present materials, by contrast, represents a host of perspectives across time. The reader can then see both where the text has been and where it is going, and contribute to those new directions. Thus, while this lack of stability creates fundamental challenges — increased initial time and continued labor and maintenance — its mutability also helps to ensure its continued relevance.</p>
<p>Finally, what we have found is the inherent communal nature — both for our students and for ourselves — in these types of digital projects. Ideally, the digital edition can reflect various perspectives. It can reflect the current cultural moment, it can retain historical elements, and it can incorporate the work of scholars at various stages in their learning. The edition incorporates, as Sahle says, “integrative technical tools” and thus is by its nature integrative; it integrates the perspectives of communities of contexts and communities of scholars<a class="footnote-ref" href="#sahle2016"> [sahle2016] </a>. Therefore, it is not only important to consider how much time such a process will take but to find good partners to work with. We were fortunate to have been brought together at that 2011 NeMLA conference, and that our interest and work styles are so similar. Not everyone starting a project will be so lucky. For now, we plan to finish the Crèvecoeur edition, continue to explore future digital humanities pedagogical opportunities with students, and look ahead to possible future collaborative projects, perhaps housed at a site of our own making.</p>
<ul>
<li id="scalar"> “About Scalar.” The Alliance for Networking Visual Culture:<a href="scalar.me/anvc/scalar/">scalar.me/anvc/scalar/</a>(accessed 28 Dec. 2018).
</li>
<li id="transcendentalism"> “Authors and Texts of American Transcendentalism.”  _American Transcendentalism Web_ :<a href="transcendentalism-legacy.tamu.edu/authors/index.html">transcendentalism-legacy.tamu.edu/authors/index.html</a>(accessed 28 Dec. 2018).
</li>
<li id="crevecoeur2018">Crèvecoeur, J. _Letters from an American Farmer_ . (ed. M. Balkun and D. Polley). Scalar: Alliance for Networking Digital Culture (2018):<a href="scalar.usc.edu/works/crevecoeur--letters-from-an-american-farmer/index">scalar.usc.edu/works/crevecoeur--letters-from-an-american-farmer/index</a>(accessed 28 Dec. 2018).
</li>
<li id="deegan2009">Deegan, M. and Sutherland, K. (2009). “Introduction.” In _Text Editing, Print and the Digital World_ . (ed. K. Sutherland and M. Deegan), 1-9. New York: Routledge.
</li>
<li id="thoreau"> _Digital Thoreau_ , Digital Thoreau,<a href="digitalthoreau.org/">digitalthoreau.org/</a>(accessed 28 Dec. 2018).
</li>
<li id="medieval"> _Digital Library of Medieval Manuscripts_ . Johns Hopkins Sheriden Libraries,<a href="dlmm.library.jhu.edu/en/digital-library-of-medieval-manuscripts/">dlmm.library.jhu.edu/en/digital-library-of-medieval-manuscripts/</a>(accessed 28 Dec. 2018).
</li>
<li id="eaves">Eaves, M., Essick, R. and Viscomi, J. _The William Blake Archive_ :<a href="www.blakearchive.org/">www.blakearchive.org/</a>(accessed 28 Dec. 2018).
</li>
<li id="eggert2009">Eggert, P. (2009). “The Book, the E-text and the Work Site. ” In _Text Editing, Print and the Digital World_ . (ed. K. Sutherland and M. Deegan), 63-82. New York: Routledge.
</li>
<li id="faherty">Faherty, D. and White, E. ed. _Just Teach One, Common-Place: The Journal of Early American Life_ :<a href="jto.common-place.org/">jto.common-place.org/</a>(accessed 28 Dec. 2018).
</li>
<li id="folsom">Folsom, E. and Price, K. ed. _The Walt Whitman Archive_ , Center for Digital Research in the Humanities at the University of Nebraska — Lincoln:<a href="whitmanarchive.org/">whitmanarchive.org/</a>(accessed 28 Dec. 2018).
</li>
<li id="gold2012">Gold, M. K. ed. (2012). _Debates in the Digital Humanities_ . Minneapolis: U of Minnesota P.
</li>
<li id="gold2016">Gold, M. K. and Klein L. F. ed. (2016). _Debates in the Digital Humanities_ . Minneapolis: U of Minnesota P.
</li>
<li id="gold2019">Gold, M. K. and Klein L. F. ed (2019). _Debates in the Digital Humanities_ . Minneapolis: U of Minnesota P.
</li>
<li id="hirsch2012">Hirsch, B. (2012) “Introduction.” In _Digital Humanities Pedagogy: Practices, Principles and Politics_ (ed. B. Hirsch), 3-30. UK: Open Book Publishers.
</li>
<li id="shakespeare"> _Internet Shakespeare Editions_ . Internet Shakespeare Editions, 6 Feb. 2015,<a href="internetshakespeare.uvic.ca/">internetshakespeare.uvic.ca/</a>(accessed 28 Dec. 2018).
</li>
<li id="jabr2013">Jabr, F. (2013). “The Reading Brain in the Digital Age: The Science of Paper versus Screens.”  _Scientific American_ 11:<a href="www.scientificamerican.com/article/reading-paper-screens/">www.scientificamerican.com/article/reading-paper-screens/</a>(accessed 28 Dec. 2018).
</li>
<li id="janeausten"> _Jane Austen Fiction Manuscripts_ . Jane Austen Fiction Manuscripts.<a href="janeausten.ac.uk/index.html">janeausten.ac.uk/index.html</a>.
</li>
<li id="jewell">Jewell, A., ed. _Willa Cather Archive_ , Center for Digital Research in the Humanities at the University of Nebraska — Lincoln:<a href="cather.unl.edu/">cather.unl.edu/</a>(accessed 28 Dec. 2018).
</li>
<li id="kumar2015">Kumar, D. Vinay; Kumar, B. T. Sampath; Parameshwarappa, D. R. ed. (2015). “URLs Link Rot: Implications for Electronic Publishing.”  _World Digital Libraries_ . 8 (1): 59-65.<a href="https://search-proquest-com.ezproxy.shu.edu/docview/1709560370?accountid=13793">https://search-proquest-com.ezproxy.shu.edu/docview/1709560370?accountid=13793</a>(accessed 8 Feb. 2020).
</li>
<li id="murphy2017">Murphy, E. C. and Smith, S. R. ed. (2017) “Imagining the DH Undergraduate: Special Issue in Undergraduate Education in DH,”  _Digital Humanities Quarterly_ , 11 (3):<a href="http://www.digitalhumanities.org/dhq/vol/11/3/000334/000334.html">http://www.digitalhumanities.org/dhq/vol/11/3/000334/000334.html</a>(accessed 8 Feb. 2020).
</li>
<li id="omekaclassic"> “Omeka Classic Directory.”  _Omeka_ , Roy Rosenzweig Center for History and New Media,<a href="omeka.org/classic/directory/">omeka.org/classic/directory/</a>(accessed 29 Oct. 2019).
</li>
<li id="omeka"> “Omeka.”  _Omeka_ , Roy Rosenzweig Center for History and New Media,<a href="omeka.org/">omeka.org/</a>(accessed 29 Oct. 2019).
</li>
<li id="openaccess"> “Open Access Directory.”  _Open Access Directory_ , Simmons University<a href="oad.simmons.edu/oadwiki/Main_Page">oad.simmons.edu/oadwiki/Main_Page</a>(accessed 29 Oct. 2019).
</li>
<li id="pierazzo2015">Pierazzo, E. (2015) _Digital Scholarly Editing: Theories, Models and Methods_ . New York: Routledge.
</li>
<li id="potter2010">Potter, T. ed. (2010) _Ponteach, or the Savages of America: A Tragedy_ . Toronto: U of Toronto P.
</li>
<li id="pressman2013">Pressman, J. and Swanstrom, L. ed. (2013). “Introduction.”  “The Literary And/As the Digital Humanities”  _DHQ_ 7 (1):<a href="digitalhumanities.org:8081/dhq/vol/7/1/000154/000154.html">digitalhumanities.org:8081/dhq/vol/7/1/000154/000154.html</a>. (accessed 8 Feb. 2020).
</li>
<li id="gutenberg"> _Project Gutenberg_ . Project Gutenberg.<a href="www.gutenberg.org/wiki/Main_Page">www.gutenberg.org/wiki/Main_Page</a>(accessed 19 Dec. 2018).
</li>
<li id="sahle2016">Sahle, P. (2016). “What is a Digital Scholarly Edition?” In _Digital Scholarly Editing: Theories and Practices_ (Ed. M. James and E. Pierazzo), 19-40. UK: Open Book.
</li>
<li id="shillingsburg2009">Shillingsburg, P. (2009). “How Literary Works Exist: Convenient Scholarly Editions”  _DHQ_ 3 (3):<a href="digitalhumanities.org:8081/dhq/vol/3/3/000054/000054.html">digitalhumanities.org:8081/dhq/vol/3/3/000054/000054.html</a>(accessed 8 Feb. 2020).
</li>
<li id="sutherland2009">Sutherland, K. (2009). “Being Critical: Paper-based Editing and the Digital Environment.”  _Text Editing, Print and the Digital World_ . (Eds. K. Sutherland and M. Deegan), 13-25. New York: Routledge.
</li>
<li id="wasteland"> _The Wasteland: T.S. Elliot’s Masterpiece_ . TouchPress (2017):<a href="thewasteland.touchpress.com/">thewasteland.touchpress.com/</a>(accessed 28 Dec. 2018).
</li>
<li id="whitley2018">Whitley, E. and Weidman, R. (2018). _The Vault at Pfaff’s: An Archive of Art and Literature by the Bohemians of Antebellum New York_ , Lehigh University:<a href="pfaffs.web.lehigh.edu/">pfaffs.web.lehigh.edu/</a>(accessed 28 Dec. 2018).
</li>
</ul>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Since this time, Harvard University Press has published (2013) a scholarly edition of <em>Letters from an American Farmer</em> (accompanied by thirteen additional Crèvecoeur pieces), edited and compiled by Dennis D. Moore. While the edition has provided an important addition to Crèvecoeur scholarship, as it was written for and marketed almost exclusively towards academics and does not include ancillary teaching materials (e.g. annotations, additional primary and secondary sources, and contextual frameworks), the text was still not the type of traditional teaching edition we refer to here.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Although not necessarily within the scope of this discussion, the question of how much academic credit is given to these types of projects towards tenure and promotion is an important one and is directly related to the feasibility of completing such work. If the academy hopes to foster the growth of the digital humanities, particularly these types of open source public projects, we need to consider ways to integrate these projects into a peer-review process to substantiate their academic and pedagogical value and give them credence for tenure and promotion review.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>I am attaching here a link to an edition my student created for the seminar and which is currently “published” ” on Scalar:<a href="http://scalar.usc.edu/works/mayberry/index">http://scalar.usc.edu/works/mayberry/index</a>## Bibliography&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry><entry><title type="html">Imagining the Continuously Present Past: Visualizing William Faulkner’s Narratives and Digital Yoknapatawpha</title><link href="https://startwords.cdh.princeton.edu/vol/15/2/000548/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://startwords.cdh.princeton.edu/vol/15/2/000548/</id><author><name>Johannes Burgers</name></author><published>2021-06-15T00:00:00+00:00</published><updated>2023-08-04T13:01:02-04:00</updated><content type="html"><![CDATA[<h1 id="heading"></h1>
<h2 id="introduction">Introduction</h2>
<p>A story, simply put, is a sequence of events. This definition has been in place at least as early as Aristotle’s <em>Poetics</em> . What is much less simple to define is the constituent terms: sequence and events. Accordingly, schools of narrative theory have conceptualized events and their connection to sequentiality from radically divergent vantage points, ranging from defining the varied shapes of the sequence, their value at a semantic level, with regard to events’ salience to the overall plot, their relationship with narrative point of view, and the interpretation of events by the reader, just to name a few<a class="footnote-ref" href="#frawley1992"> [frawley1992] </a><a class="footnote-ref" href="#genette1972"> [genette1972] </a><a class="footnote-ref" href="#herman2012"> [herman2012] </a><a class="footnote-ref" href="#labatt2005"> [labatt2005] </a><a class="footnote-ref" href="#leitch1986"> [leitch1986] </a><a class="footnote-ref" href="#reed1973"> [reed1973] </a>.<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> Indeed, as more recent work on narrative shows, what narratives are and how they can be decomposed is anything but a settled matter<a class="footnote-ref" href="#baroni2016"> [baroni2016] </a><a class="footnote-ref" href="#hansen2017"> [hansen2017] </a><a class="footnote-ref" href="#phelan2017"> [phelan2017] </a><a class="footnote-ref" href="#richardson2019"> [richardson2019] </a>.</p>
<p>Despite these definitional challenges, researchers have been productively using narratological concepts to analyze texts since the early days of computing. In his exhaustive and insightful overview of the state of the field, <em>Computational Modelling of Narrative</em> , Inderjeet Mani covers the various approaches taken by computer scientists to decompose and generate narratives, while introducing an innovative mark-up language of his own: NarrativeML<a class="footnote-ref" href="#mani2013"> [mani2013] </a>. With applications in language domains far outside literary studies and poised to disrupt everything from economics to medicine to forensics, work within the field of computational narratology has continued at break-neck speed. One recent work even claims that computational narrative generation has arrived at the moment of post-narratology<a class="footnote-ref" href="#ogata2019"> [ogata2019] </a>.</p>
<p>Despite this substantial scholarly footprint and exciting advances, “[t]he flow of influences has historically been from narratolology to computation,” as Mani already lamented in 2013<a class="footnote-ref" href="#mani2013"> [mani2013] </a>. He lists narratologists’ concern with discourse over fabula, and the relatively rudimentary quality of narrative generation, as causes of this one-way traffic. In addition to these, two large deterrents to the more widespread adoption of methods in computational narratology are the inter-related issues of scale and scalability. The first denotes the semantic level at which events are parsed. To be insightful for narratology, texts need to be broken down into meaningful narrative units that are more capacious than a predicate and more precise than a summary. Relatedly, the process needs to be scalable to a wide-range of texts to facilitate for insightful comparison. Up until now, the challenging trade-off has been that solutions at a scale appropriate for narratological analysis are not scalable because they require laborious human intervention, and unsupervised solutions are scalable but not at the right scale for narratological analysis. This is perhaps why projects that try to innovate narratology through computational methods are often mired in the proof-of-concept phase. Disappointingly, this has meant that the potential of all this groundbreaking work remains untapped by the larger community of literary scholars.</p>
<p>One single-author study that has managed to scale by dint of years of laborious coding, consistent revision, and substantial institutional support is theDigital Yoknapatawphaproject (hereafterDY). Hosted out of the University of Virginia,DYwas created through the hard work of over thirty Faulkner scholars putting in thousands of hours to code all of the locations, characters, and events in his Yoknapatawpha fictions into a relational database. Nearly a decade in the making, the site enables students, teachers, and scholars to explore fourteen novels and fifty-six short stories through a series of “deep atlases” based on maps Faulkner drew in 1936 and 1945 (<a href="http://faulkner.iath.virginia.edu/">http://faulkner.iath.virginia.edu/</a>). The main interface is supplemented by a wealth of materials including: manuscripts, archival audio, historical photographs, textual commentaries, and other data visualizations. Though the project will continue to grow,DYis now robust enough to use as a scholarly tool, and several members of the team have already leveraged it to highlight new aspects of Faulkner’s writing<a class="footnote-ref" href="#burgers2020"> [burgers2020] </a><a class="footnote-ref" href="#railton2015"> [railton2015] </a><a class="footnote-ref" href="#robbins2016"> [robbins2016] </a>.</p>
<p>The large tranche of highly-curated narrative data made available throughDYoffers an opportunity for computational narratologists to generate hereunto unimagined visualizations of narrative. This paper has far more modest ambitions. Instead, it contends that the most productive and intuitive approach to visualizing the shape of narrative is one that displays chronological order versus story order. This approach was actually devised by the Russian Formalists at the beginning of the twentieth century, and has been returned to time and again by digital humanists. By drawing on visualizations created forDigital Yoknapatawpha(<a href="http://faulkner.iath.virginia.edu/narrativeanalysis.html">http://faulkner.iath.virginia.edu/narrativeanalysis.html</a>) and other adjacent digital work, I demonstrate the comparative power of such charts and the scalability of the method across different types of fictional texts. As such, the goal of this paper is to open up a larger conversation between digital humanists and narrative theorists, and consider the best practices for translating narratological concepts into encoded narrative data that can be productively visualized. A principal part of this conversation is the necessity of establishing a meaningful and shared visual language that clearly represents fundamental narrative concepts to a broader audience. Much of this language has already been in place for some time, but it has been scattered across different knowledge domains.</p>
<h2 id="bridging-humanities-and-computational-narratology">Bridging Humanities and Computational Narratology</h2>
<p>Historically, there have been two important observations about narrative. The first, by Aristotle, is that a narrative has a beginning, middle, and an end, and that these are usually connected through causality. The second, by the Russian Formalists, is the separation of thefabula(story material) from thesyuzhet(the way the author shapes the material), an insight mirrored concurrently by E.M. Forster who more strictly defines the separation as one between story and plot<a class="footnote-ref" href="#baroni2016"> [baroni2016] </a><a class="footnote-ref" href="#forster1927"> [forster1927] </a><a class="footnote-ref" href="#shklovskii1990"> [shklovskii1990] </a>. As self-evident as these two distinctions are, operationalizing them into a hermeneutic for textual analysis is fraught with problems. As Gerald Prince aptly points out, “Narrative sequences…are semantic and not semiotic in nature” <a class="footnote-ref" href="#prince2016"> [prince2016] </a>. That is to say, narrative sequences are not easily broken down into constituent parts based on linguistic and logical properties. The scope and length of individual narrative events are influenced by a whole host of factors, including rhetorical devices, extra-textual and inter-textual connections, figures, tropes, irony, narrative frequency, narrative speed, narrative authority, and narrative reliability to name a few<a class="footnote-ref" href="#prince2016"> [prince2016] </a>. In this, it is hard to ignore the lessons of deconstruction and post-structuralist narratology<a class="footnote-ref" href="#fludernik2005"> [fludernik2005] </a>. Any attempt to grab a single narrative thread tugs at the entire warp and woof of a text’s intertextuality. Delimiting a text into discrete units always imposes an artificial structure from without. This is to say nothing of the problems that arise when decomposing narrative if the experience of the reader is considered. After all, from a functionalist approach, narrative needs a reader to (re)-constitute it, and, therefore, narrative sequence is not a type of “deep structure,” but rather something that exists in discourse. John Pier observes that for functionalists, “sequence is assimilated into the broader question of intersequentiality and the dynamic relations occurring between the telling/reading and the told” <a class="footnote-ref" href="#pier2016"> [pier2016] </a>. Which is to say, the sequencing of events in a text does not exist external to a reader. Thus, while it might be self-evident that a narrative sequence is a series of events, what constitutes those events and how they are constituted is remarkably difficult to pin down.</p>
<p>Translating narratological insights into computational methods poses a substantially different set of challenges. A major hurdle is computationally reproducing the cognitive faculties that allow human readers to understand texts with remarkable sophistication and accuracy. To decompose a narrative at any level of competency, a computer has to perform a series of inter-related and complex tasks including: natural language processing<a class="footnote-ref" href="#delmonte2017"> [delmonte2017] </a><a class="footnote-ref" href="#muzny2017"> [muzny2017] </a><a class="footnote-ref" href="#xie2019"> [xie2019] </a>, spatial organization [Tenen 2018], narrative parsing<a class="footnote-ref" href="#bartalesi2016"> [bartalesi2016] </a><a class="footnote-ref" href="#bogel2014"> [bogel2014] </a><a class="footnote-ref" href="#leonid2017"> [leonid2017] </a><a class="footnote-ref" href="#wallace2012"> [wallace2012] </a>, and understanding character entities<a class="footnote-ref" href="#barros2019"> [barros2019] </a><a class="footnote-ref" href="#nijila2018"> [nijila2018] </a>, to name but a few. Moreover, much of the work that has been done on the computational understanding of narrative falls outside the ambit of literary studies, and has focused largely on corpora within specific and, often, bounded knowledge domains, including, among others, economics<a class="footnote-ref" href="#lakoff2010"> [lakoff2010] </a>, law enforcement<a class="footnote-ref" href="#baber2011"> [baber2011] </a>, medicine<a class="footnote-ref" href="#focilarias2018"> [focilarias2018] </a>, education<a class="footnote-ref" href="#gutierrez2018"> [gutierrez2018] </a>, legal studies [Mahfouz et al. 2018], and reconstructing news narratives<a class="footnote-ref" href="#seonwoo2018"> [seonwoo2018] </a>. This research is valuable for the insights it offers, but is not directly applicable to literary studies, because so much of literary production traverses multiple knowledge domains and deliberately subverts anticipated text structures. More simply, it is more probable that the language, style, and format of two fictional texts are more dissimilar than two medical reports, legal briefs, or, even news reports. This dissimilarity makes it challenging for an unsupervised computational approach to establish and detect patterns that can be iterated across a large corpus with a high degree of consistency.</p>
<p>Still, there have been significant attempts to generate a functional model for doing narrative analyses using digital methods. For example,PlotViswas a tool developed by a team at the University of British Columbia. It visualized narratives encoded in Extensible Mark-up Language (XML) and could be “customized by the teachers and students in order to accommodate various interpretations of a single piece of fiction”<a class="footnote-ref" href="#brown2013"> [brown2013] </a>. Operational from 2013-2016,heureCLÉAused human-annotation and machine learning techniques to produce a corpus of 21 annotated short stories, it was supplemented by the textual annotation and analysis tool “CATMA” <a class="footnote-ref" href="#meister"> [meister] </a>. Using the data from heureCLÉA, another team designedNarrelations, an application that visualizes multiple levels of narrative<a class="footnote-ref" href="#schwan2019"> [schwan2019] </a>. Meanwhile, Mark Finlayson created theProppLearnercorpus by annotating fifteen folk tales. Once annotated, the training data was used to analyze the morphology of different folktales using Propp’s method. Yet, even here a significant amount of human intervention was required to make sure that events were parsed properly<a class="footnote-ref" href="#finlayson2017"> [finlayson2017] </a>. Indeed, as exciting as the results of the study were, the data collection process was necessarily labor-intensive and costly<a class="footnote-ref" href="#finlayson2017"> [finlayson2017] </a>.<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup></p>
<p>Other recent investigations of narrative have relied more on natural language processing or some form of machine learning to aid with the parsing of text. Among these is the aptly named, “Syuzhet” package, developed by Matthew Jockers and available on CRAN (Comprehensive R Archive Network). The package allows users to draw on four different sentiment dictionaries to score the overall sentiment of a text or to chart how “positive and negative sentiments are activated across the text” <a class="footnote-ref" href="#jockers2017"> [jockers2017] </a>. The advantage of this approach is that text analysis can be automated. While Jockers’s work has sparked a lot of interest, it has also received some criticism<a class="footnote-ref" href="#swafford2015"> [swafford2015] </a>. One issue is that his use of the word “plot” has no bearing on the concept of chronological order of events, but rather how different sentiment structures are articulated across a text. An alternative approach taken by Koichi Takeuchi uses a predicate-argument structure thesaurus to determine narrative states, actions, and change-in-states. This allows for “multi-dimensional relations between predicates with their arguments containing relations between the change-of-state and its goal” <a class="footnote-ref" href="#takeuchi2016"> [takeuchi2016] </a>. Though Takeuchi’s study is geared towards automatic narrative generation, it can also be used to decompose a narrative. Being able to detect a change-in-state in a predicate is useful for understanding the syntactic building blocks of a narrative, but is too granular for addressing narrative changes in a text for narratological analysis. Another highly promising system under development isYarn, which uses Hierarchal Task Network (HTN) planning to generate visualizations of possible storylines<a class="footnote-ref" href="#padia2019"> [padia2019] </a>. Still, here too the plot composition is presently too crude to be able to meaningfully differentiate between narratives for the purposes of narratology.</p>
<p>Finally, there has been a spate of projects that use film as a basis of narrative analysis. As Eric Hoyt et al. explain, “[o]f all narrative forms, the motion picture screenplay may be the most perfectly pre-disposed for computational analysis” <a class="footnote-ref" href="#hoyt2014"> [hoyt2014] </a>. This is because the text is already semi-structured with characters identified by dialogue and narrative units separated into scenes. Since the process of narrative analysis of scripts lends itself to automation, a number of analogous projects have emerged independent of one another that generate a visualization of narrative progression over time. These include work done by Sharma and Rajamanickam<a class="footnote-ref" href="#sharma2013"> [sharma2013] </a>,ScripThreads<a class="footnote-ref" href="#hoyt2014"> [hoyt2014] </a>, andStory Curves<a class="footnote-ref" href="#kim2018"> [kim2018] </a>. The underlying assumption with all of these projects is that the script scene sequence is consonant with the plot event sequence. From a strict narratological perspective this need not always be the case. Events can also be conveyed to the audience through diegetic and extradiegetic elements, as, for example, when a character reveals his or her backstory through dialogue with another character, or when there is a voice-over that relates past or future events. Presumably, these events occur at a different date than the chronological order of the scene. To account for this discrepancy in any automated parsing method adds a substantial level of complication. While this by no means invalidates such projects, it merely underscores that the question of the appropriate scale of a narrative event is still unsettled.</p>
<p>In sum, the twinned foundational challenges facing a more widespread adoption of computational narrative analysis are those of scale and scalability. The projects that rely on manual encoding are generally fine-grained enough to provide consistent narratological insight, but the labor involved does not scale well. Meanwhile, automated methods tend to be too course-grained to allow for meaningful comparison between texts, or, conversely, they provide analysis at the level of the predicate, which is not functional for narrative analysis. As Prince points out, “narratologists agree that narrative sequences represent linked series of situations and events and fur¬ther agree that these sequences can be expanded or summarized, that they can be combined with other sequences in specifiable ways such as conjunc¬tion, embedding, or alternation, and that they can be extracted from larger sequences” <a class="footnote-ref" href="#prince2016"> [prince2016] </a>. This type of narrative modularity can only be achieved if event units contain more than predicate-level information, and account for a unified ontology of space, time, and character within an event, while at the same time being more specific than a summary of the text. While there is no precise determination as to what this scale might be, the method developed forDYprovides a flexible and reproducible framework.</p>
<h2 id="punctuating-the-long-sentence-event-driven-narrative-encoding-in-faulkner">Punctuating the Long Sentence: Event-Driven Narrative Encoding in Faulkner</h2>
<p>Faulkner Studies has always had a special relationship with narratology. Because Faulkner’s texts are so narratively intricate, one consistent topic of exploration has been dis-entangling his plot lines and understanding his use of time, so much so that it is somewhat of a cottage industry<a class="footnote-ref" href="#going1958"> [going1958] </a><a class="footnote-ref" href="#harris1993"> [harris1993] </a><a class="footnote-ref" href="#inge1970"> [inge1970] </a><a class="footnote-ref" href="#nebeker1971"> [nebeker1971] </a><a class="footnote-ref" href="#perry1979"> [perry1979] </a><a class="footnote-ref" href="#reed1974"> [reed1974] </a><a class="footnote-ref" href="#schwab1991"> [schwab1991] </a><a class="footnote-ref" href="#stewart1958"> [stewart1958] </a><a class="footnote-ref" href="#volpe2003"> [volpe2003] </a><a class="footnote-ref" href="#wilson1972"> [wilson1972] </a>. With the advent of digital technologies this exploration has continued unabated. John Padgett’s “William Faulkner on the Web” still provides a rich resource for Faulkner novices and experts alike [Padgett]. “The Sound and the Fury: a Hypertext Edition” by Stoicheff, Muri, Deshaye, et al. was a highly innovative project that tackled the problem of visualizing one of Faulkner’s most challenging narratives as early as 2003<a class="footnote-ref" href="#stoicheff"> [stoicheff] </a>. BeforeDY, the most sophisticated visualization of a Faulkner narrative was actually the 2003 Adobe Flash-based chronology of <em>Absalom, Absalom!</em> created by current DY director, Stephen Railton<a class="footnote-ref" href="#railtonrourk2003"> [railtonrourk2003] </a>.<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup></p>
<p>In its sheer openness and scale,DYsupersedes these early Faulknerian projects, while simultaneously being heavily indebted to them. The data currently available represents nearly five-thousand character records, over two thousand locations, and more than eight thousand events, each with their own individual attributes. Aggregated, these data tables represent around a quarter-million data points across several dozen different data fields.<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup> This data drives the main interface, but can also be used to create alternative visualizations. For example, Raphael Alvarado designed a platform to generate force directed graphs that show the co-occurrence of characters and locations<a class="footnote-ref" href="#alvarado2018"> [alvarado2018] </a>.</p>
<p>Included in this data set are also a number of variables that allow for the study of Faulkner’s narrative: chronology, narrative status, and event dates. In order to arrive at these more abstracted data points, all of Faulkner’s Yoknapatawpha fictions had to be entered into a relational database containing the entities: “Text,”  “Locations,”  “Characters,” and “Events.”  “Texts” contains the individual “Locations” and “Characters” for that text, and “Events” are the combination of a character or characters at a location for a unified action.<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup> An overview is provided in the entity relationship diagram below (see Figure 1):<br>




























<figure ><img loading="lazy" alt="Chart showing relationship between entities." src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Entity Relationship Diagram Digital Yoknapatawpha DatabaseEntity Relationship Diagram <em>Digital Yoknapatawpha</em> Database
        </p>
    </figcaption>
</figure></p>
<p>While entering all the locations and characters has been by no means uncontentious, encoding events has presented some of the most difficult theoretical and practical challenges. Since a database can only store discrete information, delimiting events, which tend to be non-discrete, necessarily requires interpretation because the boundaries that separate beginnings and endings are unclear and subjective<a class="footnote-ref" href="#mani2013"> [mani2013] </a>. In an ideal scenario, events are entered at the same level of granularity with the same consistency across the corpus. Impinging on this ideal are practical considerations. Digital projects that require manual encoding quickly run into limitations like data collection scope, the labor available, and the possibility of introducing human error. The slightest change in the definition of event boundaries can exponentially increase project completion time. An example is instructive here. Faulkner’s “Red Leaves,” is a short story about the ritualistic burial of Issetibbeha that requires a “hunt” of his body-servant who is to be buried next to him. In the following passage, the African American servant is returning to the burial site and along the way he comes across one of the Chickasaw:</p>
<blockquote>
<p><a href="1">A</a> In the middle of the afternoon he came face to face with an Indian. (2) They were both on a footlog across a slough — the Negro gaunt, lean, hard, tireless and desperate; the Indian thick, soft-looking, the apparent embodiment of the ultimate and the supreme reluctance and inertia. (3) The Indian made no move, no sound; he stood on the log and (4) watched the Negro plunge into the slough and swim ashore and crash away into the undergrowth.<br>
<a href="5">B</a>Just before sunset he lay behind a down log<a class="footnote-ref" href="#faulkner1995"> [faulkner1995] </a>.<br>
There are many ways to divide up the events in this text. The first is to say that the entire paragraph and the following sentence constitute an event because it starts in the middle of the afternoon and goes till sunset. This is a period of several hours, and should therefore be considered one extended event: the servant running away. The other option, [AB], uses the paragraph division to split the passage in two and considers [A], the meeting with the “Indian,” and [B], lying behind a log, as two different moments. The last option would be to divide every individual action and description into an event (1,2,3,4,5). This approach appears ideal from a data perspective, because it gives the greatest amount of detail. The drawback is that entering five different records is much more laborious than the first approach. Since, each event record requires the entry of thirteen variables, the difference is between entering thirteen or sixty-five data points. Scaled to the level of the text, a story that might take thirty hours to encode suddenly takes a hundred and fifty hours. As each story in theDYdatabase is also peer-reviewed and subsequently curated for additional data entry, changing the scale also increases the labor required downstream in the production cycle.</p>
</blockquote>
<p>For DY, the scale of an event is defined as, “1 setting, 1 unbroken length of time, 1 main focus and 1 narrative style is 1 continuous Event” <a class="footnote-ref" href="#railton2015"> [railton2015] </a>. This definition has remained remarkably durable throughout the production process. The narrative is capsulized into units that are intelligible as self-sufficient ontologies. An event can therefore be de-contextualized from the larger narrative, and still make sense as an action performed by a character or characters at a particular place. Needless to say, applying this definition consistently throughout the encoding process does not happen without debate. Usually, the bone of contention is whether an event remains one continuous action when a key character enters or exits a location, and how to break up events where characters are travelling for an indeterminate amount of time and space. Notably,DY’s definition of an event uses page number as a proxy for discourse time (the amount of time it takes to read the passage) and does not measure story time, (the duration of the event in the text)<a class="footnote-ref" href="#chatman1978"> [chatman1978] </a>. An event may span several pages and describe an action that happens in an instant, or, conversely, may only be one sentence and take years. Thus, it is possible to show the order of events, but not the narrative pacing that dictates that order.</p>
<p>Along with delimiting events, another challenge is ordering them. For the purposes of the main visualization, the events need to be placed in chronological order. There are few literary texts that adhere to a strict chronological order, and Faulkner’s are certainly not among these. This makes for great reading, but challenging encoding. Fortunately, while many of Faulkner’s events are narrated out of order, they do follow an underlying total order<a class="footnote-ref" href="#richardson2012"> [richardson2012] </a>.</p>
<p>This is not always the case though. There are occasionally “orphan” events that cannot be unambiguously slotted into a chronology. For example, in their meticulous chronological study of <em>The Sound and the Fury</em> , George Stewart and Joseph Backus document three, relatively minor, orphan events. In their opinion, “the matter is too minute to warrant further discussion” <a class="footnote-ref" href="#stewart1958"> [stewart1958] </a>. Though this may certainly be true of their research, it presents a significant database entry problem forDY. These events must be entered somewhere so they can be sequenced in the animation, and whatever position they are given changes the total order of the chronology.<sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup></p>
<p>On occasion, computation was used to provide a rough sort of events, but, by and large, the final chronologies for each text were ultimately the result of textual scholarship. It should be noted that sorting events through computation is possible, however; as Burg et al., have shown in their work using constraint logic programming to infer the most probable order of events for “A Rose for Emily” <a class="footnote-ref" href="#burg2000"> [burg2000] </a>.<sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup> Unfortunately, computational sorting cannot account for any authorial irregularities in a plot. As Faulkner likely did not anticipate that his fiction would be manually encoded by a group of scholars for the purposes of visualization, he did not iron out plot timeline inconsistencies. Perhaps the most famous example of this is his work on <em>The Mansion</em> . For the final drafts of the novel, Faulkner was extremely reluctant to correct timeline discrepancies with his previous novels and adjust misalignments with historical events<a class="footnote-ref" href="#railton2015"> [railton2015] </a>. Any system that relies on causality and strict date ordering would run into trouble sorting this text.</p>
<p>Stepping back, it is clear that the <em>DY</em> data is the result of interpretation, collaboration, and compromise. Nevertheless, part of the success of the encoding process has been the intuitive ease of data entry achieved by framing the data within fundamental narratological concepts. Annotators with various levels of technical expertise could be onboarded and taught to encode by working on a short-story that was then checked for consistency. On average each text event coding went through seven passes. The initial pass of hand-coding the events in the paper copy of the text, transcription to a spreadsheet, sorting the events by chronology in a spreadsheet, entry into the database, peer-review by a fellow database editor, another review by the director or one of the associate directors, and another final review for inconsistencies before being brought online. This very intensive vetting process reduces the possibility of human error and inconsistency, but, of course, cannot guarantee its elimination. Importantly, one of the valuable lessons about the encoding process is that the data should only go live once all errors have been removed. Once an erroneous entry makes its way into the database it is very hard to discover and revise. Furthermore, as all the encoders were Faulkner scholars, there was a continuous temptation by editors to introduce ever more narrative features to capture the richness of Faulkner’s writing. Needless to say, this would have meant continuously recoding eight thousand different entries representing six thousand pages of narrative text, significantly adding to the prospective workload. Such efforts were thus held in abeyance until the completion of the initial encoding of all the texts. Currently, theDYteam is adding an additional level of nuance to each event by labelling each event with keywords. This process has greatly profited from the fact that the event structure is already in place.</p>
<p>Taking the narrative data and turning it into visualizations presents its own suite of challenges. There are no established conventions for representing digitally encoded narratives. Without standard practices, it is hard to interpret and compare different narrative data visualizations. In this regard, Faulkner’s texts are particularly tricky to visualize because he experimented with narrative structures throughout his career. Scholars have described his complex narrative techniques as everything from the “frozen moment” to enclosure of past, present, and future<a class="footnote-ref" href="#riojellifffe2001"> [riojellifffe2001] </a><a class="footnote-ref" href="#skei1999"> [skei1999] </a>. Reflecting on his writing, Faulkner once famously conceived of the past and present as one long sentence:</p>
<blockquote>
<p>There is no such thing really as was because the past is. It is a part of every man, every woman, and every moment. All of his and her ancestry, background, is all a part of himself and herself at any moment. And so a man, a character in a story at any moment of action is not just himself as he is then, he is all that made him, and the long sentence is an attempt to get his past and possibly his future into the instant in which he does something.<br>
<a class="footnote-ref" href="#faulkner1965"> [faulkner1965] </a>Faulkner’s theory of the long sentence and the continuously present past, represents a fundamental complication for visualization. A faithful rendering of his vision would have to project the narrative past and the narrative present on top of one another, which would, at best, lead to an amorphous blob. On the other hand, showing events as a discreet sequence does not do justice to the way Faulkner weaves the past through the present in his work.</p>
</blockquote>
<p>A related challenge is that Faulkner’s idea of the past being eternally present led him to experiment with time throughout his career; arguably each of the novels is a reworking of the same theory in a different form. Ideally, a visualization that captures Faulkner’s use of time has to be consistent across all his works to provide a basis for comparison, but also flexible enough to capture the idiosyncrasies of each text. For instance, the <em>Sound and the Fury</em> and <em>Absalom, Absalom!</em> are both about the past, but “[o]ne is a drama about knowing events, the other a drama of events defused and disconnected” <a class="footnote-ref" href="#labatt2005"> [labatt2005] </a>. These differences in the way time is constructed are compounded when expressed across the entire corpus. Any visualization of his narrative, including the ones provided by the narrative structure analysis dashboard, can only provide a provisional and limited insight into the texts.</p>
<p>One final hurdle to visualizing narrative in Faulkner is that the unique narrative problems his texts present may not necessarily be useful for creating a shared visual language with other, non-Faulkner texts. This is an obstacle that narrative theory is particularly adept at surmounting. After all, there are certain generalizable textual aspects based on narrative theory that can elucidate cross-author comparison.</p>
<h2 id="matchless-times-visualizing-faulkners-plots">Matchless Times: Visualizing Faulkner’s Plots</h2>
<p>At first blush, it would appear that there is no accepted standard for visualizing narrative data. This is only because the common means of doing this have been scattered across different knowledge domains. In fact, a number of different projects have proposed analogous models, even if they were not necessarily aware of one another. The most consistently used model relies on graphing the chronological order in relation to story time. This technique was already anticipated by Vladimir Propp, and he hints at versions of the models in the appendix of <em>Morphology of the Folktale</em> in 1928<a class="footnote-ref" href="#propp1979"> [propp1979] </a>. Earlier work by Faulkner scholars has also adopted the same technique of contrasting plot with story<a class="footnote-ref" href="#railtonrourk2003"> [railtonrourk2003] </a><a class="footnote-ref" href="#stoicheff"> [stoicheff] </a>. The storyline models created by Randall Munroe on xkcd.com have inspired much productive subsequent research<a class="footnote-ref" href="#liu2013"> [liu2013] </a><a class="footnote-ref" href="#munroe2009"> [munroe2009] </a><a class="footnote-ref" href="#padia2019"> [padia2019] </a><a class="footnote-ref" href="#tanahashi2012"> [tanahashi2012] </a>, and are similar to Propp’s initial insights, even if he is not mentioned. Thus, when Kim et al. claim that their project Story Curve is the “first scientific investigation and systematic exploration of this visualization technique,” it is perhaps somewhat overstated<a class="footnote-ref" href="#kim2018"> [kim2018] </a>. The deep parallels between current work and that of Propp nearly a century earlier should not be seen as evidence of stagnation within in the field, but rather as a testament to the intuitive power of this type of visualization design.</p>
<p>Respecting this observation, I used the plotly.js graphing library and vanilla JS to design a data dashboard that would allow users ranging from first-time readers of Faulkner to seasoned scholars to compare his texts with one another on the basis of plot shape. As <em>DY</em> caters to the broadest possible audience, users have different levels of technical proficiency, and the usability of the charts needed to be immediate and intuitive. The design goal was therefore to create something that required very little input from the user, and generated insights that would be familiar and meaningful to literary scholars. The resulting interface allows users to compare up to four charts, and toggle between chronological order and date range. The charts also have some functionality that is native to plotly.js that enables a more scoped view of the data, and the ability to download the chart. Along with the main narrative structure chart, there are also charts showing the percentage of different forms of narration; the ratio of flashbacks, flashforwards, and linear event sequences; and a frequency diagram of events across the dates of the story. The various tools all work together to allow users to compare the narrative structure of fourteen novels and fifty-six short stories.</p>
<h2 id="understanding-chronology-through-plot-shape">Understanding Chronology through Plot Shape</h2>
<p>The step charts below represent the progression of the story relative to the order in which it is told. They are meant to showcase the plot structure of a text in one, easy to compare, view. Each line segment represents one event. Events that comprised less than a page come across as a dot, while events that amounted to multiple pages are line segments, though for novels most line segments appear as dots due to scaling. The length of events only captures their discourse time, and does not correspond to their significance or textual duration. These segments are plotted by page number on the x-axis and chronological order on the y-axis.<sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup> When events happen earlier, they are lower down. Conversely, events that happen later in the chronology are higher up on the chart. If the story and the plot coincide, and events are told in the order that they happen, the slope is forty-five degrees. This is rarely the case for Faulkner, or any author. Instead, there are usually troughs or peaks in the line. These are indications of analepsis (flashback) or prolepsis (flashforward), respectively.<sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup> Borrowing from an example by David Herman, if story and plot coincide, the order of events plotted out as a line segment is ABC (see Figure 2). If Faulkner tells the story in a different order and uses a flashback, the order is BAC. Visually, line segment A will come sequentially after B on the x-axis, but appear lower down on the y-axis (see Figure 3). In that same vein, if there were a flashforward the story would be told ACB and segment C would precede B on the x-axis, but be higher up on the y-axis (see Figure 4)<a class="footnote-ref" href="#herman2002"> [herman2002] </a>.</p>




























<figure ><img loading="lazy" alt="Events plotted as a line segment." src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Story and Chronology
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="Events plotted as if there is a flashback." src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Flashback
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="Events plotted if there is a flashforward." src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Flashforward
        </p>
    </figcaption>
</figure>
<p>Along with chronological data,DYalso identifies the narrative status of an event. Narrative status answers the question: “Who/what is responsible as the source of this Event?” <a class="footnote-ref" href="#railton2015"> [railton2015] </a>. This concept does not map neatly onto any language available in narrative theory, and using any adjacent definition likely leads to more confusion than clarity. Be that as it may, each event is identified as having one of five narrative statuses: 1) <em>Narrated</em> by a first or third person narrator; 2) <em>Told</em> by a character in the story; 3) <em>Remembered</em> through a character’s consciousness; 4) <em>Hypothesized</em> when something might have happened; 5) <em>Narrated</em> + <em>consciousness</em> when Faulkner combines narration with stream of consciousness. The last case was created specifically for <em>Light in August</em> , though examples appear sporadically throughout other texts. Using these categories, the chronological data can be subsetted according to narrative status. Encoding narrative status for a text is not a necessary compotent for creating storyline charts, and the applicability of the narrative categories beyond Faulkner remains to be seen. It would be challenging, for example, to encode James Joyce’s <em>Ulysses</em> with purely these type of narrative status classifiers. Nonetheless, the resulting visualization shows events as both a change in location or character, and as a change in narrative status (Figure 5).</p>




























<figure ><img loading="lazy" alt="Plot of narrative status in _The Sound and the Fury_ ." src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Events in <em>The Sound and the Fury</em> by chronological order and page number.
        </p>
    </figcaption>
</figure>
<p>The above visualization compresses a lot of complex information. On the y-axis is the rank order of chronological events, and on the x-axis are the page and event number. The plotly.js library allows users several ways to drill down and manipulate the data. The legend is interactive and users can hide and show the various traces. Users can also hover over points to reveal specific information like rank, page and event number accompanied by the first 6-8 words of the event.</p>
<p>In this particular view of the <em>Sound and the Fury</em> several salient features of the text are visible. First, Benjy’s chapter is the first in the novel, but is actually meant to be third sequentially. This is clear visually because the first series of yellow dots up until page 74 can be slotted into the space on the y-axis between 264 and 265. This is not particularly revelatory since the chapter titles are dated, but it does confirm that the sequence is in the right order. Second, many of the details in the narrative present are actually quite regular in their sequence, it is the past that appears confused and is frequently nonlinear. Upon closer inspection, even here there are patterns in the sequence. In terms of narrative status, it is interesting to note just how much of the <em>Sound and the Fury</em> is told through memory. The first two chapters are recounted to a large extent to through the memories Benjy and Quentin have. Meanwhile, in the last two chapters the event sequence is far more linear, and the narrative status is more consistently narrated than the previous two sections. The novel appears to move from chaos and disorder to order and stability, and it is perhaps all the more poetic that the novel ends with “each in its ordered place” <a class="footnote-ref" href="#faulkner1990"> [faulkner1990] </a>.</p>
<h2 id="interpreting-temporal-positioning-through-date-range">Interpreting Temporal Positioning through Date Range</h2>
<p>Along with chronology and narrative status, the date range during which events occur is another insightful way to visualize these stories. What is particularly revealing about the date information is how Faulkner structured the past in his narratives. At times, this is very precisely defined, as with the date titles of the chapters in <em>The Sound and the Fury</em> . In other instances, time is very vaguely indicated. TheDYteam enters an exact date whenever possible, but resorts to a date range when there is insufficient information for a specific date. Sometimes the range is a week, a month, or even a couple of years. In order to identify and delimit date ranges as much as possible, textual references to real historical events, such as the Louisiana Purchase or the Battle of First Manassas, are used as anchors to which other relative dates are tethered. Even this is not always possible, since relative time indications like “earlier” and “later” have no specific value. In such cases, it is indicated that the date range is indeterminate.</p>
<p>The aforementioned challenges with establishing the dates for events notwithstanding, it is still possible to visualize them in a productive manner. The graph below shows the latest possible date for each event indicated by a red line and the earliest possible date with a blue line (see Figure 6). The area in between represents the range of dates possible. As opposed to the chronological graphs, these have a continuous line because it provides more visual clarity than a segmented line, even if it might give the false impression that there is a smooth transition between past and present.</p>




























<figure ><img loading="lazy" alt="Plot depicting possible dates for events." src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Events in <em>Absalom, Absalom!</em> by date range and page number.
        </p>
    </figcaption>
</figure>
<p>To a certain extent, the date range graphs parallel the chronology graphs, because both order the events relative to page number. The key distinction is that the date range charts demonstrate the difference in time between events. The chronological graphs are insightful with regard to the sequencing of time; the date range graphs give a better sense of how Faulkner is using historical time.</p>
<p>In the chart of <em>Absalom, Absalom!</em> this use of the past is particularly powerful. Even though the narrative present is 1909-1910, many of the events take place at an earlier date. Looking across the graph, it is possible to see that the past predominates the beginning of the novel, but over the course of the text the past and present grow together, until, finally, they almost intertwine at the end. The chart demonstrates just how closely interknit past and present are in Faulkner’s imagining.</p>
<h2 id="conclusion">Conclusion</h2>
<p>While it is tempting to make inferences about Faulkner’s work using the narrative structure analysis dashboard, those conclusions have been deliberately forestalled here. Instead, the goal ofDYis to provide a platform for other scholars to use the data for their own work. It makes little sense to create a dynamic user-driven visualization if it is only meant to lead to predetermined answers. That being said, there are a number of fruitful areas of investigation that the dashboard makes possible. The first of these is to see if there is a pattern in the way Faulkner structures the plots of his texts. One of Faulkner’s central concerns is how the past inhabits the present, but it is unclear if he does so in continuously new ways or if there is a signature “Faulknerian style” with which he works and reworks this material. A related issue is the change or consistency in his use of chronology, narrative status, and date ranges throughout his career. In Faulkner studies, there is a generally accepted arc of Faulkner’s development that divides his literary output into several distinct periods. It would be interesting to see if this structured division of his writing is visible in the visualizations or if these tell a different story.</p>
<p>Beyond Faulkner, the visual language used in this paper and the accompanying dashboard are prompts for a larger discussion about representing and interpreting digitally encoded narratives. The framework for narrative analysis provided byDYis precise enough to highlight meaningful differences, while being broad enough not to be limited to Faulkner. The visual language used for the dashboard is similar to projects within disparate knowledge domains, suggesting their intelligibility. A broader adoption of plot/story charts would usher in the ability to start comparing different authors on reasonably equal narratological footing. One obvious point of contrast for Faulkner is a contemporaneous author like Ernest Hemingway, many of whose short stories take place almost entirely in the narrative present. Yet, the trauma of the past is always lurking just under the surface about to explode. Similarly, it would be interesting to compare Faulkner’s writing to that of realist authors like Kate Chopin, who more strictly adheres to the conventions of chronological unity.</p>
<p>Whatever path researchers might strike out, narrative theory still provides an indispensable map for conceptualizing, and encoding narrative data. No doubt, this encoding is an artificial construct that does not reveal any type of fundamental truth about a text, as early structuralist narrative theorists might have imagined. Nevertheless, these rasters for interpretation slice up texts in unexpected ways, providing new insights yet to be imagined, much less visualized.</p>
<ul>
<li id="aldawsari2019">Aldawsari, M. & Finlayson, M. “Detecting Subevents Using Discourse and Narrative Features” . _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_ , July 2019 Florence, Italy. Association for Computational Linguistics, 4780-90.
</li>
<li id="alvarado2018">Alvarado, R. “Characters and Locations in Force Directed Graph” . Available:<a href="http://faulkner.iath.virginia.edu/characters-force.html?text_na=FD">http://faulkner.iath.virginia.edu/characters-force.html?text_na=FD</a>.
</li>
<li id="baber2011">Baber, C., Andrews, D., Duffy, T. & Mcmaster, R. “Sensemaking as Narrative: Visualization for Collaboration” . _The 3rd International UKVAC Workshop on Visual Analytics_ , 2011 London.
</li>
<li id="baroni2016">Baroni, R. L. “The Many Ways of Dealing with Sequence in Contemporary Narratology” . In: Baroni, R. L. & Revaz, F. O. (eds.) _Narrative Sequence in Contemporary Narratology._ The Ohio State University Press, Columbus (2016).
</li>
<li id="baronirevaz2016">Baroni, R. L. & Revaz, F. O. _Narrative Sequence in Contemporary Narratology._ The Ohio State University Press, Columbus (2016).
</li>
<li id="barros2019">Barros, C., Vicente, M. & Lloret, E. “Tackling the Challenge of Computational Identification of Characters in Fictional Narratives” . _2019 IEEE International Conference on Cognitive Computing (ICCC)_ , July 2019. 122-29.
</li>
<li id="bartalesi2016">Bartalesi, V., Meghini, C. & Metilli, D. “Steps Towards a Formal Ontology of Narratives Based on Narratology” . OpenAccess Series in _Informatics_ , 2016. 4.1-4.10.
</li>
<li id="bogel2014">Bögel, T., Strötgen, J. & Gertz, M. “Computational Narratology: Extracting Tense Clusters from Narrative Texts” . _Proceedings of the Ninth International Conference on Language Resources and Evaluation_ (LREC 14), May 2014 Reykjavik, Iceland. European Language Resources Association (ELRA), 950-55.
</li>
<li id="brown2013">Brown, M., Dobson, T., Grue, D. & Ruecker, S. “Challenging New Views on Familiar Plotlines: A Discussion of the Use of XML in the Development of a Scholarly Tool for Literary Pedagogy” . _Literary & Linguistic Computing_ , 28 (2013): 199-208.
</li>
<li id="burg2000">Burg, J., Boyle, A. & Lang, S.-D. “Using Constraint Logic Programming to Analyze the Chronology in A Rose for Emily ” . _Computers and the Humanities_ , 34 (2000): 377-92.
</li>
<li id="burgers2020">Burgers, Johannes. “Familial Places in Jim Crow Spaces: Kinship, Demography, and the Color Line in William Faulkner’s Yoknapatawpha County” . _Journal of Cultural Analytics_ , 1 (2020).
</li>
<li id="chatman1978">Chatman, S. B. _Story and Discourse: Narrative Structure in Fiction and Film_ . Cornell University Press, Ithaca, NY (1978).
</li>
<li id="delmonte2017">Delmonte, R. & Marchesini, G. “A Semantically-Based Computational Approach to Narrative Structure” . _IWCS 2017 — 12th International Conference on Computational Semantics — Short papers_ , 2017.
</li>
<li id="faulkner1965">Faulkner, W. _Faulkner in the University: Class Conferences at the University of Virginia, 1957-1958_ . Vintage, New York (1965).
</li>
<li id="faulkner1990">Faulkner, W. _The Sound and the Fury_ . Vintage International, New York (1990).
</li>
<li id="faulkner1995">Faulkner, W. _Red Leaves. Collected Stories_ . Vintage International, New York (1995).
</li>
<li id="finlayson2017">Finlayson, M. A. “ProppLearner: Deeply Annotating a Corpus of Russian Folktales to Enable the Machine Learning of a Russian Formalist Theory” . _Digital Scholarship in the Humanities_ , 32 (2017): 284-300.
</li>
<li id="fludernik2005">Fludernik, M. “Histories of Narrative Theory (II): From Structuralism to the Present” . In: Phelan, J. & Rabinowitz, P. J. (eds.) _A Companion to Narrative Theory_ . Blackwell Publishing, Malden, MA (2005).
</li>
<li id="focilarias2018">Fócil-Arias, C., Sidorov, G., Gelbukh, A., Arce, F., Pinto, Singh, Villavicencio, Mayr, S. & Stamatatos. “Extracting Medical Events from Clinical Records Using Conditional Random Fields and Parameter Tuning for Hidden Markov Models” . _Journal of Intelligent & Fuzzy Systems_ , 34 (2018): 2935-47.
</li>
<li id="forster1927">Forster, E. M. _Aspects of the Novel_ . Harcourt, New York (1927).
</li>
<li id="frawley1992">Frawley, W. _Linguistic Semantics_ . Lawrence Erlbaum Associates, Hillside, NJ (1992).
</li>
<li id="genette1972">Genette, G. 1972. Discours du récit. Essai de méthode. Figures III. Paris: Seuil.
</li>
<li id="going1958">Going, W. T. “Chronology in Teaching A Rose for Emily ” . _Exercise Exchange_ , 5 (1958): 8-11.
</li>
<li id="gutierrez2018">Gutiérrez, G., Canul-Reich, J., Zezzatti, A. O., Margain, L. & Ponce, J. “Mining: Students Comments about Teacher Performance Assessment using Machine Learning Algorithms” . _International Journal of Combinatorial Optimization Problems & Informatics_ , 9 (2018): 26-40.
</li>
<li id="hansen2017">Hansen, P. K., Pier, J., Roussin, P. & Schmid, W. _Emerging Vectors of Narratology_ . De Gruyter, Boston (2017).
</li>
<li id="harris1993">Harris, P. A. “Fractal Faulkner: Scaling Time in Go Down, Moses ” . _Poetics Today_ , 14 (1993): 625-51.
</li>
<li id="herman2002">Herman, D. _Story Logic: Problems and Possibilities of Narrative_ . University of Nebraska Press, Lincoln, NE (2002).
</li>
<li id="herman2012">Herman, D. _Narrative Theory: Core Concepts and Critical Debates_ . Ohio State University Press, Columbus (2012).
</li>
<li id="hoyt2014">Hoyt, E., Ponto, K. & Roy, C. “Visualizing and Analyzing the Hollywood Screenplay with ScripThreads” . _DHQ: Digital Humanities Quarterly_ , 8 (2014).
</li>
<li id="inge1970">Inge, T. M. (ed.). _A Rose for Emily_ . Merrill, Columbus, OH (1970).
</li>
<li id="jockers2017">Jockers, M. “Introduction to the Syuzhet Package” . Available:<a href="https://cran.r-project.org/web/packages/syuzhet/vignettes/syuzhet-vignette.html">https://cran.r-project.org/web/packages/syuzhet/vignettes/syuzhet-vignette.html</a>.
</li>
<li id="kim2018">Kim, N. W., Bach, B., Im, H., Schriber, S., Gross, M. & Pfister, H. “Visualizing Nonlinear Narratives with Story Curves” . _IEEE Transactions on Visualization and Computer Graphics_ , 24 (2018): 595-604.
</li>
<li id="labatt2005">Labatt, B. _Faulkner the Storyteller_ . Universtity of Alabama Press, Tuscaloosa, AL (2005).
</li>
<li id="lakoff2010">Lakoff, G. & Narayanan, S. “Toward a Computational Model of Narrative” . AAAI Fall Symposium - Technical Report, (2010).
</li>
<li id="leitch1986">Leitch, T. M. _What Stories Are: Narrative Theory and Interpretation_ . Pennsylvania State University Press, University Park, PA (1986).
</li>
<li id="leonid2017">Leonid, B. “Towards a Computational Measure of Plot Tellability” . _AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment; Thirteenth Artificial Intelligence and Interactive Digital Entertainment Conference_ , (2017).
</li>
<li id="liu2013">Liu, S., Wu, Y., Wei, E., Liu, M. & Liu, Y. “StoryFlow: Tracking the Evolution of Stories” . _IEEE Transactions on Visualization and Computer Graphics_ , 19 (2013): 2436-45.
</li>
<li id="mahfouz2018">Mahfouz, T., Kandil, A. & Davlyatov, S. “Identification of Latent Legal Knowledge in Differing Site Condition (DSC) Litigations” . _Automation in Construction_ , 94 (2018): 104-11.
</li>
<li id="mani2013">Mani, I. _Computational Modeling of Narrative_ . Morgan & Claypool Publishers, (2013).
</li>
<li id="meister">Meister, J. C. & Geertz, M. “heureCLÉA: Collaborative Literature Exploration & Annotation” . Available:<a href="http://heureclea.de/">http://heureclea.de/</a>.
</li>
<li id="munroe2009">Munroe, R. 2009. “Movie Narrative Charts” . _xkcd_ [Online]. Available from:<a href="https://xkcd.com/657/">https://xkcd.com/657/</a>.
</li>
<li id="murphy2003">Murphy, K. 2003. “Sequential Display of Narrative Time in Benjy's Section” . In: Stoicheff, M., Deshaye, Et Al. (ed.). _The Sound and the Fury_ : a Hypertext Edition: U of Saskatchewan.
</li>
<li id="muzny2017">Muzny, G., Algee-Hewitt, M. & Jurafsky, D. “Dialogism in the Novel: A Computational Model of the Dialogic Nature of Narration and Quotations” . _Digital Scholarship in the Humanities_ , 32 (2017): ii31-ii52.
</li>
<li id="nebeker1971">Nebeker, H. E. “Chronology Revised” . _Studies in Short Fiction_ , 8 (1971): 471.
</li>
<li id="nijila2018">Nijila, M. & Kala, M. T. “Extraction of Relationship Between Characters in Narrative Summaries” . _2018 International Conference on Emerging Trends and Innovations In Engineering and Technological Research (ICETIETR)_ , 11-13 July 2018 2018. 1-5.
</li>
<li id="ogata2019">Ogata, T. “Toward a Post-Narratology or the Narratology of Narrative Generation” . In: Akimoto, T. (ed.) _Post-Narratology through Computational and Cognitive Approaches_ . Information Science Reference, Hershey, PA (2019).
</li>
<li id="padgett">Padgett, J. “William Faulkner on the Web” . Available:<a href="http://cypress.mcsr.olemiss.edu/~egjbp/faulkner/faulkner.html">http://cypress.mcsr.olemiss.edu/~egjbp/faulkner/faulkner.html</a>.
</li>
<li id="padia2019">Padia, K., Bandara, K. H. & Healey, C. G. “A System for Generating Storyline Visualizations Using Hierarchical task network planning” . _Computers & Graphics_ , 78 (2019): 64-75.
</li>
<li id="perry1979">Perry, M. “Literary Dynamics: How the Order of a Text Creates its Meanings [with an Analysis of Faulkner's A Rose for Emily ]” . _Poetics today_ , 1 (1979): 35-64; 311-61.
</li>
<li id="phelan2017">Phelan, J. _Somebody Telling Somebody Else: A Rhetorical Poetics of Narrative_ . Ohio State University Press, Columbus (2017).
</li>
<li id="pier2016">Pier, J. “The Configuration of Narrative Sequences” . In: Baroni, R. L. & Revaz, F. O. (eds.) _Narrative Sequence in Contemporary Narratology_ . The Ohio State University Press, Columbus, OH (2016).
</li>
<li id="prince2016">Prince, G. “On Narrative Sequence, Classical and Postclassical” . In: Baroni, R. L. & Revaz, F. O. (eds.) _Narrative Sequence in Contemporary Narratology_ . The Ohio State University Press, Columbus, OH (2016).
</li>
<li id="propp1979">Propp, V. _Morphology of the Folktale._ University of Texas Press, Austin (1979).
</li>
<li id="railtona">Railton, S. “Instructions” . _Digital Yoknapatawpha_ . Available:<a href="http://faulkner.drupal.shanti.virginia.edu/content/instructions">http://faulkner.drupal.shanti.virginia.edu/content/instructions</a>.
</li>
<li id="railtonb">Railton, S. “Manuscripts &c: The Mansion” . _University of Virginia_ . Available:<a href="http://faulkner.drupal.shanti.virginia.edu/node/18732?canvas">http://faulkner.drupal.shanti.virginia.edu/node/18732?canvas</a>.
</li>
<li id="railtonrourk2003">Railton, S. & Rourk, W. “ _Absalom, Absalom!_ Chronology” . _University of Virginia_ . Available:<a href="http://twain.lib.virginia.edu/absalom/index2.html">http://twain.lib.virginia.edu/absalom/index2.html</a>.
</li>
<li id="railton2015">Railton, S., Towner, T. M., Burgers, J., Corrigan, J., Joiner, J. J., Hagood, T., Carothers, J. B. & Cornell, E. “Roundtable: Digital Yoknapatawpha” . _Mississippi Quarterly_ , 68 (2015): 456-85.
</li>
<li id="reed1973">Reed, J. W. J. _Faulkner's Narrative_ . Yale University Press, New Haven (1973).
</li>
<li id="reed1974">Reed, R. “The Role of Chronology in Faulkner's Yoknapatawpha Fiction” . _The Southern Literary Journal_ , 7 (1974): 24-48.
</li>
<li id="richardson2012">Richardson, B. “Time, Plot, Progression” . In: Herman, D. (ed.) _Narrative Theory: Core Concepts and Critical Debates_ . Ohio State University Press, Columbus, OH (2012).
</li>
<li id="richardson2019">Richardson, B. _A Poetics of Plot for the Twenty-First Century: Theorizing Unruly Narratives_ . Ohio State University Press, Columbus, OH (2019).
</li>
<li id="riojellifffe2001">Rio-Jellifffe, R. _Obscurity's Myriad Components: The Theory and Practice of William Faulkner_ . Bucknell University Press, Lewisburg (2001).
</li>
<li id="robbins2016">Robbins, B. “Reading for Data: Temporal Speed Shifts in Faulkner's Death Drag and the Process of Textual Digitization” . _Studies in American Culture_ , 39 (2016): 7-20.
</li>
<li id="schwab1991">Schwab, M. “A Watch for Emily” . _Studies in Short Fiction_ , 28 (1991): 215.
</li>
<li id="schwan2019">Schwan, H., Jacke, J., Kleymann, R., Stange, J.-E. & Dörk, M. “Narrelations — Visualizing Narrative Levels and their Correlations with Temporal Phenomena” . _Digital Humanities Quarterly_ , 013 (2019).
</li>
<li id="seonwoo2018">Seonwoo, Y., Oh, A. & Park, S. “Hierarchical Dirichlet Gaussian Marked Hawkes Process for Narrative Reconstruction in Continuous Time Domain” . _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing_ , 2018 2018. 3316-25.
</li>
<li id="sharma2013">Sharma, R. & Rajamanickam, V. “Using Interactive Data Visualization to Explore Non-Linear Movie Narratives” . _Parsons Journal for Information Mapping_ , (2013).
</li>
<li id="shklovskii1990">Shklovskiĭ, V. _Theory of Prose_ . Dalkey Archive Press, Elmwood Park, IL (1990).
</li>
<li id="skei1999">Skei, H. H. _Reading Faulkner's Best Short Stories_ . University of South Carolina, Columbia, SC (1999).
</li>
<li id="stewart1958">Stewart, G. R. & Backus, J. M. “ Each in its Ordered Place : Structure and Narrative in "Benjy's Section" of _The Sound and the Fury_ ” . _American Literature_ , 29 (1958): 440-56.
</li>
<li id="stoicheff">Stoicheff, P., Muri, A., Deshaye, J., Truchan-Tataryn, Bath, J., Mitchell, D. & Murphy, K. “The Sound and the Fury: a Hypertext Edition” . Available:<a href="http://drc.usask.ca/projects/faulkner/">http://drc.usask.ca/projects/faulkner/</a>.
</li>
<li id="swafford2015">Swafford, A. 2015/03/02 2015. “Problems with the Syuzhet Package” . _Anglophile in Academia: Annie Swafford's Blog_ [Online]. Available from:<a href="https://annieswafford.wordpress.com/2015/03/02/syuzhet/">https://annieswafford.wordpress.com/2015/03/02/syuzhet/</a>.
</li>
<li id="takeuchi2016">Takeuchi, K. “Thesaurus with Predicate-Argument Structure to Provide Base Framework to Determine States, Actions, and Change-of-States” . In: Ogata, T. & Akimoto, T. (eds.) _Computational and Cognitive Approaches to Narratology. Information Science Reference_ , Hersey, PA (2016).
</li>
<li id="tanahashi2012">Tanahashi, Y. & Ma, K.-L. “Design Considerations for Optimizing Storyline Visualizations” . _IEEE Transactions on Visualization and Computer Graphics_ , 18 (2012): 2679-88.
</li>
<li id="tenen2018">Tenen, D. Y. “Toward a Computational Archaeology of Fictional Space” . _New Literary History_ , 49 (2018): 119-47.
</li>
<li id="volpe2003">Volpe, E. L. _A Reader's Guide to William Faulkner: The Novels_ . Syracuse University Press, Syracuse, NY (2003).
</li>
<li id="wallace2012">Wallace, B. “Multiple Narrative Disentanglement: Unraveling Infinite Jest” . _Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_ , 2012 Montréal, Canada. Association for Computational Linguistics, 1-10.
</li>
<li id="wilson1972">Wilson Jr, G. R. “The Chronology of Faulkner’s A Rose for Emily Again” . _Notes on Mississippi Writers_ , 5 (1972): 44.
</li>
<li id="xie2019">Xie, H., Lu, X., Chen, X., Tong, J. & Tang, Z. “ES-ESens: Detection of Event Sentences Based on Evaluation of the Explicitness and Significance of Information” . 2019 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR), March 2019. 32-35.
</li>
</ul>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Of these, the classic work is Gérard Genette’s <em>Discourse du récit</em> , which has been productively mined by literary scholars and computer scientists for his insights into narrative. For an overview of a semantic analysis see:<a class="footnote-ref" href="#frawley1992"> [frawley1992] </a>. Blair Labatt uses event salience for a narratological analysis specifically based on Faulkner’s work<a class="footnote-ref" href="#labatt2005"> [labatt2005] </a>. Joseph Reed’s work is an early attempt at using narrative theory to analyze Faulkner. It is somewhat idiosyncratic in its outlook though<a class="footnote-ref" href="#reed1973"> [reed1973] </a>. James Phelan and Peter J. Rabinowitz’s critical introduction to narrative theory provides an excellent overview of rhetorical, feminist, mind-oriented, and antimimetic approaches with regard to time, plot, and progression<a class="footnote-ref" href="#herman2012"> [herman2012] </a>. Other definitions include Leitch, who sees plot as a function of the a teleological principle<a class="footnote-ref" href="#leitch1986"> [leitch1986] </a>.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Finlayson’s more recent related work is extremely promising in its potential to parse sub-events from narratives<a class="footnote-ref" href="#aldawsari2019"> [aldawsari2019] </a>.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Unfortunately, neither digital text currently functions. Due to copyright issues the full hypertext created by Stoicheff, Muri, Deshaye, et al. is no longer available. Though it is still a good resource for digital renderings of the text. In fact, a graph created by Kathleen Murphy of narrative time in the Benjy section of <em>Sound and the Fury</em> shares many similarities with the chronology graphs this paper presents<a class="footnote-ref" href="#murphy2003"> [murphy2003] </a>. Likewise, the chronology of <em>Absalom, Absalom!</em> created by Railton and Rourke no longer functions because it was coded in Flash. Currently, attempts are being made to revive the chronology in a new format.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>As of this writing the exact numbers are 2,152 locations, 4,988 characters, and 8,435 events, though these are always subject to change.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>The database also contains a location key that keeps track of all the locations being used across the corpus, but this does not have any bearing on the research here.&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>This approach precludes the possibility of simultaneity, something that other storyline visualizations are able to do.&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>Their findings are intriguing as a proof of concept for constraint logic programming, though they readily admit that perhaps “to understand Emily, we must give up our orderly sorting of experience.” That is to say, that the “fuzzy” dating of the text may not be a puzzle to be solved but a confusion to be experienced. After all, it seems unlikely that Faulkner hid within his story an obscure dating and chronological system that he wanted to be discovered through an advanced programming language seventy years later.&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>The reason for choosing page number, which is dependent on edition, versus word count, which is independent of layout and more precise, was a practical one. Only in some cases was there a clean digital version available. In the future, this data will eventually have to be linked to word count as well as page number.&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>Nonlinear sequencing is not always a sign of prolepsis or analepsis can also be a sign of event parallelism. Since the events have to be entered in rank order, events that happen at the same time are forced into a linear sequence.## Bibliography&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry><entry><title type="html">Towards Hermeneutic Visualization in Digital Literary Studies</title><link href="https://startwords.cdh.princeton.edu/vol/15/2/000547/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://startwords.cdh.princeton.edu/vol/15/2/000547/</id><author><name>Rabea Kleymann</name></author><author><name>Jan-Erik Stange</name></author><published>2021-06-15T00:00:00+00:00</published><updated>2023-08-04T13:01:02-04:00</updated><content type="html"><![CDATA[<h2 id="1-introduction">1. Introduction</h2>
<p>Data visualization has become a prolific method in digital literary studies to represent the results of a research process. While it is most commonly used to communicate these results to a scholarly audience, there is an increasing number of cases that exhibit an analytical use of the method in order to gain a better understanding of the textual data under investigation (cf.<a class="footnote-ref" href="#jessop2008"> [jessop2008] </a>;<a class="footnote-ref" href="#sinclair2013"> [sinclair2013] </a>). A common definition of data visualization is: “The use of computer-supported, interactive, visual representations of abstract data to amplify cognition” <a class="footnote-ref" href="#card1999"> [card1999] </a>. Although literary text can be transformed into abstract data with the help of statistical or computational methods like natural language processing, this is not the kind of data that literary scholars practicing an hermeneutic approach are concerned with. Here, scholars are dealing with subjective and ambiguous data, usually in the form of text annotations, for which conventional data visualization is not adequate (cf.<a class="footnote-ref" href="#drucker2018"> [drucker2018] </a>). Precisely this discrepancy between conventional data visualizations used and the interpretative data and processes to be visualized is the starting point of our considerations for a more reflective interface and visualization concept for literary analysis and interpretation processes.</p>
<p>As Meister et al. point out: “Most current DH visualizations are thus epistemological one-way avenues toward knowledge, from data via rendering algorithm to visual display” <a class="footnote-ref" href="#meister2017"> [meister2017] </a>. But the process of understanding texts and other cultural artifacts in the humanities is a continuous, dynamic interplay of modeling as well as reasoning operations. These operations repeatedly affect a dynamic data model in form of enrichment and reconfiguration (cf.<a class="footnote-ref" href="#gius2017"> [gius2017] </a>). The hermeneutic approach is a specific form of the process of understanding, which has significance especially for literary and cultural studies. In the humanities, the termhermeneutic(hermeneueinin Greek means “to say, to explain, to translate” <a class="footnote-ref" href="#palmers1969"> [palmers1969] </a>) generally stands “for the practice of exegesis (i. e. interpretation) that leads to understanding, [&hellip;] [as well as] for the theory of interpretation as a reflection on the conditions and norms of understanding and linguistic utterance in general” <a class="footnote-ref" href="#mittelstrab2008"> [mittelstrab2008] </a><sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> . Already developed as a practice in ancient times, hermeneutics took different forms over the 19th century, especially in Schleiermacher’s and Herder’s approach, then around 1900 in Dilthey’s conception and finally in the 20th century in the thinking of Heidegger, Gadamer and the “Konstanzer Schule” . As a result, the following distinction has become established in the humanities. On the one hand, hermeneutic refers to a literary-philological art theory of the interpretation of texts (and other cultural artefacts). We speak ofliterary hermeneutics. On the other hand, hermeneutic is a philosophical discipline with an universalistic claim that is concerned with the conditions of humanistic understanding per se (cf.<a class="footnote-ref" href="#nunning2008"> [nunning2008] </a>;<a class="footnote-ref" href="#stiening2016"> [stiening2016] </a>). In this context, we are talking of <em>hermeneutic philosophy</em> . However, both meanings of the term are strongly interdependent and partly overlap in different theories of the humanities.</p>
<p>In digital humanities research, there has been some discussion on how the digital humanities might live up to expectations and methodological requirements associated with a digitally supported hermeneutic practice (cf.<a class="footnote-ref" href="#zundert2016"> [zundert2016] </a>;<a class="footnote-ref" href="#rockwellandsinclair2016"> [rockwellandsinclair2016] </a>). Often a very broad notion of hermeneutics as a theory of understanding (as opposed to explaining) is applied. We can observe results that have arisen from these discussions in a number of software tools (e.g. <em>Catma</em> , <em>Voyant</em> ). These tools are, for example, replicating traditional scholarly activities that are considered a part of the interpretation process. Unsworth<a class="footnote-ref" href="#unsworth2000"> [unsworth2000] </a>gives a systematic account of “scholarly primitives” , as he calls these activities, some of which are applied by scholars during the interpretative process. Among these are annotation, comparison and representation. As for annotation, this is often the starting point of hermeneutic practice: highlighting parts of a document and writing down comments in the margins are two of the oldest scholarly techniques.</p>
<p>While the integration of these primary activities into digital tools certainly is a step towards hermeneutics in the digital realm (ordigital literary hermeneutics), most of these efforts have been rather agnostic about the epistemological premises of hermeneutic theory. We argue that visualization could serve as the missing link between fundamental hermeneutic premises and digital (literary) hermeneutics. We claim that visualizations not only have to fulfill certain conditions to adequately support literary analysis and interpretation. Rather, we assert that, in referring to the traditional theory of hermeneutics, these qualities can be distinctively described and designed for. We definehermeneutic visualizationas:</p>
<blockquote>
<p>The use of computer-supported, interactive, visual representations of text annotations to manipulate, reconfigure and explore them in order to create visual interpretations that can be used as arguments and allow a critical reflection of the hermeneutic process in light of a research question.<br>
By clarifying the premises and postulates for hermeneutic visualization, we address two research desiderata. First, a systematic elaboration of the implicit premises of hermeneutic text interpretations is still missing. What premises of hermeneutics do we have to consider when we want to use visualizations as tools for the interpretation process? We propose four premises for hermeneutic visualizations. These can be summarized as (1) differentiation author/text, (2) hermeneutic circle and (3) dependency text/recipient.</p>
</blockquote>
<p>Second, data visualizations within the DH scholarship are often limited to a representational usage. The current data visualization paradigm in the digital humanities foregrounds operations and transformations of input data at the expense of the human user whose agency as producer and interpreter of visualizations is largely ignored. This has triggered a conceptual and technical reification of visualization that invites its misinterpretation as an objective representation. At worst, visualizations are taken as quasi-objects that appear to exist on the same ontological level as the objects whose properties they claim to faithfully represent. But especially in digital literary studies, we are dealing with data that rather refer to an interpretation process. Polyvalence is a characteristic of this data generated by interpretation. Because of the assumed objective nature of the data, polyvalence is not accounted for in the visual representation of the visual variables, which raises a range of questions: What demands must be made on a critical interface and visualization concept for hermeneutic text interpretations? What do hermeneutic visualizations look like? How do we create them? In order to tackle these questions we propose four postulates for hermeneutic visualizations: <em>Two Way Screen, Quality, Parallax</em> and <em>Discourse</em> [cf.<a class="footnote-ref" href="#meister2017"> [meister2017] </a>;<a class="footnote-ref" href="#drucker2018"> [drucker2018] </a>]. These four postulates serve as guidelines for creating hermeneutic visualizations and embedding them in user interfaces. To get our postulates on a concrete footing, we will demonstrate their usefulness with the help of the interactive visualization prototype “Stereoscope” .</p>
<p>In our paper, we want to raise awareness for the extent to which research in digital humanities always contributes to the “epistemic self-enlightenment” <a class="footnote-ref" href="#albrecht2015"> [albrecht2015] </a>of one’s own discipline. This article aims to show how digital humanities research is based on a thorough and continuous reflection of the epistemological principles at work. Our article represents an attempt to think visualization in terms of literary hermeneutics. Contrary to current narratives of an “end of theory” (cf.<a class="footnote-ref" href="#anderson2008"> [anderson2008] </a>) or a post-theoretical era (cf.<a class="footnote-ref" href="#scheinfeldt2012"> [scheinfeldt2012] </a>) in the DH, we argue for a productive discussion of theories, in our case literary theories. Against this background, we believe that the question of hermeneutic visualizations gains an exemplary status. We are not only interested in demonstrating possibilities of operationalization for the singular case of hermeneutic visualizations. Rather, we believe that our case of hermeneutic visualization can be regarded as a prototypical procedure with respect to the question of methodology in digital humanities research in general. The issue of “hermeneutic visualization” is, we argue, both a demonstration object and a medium of reflection.</p>
<p>Here is the structure of our argument: In section 2, we will begin with a short synopsis of the development of classic hermeneutics and an exposition and explanation of the three epistemological premises. Against this backdrop, we will discuss how visualizations are well-suited to represent these activities, but are also lacking qualities in order to meet the epistemological premises. Section 3 will address these lacking qualities by discussing four postulates. In section 4 we will then demonstrate how these postulates have been addressed in a software prototype. In closing, we reflect on the results and discuss directions for future research.</p>
<h2 id="2-from-the-hermeneutic-foundation-to-digital-hermeneutic-visualizations">2. From the Hermeneutic Foundation to Digital Hermeneutic Visualizations</h2>
<p>This section is initially dedicated elaborating implicit premises in hermeneutic literary theory. These premises are then the lens through which we discuss current hermeneutical approaches in digital humanities, especially with regard to the role of visualizations.</p>
<h2 id="21-interpretation-theory-method-and-argument-in-literary-studies">2.1 Interpretation, Theory, Method, and Argument in Literary Studies</h2>
<p>Among the main activities of researchers working in the field of literary studies one can count the following: interpretation, development and application of literary theory and of methods, argumentation. These core activities form part of the wider literary studies framework and therefore warrant closer inspection. In order to investigate hermeneutic visualizations we think that work on our terminology is required.</p>
<p>Interpretation is considered one of the main activities of literary studies (cf.<a class="footnote-ref" href="#albrecht2015"> [albrecht2015] </a>).<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> The terminterpretation, a derivation from the Latin word “interpretatio: understand, explain, translate” , is defined as “the methodically induced result of understanding texts in their totality” <a class="footnote-ref" href="#spree2000"> [spree2000] </a>as well as “the formulation of hypotheses about aspects of meaning in literary texts” <a class="footnote-ref" href="#gius2017"> [gius2017] </a>. Following Winko<a class="footnote-ref" href="#winko2003"> [winko2003] </a>the synthesizing interpretation can, on the one hand, presuppose the results of a rather descriptive and analytical text analysis. Analysis and interpretation are thus considered in a procedural relationship. On the other hand, text analysis and interpretation can be determined as synonyms, since both terms are based on the same rules of meaning-making treatment of the text. Despite the idea of a reasoning process that does not require any principles or the claim of some literary scholars “that literary texts are ambiguous or polyvalent by nature” , as Gius &amp; Jacke<a class="footnote-ref" href="#giusjacke2017"> [giusjacke2017] </a>point out, a literary interpretation is based on rules [cf.<a class="footnote-ref" href="#jannidis2003"> [jannidis2003] </a>]. These rules, which are applied in a reasoning process, can be provided by different theoretical approaches (cf.<a class="footnote-ref" href="#spree2000"> [spree2000] </a>).</p>
<p>A <em>literary theory</em> can be defined as an</p>
<blockquote>
<p>explicit, elaborated, logical structured system of categories in order to describe, explore or explain certain issues [of texts]<br>
<a class="footnote-ref" href="#nunning2010"> [nunning2010] </a>. Literary theories provide not only specific epistemological implications regarding, for example, the concept of authorship or the relevance of contexts, but also contain an implicit idea of meaning. During decades of theoretical debates and throughout different turns, the parameters indicating or representing meaning have shifted (cf.<a class="footnote-ref" href="#jannidis2003"> [jannidis2003] </a>). Besides the epistemological implications, this also changed the definition of what actually constitutes a research object as such.<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup> Methods, however, differ from theories. A literary method is a procedure for accomplishing knowledge in a research inquiry. Methods can be characterized as purposeful and rule-based (cf.<a class="footnote-ref" href="#nunning2010"> [nunning2010] </a>;<a class="footnote-ref" href="#winko2003"> [winko2003] </a>). A theory could not only encompass one or a set of several methods, but also demand the application of methods with varying degrees of specification (for example, deductive or dialectical methods compared to the more general operations such as reading or generating hypotheses).</p>
</blockquote>
<p>Argumentation is described as the “unfolding of given proofs” by Cicero in De partitione Oratoria (cf.<a class="footnote-ref" href="#radle2000"> [radle2000] </a>). In literary studies, argumentation plays an important role for the process of generating and validating interpretations (cf.<a class="footnote-ref" href="#albrecht2015"> [albrecht2015] </a>). Argumentation can be described as a formal or logical organization of single observations, that serve as arguments “to provide evidence in favor of some point of view” <a class="footnote-ref" href="#groarke2017"> [groarke2017] </a>. Krämer<a class="footnote-ref" href="#kramer2015"> [kramer2015] </a>speaks of “practices of arguing” , that means “patterns of links, in which certain types of text data can be associated with certain ways of attributing meaning.” An argumentation explicates the interpretative process in a textual or visual form by structuring, connecting, and subsuming single observations (cf.<a class="footnote-ref" href="#eemeren2009"> [eemeren2009] </a>). Von Savigny, on the one hand, distinguishes eight types of literary arguments, for example “arguments of understanding” or “poetic arguments” <a class="footnote-ref" href="#savigny1976"> [savigny1976] </a>. Kindt and Schmidt<a class="footnote-ref" href="#kindt1976"> [kindt1976] </a>, on the other hand, mention three attributes for the evaluation of an argumentation: rigor, intersubjectivity, validation. One issue, for example, is the idea of “evidentiary transparency” <a class="footnote-ref" href="#piper2020"> [piper2020] </a>. How can we verify a literary hypothesis? Is it acceptable for a hypothesis to merely resist falsification, or does it need to be positively confirmed via case studies? (cf.<a class="footnote-ref" href="#albrecht2015"> [albrecht2015] </a>).</p>
<h2 id="22-foundations-of-literary-hermeneutics">2.2 Foundations of Literary Hermeneutics</h2>
<p>The next point to be addressed concerns the foundations of the type of hermeneutics practiced in the field of literary studies. Thus, the first sentence of Szondi’s <em>Introduction to Literary Hermeneutics</em> <a class="footnote-ref" href="#szondi1995"> [szondi1995] </a>reads: “Literary hermeneutics is the study of the interpretation [&hellip;] of literary works.” Köppe &amp; Winko<a class="footnote-ref" href="#winko2003"> [winko2003] </a>point out that the hermeneutic approach is a “precursor” theory, which is not practiced anymore. But according to introductions into literary studies, hermeneutic theory is not only still widespread, it is often also mentioned before all other theories (cf.<a class="footnote-ref" href="#jessing2007"> [jessing2007] </a>;<a class="footnote-ref" href="#nunning2010"> [nunning2010] </a>;<a class="footnote-ref" href="#jahraus2002"> [jahraus2002] </a>). While Stiegler notes a kind of “hermeneutics bashing” <a class="footnote-ref" href="#stiegler2015"> [stiegler2015] </a>conducted by other theoretical approaches, hermeneutic theory and its methodological tradition constitute an essential approach to text interpretation.</p>
<p>Essential for hermeneutic theory is the idea of an understanding, which aims to reach a deeper meaning or hidden reason of a text. Consequently, it is assumed that (literary) texts have a meaning, which can be exposed under certain conditions. This meaning does not have an objective, but rather an observer-dependent and contextual status. In that regard, the hermeneutic approach differs widely, for example, from Derrida’s deconstruction (cf.<a class="footnote-ref" href="#derrida1967"> [derrida1967] </a>). In his work <em>Hermeneutik und Kritik mit besonderer Beziehung auf das Neue Testament</em> (1838) Friedrich Schleiermacher (1768–1834) stresses two important epistemological premises of the hermeneutic understanding of meaning. First, Schleiermacher differentiates between the intentions of the author and the expressions in the text. Schleiermacher’s distinction leads to the idea of an autonomous intention of the text, which is not necessarily congruent with the intention of the author. Therefore, the text is regarded as an artificial and aesthetic work of art with a specific meaning (cf.<a class="footnote-ref" href="#selbmann2002"> [selbmann2002] </a>). Second, Schleiermacher argues that a profound understanding of the text corresponds with the holistic dependency of parts and the whole. He proposes the “Grundsatz der Ganzheit” : “[T]he same way that the whole is, of course, understood in reference to the individual, so too, the individual can only be understood in reference to the whole” (cf.<a class="footnote-ref" href="#schleiermacher1838"> [schleiermacher1838] </a>,<a class="footnote-ref" href="#mantzavinos2016"> [mantzavinos2016] </a>).</p>
<p>Moreover, the philologist Friedrich Ast (1778–1841) and later Schleiermacher emphasize the circular procedure of interpretation, i.e., the hermeneutic circle. In literary studies, the hermeneutic circle or spiral is regarded as an instrument for the formulation of a hypothesis connecting a meaningful whole and its elements [cf.<a class="footnote-ref" href="#otoole2018"> [otoole2018] </a>]. In Gadamer’s conception of hermeneutic experience, understanding is also determined as a circular movement that arises precisely through the examination of the text. The circular structure of understanding thereby aims to change the view of the world in a way that reveals something new or revises old experiences and prejudices (cf.<a class="footnote-ref" href="#rese2010"> [rese2010] </a>). The negation of certain experiences through the reading and analysis of a text leads to the transformation of the horizon of understanding (cf.<a class="footnote-ref" href="#gadamer1990"> [gadamer1990] </a>). “Textual understanding” , as Gius &amp; Jacke<a class="footnote-ref" href="#gius2017"> [gius2017] </a>describe it, “is attained in the interplay between (contextual) assumptions about the text on the one hand, and textual data on the other hand […].” Thus, the act of interpretation constitutes a specific practice of a “reading and questioning […], back and forth, shifting the focus of one’s attention and revising interim interpretations and judgements along the way” <a class="footnote-ref" href="#chamber2006"> [chamber2006] </a>.<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup></p>
<p>Another premise of the hermeneutic method – besides the differentiation between author and text intention and the holistic premise – is the highly valued co-dependency between the text and the recipient. The co-dependency is linked to the issue of context and subjective or social perceptions and views. According to Gadamer’s <em>fusion of horizons</em> , a recipient, who engages with the text in a productive way, generates partial and subjective knowledge. This generated knowledge in the form of meaning “can neither be deduced theoretically, nor be fully articulated, but rests on a kind of tact or sensitivity that is only exhibited in the form of exemplary judgments and interpretations” <a class="footnote-ref" href="#ramberg2005"> [ramberg2005] </a>. Gius &amp; Jacke<a class="footnote-ref" href="#giusjacke2017"> [giusjacke2017] </a>explain: “Because these reasoning processes are non-deductive, i.e., they are not strictly based on rules of deductive logic, they may result in more than one account of meaning.”</p>
<p>In summary, we define literary hermeneutics as a specific approach to produce meaning through an iterative, non-deterministic, and subjective procedure. Essential for this approach are three premises, as explicated in the previous paragraphs:<br>
The differentiation between intentions of author and textThe holistic premise (hermeneutic circle)The dependency between text and recipient</p>
<p>The hermeneutic approach is one possibility to tackle the complexity of text comprehension. Further research could investigate visualization in other literary interpretative processes based on Derrida’s idea of deconstruction or Foucauldian parameters of discourse, for example.</p>
<h2 id="23-conceptions-of-digital-literary-hermeneutics">2.3 Conceptions of Digital Literary Hermeneutics</h2>
<p>So far, hermeneutics, as understood in traditional literary studies and based on these three premises, has not played a prominent role in digital humanities. As van Zundert<a class="footnote-ref" href="#zundert2016"> [zundert2016] </a>states:</p>
<blockquote>
<p>The dialogue surrounding hermeneutics seems not to have developed fully yet in digital humanities – references to hermeneutics are scant and often at a concrete level of the practice of text interpretation, such as when Katherine Hayles (2012) uses the phrasehermeneutic close reading. Yet from several paragraphs and sections in the literature the emergence of a debate seems traceable.</p>
</blockquote>
<p>Literary scholars participating in this debate on hermeneutics in digital humanities or <em>digital hermeneutics</em> , as it is often called, have different views on how (the use) of digital technology might shape literary hermeneutics and what digital literary hermeneutics should encompass. Generally speaking, the debate is dominated by attempts to digitally replicate interpretative processes known from the analog world. However, a systematic effort to reflect on how the hermeneutic premises might be answered by digital technology is still missing.</p>
<p>Commonly, approaches toward a digital literary hermeneutics, or more generally toward interpretation, share the notion that it involves a process of “reconfiguration, reorganization or restructuring” <a class="footnote-ref" href="#armaselu2017"> [armaselu2017] </a>, or as Samuels &amp; McGann<a class="footnote-ref" href="#samuels1999"> [samuels1999] </a>describe it, “deformance” . Rockwell<a class="footnote-ref" href="#rockwell2003"> [rockwell2003] </a>calls the results of algorithmic analysis of texts “hybrid texts” that operate as “interpretive aids” : “[T]hey are generated by processes of taking information apart and putting it back together into new configurations for the purposes of discovery and reflection.” This reconfiguration can be carried out automatically by an algorithm, as is the case, for example, in concordances, or by manual annotations and comments of text passages by scholars [cf.<a class="footnote-ref" href="#rapp2017"> [rapp2017] </a>;<a class="footnote-ref" href="#jacke2018"> [jacke2018] </a>]. While the former is idiosyncratic to the digital realm, the latter has been practiced in literary hermeneutics for a long time. In terms of possibilities to reconfigure and restructure, however, the digital world grants considerably more freedom than analog annotations. Bradley<a class="footnote-ref" href="#bradley2008"> [bradley2008] </a>describes a research software prototype called <em>Pliny</em> that is guided by scholarly practice of interpretation in the analog world:</p>
<blockquote>
<p>Notetaking, and this kind of juggling of notes to discover previously unrecognised patterns and relationships and to stimulate new ideas is one of the long established methods of scholarship.</p>
</blockquote>
<p><em>Pliny</em> allows scholars to annotate texts, images and other media by creating digital notes that can be arranged to one’s likings on a plane. Relationships between notes can be conveyed by placing them in spatial proximity or by nesting notes to account for hierarchical relationships. In contrast to analog environments, notes can be reused in different structures and contexts as they are references, not actual objects. References between all the notes can be visualized in a special graph view.</p>
<p>Boot (2009) takes up Bradley’s tripartition of the scholarly process into “Reading and Annotation (Resource)” , “Developing Interpretation” and “Presentation of Interpretation (Article/Argument)” and describes the structure of annotations as “mesotext” that is made out of “mesodata” (individual annotations). <em>Mesotext</em> acts as a connector between the primary text (which it references) and “secondary texts” or “narratives” (the article a scholar is working on), for which it provides arguments. Similar to <em>Pliny</em> , allowing scholars to adjust the <em>mesotext</em> structure, when new insights have been gained, the concept comes close to the traditional analog annotation process.</p>
<p>As a clear differentiation from the scientific method and a way of strengthening the literary hermeneutic approach, some scholars argue for exploration or a “hermeneutic of play” <a class="footnote-ref" href="#rockwell2003"> [rockwell2003] </a>. van Zundert<a class="footnote-ref" href="#zundert2016"> [zundert2016] </a>calls for a usage of data not so much as evidence in the scientific sense, but rather as a resource to “provoke new questions and explorations” that can be utilized in a “playful iterative approach” . Ramsay<a class="footnote-ref" href="#ramsay2007"> [ramsay2007] </a>even speaks of a “Screwmeneutical Imperative” that scholars should follow, an obligation to be playful and try out things.</p>
<h2 id="24-towards-hermeneutic-visualizations">2.4 Towards Hermeneutic Visualizations</h2>
<p>In addition to the still pending consideration of hermeneutic premises in digital literary studies, we now take a look at approaches from data visualization. How are visualizations used in digital literary studies so far? Why does an “uncritical” use of data visualizations possibly interfere with an interpretation process? For this purpose, we discuss two ways of incorporating visualization into digital humanities research: humanizing visualizations and visual text analysis.</p>
<p>Firstly, research approaches can be brought together that ask about the critical potential of data visualizations in the humanities. The starting point is often the criticism of the purely instrumental function of visualizations. Visualizations serve, following Card et al.<a class="footnote-ref" href="#card1999"> [card1999] </a>, as “external aids” . Hinrichs et al.<a class="footnote-ref" href="#hinrichs2019"> [hinrichs2019] </a>critically conclude that visualizations have been considered only as tools “to facilitate quantitative and qualitative analysis processes, potentially within any research discipline or practice.” Further the authors explain that “this pragmatic approach risks overlooking the research value of visualization and relegating computer science and design to service-based roles.” Moreover, Correll<a class="footnote-ref" href="#correll2019"> [correll2019] </a>even argues “that visualization is a bad neighbor to the digital humanities: it exacerbates the worst tendencies of DH scholarship and promotes parasitic, technocratic collaborations.” In this context, he pleas not only for new alliances between the humanities and visualization research. Rather, he points out, that “[w]e need to humanize visualization before we visualize the humanities.” Traditionally, visualizations have been developed and used in scientific contexts. Drucker<a class="footnote-ref" href="#drucker2011"> [drucker2011] </a>explains that “realist models of knowledge” have been instrumental in forming these representations and that “we need to take on the challenge of developing graphical expressions rooted in and appropriate to interpretative activity.”</p>
<p>In order to re-explore new alliances, visualizations are on the one hand reconsidered by means of concepts or function in a humanities context. Seifert et al. speak of visualization as “an effective enabler for exploratory analysis, making it a powerful tool for gaining insight into unexplored data sets.” <a class="footnote-ref" href="#seifert2014"> [seifert2014] </a>Dörk et al.<a class="footnote-ref" href="#dork2013"> [dork2013] </a>go one step further and show a critical approach to information visualization. Disclosure, plurality, contingency, and empowerment are presented as the main characteristics of critical information visualization. In the context of trans-disciplinary research in the digital humanities, Hinrichs et al. advocate for the concept of “sandcastles” , which they describe as “tailored, unique, often stunning yet also transient and unstable interactive visualizations.” <a class="footnote-ref" href="#hinrichs2019"> [hinrichs2019] </a>With this, the authors oppose the reification of data and refer to an iterative process of understanding. In contrast to a conception of visualization as tools, they “elicit critical insights, interpretation, speculation and discussions within and beyond scholarly audiences.”</p>
<p>While Galey &amp; Ruecker<a class="footnote-ref" href="#galey2010"> [galey2010] </a>do not refer to visualizations in particular (although they use visualizations as case studies), they argue for the use of digital artifacts as arguments:</p>
<blockquote>
<p>The digital humanities must not lose sight of the design of artifacts as a critical act, one that may reflect insights into materials and advance an argument about an artifact’s role in the world. Our purpose here is to follow the implications of a hermeneutical approach to design for digital humanities projects that entail the strategic prototyping of digital artifacts.</p>
</blockquote>
<p>However, as Ramsay &amp; Rockwell<a class="footnote-ref" href="#ramsay2012"> [ramsay2012] </a>convincingly point out, Galey &amp; Ruecker’s concept of argument refers rather to the interface of the digital artifact than to the contents of the text which is supposed to be analyzed with the digital tool. Relating to our earlier comment on argumentation in the context of literary studies as a formal or logical organization of single observations that serve as arguments “to provide evidence in favor of some point of view” <a class="footnote-ref" href="#groarke2017"> [groarke2017] </a>, visualizations could serve as another non-linear form of argument that complements textual explications in an argumentation (cf.<a class="footnote-ref" href="#meirelles2019"> [meirelles2019] </a>). In a similar vein, Ramsay &amp; Rockwell also sees visualization tools behaving “like hermeneutical theories” <a class="footnote-ref" href="#ramsay2012"> [ramsay2012] </a>that offer new perspectives on the research object. Kath et al.<a class="footnote-ref" href="#kath2015"> [kath2015] </a>have already brought to attention the need for a second order hermeneutics, termed <em>New Visual Hermeneutics</em> , which can guide the interpretation of such visualizations.</p>
<p>A second research field that plays a role in our demand for hermeneutic visualizations are the approaches from visual text analysis. Although there is a large selection of mainly generic visualization tools scholars can choose from (The TAPoR website<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup> with its large collection gives a good overview), some tools have gained significant popularity for certain use cases and can almost be considered the de facto standard. Stylometry, for example, is usually conducted with the <em>Stylo</em> <sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup> package of the statistical language <em>R</em> , <em>Gephi</em> <sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup> is very popular for visualizing character networks (cf.<a class="footnote-ref" href="#barbot2019"> [barbot2019] </a>). These visualization tools and techniques offer quite complex user interfaces to control the appearance of a visualization. However, a direct manipulation of the resulting visualization is often not possible and so the process is split into two parts: Manipulation of the interface followed by the generation of a static visualization image as an end result. The image does not contain any information on how the scholar got there and what other possible visual configurations might have been possible by changing the parameters and the data used.</p>
<p>Static visualization images produced by visualization tools are decoupled from the raw text and the modeled text data (produced by the scholar and/or an algorithm), neither allowing a direct manipulation of the data, nor a back-and-forth between data and text.</p>
<p>Other software applications for example <em>Voyant Tools</em> <sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup> offer a coexistence of text and data in one user interface, thereby creating the general possibility for hermeneutic process. A constant movement between text and data is a prerequisite formulated by the holistic premise and as a representation of the data the visualization should be tightly linked to the text and allow to view text and visualization side-by-side or at least enable quick changes between these two views. In order to enable that we need to take into account not only the visualization but the user interface surrounding it.<sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup> Most of these tools need to be bent to comply with hermeneutic practice. Especially the third premise, the dependency between recipient and text and how it could be represented, has only started to become an object of special attention in the digital humanities (cf.<a class="footnote-ref" href="#drucker2018"> [drucker2018] </a>,<a class="footnote-ref" href="#binder2014"> [binder2014] </a>,<a class="footnote-ref" href="#theron2018"> [theron2018] </a>,<a class="footnote-ref" href="#piotrowski2019"> [piotrowski2019] </a>).</p>
<p>A special use case of visual text analysis is annotation visualization. As Jannidis et al.<a class="footnote-ref" href="#jannidis2017"> [jannidis2017] </a>explain: “The creation of annotation categories, the discovery of corresponding phenomena and the enrichment with appropriate annotations is part of the hermeneutical analysis process.” Meister<a class="footnote-ref" href="#meister2020"> [meister2020] </a>even claims, that we need to understand annotation as “a methodological mediator that can prove the connectivity of digital methods also and especially for traditionally hermeneutically oriented literary scholars.” Baumann et al.<a class="footnote-ref" href="#baumann2020"> [baumann2020] </a>give a differentiated overview of current approaches to annotation visualization. But what we think has not yet been sufficiently considered in visualization approaches is the hermeneutic profit of annotation following Gius &amp; Jacke<a class="footnote-ref" href="#giusjacke2017"> [giusjacke2017] </a>. The authors refer to Piez’s<a class="footnote-ref" href="#piez2010"> [piez2010] </a>conception of a hermeneutic markup:</p>
<blockquote>
<p>Byhermeneutic markupI mean markup that is deliberately interpretive. It is not limited to describing aspects or features of a text that can be formally defined and objectively verified. Instead, it is devoted to recording a scholar’s or analyst’s observations and conjectures in an open-ended way.</p>
</blockquote>
<p>According to Gius &amp; Jacke, what constitutes Piez’s approach to hermeneutical markup is that it is precisely the polyvalence and context sensitivity of literary texts that are taken into account. In order for visualizations to meet hermeneutical requirements, they need to be linked to annotations as interpretative endeavors [cf.<a class="footnote-ref" href="#zirker2017"> [zirker2017] </a>,<a class="footnote-ref" href="#nantke2020"> [nantke2020] </a>]. Polyvalence in particular is not only the decisive feature of interpretative data. Rather, the acknowledgement of polyvalence is one of the conceptual requirements that need to be taken into account in the sense of our hermeneutical premises when using visualizations. This is also linked to the distinction of data and capta, as emphasized by Drucker. In other words, in order to serve this purpose of polyvalence, hermeneutic visualizations must value the performative and theoretical aspect of an interpretative endeavor (modelling, theorizing, critique and discourse) higher than the declarative visual output (results, experimental validation, applicability).</p>
<p>We have now discussed approaches of digital hermeneutics and data visualization against the background of our premises. Based on our research approach, the question now arises which requirements visualizations have to fulfill in order to support hermeneutic interpretation processes. For this, we will propose the four postulates that help creating and embedding such visualizations into a user interface.<sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup></p>
<h2 id="3-hermeneutic-visualization-four-postulates">3. Hermeneutic Visualization: Four Postulates</h2>
<p>As we have seen, one issue of the current approaches to digital hermeneutics is the disregard of the epistemological premises of hermeneutics, i.e., the differentiation between intention of author and text, the holistic idea of the understanding of the whole and its parts (circularity of the interpretation process), as well as the dependency between the text and the recipient (subjective and context-dependent reasoning process). Second, the visualizations used in the DH seem to contradict the hermeneutical premises. We see a distortion of the hermeneutic interpretation process, for example, in the use of visualizations that do not take into account the polyvalence of interpretative data. However, in order for visualizations to be used as the missing link in the process of “becoming hermeneutical” in the digital humanities, we need a concept of visualizations that meets the following four postulates as guidelines: <em>Two Way Screen, Quality, Parallax and Discourse</em> . We understand these postulates as transformers of and mediators between the theoretical hermeneutic model and the concrete visual arrangement.</p>
<p>The postulate of the <em>Two Way Screen</em> refers to the interface, which should not be restricted to rendering, but allow manipulation as well. More precisely, a commitment to the <em>Two Way Screen</em> implies that the screen serves as an interactive and visual environment in which interpretation (ranging from low-level annotation and structuring to high-level theorizing activity) takes place, not only gets displayed [cf.<a class="footnote-ref" href="#drucker2018"> [drucker2018] </a>].</p>
<p>The structure of the interface does not serve as a mere representation space for an interpretative result. Rather, the interface provides incentives to engage and to change bidirectionally between the representation of text data and the modelling of text data. This means that actions taken by changing any graphical feature as an act of interpretation are registered as new data and/or as changes in the data model on the fly. The underlying principle is to get away from the flat screen as a space of display by acknowledging the additional dimension of interpretative activity [cf.<a class="footnote-ref" href="#drucker2016"> [drucker2016] </a>]. The postulate of the <em>Two Way Screen</em> is based on the holistic premise, as it allows a continuous shift between exploring the visualization to learn something about the text (the whole) and applying that new knowledge to change text data (the part), in consequence, creating new representations. Here, as well as in the postulate of <em>Quality</em> , the constructedness of the data becomes apparent and we can grasp it “as capta, taken and constructed” <a class="footnote-ref" href="#drucker2011"> [drucker2011] </a>.</p>




























<figure ><img loading="lazy" alt="A hand-drawn representation of a 3DH project showing the relationship between data and the display." src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Drucker<a class="footnote-ref" href="#drucker2016"> [drucker2016] </a>: Conception of the Two-Way-Screen (draft originated in the 3DH project)
        </p>
    </figcaption>
</figure>
<p>While the postulate of the <em>Two Way Screen</em> formulates the necessity of providing means for changing and constructing interpretative data through the visualization, it does not specify how the data might be visually represented. To this end, the postulate of <em>Quality</em> demands the incorporation of the epistemological qualities of hermeneutic practice into the visualization. Responding to the hermeneutic premise of the dependency between text and recipient, <em>Quality</em> takes into account the subjective and contextual quality of the data by showing the annotated text data as <em>capta</em> . We suggest an extension of the use of Bertin’s visual variables (position, color, tone, size, shape etc.) [cf.<a class="footnote-ref" href="#bertin1983"> [bertin1983] </a>;<a class="footnote-ref" href="#meirelles2019"> [meirelles2019] </a>] to the encoding of <em>capta</em> , allowing literary scholars to express interpretative dimensions like salience or relatedness [cf.<a class="footnote-ref" href="#drucker2018"> [drucker2018] </a>].</p>
<p>The third postulate of <em>Parallax</em> stresses the importance of providing multiple views on the object of hermeneutic inquiry [cf.<a class="footnote-ref" href="#drucker2018"> [drucker2018] </a>]. The termparallax “(Greek παράλλαξις (parallaxis)), meaning alternation ” [cf.<a href="https://en.oxforddictionaries.com/definition/parallax">Oxford English Dictionary</a>] is a metaphorizedterminus technicusof optics. We understand Parallax as visual multiperspectivity or multiple points of view that reveal the ambiguity of a text. Coles<a class="footnote-ref" href="#mccurdy2016"> [mccurdy2016] </a>stresses the aesthetic importance of the “ambiguity of meaning” in contrast to scientific replicability. Ambiguity, as Berndt<a class="footnote-ref" href="#berndt2009"> [berndt2009] </a>points out, “denotes a fundamental ‘equivocalness’ that engenders uncertainty and doubt .” The visualization in its parallax function, hence, provokes an ambiguity of a maybe “assumed” certainty or evidence. This provocation generated by the visualization relates to the premise of dependency between text and recipient once more and puts the situatedness and partialness of the hermeneutic reasoning process into effect. Moreover, the ambiguity evokes a “questionability whose astonishment gives cause to further research” <a class="footnote-ref" href="#mersch2009"> [mersch2009] </a>. Instead of limiting the points of view, the postulate of <em>Parallax</em> increases the possibility for productive contradiction in the reasoning process.</p>
<p>The last postulate <em>Discourse</em> defines the role of the visualization in the argumentation. Following Latour<a class="footnote-ref" href="#latour1986"> [latour1986] </a>, who claims that “the ways in which we represent our arguments changes the way in which we argue” (cf.<a class="footnote-ref" href="#hinrichs2017"> [hinrichs2017] </a>), we think that a hermeneutic visualization fosters the critical reflection of the hermeneutic process itself. An argumentation comprised of text as well as visualizations as single observations differs from the mere textual form. The connection between visualization, annotations (the object of study) and textual arguments enables a complex, non-linear movement between these entities that does not restrict scholars to one possible reading, but allows a multitude of readings. Furthermore, the direct connection between annotations and visualizations creates a transparency of individual arguments that invites the author as well as the audience to critically reflect the argumentation. In this way, it lives up to the evaluation criteria rigor, intersubjectivity and validation mentioned in 2.1 and leads to an iterative refinement of the argumentation and an oscillation between part and whole, addressing the holistic premise in that way.</p>
<p>The postulates describe four interrelated aspects, under which visualizations can be beneficial for the hermeneutic process in the digital realm and act as hermeneutic visualizations. In the next part, we would like to underpin their validity by presenting their exemplary application in an interface concept and its prototypical implementation.</p>
<h2 id="4-the-four-postulates-used-as-guidelines-for-prototypical-implementation">4. The Four Postulates Used as Guidelines for Prototypical Implementation</h2>
<p>Referring to Boot’s<a class="footnote-ref" href="#boot2009"> [boot2009] </a>model of <em>mesotext</em> as a particular configuration of annotations ( <em>mesodata</em> ) that relates to the primary text, as well as to the secondary text (an article for example), we incorporated a tripartite user interface in our concept. There is a text area on the left side holding the primary text a scholar is studying, a canvas in the middle that can represent different configurations of annotations of the text with different visualizations and a views area on the right side that allows a scholar to build arguments by saving different views of canvasses with tags and comments assigned to them (Fig. 2). All these parts are connected with each other, so that interacting with one part of the interface, such as a mouse hover over an annotation in the text area, leads to a highlight of that annotation in the canvas area (or highlight of the visually represented annotation, respectively).</p>
<p>Our concept will be illustrated by screenshots of our prototype <em>Stereoscope</em> that implements the most important features of the concept. Stereoscope is a web-based software prototype for visualizing two core processes of literary studies: hermeneutic exploration of textual meaning and construction of arguments about texts. In Stereoscope scholars can represent their manually created digital annotations with multiple visualizations to record and convey qualitative statements.<sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup> In this article, we use a German-based text, but Stereoscope can be applied to texts in other languages, too.</p>




























<figure ><img loading="lazy" alt="A screenshot of a Stereoscope scatter plot of various colored circles in clusters." src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>User interface of the Stereoscope prototype with “overlays” layout selected
        </p>
    </figcaption>
</figure>
<h2 id="text">Text</h2>
<p>The area on the left side shows the primary text a scholar is working on and the parts of the text that have already been annotated. Hovering over annotations produces a pop-up that informs about the categories of the annotations for the respective text passage. Annotations can be saved to a selection by clicking them. A toggle switch at the top of the text area allows scholars to switch between the linear text view and a view of the selected annotations.</p>
<p>The prototype allows to upload a text file together with an annotation file created by the software <em>CATMA</em> . For practical purposes the prototype was developed to work with the CATMA format, however, a compatibility with other formats would be desirable.</p>
<h2 id="canvas">Canvas</h2>
<p>The canvas is the larger area in the middle of the interface that serves as a plane for creating configurations expressed through visualizations. Each visualization contains circles of different sizes that represent annotations. We call these circles <em>glyphs</em> . Their size informs the length of individual annotations. While the circles themselves are immutable, the position of glyphs on the canvas can change depending on the visualization layout scholars have selected. Furthermore, different types of relationships between annotations can be expressed with connecting lines between glyphs. Currently, there is only one type of relationship that depicts the degree of textual proximity of text passages in the <em>overlaps</em> layout.</p>
<p>When hovering over a glyph a little pop-up reveals the type of annotation category. The category is also expressed by the color of the circle. Clicking on a glyph causes the text area to scroll to the corresponding annotated text passage. Analogous to the text area, alt-clicking on glyphs allows scholars to collect annotated text passages that can be viewed in the text area in the selected <em>annotations</em> mode (Fig. 3).</p>




























<figure ><img loading="lazy" alt="Screenshot of a scatter plot with various colored circles depicting glyphs." src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Selected annotations mode: Collected annotations shown on the left and corresponding glyphs highlighted on the canvas
        </p>
    </figcaption>
</figure>
<p>Using the scroll wheel of the mouse or a pinch gesture on the track pad visualizations can be zoomed in and out of and parts of the visualization can be moved into focus.</p>
<p>Above the canvas area different controls allow scholars to change the layout, show and hide panels and labels and export the current view as an image. We call the current state of the visualization on the canvas a “view” .</p>
<p>There are three selectable panels for filtering by annotation category, adding comments to a canvas view, and adjusting settings for individual visualizations. When activated, these overlay the canvas in the bottom half (Fig. 4). Scholars can currently select from three different types of visualizations: grid, scatterplot, and overlaps (network diagram). These visualizations are integrated into the prototype as template files and the list can be extended to incorporate further visualization techniques. Scholars are encouraged to add new visualizations that are suited to their individual research questions and needs.</p>




























<figure ><img loading="lazy" alt="A screenshot of a scatter plot with various colored circles." src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Panels filter, comment and layout shown with three categories selected in the filter panel (Annotated text not falling under these categories is grayed out in the text area)
        </p>
    </figcaption>
</figure>
<h2 id="views">Views</h2>
<p>The narrow column on the right side offers space for saving different views of the canvas as small thumbnails. Each view in this area consists of a miniature static image of the selected layout for the view, a title, the name of the layout, tags, and a button to assign tags. If a comment has been written for a particular canvas, it is shown here as well. The currently selected view is marked by an orange border. All manipulations of the canvas, like selected filters or glyphs, adjusted settings or a change in zoom state are saved automatically for each view and are re-established, when scholars click on other canvasses to switch to them.</p>
<p>Clicking the plus sign at the top opens a dialog window for adding a new view. Here, title, layout, and comment can be filled in. All the comments assigned to the views can be searched with a search field at the top. Typing something in there filters the list of views, fading out views that do not contain comments that match the search term. If tags have been assigned to views, clicking on one of them filters the list with the respective tag. In that way, either ad-hoc search strings or tags can be used to create temporary subselections of the list. Individual views can also be exported as images.</p>
<h2 id="using-the-four-postulates-as-guidelines-for-implementation">Using the four postulates as guidelines for implementation</h2>
<p>In this section, we will elaborate on the concrete development of a prototype using the four postulates. Naturally, this prototypical implementation is exemplary and not exhaustive. There are alternative ways of adhering to the postulates when developing a user interface.</p>
<p>As described in the previous part, understanding annotations as <em>capta</em> rather than data, we accounted for this in the interface of the prototype with the possibility of assigning different attribute values to selected annotations (or glyphs, respectively). This is exemplified with two attributes scholars can add: certainty and importance. Both attributes take values on a scale from 1 to 5. Setting these values changes the appearance of the glyphs and saves the changes in the underlying JSON format (see Fig. 5). The altered JSON file can be downloaded for each individual view (by clicking on the respective icon on the thumbnail image in the views area). This functionality provides an example of the postulate of the <em>Two Way Screen</em> , that could be extended to further functionality, like assigning other attributes or the change of categories, for example. Generally, when thinking about applying our concept to a full-fledged software tool, it would be desirable to integrate full annotation functionality into the system, while allowing the manipulation of annotation metadata via text as well as visualization.</p>




























<figure ><img loading="lazy" alt="A screenshot of a scatterplot showing one annotation." src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Changing certainty and importance values changes the appearance of the glyphs and writes these changes to the JSON file
        </p>
    </figcaption>
</figure>
<p>When assigning certainty or importance values scholars create a qualitative statement about the epistemic status of annotations, in that way addressing the <em>Quality</em> postulate. Qualitative statements are not restricted to the individual annotation. Adding comments and tags to views offers a way of making qualitative assessments about a particular configuration of annotations or collection of configurations, respectively (Fig. 6).</p>
<p>In the prototype, visualizations are always based on an automatic structuring algorithm, be it the two scales of the scatterplot or the forces operating in the network layout. In addition to it, the interface concept also includes a functionality that allows scholars to define the spatial structures themselves, for example, by positioning glyphs freely on the canvas or allowing to group them by encircling them with lines drawn on the canvas. Interacting with the glyphs on the canvas in such a way could also be a way to offer meta annotations.</p>




























<figure ><img loading="lazy" alt="A screenshot of a visualization showing a network with various colored circles connected by lines." src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Canvas with assigned comment and tags (Tags visible in the views area on the right side)
        </p>
    </figcaption>
</figure>
<p>In the most basic way, the postulate of <em>Parallax</em> is accomplished by presenting annotations in the context of the surrounding text in a linear fashion side by side with the different non-linear configurations represented by the visualization layouts. Furthermore, with the views area on the right side of the prototype it becomes possible to compare different configurations with each other by switching between them. On another level, the ambiguity mentioned in the postulate is exemplified by the certainty attribute values assigned to glyphs. When looking at a particular visualization on the canvas, the filter and settings panel allow scholars to change the foundation for the representation, for example, by showing only certain categories of annotations in the visualization or to change parameters regarding the visualization layout (Fig. 7).</p>
<p>The views column on the right side of the interface responds to the <em>Discourse</em> postulate. Here, scholars are encouraged to build an argumentation out of visualizations (views) and texts (comments) as single observations.</p>




























<figure ><img loading="lazy" alt="A screenshot of a visualization of overlaps of various colored circles." src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>“Enclosed” lines deselected in the settings panel (bottom right) for the overlaps layout
        </p>
    </figcaption>
</figure>
<p>Tags assigned to views provide a structuring mechanism that can be used to form different argumentations out of the same views, thus presenting different possible readings to compare with each other.</p>
<p>Scholars can jump between views and, by clicking on them, in that manner read the argumentation in a non-linear way. By investigating individual views they can follow the argument down to the specific annotations in the text that constitute the foundation for the argument. This possibility to drill down creates a transparency in the argumentation that allows critical reflections on the rigor, intersubjectivity, and validation of the argumentation. Figure 8 shows the usage of several views for different argumentations (Fig. 8).</p>




























<figure ><img loading="lazy" alt="A screenshot showing varies views of the interface." src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>A scholar scrolls through views belonging to two different argumentations (three leftmost images). In the right image the tag “Häufungen und Lücken” representing one of the two argumentations has been selected.
        </p>
    </figcaption>
</figure>
<h2 id="5-conclusions">5. Conclusions</h2>
<p>In this article, we suggested four postulates as guidelines for developing hermeneutic visualizations. The resulting visualizations are designed to promote the connection of digital literary hermeneutics with digital approaches, since they address the epistemological premises of hermeneutics, as we have shown in our exemplary prototypical implementation guided by the postulates.</p>
<p>Looking at the hermeneutic visualizations in the prototype, one might notice that they have an appearance similar to traditional visualizations. This leads us back to the question formulated in the introduction: What do hermeneutic visualizations look like? In other words: Are we able to name distinctive qualities of hermeneutic visualizations?</p>
<p>The answer to this is to be found in the nature of hermeneutic theory expressed by the three premises and operationalized by the four postulates. While certainty and importance are typical examples of partial, contextual, and subjective knowledge and are expressed with the help of visual variables in the prototype, the central holistic premise demands an iterative, circular process of generating meaning and forming arguments that becomes visible in the structure of the user interface, but not primarily in individual visualizations.</p>
<p>Future implementations might put a stronger focus on the premise of the dependency between text and recipient (represented by the <em>Qualitative</em> and <em>Parallax</em> postulate), which might result in more examples of visual variables depicting partial, contextual and subjective knowledge or even completely new visualizations. Newness for its own sake, however, has not been our concern here.</p>
<p>Although the presented prototype has been iteratively developed and reviewed by the researchers within our team based on a real-world hermeneutic scenario (Interpretation of Franz Kafka’s <em>In der Strafkolonie</em> ), we are interested to learn more about other scholars’ experience with the prototype in order to further study the appropriateness of the postulates and the idea of hermeneutic visualization. To this end, the prototype has been launched on a website for other scholars to use.<sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup> In addition, the source code has been published on BitBucket<sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup> in order to give interested scholars the opportunity to contribute to the development. Being aware that our prototype cannot address all eventualities of hermeneutic activity, we deemed it important to enable scholars to extend the repertoire of hermeneutic visualizations that can be used as arguments. The source code provides a visualization template that can be used to develop other visualizations. Following Hinrichs et al.<a class="footnote-ref" href="#hinrichs2019"> [hinrichs2019] </a>, we encourage scholars to come up with new visualizations and adjust existing hermeneutic practices, in that way building “sandcastles” and experimenting with hermeneutic visualizations. The prototype itself necessarily has to be a generic tool, that is capable of supporting a diverse range of hermeneutic scenarios.</p>
<p>Finally, we hope that our research might inspire other researchers to investigate what premises need to be considered in order for visualization to benefit other interpretative approaches. Since some attributes like ambiguity are not specific to hermeneutics, but common to all theories of interpretation, this research might serve as a starting point for the development of respective approaches in other areas.</p>
<h2 id="6-acknowledgements">6. Acknowledgements</h2>
<p>The research and the software prototype Stereoscope was developed as part of the 3DH project <em>Three-Dimensional Dynamic Data Visualisation and Exploration for digital humanities Research</em> at the University of Hamburg (04/2016–12/2018).<sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup> The project is conducted by Jan Christoph Meister. Associated members are Marian Dörk (University of Applied Sciences Potsdam), Johanna Drucker (University of California), Evelyn Gius (TU Darmstadt), Geoffrey Rockwell (University of Alberta), Florian Windhager (Danube University Krems).</p>
<p>Furthermore, we would like to thank Jan Christoph Meister, Jan Horstmann and Marian Dörk for comments and suggestions on this paper.</p>
<h2 id="7-links">7. Links</h2>
<ul>
<li>Article “Parallax” in <em>Oxford English Dictionary</em> . Available at:<a href="https://en.oxforddictionaries.com/definition/parallax">https://en.oxforddictionaries.com/definition/parallax</a></li>
<li><a href="https://voyant-tools.org/">https://voyant-tools.org/</a></li>
<li><a href="https://github.com/computationalstylistics/stylo">https://github.com/computationalstylistics/stylo</a></li>
<li><a href="https://gephi.org/">https://gephi.org/</a></li>
<li><a href="http://tapor.ca">http://tapor.ca</a></li>
<li><a href="http://threedh.net/">http://threedh.net/</a></li>
<li><a href="http://catma.de/">http://catma.de/</a></li>
<li><a href="https://github.com/janerikst/stereoscope">https://github.com/janerikst/stereoscope</a></li>
</ul>
<ul>
<li id="albrecht2015">Albrecht, A., Danneberg, L., Krämer, O., Spoerhase, C., (eds.) (2015). _Theorien, Methode und Praktiken des Interpretierens_ . Berlin: de Gruyter.
</li>
<li id="anderson2008">Anderson, C. (2008). “The End of Theory: The Data Deluge Makes the Scientific Method Obsolete” , in _Wired_ , 23/06/2008. Available at:<a href="https://www.wired.com/2008/06/pb-theory/">https://www.wired.com/2008/06/pb-theory/</a>.
</li>
<li id="armaselu2017">Armaselu, F., van den Heuvel, C., (2017). “Metaphors in Digital Hermeneutics: Zooming through Literary, Didactic and Historical Representations of Imaginary and Existing Cities” , in _DHQ: Digital Humanities Quarterly_ , 11(3). Available at:<a href="http://www.digitalhumanities.org/dhq/vol/11/3/000337/000337.html">http://www.digitalhumanities.org/dhq/vol/11/3/000337/000337.html</a>.
</li>
<li id="barbot2019">Barbot L., Fischer, F., Moranville, Y., Pozdniakov I. (2019). “Which DH Tools are actually used in Research?” , in weltliteratur.net, A Black Market for the digital humanities, Available at:<a href="https://weltliteratur.net/dh-tools-used-in-research/">https://weltliteratur.net/dh-tools-used-in-research/</a>.
</li>
<li id="baumann2020">Baumann, M., Koch, S., John, M., Ertl, T. (2020). “Interactive Visualization for Reflected Text Analytics” , in Reiter, N., Pichler, A., Kuhn, J. (eds.) _Reflektierte algorithmische Textanalyse. Interdisziplinäre(s) Arbeiten in der CRETA-Werkstatt_ , Berlin: de Gruyter, pp. 269-296.
</li>
<li id="berndt2009">Berndt, F. (2009). “In the Twilight Zone. Ambiguity and Aesthetics in Baumgarten” , in Berndt, F., Kammer, S. (eds.) _Amphibolie, Ambiguität, Ambivalenz_ . Würzburg: Königshausen & Neumann, pp. 121-137.
</li>
<li id="bertin1983">Bertin, J. (1983). _Semiology of Graphics: Diagrams, Networks, Maps_ . Madison: University of Wisconsin Press.
</li>
<li id="binder2014">Binder, F., Entrup, B., Schiller, I., Lobin, H. (2014). “Uncertain about Uncertainty: Different ways of processing fuzziness in digital humanities data” , in _Conference Abstracts_ , DH2014, pp. 95-98. Available at:<a href="https://d-nb.info/1164023926/34">https://d-nb.info/1164023926/34</a>.
</li>
<li id="boot2009">Boot, P. (2009). _Mesotext: digitised emblems, modelled annotations and humanities scholarship_ . Amsterdam: Amsterdam University Press.
</li>
<li id="bradley2008">Bradley, J. (2008). “Thinking about interpretation: Pliny and scholarship in the humanities” , in _Literary and linguistic computing_ , 23(3), pp. 263-279.
</li>
<li id="card1999">Card, S., Mackinlay, J., Shneiderman, B. (1999). _Readings in information visualization: Using vision to think_ . San Francisco: Kaufmann.
</li>
<li id="chamber2006">Chambers, E., Marshall G. (2006). _Teaching & Learning English Literature_ . Thousand Oaks: SAGE Publications.
</li>
<li id="correll2019">Correll, M. (2019). _Counting, Collaborating, and Coexisting: Visualization and the digital humanities_ . Available at:<a href="https://mcorrell.medium.com/counting-collaborating-and-coexisting-visualization-and-the-digital-humanities-1bf157400d8">https://mcorrell.medium.com/counting-collaborating-and-coexisting-visualization-and-the-digital-humanities-1bf157400d8</a>.
</li>
<li id="danneberg1995">Danneberg, L. (1995). “Die Historiographie des hermeneutischen Zirkels: Fake und fiction eines Behauptungsdiskurses” , in _Zeitschrift für Germanistik_ , 5(3), pp. 611-624.
</li>
<li id="derrida1967">Derrida, J. (1967). _De la grammatologie_ [dt. Grammatologie 1974, 2004]. Paris: Editions de Minuit, Frankfurt: Suhrkamp.
</li>
<li id="dork2013">Dörk, M., Feng P., Collins, C., Sheelagh C. (2013). _Critical InfoVis: Exploring the Politics of Visualization_ , in _alt.chi 2013_ : Extended Abstracts of the SIGCHI Conference on Human Factors in Computing Systems, ACM, pp. 2189-2198.
</li>
<li id="drucker2011">Drucker, J. (2011). “Humanities approaches to graphical display” , in _DHQ: Digital Humanities Quarterly_ , 5(1). Available at:<a href="http://www.digitalhumanities.org/dhq/vol/5/1/000091/000091.html">http://www.digitalhumanities.org/dhq/vol/5/1/000091/000091.html</a>.
</li>
<li id="drucker2016">Drucker, J. (2016). _3DH Visualizations: Three dimensional / digital humanities_ . Available at:<a href="https://pages.gseis.ucla.edu/faculty/drucker/3DH_Gallery/Text_3DH_Gallery.html">https://pages.gseis.ucla.edu/faculty/drucker/3DH_Gallery/Text_3DH_Gallery.html</a>.
</li>
<li id="drucker2018">Drucker, J. (2018). “Non-representational approaches to modeling interpretation in a graphical environment” , in _Digital Scholarship in the Humanities_ , 33(2), pp. 248-263.
</li>
<li id="eemeren2009">Eemeren, F., Grootendorst, R., Snoeck Henkemans, A. F. (1996, 2009). _Fundamentals of argumentation theory : a handbook of historical backgrounds and contemporary developments_ . New York, London: Routledge [Erlbaum].
</li>
<li id="gadamer1990">Gadamer, H.-G. (1960, 1990). _Hermeneutik I. Wahrheit und Methode. Grundzüge einer philosophischen Hermeneutik_ . Tübingen: Mohr.
</li>
<li id="galey2010">Galey, A., Ruecker, S. (2010). “How a prototype argues” , in _Literary and Linguistic Computing_ , 25(4), pp. 405-424.
</li>
<li id="gibson2014">Gibson, J. (2014). _The ecological approach to visual perception_ . London: Psychology Press.
</li>
<li id="giusjacke2017">Gius, E., Jacke, J. (2017). “The hermeneutic profit of annotation: on preventing and fostering disagreement in literary analysis” , in _International Journal of Humanities and Arts Computing_ , 11(2), pp. 233-254.
</li>
<li id="gius2017">Gius, E., Kleymann, R., Meister J.C., Petris M. (2017). “Datenvisualisierung als Aisthesis” , in _Conference Abstracts_ , DHd 2017, pp. 115-120. Available at:<a href="http://doi.org/10.5281/zenodo.4646123">http://doi.org/10.5281/zenodo.4646123</a>.
</li>
<li id="glinka2017">Glinka, K., Pietsch, C., Dörk, M. (2017). “Past Visions and Reconciling Views: Visualizing Time, Texture and Themes in Cultural Collections” , in _DHQ: Digital Humanities Quarterly_ , 11(2). Available at:<a href="http://www.digitalhumanities.org/dhq/vol/11/2/000290/000290.html">http://www.digitalhumanities.org/dhq/vol/11/2/000290/000290.html</a>.
</li>
<li id="groarke2017">Groarke, L. (2017). “Informal Logic” , in Zalta, E. (ed.) _The Stanford Encyclopedia of Philosophy_ . Available at:<a href="https://plato.stanford.edu/archives/spr2017/entries/logic-informal/">https://plato.stanford.edu/archives/spr2017/entries/logic-informal/</a>.
</li>
<li id="hinrichs2017">Hinrichs, U., Forlini, S. (2017). “In defense of sandcastles: research thinking through visualization in DH” , in _Proceedings of the conference on Digital Humanities_ . International Alliance of Digital Humanities Organizations (ADHO). Available at:<a href="https://dh2017.adho.org/abstracts/133/133.pdf">https://dh2017.adho.org/abstracts/133/133.pdf</a>.
</li>
<li id="hinrichs2019">Hinrichs, U., Forlini, S., Moynihan, B. (2019). “In defense of sandcastles: Research thinking through visualization in digital humanities” in _Digital Scholarship in the Humanities_ , 34(1), pp. 180-199. DOI:<a href="https://doi.org/10.1093/llc/fqy051">https://doi.org/10.1093/llc/fqy051</a>.
</li>
<li id="jacke2018">Jacke, J. (2018). “Manuelle Annotation” , in _forTEXT. Literatur digital erforschen_ . Available at:<a href="http://fortext.net/routinen/methoden/manuelle-annotation">http://fortext.net/routinen/methoden/manuelle-annotation</a>.
</li>
<li id="jahraus2002">Jahraus, O., Neuhaus, S. (eds.) (2002). _Kafkas Urteil und die Literaturtheorie. Zehn Modellanalysen_ . Stuttgart: Reclam.
</li>
<li id="jannidis2003">Jannidis, F., Lauer, G., Martínez, M., Winko, S. (eds.) (2003). “Der Bedeutungsbegriff in der Literaturwissenschaft. Eine historische und systematische Skizze” , in _Regeln der Bedeutung. Zur Theorie der Bedeutung literarischer Texte_ . Berlin: de Gruyter, pp. 3-32.
</li>
<li id="jannidis2017">Jannidis, F., Kohle, H., Rehbein, M. (eds.) (2017). _Digital Humanities. Eine Einführung_ , Stuttgart: Metzler.
</li>
<li id="jessop2008">Jessop, M. (2008). “Digital Visualization as a scholarly activity” , in _Literary and Linguistic Computing_ , 23(3), pp. 281-293.
</li>
<li id="jessing2007">Jeßing, B., Köhnen, R. (2007). _Einführung in Die Neuere Deutsche Literaturwissenschaft_ . Stuttgart [u.a.]: Metzler.
</li>
<li id="kath2015">Kath, R., Schaal, G., Dumm, S. (2015). “New Visual Hermeneutics” , in _Zeitschrift für germanistische Linguistik_ , 43(1), pp. 27-51.
</li>
<li id="kindt1976">Kindt, W., Schmidt, S. (1976). _Interpretationsanalysen: Argumentationsstrukturen in Literaturwissenschaftlichen Interpretationen_ . München: Fink.
</li>
<li id="kramer2015">Krämer, O. (2015). “Goethes Wahlverwandtschaften in Interpretationen von der Geistesgeschichte bis zum Poststrukturalismus” , in Albrecht, A., Danneberg, L., Krämer, O., Spoerhase, C., (eds.) (2015) _Theorien, Methode und Praktiken des Interpretierens_ . Berlin: de Gruyter, pp. 159-203.
</li>
<li id="koppe2013">Köppe, T., Winko, S. (2013). _Neuere Literaturtheorien. Eine Einführung_ . Stuttgart: Metzler.
</li>
<li id="latour1986">Latour, B. (1986). “Visualization and Cognition: Drawing Things Together” , in _Knowledge and Society: Studies in the Sociology of Culture Past and Present_ , 6, pp. 1-40.
</li>
<li id="maceachren2012">MacEachren, A. M., Roth, R. E., O'Brien, J., Li, B., Swingley, D., Gahegan, M. (2012). “Visual semiotics & uncertainty visualization: An empirical study” , in _IEEE Transactions on Visualization and Computer Graphics_ , 18(12), pp. 2496-2505.
</li>
<li id="mantzavinos2016">Mantzavinos, C. (2016). “Hermeneutics,”  _The Stanford Encyclopedia of Philosophy_ (Fall 2020 Edition), Edward N. Zalta (ed.).<a href="https://plato.stanford.edu/archives/fall2020/entries/hermeneutics/">https://plato.stanford.edu/archives/fall2020/entries/hermeneutics/</a>.
</li>
<li id="mccurdy2016">McCurdy, N., Lein, J., Coles, K., Meyer, M. (2016). “Poemage: Visualizing the Sonic Topology of a Poem” , in _IEEE Transactions on Visualization and Computer Graphics_ , 22(1), pp. 439–448.<a href="https://doi.org/10.1109/TVCG.2015.2467811">https://doi.org/10.1109/TVCG.2015.2467811</a>.
</li>
<li id="mersch2009">Mersch, D. (2009). “The Chiasmus of Language - Six Theses of Language and Alterity” , in Berndt, F., Kammer, S. (eds.) _Amphibolie, Ambiguität, Ambivalenz_ . Würzburg: Königshausen & Neumann, pp. 107-120.
</li>
<li id="meirelles2019">Meirelles, I. (2019). “Visualizing information” , in Flanders, J., Jannidis, F. (eds.) _The Shape of Data in the Digital Humanities. Modeling Texts and Text-based Resources_ . London, New York: Routhledge, pp. S. 167-177.
</li>
<li id="meister2017">Meister, J.C., Drucker, J., Rockwell, G. (2017). “Modeling Interpretation in 3DH: New dimensions of visualization” in _Proceedings of the conference on Digital Humanities_ . International Alliance of Digital Humanities Organizations (ADHO). Available at:<a href="https://dh2017.adho.org/abstracts/058/058.pdf">https://dh2017.adho.org/abstracts/058/058.pdf</a>.
</li>
<li id="meister2020">Meister. J.C., (2020). “Annotation als Mark-Up avant la lettre” , in Jannidis F., Winko, S., Rapp, A., Meister J.C., Stäcker T. (eds.) _Digitale Literaturwissenschaft_ . DFG-Symposium Villa Vigoni, 2017. Berlin, Boston: de Gruyter (to appear).
</li>
<li id="mittelstrab2008">Mittelstraß, J. (ed.) (2008). _Enzyklopädie Philosophie und Wissenschaftstheorie_ . Vol. 3. Stuttgart: Metzler.
</li>
<li id="nantke2020">Nantke, J., Schlupkothen, F. (eds.) (2020). _Annotations in Scholarly Editions and Research. Functions, Differentiation, Systematization_ , Berlin, Boston: de Gruyter.
</li>
<li id="nunning2008">Nünning, A. (2008). _Metzler Lexikon Literatur- und Kulturtheorie: Ansätze – Personen – Grundbegriffe_ . Stuttgart: Metzler.
</li>
<li id="nunning2010">Nünning, V., Nünning, A. (2010). _Methoden der literatur- und kulturwissenschaftlichen Textanalyse. Ansätze – Grundlagen – Modellanalysen_ . Stuttgart: Metzler.
</li>
<li id="otoole2018">O'Toole, M. (2018). _The Hermeneutic Spiral and Interpretation in Literature and the Visual Arts_ . New York: Routledge.
</li>
<li id="palmers1969">Palmers, R. E. (1969). _Hermeneutics. Interpretation Theory in Schleiermachers, Dilthey, Heidegger, and Gadamer_ . Evanston: Northwestern University Press.
</li>
<li id="piez2010">Piez, W., (2010). _Towards Hermeneutic Markup: An architectural outline_ . Available at:<a href="http://dh2010.cch.kcl.ac.uk/academic-programme/abstracts/papers/pdf/ab-743.pdf">http://dh2010.cch.kcl.ac.uk/academic-programme/abstracts/papers/pdf/ab-743.pdf</a>.
</li>
<li id="piotrowski2019">Piotrowski, M. (2019). “Accepting and Modeling Uncertainty” , in Kuczera, A., Wübbena, T., Kollatz, T. (eds.) _Die Modellierung des Zweifels – Schlüsselideen und -konzepte zur graphbasierten Modellierung von Unsicherheiten_ . Zeitschrift für digitale Geisteswissenschaften. Wolfenbüttel.<a href="https://zfdg.de/sb004_006">DOI: 10.17175/sb004_006a</a>.
</li>
<li id="piper2020">Piper, A. (2020). _Can We Be Wrong? The Problem of Textual Evidence in a Time of Data_ . Cambridge: Cambridge University Press.
</li>
<li id="radle2000">Rädle, F. (2000). “Argumentum” , in Fricke, H. (ed.) _Reallexikon der deutschen Literaturwissenschaft_ . Vol. 1. Berlin: de Gruyter, pp. 130-132.
</li>
<li id="rapp2017">Rapp, A. (2017). “Manuelle und automatische Annotation” , in Jannidis, F., Kohle, H., Rehbein, M. (eds.) _Digital Humanities. Eine Einführung_ . Stuttgart: Metzler, pp. 253-267.
</li>
<li id="ramsay2007">Ramsay, S. (2007). “Algorithmic criticism” , in Siemens, R., Schreibmann, S. (eds.) _A Companion to Digital Literary Studies_ . Oxford: Blackwell. Available at:<a href="http://www.digitalhumanities.org/companion/view?docId=blackwell/9781405148641/9781405148641.xml&chunk.id=ss1-6-7&toc.depth=1&toc.id=ss1-6-7&brand=9781405148641_brand">http://www.digitalhumanities.org/companion/view?docId=blackwell/9781405148641/9781405148641.xml&chunk.id=ss1-6-7&toc.depth=1&toc.id=ss1-6-7&brand=9781405148641_brand</a>.
</li>
<li id="ramsay2012">Ramsay, S., Rockwell, G., (2012). “Developing Things: Notes toward an Epistemology of Building in the Digital Humanities” , in Gold, M. (ed.) _Debates in the Digital Humanities_ . Available at:<a href="http://dhdebates.gc.cuny.edu/debates/text/11">http://dhdebates.gc.cuny.edu/debates/text/11</a>.
</li>
<li id="ramberg2005">Ramberg, B., Gjesdal, K. (2005). “Hermeneutics” , in Zalta, E. (ed.) _The Stanford Encyclopedia of Philosophy_ . Available at:<a href="https://stanford.library.sydney.edu.au/archives/sum2010/entries/hermeneutics/">https://stanford.library.sydney.edu.au/archives/sum2010/entries/hermeneutics/</a>.
</li>
<li id="rese2010">Rese, F. (2010). “Hans-Georg Gadamer” , in Martínez, M., Scheffel, M. (eds.). _Klassiker der modernen Literaturtheorie_ . München: Beck, pp. 168-190.
</li>
<li id="rockwell2003">Rockwell, G. (2003). “What is text analysis, really?” , in _Literary and linguistic computing_ , 18(2), pp. 209-219.
</li>
<li id="rockwellandsinclair2016">Rockwell, G., Sinclair, S. (2016). _Hermeneutica: Computer-assisted interpretation in the humanities_ . Cambridge, Massachusetts, London, England: The MIT Press.
</li>
<li id="samuels1999">Samuels, L., McGann, J. (1999). “Deformance and interpretation” , in _New Literary History_ , 30(1), pp. 25-56.
</li>
<li id="savigny1976">von Savigny, E. (1976). _Argumentation in der Literaturwissenschaft_ . München: Beck.
</li>
<li id="scheinfeldt2012">Scheinfeldt, T. (2012). “Where’s the Beef? Does Digital Humanities Have to Answer Questions?” , in Gold, M. (ed.) _Debates in Digital Humanities_ . Minnesota: University of Minnesota Press. Available at:<a href="http://dhdebates.gc.cuny.edu/debates/text/18">http://dhdebates.gc.cuny.edu/debates/text/18</a>.
</li>
<li id="schleiermacher1838">Schleiermacher, F. (1838). _Hermeneutik und Kritik_ . Deutsches Textarchiv. Available at:<a href="http://www.deutschestextarchiv.de/schleiermacher_hermeneutik_1838/8">http://www.deutschestextarchiv.de/schleiermacher_hermeneutik_1838/8</a>.
</li>
<li id="schwan2019">Schwan, H., Jacke, J., Kleymann, R., Stange, J. E., Dörk, M. (2019). “Narrelations – Visualizing Narrative Levels and their Correlations with Temporal Phenomena” , in _DHQ: Digital Humanities Quarterly_ , 13(3). Available at:<a href="http://www.digitalhumanities.org/dhq/vol/13/3/000414/000414.html">http://www.digitalhumanities.org/dhq/vol/13/3/000414/000414.html</a>.
</li>
<li id="selbmann2002">Selbmann, R. (2002). “Kafka als Hermeneutiker. Das Urteil im Zirkel der Interpretation” , in Jahraus, O., Neuhaus, S. (eds.) _Kafkas Urteil und die Literaturtheorie. Zehn Modellanalysen_ . Stuttgart: Reclam, pp. 36-58.
</li>
<li id="seifert2014">Seifert, C., Sabol, V., Kienreich, W., Lex, E., Granitzer, M. (2014). “Visual analysis and knowledge discovery for text” , in _Large-Scale Data Analytics_ . New York: Springer, pp. 189-218.
</li>
<li id="sinclair2013">Sinclair, S., Ruecker, S., Radzikowska, M. (2013). “Information Visualization for Humanities Scholars” , in Price, K. M., Siemens, R. (eds.). _Literary studies in the digital age: An evolving anthology. New York: Modern Language Association_ . Available at:<a href="https://www.researchgate.net/publication/273450219_Information_Visualization_for_Humanities_Scholars">DOI: 10.1632/lsda.2013.6</a>.
</li>
<li id="spree2000">Spree, A. (2000). “Interpretation” , in Fricke, H. (ed.): _Reallexikon der deutschen Literaturwissenschaft_ . Vol. 2. Berlin, New York: de Gruyter, pp. 168-172.
</li>
<li id="stiegler2015">Stiegler, B. (2015). _Theorien der Literatur- und Kulturwissenschaften_ . Paderborn: Schöningh.
</li>
<li id="stiening2016">Stiening, G. (2016). “Hermeneutik. Über die Grenzen des Verstehens und die Gefahren ihrer Missachtung” , in Jahraus, O. (ed.) _Zugänge zur Literaturtheorie_ . Stuttgart: Reclam, pp. 54-70.
</li>
<li id="szondi1995">Szondi, P. (1995). _Introduction to literary hermeneutics_ . Trans. by Martha Woodmansee. Cambridge: Cambridge University Press.
</li>
<li id="theron2018">Therón, R., Losada, A. G., Benito, A., Santamaría, R. (2018). “Toward supporting decision-making under uncertainty in digital humanities with progressive visualization” , in _Proceedings of the Sixth International Conference on Technological Ecosystems for Enhancing Multiculturality_ , pp. 826-832.
</li>
<li id="unsworth2000">Unsworth, J. (2000). “Scholarly primitives: What methods do humanities researchers have in common, and how might our tools reflect this” , in _Symposium on Humanities Computing: Formal Methods, Experimental Practice_ . King’s College, London, 3.
</li>
<li id="weimar2000">Weimar, K. (2000). “Hermeneutik” , in Fricke, H. (ed.): _Reallexikon der deutschen Literaturwissenschaft_ . Vol. 2. Berlin, New York: de Gruyter, pp. 25-29.
</li>
<li id="winko2003">Winko, S. (2003). “Textanalyse” , in Fricke, H. (ed.): _Reallexikon der deutschen Literaturwissenschaft_ . Vol. 2. Berlin, New York: de Gruyter, pp. 597-601.
</li>
<li id="zirker2017">Zirker, A., Bauer, M. (2017). “Explanatory Annotation in the Context of the Digital Humanities: Introduction” , in _International Journal of Humanities and Arts Computing - A Journal of Digital Humanities_ , 11 (2017), pp. 145-152.
</li>
<li id="zundert2016">van Zundert, J. (2016). “Screwmeneutics and hermenumericals: the computationality of hermeneutics” , in Schreibmann S., Siemens, R., Unsworth J. (eds.) _A New Companion to Digital Humanities_ . Oxford: Wiley Blackwell, pp. 331-347.
</li>
</ul>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Original German text passages quoted or paraphrased above are translated by the authors of this paper.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>We acknowledge that the chosen terms overlap with terms for example likeheuristic, technique, practice.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Krämer<a class="footnote-ref" href="#kramer2015"> [kramer2015] </a>points out that in the practice of text interpretation different theoretical approaches are often mixed up.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Problems and critics of the hermeneutic circle cf. Danneberg (1995).&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p><a href="http://tapor.ca">http://tapor.ca</a>&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p><a href="https://github.com/computationalstylistics/stylo">https://github.com/computationalstylistics/stylo</a>&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p><a href="https://gephi.org/">https://gephi.org/</a>&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p><a href="https://voyant-tools.org/">https://voyant-tools.org/</a>&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>These insufficiencies which many tools have in common do not prevent hermeneutic practice with them entirely. Literary scholars practicing an hermeneutic approach have found workarounds to enable a critical handling of these visualization tools despite their shortcomings.&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>In our understanding hermeneutic visualizations have to be developed with respect to the user interface that is holding these visualizations. To be able to reconfigure, explore and form arguments with hermeneutic visualizations there has to be a user interface surrounding these visualizations that is oriented towards the hermeneutic process as a whole and allows the manipulation of the visualizations as well as the arrangement of them.&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p><a href="http://stereoscope.threedh.net/">http://stereoscope.threedh.net/</a>&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p><a href="http://threedh.janerikstange.com/">http://threedh.janerikstange.com/</a>(temporary domain)&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13">
<p><a href="https://github.com/janerikst/stereoscope">https://github.com/janerikst/stereoscope</a>&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14">
<p><a href="http://threedh.net/">http://threedh.net/</a>## Bibliography&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry><entry><title type="html">Why Digital Humanists Should Emphasize Situated Data over Capta</title><link href="https://startwords.cdh.princeton.edu/vol/15/2/000556/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://startwords.cdh.princeton.edu/vol/15/2/000556/</id><author><name>Matthew Lavin</name></author><published>2021-06-15T00:00:00+00:00</published><updated>2023-08-04T13:01:02-04:00</updated><content type="html"><![CDATA[<p>As we pass the ten-year anniversary of Johanna Drucker’s “Humanities Approaches to Graphical Display” (2011), I believe it is an opportune time to revisit Drucker’s call to “reconceive all data as capta” <a class="footnote-ref" href="#drucker2011"> [drucker2011] </a>.<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> Drucker’s argument for such a reimagining is based on “the etymological roots” of data and capta — data comes from the Latingiven,and capta from the Latintaken— and raises a larger point about the differences between scientific realism and constructivist critiques of realism.<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> Realist approaches to visualization, Drucker argues, assume “transparency and equivalence, as if the phenomenal world were self-evident and the apprehension of it a mere mechanical task” <a class="footnote-ref" href="#drucker2011"> [drucker2011] </a>. Drucker goes further in stating, “Nothing in intellectual life is self-evident or self-identical, nothing in cultural life is mere fact, and nothing in the phenomenal world gives rise to a record or representation except through constructed expressions” <a class="footnote-ref" href="#drucker2011"> [drucker2011] </a>. Drucker lays out a lexical argument between data and capta that more or less parallels the realist and constructivist modalities of thought as she describes them (i.e., data suggests realism and capta suggests constructivism).<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup></p>
<p>In this piece, I want to argue in favor of embracing some of Drucker’s points about humanistic inquiry while simultaneously arguing against capta as a term to be used in place of data. As a humanist, I do see value in emphasizing that data are taken and not given, but I believe there is a richer etymological narrative, and a richer history of the worddatain English, to be described. There is also the more complicated question of whether all forms of empiricism require proceeding “as if the phenomenal world were self-evident,” and the degree to which social constructions mediate our experiences and understanding of the world. I will gesture at these larger questions in this essay, but my primary concern is with how scholars in digital humanities should approach conversations about data.</p>
<p>The bulk of my essay is a revised and expanded etymology for the termsdataandcapta, which considers their Latin roots, as well as the close ties of the worddatato the publishing history of <em>Euclidis Data</em> (1625) and subsequent translations and editions of that work. Euclid’s own use of the termgivenwas a subject for discussion and debate, as was the degree to which his use ofgivenresembled other Greeks’ use of the term. A fully fleshed out history of the termdatain English includes its use in two related but distinct senses: geometrical and empirical, the latter of these emerging from the prior. To appreciate the difference between these senses, we must understand the degree to which early modern thinkers synthesized geometrical and numerical thought into what is now regarded as the singular discipline of mathematics. Further, these thinkers adapted and extended mathematical thinking to translate observations of or measurements from the natural world into generalizations and explanatory systems.</p>
<p>In the closing section of this essay, I will turn to the benefits of embracing concepts such as situated data over capta. Such an approach allows humanists to contest oversimplifications of poorly executed data-driven inquiry and simultaneously to create more opportunities for conversation with other disciplines. The tone of these conversations could be positively affected, as well, with digital humanities speaking and listening in equal proportion. Lastly, I will discuss the new possibilities that this rhetorical shift could create for how we teach data analysis, which has the potential to advance digital humanities as a discipline.</p>
<h2 id="druckers-conceptual-and-terminological-intervention">Drucker’s Conceptual and Terminological Intervention</h2>
<p>My response to Drucker begins with a certain degree of uncertainty about how “Humanities Approaches to Graphical Display” has been received in digital humanities. Generally, I take it to have made a strong, positive impression. Drucker was not the first scholar to point out thatcaptamight be a more appropriate term thandata, but her work is arguably the most influential to make this point, especially in digital humanities.<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup> This essay, or the corresponding material in Drucker’s book <em>Graphesis</em> (2014), is widely referenced in digital humanities, and I have seen it listed on many DH course syllabi.<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup> There appear to have been numerous conference presentations, blog posts, and discussions of capta on social media. In various contexts, public displays of one’s agreement with Drucker might take the form of statements like, “There is no such thing as data. There are only capta. ” <sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup> Broadly speaking, Drucker has set the terms of the debate on the subject of data and capta, and this influence has been both descriptive and normative. Simultaneously, I have not seen signs of a large-scale movement to purge documents of the worddata, nor have the wordcaptaor its cognates become especially prevalent in digital humanities scholarship.<sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup></p>
<p>One might argue that Drucker makes a conceptual intervention and not a terminological one. I would concede that the essay is most concerned with reconceptualizing data as capta, and the importance of such a shift. With regard to data visualization, Drucker argues that “the rendering of statistical information into graphical form gives it a simplicity and legibility that hides every aspect of the original interpretative framework on which the statistical data were constructed” <a class="footnote-ref" href="#drucker2011"> [drucker2011] </a>. Data do not “pre-exist their parameterization” because they are “constructed as an interpretation of the phenomenal world, not inherent in it” <a class="footnote-ref" href="#drucker2011"> [drucker2011] </a>. Drucker does not explicitly argue that humanities scholars should never say or write the worddata, but she does claim that “all data is capta” nd that because of the etymological roots of the terms,capta “is taken actively while  data  is assumed to be a given able to be recorded and observed” <a class="footnote-ref" href="#drucker2011"> [drucker2011] </a>. As a result, I reject the idea that one can so easily separate Drucker’s conceptual intervention from the terminological one supporting it.</p>
<p>Another way of looking at Drucker’s influence would be to say that there is widespread agreement with her views on data and capta, but practical considerations outweigh other concerns. Such practical considerations include (1) maximizing clarity for particular audiences, (2) limits on space that would make it difficult to explain the choice to usecapta, and (3) the question of how to invoke concepts like big data, data visualization, databases, datasets, metadata, and open data. One might attempt to stake out a rhetorical middle position by mentioning that data are really capta and to proceed with the worddatathereafter, as Rob Kitchin does in <em>The Data Revolution: Big Data, Open Data, Data Infrastructures and Their Consequences</em> (2014). Kitchin adds, “since the term data has been so thoroughly ingrained to mean capta, rather than confuse the matter further it makes sense to continue to use the term data where capta would be more appropriate” <a class="footnote-ref" href="#kitchin2014"> [kitchin2014] </a>. This seems like it would be an appealing option for many people, and I suspect it is widespread, although I have not investigated this question in any serious way.</p>
<p>Regardless, I am aware of no published work that disputes Drucker’s core points. That is:<br>
the worddataconnotes something given andreconceptualizing data as capta is integral to rejecting realist assumptions of transparency and equivalence between data and the phenomena they purport to represent.</p>
<h2 id="a-revised-and-expanded-etymology-for-data-and-capta">A Revised and Expanded Etymology for Data and Capta</h2>
<p>In direct response to Drucker, I hope to demonstrate that the worddatadoes not assume a given that can be transparently and unproblematically recorded and observed. If this first point can be satisfactorily established, it should follow that one can say and write the worddata, embrace “the situated, partial, and constitutive character of knowledge production,” and recognize that “knowledge is constructed,  <em>taken</em> , not simply given as a natural representation of pre-existing fact” <a class="footnote-ref" href="#drucker2011"> [drucker2011] </a>. This line of reasoning is crucial to how digital humanists understand and talk about the concept of data, as well as how we teach computational methods. However, making such an argument first requires revisiting the origin ofdataas a term of use in English and discussing how the meaning of the termdatahas changed over time.</p>
<p>To begin, the worddataindeed comes from the Latin forgiven,but the etymology of the English language word is less straightforward than Drucker suggests. A more detailed account of “the early history of the concept of data ” is Daniel Rosenberg’s “Data Before the Fact” (2013), which discusses much of the early history of data’s English usage. As Rosenberg describes, “In English, data was first used in the seventeenth century. Yet it is not wrong to associate the emergence of the concept and that of modernity. The rise of the concept in the seventeenth and eighteenth centuries is tightly linked to the development of modern concepts of knowledge and argumentation” <a class="footnote-ref" href="#rosenberg2013"> [rosenberg2013] </a>. Rosenberg emphasizes the “specifically rhetorical” status of the worddataas something both “pre-analytical” and “pre-factual,” more so even than the related concepts offactsandevidence<a class="footnote-ref" href="#rosenberg2013"> [rosenberg2013] </a>. As he further explains, “When a fact is proven false, it ceases to be a fact. False data is data nonetheless” <a class="footnote-ref" href="#rosenberg2013"> [rosenberg2013] </a>. Rosenberg’s telling of the early history of data in English adequately distinguishes between the two earliest senses of the termdata, the first in “the realm of mathematics, where it retained the technical sense that it has in Euclid, as quantities <em>given</em> in mathematical problems, as opposed to the quaesita , or quantities <em>sought</em> ” and the second “in the realm of theology, where it referred to scriptural truths — whether principles or facts — that were given by God and therefore not susceptible to questioning” <a class="footnote-ref" href="#rosenberg2013"> [rosenberg2013] </a>. However, I want to argue that these two senses are more closely related than one might first assume, and there is a third early sense of the word that Rosenberg does not fully distinguish.</p>
<p>First, the <em>Oxford English Dictionary</em> provides an entry for data as “something given or granted; something known or assumed as fact, and made the basis of reasoning; an assumption or premise from which inferences are drawn” <a class="footnote-ref" href="#oed2020c"> [oed2020c] </a>. We might call thisdata in the geometrical sense.This phrasing is intended to echo Rosenberg’s definition, except for the distinction that our contemporary concept of mathematics depends on a synthesis of geometrical and numerical paradigms of thought that had not yet fully unfolded in Europe circa 1600.<sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup> The story of the data’s entry into English must include some history of the translation of Euclid’s work from Greek to Latin and English, a story Rosenberg alludes to but does not detail in full. The Greek word δεδομένα (dedoména) meansgiven.Δεδομένα (Dedoména) is the title of a work by Euclid, which remains extant in manuscript form in Greek and Arabic. It was translated into Latin by the French mathematician Claude Hardy and printed alongside its original Greek as a volume titled <em>Euclidis Data</em> in 1625. An English translation with the title <em>Euclid’s Data</em> was included with an edition of <em>Euclid’s Elements of Geometry</em> (John Leeke &amp; George Serle) in 1661.<sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup> The <em>OED</em> traces this sense of the worddatato Henry Hammond’s <em>A copy of some papers past at Oxford, betwixt the author of the Practicall catechisme, and Mr. Ch.</em> in 1646<a class="footnote-ref" href="#oed2020c"> [oed2020c] </a>, which refers to “a heape of data,” but, as Rosenberg states, this heap “is not a pile of numbers but a list of theological propositions accepted as true for the sake of argument” <a class="footnote-ref" href="#rosenberg2013"> [rosenberg2013] </a>. It may seem counter-intuitive to associate the Greek-to-Latin geometrical usage ofdatawith what Jonathan Furner calls “data as gifts from God,” but the theological or ecclesiastical notion of data is a direct antecedent of how Euclid used the termdata/dedoména<a class="footnote-ref" href="#furner2016"> [furner2016] </a>. Furner elaborates that such uses go back to Thomas Tuke’s <em>Nevv Essayes</em> of 1614, and that the Latin worddatais found in other Latin phrases in “religious texts of early seventeenth-century England — for example, gratia gratis data ( grace freely given ), and data desuper ( given from above )” <a class="footnote-ref" href="#furner2016"> [furner2016] </a>. Like Rosenberg, Furner differentiates betweendatain the geometrical sense anddatain the ecclesiastical sense, but they both owe their meaning to a fairly literal translation of the Latin word as “given.”</p>
<p>Quite different from this first cluster is what we might call “data in the empirical sense.” The <em>OED</em> refers to datum “chiefly in plural” (i.e., data), which describes “an item of (chiefly numerical) information obtained by scientific work, a number of which are typically collected together for reference, analysis, or calculation” with first use listed as 1630<a class="footnote-ref" href="#oed2020c"> [oed2020c] </a>.<sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup> This sense of the word seems to relate to the <em>OED</em> ’s entry for data “as a count noun: an item of information; a datum; a set of data” (first use listed as 1645) and later, as “a mass noun,” meaning “related items of (chiefly numerical) information considered collectively, typically obtained by scientific work and used for reference, analysis, or calculation” with its first use listed as 1702<a class="footnote-ref" href="#oed2020c"> [oed2020c] </a>.<sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup> There are likely examples of historic usage where data appears to be something in between the geometrical sense and the empirical sense, but the difference between the two in their sharpest forms is quite significant, and assuming the associations of the first sense carry over to the second sense would be a mistake. To better understand the dissimilarities between these senses, we must reexamine the first English translation of Euclid’s <em>Data</em> .</p>
<p>How Euclid had useddedoména/datais the subject of a commentary by Marinus of Neapolis, translated and included in Leeke and Serle’s edition, for “the Ancients have defined it in one manner, and later Writers after another” <a class="footnote-ref" href="#marinus1661"> [marinus1661] </a>. For Marinus, the question was whether Euclid’s notion of data could be defined as a combination ofordinatum(regulated or orderly) andporiman(available or provided) or as a combination ofcognitum(infamous or known) andporimon. Marinus’ two candidate definitions may appear to map directly to the English language senses of geometrical data and empirical data, but this is not the case. Neither of Marinus’ notions would have been used as a term to describe aspects of the natural world, only for abstractions like the following:<br>
The length of a line C when line C equals the length of line A minus the length of line B, and the lengths of lines A and B are both given.The length of line Z connecting two points A and B, when the positions of points A and B are given.</p>
<p>Drawing on examples from Euclid and other ancient texts, Marinus concludes that Euclid is referring to data as a combination ofporimon, “that which may be exhibited by Demonstration, or which is apparent without Demonstration” andcognitum, that which is “clear and comprehended of us” <a class="footnote-ref" href="#marinus1661"> [marinus1661] </a>. However, his recognition of a second candidate definition, describing data as a combination ofordinatumandporimanshould be noted, as it could point to others forming a similar impression.</p>
<p>More recently, Christian Taisbak notes that Euclid’s use of the worddatameans both the clear and comprehended premise, and the demonstrated conclusion:</p>
<blockquote>
<p>When I started to translate the <em>Data</em> , I found it very longwinded that a certain phrase kept popping up time and again, several times in every proposition: <em>if this item is given, that item is</em> also <em>given</em> . I decided to cancel all those <em>alsos</em> and restore them only where they were absolutely necessary. But then I discovered that I was leaving out an essential feature of the <em>Data</em> : the Givens hang together in chains, the purpose of any proposition being to produce more links to them.<br>
<a class="footnote-ref" href="#taisbak2003"> [taisbak2003] </a>Taisbak is essentially describing deductive reasoning, “that if some items are given, some other items are also given, <em>into the bargain</em> so to speak” <a class="footnote-ref" href="#taisbak2003"> [taisbak2003] </a>. For Euclid,dedoménareferred “not only to the input of a problem, but also to the output” <a class="footnote-ref" href="#taisbak2003"> [taisbak2003] </a>. In this context, one might also compare Euclid’s use ofdatatoakolouthia(ἀκολουθία), a term favored by Aristotle and others, which “indicates the necessary relationship between two propositions when one of them is the consequence of the other” <a class="footnote-ref" href="#seco2010"> [seco2010] </a>).<sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup> This perspective complicates Rosenberg’s gloss ofdataas quantities given andquaesitaas quantities sought, since quantities sought can in fact become given through demonstration or deduction (ὅπερ ἔδει δεῖξαι /hóper édei deîxai).<sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup> These nuances remain specific to the geometrical rather than the empirical concept of data that I have so far discussed, but they are important aspects of the history of the termdata, as well as the history of how that term became associated with quantitative reasoning. As a result, these details already suggest the beginnings of a pedagogical intervention in digital humanities, where thegiven-nessof data is more complex that it might initially appear.</p>
</blockquote>
<p>Data in the empirical sense appears to draw upon aspects of data as a geometrical given and extend these qualities to features of the natural world. For the likes of Apollonius, Euclid, and Marinus, the concept of data would never imply collecting information in this way. Observations of the sort said to be described in Eratosthenes’ <em>Γεωγραφία</em> ( <em>Geografíka</em> ), would be called φαινόμενο (fainómeno), orphenomenon,which translates to “that which appears or is seen.” <sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup> Phenomena included anything that appeared to be true, such as illusions, mirages, and dreams. James Evans and J. Lennart Berggren explain that “the word phenomena is a participle of the passive verb phanomai , which carries the meanings of to come to light, come to sight, be seen, appear. ” They add, “The last two are definitive for the astronomical sense of the word, which is things that are seen/appear in the heavens ” <a class="footnote-ref" href="#evans2018"> [evans2018] </a>. An entire genre of Greek philosophical thought was dedicated to the idea that, from careful observation of the heavens, one could understand the positions of the earth, the planets, and the stars, as well as explain the circular motions of all celestial bodies. Hence the notion, originating in Greek astronomy, of σωζειν τα φαινόμενα (sozein ta fainómena) or “save the phenomena” <a class="footnote-ref" href="#heath1921"> [heath1921] </a>. Works within this genre included Euclid’s <em>The Phenomena</em> , Autolykos’ <em>On the Moving Sphere</em> , Aristotle’s <em>On the Heavens</em> , Gemino’s <em>Introduction to the Phenomena</em> , and Theodesius’ <em>Sphaerics</em> , which is thought to extend the work of Eudoxos. Contributions to this genre continued into the Middle Ages.</p>
<p>These writers did not appear to distinguish between their observations and the way they were structured or recorded. Much later, the genre ofAlmanack of EphemerisorEphemeridesseems to suggest a potential term that makes this distinction. The earliest use of the termalmanac, meaning “an annual table, or (more usually) a book of tables,” can be traced to <em>The Equatorie of the Planetis</em>  (circa CE 1392), whereas the term <em>ephemeris</em> was used as early as the mid-16th century<a class="footnote-ref" href="#oed2020a"> [oed2020a] </a>. According to Arthur L. Norberg, an almanac of ephemeris, by the nineteenth century, “contained information on the phenomena for that year: eclipses of the Sun, Moon, and Jupiter’s satellites; the orientation of Saturn’s rings and the apparent discs of Venus and Mars” <a class="footnote-ref" href="#campbell-kelly2003"> [campbell-kelly2003] </a>. For whatever reason, neither of these terms caught on as a more generalized word for the kind of structured information one might generate by making and recording observations. As with Taisbak’s point about givens, for Euclid, functioning as the inputs and outputs of problems, the seeds of a pedagogical intervention are here planted. Phenomena were recorded and published with the expectation that future observations might supplement, refine, or replace previous ones, which suggests a fundamental awareness that representations of phenomena were partial and imperfect. Certainly, at almost any moment in history, some were expressing their belief in a unified, coherent reality just beyond our reach, as well as their confidence that their tabulations would allow them to decipher its governing dynamics. I will return to this subject in this essay’s conclusion but, for now, it should suffice to say that this belief is best understood not as a position on the distinguishing properties of data, but rather a position on the possibility of obtaining objective knowledge through empirical means.</p>
<p>It is also crucial to note that the idea of a table as a textual and material device for structuring numerical information is at least 4,500 years old. According to Martin Campbell Kelly and his co-authors, “While the list has been hailed as a major breakthrough in cognitive history&hellip;the table as a pre-modern phenomenon of structured thought has been completely neglected” <a class="footnote-ref" href="#campbell-kelly2003"> [campbell-kelly2003] </a>. In ancient Sumer, Babylonia, and Assyria, clay tablets were often if not primarily a repository of numerical information, often used as a memorization or calculation aid. Etymologically, the wordtablecomes from the Latintabula, which in turn, originates with the Greek termτάβλι(tabla), meaning a plank or board<a class="footnote-ref" href="#austin1934"> [austin1934] </a>. The term was used to refer to a particular board game that is thought to be the ancestor of backgammon, and was also a general term for a tablet, a slate, or any flat piece of wood. In English, it eventually a particular piece of furniture “on which food is served, and at or around which people sit at a meal” (circa CE 1300). The English wordtablemeaning “a schematic arrangement of information” or “an orderly arrangement of particulars” can be traced as far back as Byrhtferth of Ramsey’s <em>Enchiridion</em> (CE 986-1016), but the association between a tablet/slate and structured information recorded upon it appears to be much older<a class="footnote-ref" href="#oed2020e"> [oed2020e] </a>.</p>
<p>Today, the phrasetabular datasuggests a strong association between tables and data, and the termobservationis widely used to denote “all values measured on the same unit (like a person, or a day, or a race) across attributes” or, in the context of rectangular data, a row of values.<sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup> As Lisa Gitelman notes, the idea of converting that which one observes into data — a conceptual object that structures and represents that which has been perceived — seems like second nature to us. “When phenomena are variously reduced to data, they are divided and classified, processes that work to obscure — or <em>as if</em> to obscure — ambiguity, conflict, and contradiction” <a class="footnote-ref" href="#gitelman2013"> [gitelman2013] </a>. For Gitelman, this relationship suggests both a dependency on hierarchy, and a kind of epistemological cover that protects “users who perform logical operations on the data&hellip;from having to know how the data have been organized” <a class="footnote-ref" href="#gitelman2013"> [gitelman2013] </a>. When terms likedata-drivenare invoked, there is often an assumption of order, imposed or revealed, by those engaging in analysis.</p>
<p>Nevertheless, the idea of ordering and standardizing information for the purposes of facilitating numerical operations is much older than the English worddata. What seems to have changed is that the termdataadopted a secondary meaning in addition to the geometrical sense of term associated with Euclid. An entry in Ephraim Chambers’ <em>Cyclopædia</em> (1728) offers two definitions for the termdata. First, it continued to refer to a geometrical given. Second, “From the primary Use of the Word <em>Data</em> in mathematicks, it has been transplanted into other Arts; as Philosophy, Medicine, &amp;c. where it expresses any Quantity, which for the Sake of a present Calculation, is taken for granted to be such, without requiring an immediate Proof for its Certainty” <a class="footnote-ref" href="#chambers1728"> [chambers1728] </a>. According to Chambers, data had come to be used to describe a quantity, in a wide range of contexts, that was meant to be treated as a given for the purposes of analysis. This definition reinforces the distinction between data in the strictly geometrical sense and data in other contexts, and it is compatible with religious uses of the term, since information from scripture would qualify as data of the geometrical sort, or knowledge beyond question.</p>
<p>This explanation does not describe a specific process by which phenomena become data. However, the clausefor the Sake of a present calculationseems to limit why and how the data are permitted to be taken for granted, which undercuts the idea that data would be regarded as natural or pre-existing facts. In turn, it contradicts the idea that data in this sense could ever be viewed as pre-factual, as Rosenberg describes. Among the natural philosophers of the 17th and 18th centuries, the idea that Nature would reveal its hidden structure or laws to those who spoke its language was predominant.<sup id="fnref:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup> In this light, it seems likely if not obvious that empirical observations would be considered facts given by God. However, if data were available or provided because Nature had provided them for us to analyze, then there would be no need for Chambers’ qualifying phrase. The entry in Chambers’ <em>Cyclopædia</em> seems more compatible with the idea that phenomena become data, or are granted provisional status as data, which suspends the need for “an immediate proof” and permits calculation to occur.<sup id="fnref:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup></p>
<p>Returning to Marinus’ two candidate definitions for Euclid’s notion of data — asporimonpluscognitumorporimonplusordinatum— we can see a case for either or both.<sup id="fnref:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup> Imposing Marinus’ analysis on Chambers’ definition is potentially problematic, but I would argue that our contemporary use of the termdatais best explained by starting with the notion of data asporimonplusordinatum. Recall that this sense — as a combination of something provided in a way that gives it order — was <em>not</em> the sense Euclid was not using, according to Marinus. As Taisbak notes, however,ordinatumfor Marinus was the Greek word τεταγμένων (tetagmenon), literallyfixed,but also used to mean “organized according to some <em>taxis</em> , order ” <a class="footnote-ref" href="#taisbak2003"> [taisbak2003] </a>. Regarding the Latinporimon, the Greek term πορίσαοθαι (porizesthai), Taisbak explains that Euclid uses the term to describe “several operations,” includingput together,draw,describe,make,orproduce<a class="footnote-ref" href="#taisbak2003"> [taisbak2003] </a>.<sup id="fnref:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup> By this logic, definingdataas bothporimonandordinatumwould emphasize its being available, collected, or made, as well as being brought to order or revealed to have latent order by the act of demonstration.</p>
<p>Broadly speaking, then, a full-fledged etymology of the termdatasuggests a narrative much more complex than data asgivenand capta astaken.The Latin word forgivenwas used by Euclid in a manner very different from how data was used by the mid-18th century, and the true meaning of the Euclidian concept of dedoména/data has disputed definitions among classical thinkers, as Marinus summarizes. Concepts such as saving the phenomena, an almanac of ephemeris, and tabular information suggest an ancient and longstanding tradition of making precise observations, placing them a structured format, and using that structure to facilitate calculation. Taking Chambers into account, it seems likely that facts became eligible to be regarded as data by being provided or made available by a person or group of people who had made observations or measurements, and then imposed some form of order or structure upon them in order to facilitate analysis. Drucker’s advocacy for capta as a key term and a guiding principle is rooted in the notion that the termdataassumes a pre-existing given that can be naively recorded without being shaped by the observer. The story I have told suggests a more nuanced history of data, both as a term and a concept. There is room to embrace the worddataand understand data to be inevitably fragmentary, imperfect, and tentative, yet simultaneously useful in their ability to organize knowledge and facilitate modes of analysis that would otherwise be impossible. In the section that follows, I will make the case for embracing concepts like situated data and data-rich literary history to convey this message, which includes potential benefits to scholarly discourse and digital humanities pedagogy.</p>
<h2 id="situating-data-for-the-sake-of-interdisciplinarity-and-digital-humanities-pedagogy">Situating Data for the Sake of Interdisciplinarity and Digital Humanities Pedagogy</h2>
<p>Up to now, I have attempted to show that the termdatahas a much more nuanced history that its translation to the English wordgiven. This revised etymology leaves room for an understanding of both the term and concept as less closely linked to naive realism than Drucker suggests. As S.I. Hayakawa once famously wrote, “the writer of a dictionary is a historian, not a lawgiver” <a class="footnote-ref" href="#hayakawa1990"> [hayakawa1990] </a>. The meaning ofdatahas changed over time and, at any given time, no singular meaning for the term was universal. I have pointed to one important sea change, by which data came to have special prominence in the context of empiricism. A longer essay could say much more on the subject of additional changes in prevailing notions of the term, or additional meanings that persist in specific contexts or among specialized groups. Along with the accepted range of definitions fordata, their various connotations and their preferred rhetorical uses, have also changed over time. I think Drucker is right to point out that many today use the termdatato suggest a kind unimpeachable concreteness rather than something partly born out of the assumptions made when selecting and organizing said data. I suspect that there is widespread disagreement among self-proclaimed empiricists about the degree to which data arenatural representationsas opposed to “situated, partial, and constitutive,” but I agree that scientific rhetoric often downplays or denies the artificial, conditional, and fragmentary aspects of data<a class="footnote-ref" href="#drucker2011"> [drucker2011] </a>.</p>
<p>As an alternative to avoiding the termdata, I would argue that digital humanities should focus on challenging and complicating beliefs about the purity, objectivity, or totality of data, all the while using the worddatafrequently, and without any shame. There is a strong defense for using the worddatamindfully and unapologetically based on the understanding that data are collected, assembled, and recorded by people (or their instruments). Data have structure, but this structure comes (at least in part) from how observations are gathered and organized, as well as shaped by the interpretive decisions made at their inception. Contextual information about these interpretive decisions is a vital component of a dataset. Many in digital humanities have already moved in this direction, shifting from notions ofgood dataandbad datato concepts like data as situated knowledge, as described in Catherine D’Ignazio and Lauren F. Klein’s <em>Data Feminism</em> (2020) or data-rich literary history as described in Katherine Bode’s <em>A World of Fiction: Digital Collections and the Future of Literary History</em> (2019). The benefits of these strategies are both discursive and pedagogical.</p>
<p>The first benefit of this approach is to move away from an argument based on word origin. A word’s origins seldom tell an accurate story of a word’s historical use or contemporary valences. There are numerous Latin words with English homographs, and often their meanings have changed over time. Words such asplastic,stigma,campus,focus,versus,stimulus,andsinisterall have Latin antecedents with meanings that differ greatly from their contemporary use in English. With the worddata, there is added ambiguity because the English wordgivenhas more than one meaning. Describing an idea asa givenora warrantin the context of a rhetorical argument remains common, and seems to carry no implication of pre-factualness or ineligible for questioning.<sup id="fnref:20"><a href="#fn:20" class="footnote-ref" role="doc-noteref">20</a></sup></p>
<p>Capta, likewise, is Latin for taken and therefore has a certain surface appeal as a mirror term for data. However, capta can also meancaught,captured,andcaptive.The phraseJudæa captarefers to the siege and capture of Jerusalem by the Romans, as well as a series of coins issued by the Roman Emperor Vespasian to celebrate these actions<a class="footnote-ref" href="#elizabeth1845"> [elizabeth1845] </a>. In the Roman Empire, the phrasesmanu capta<a class="footnote-ref" href="#sandars1917"> [sandars1917] </a>andpraeda manu captadenoted that which wascaptured by handoracquired by force of handsfrom among Rome’s enemies.<sup id="fnref:21"><a href="#fn:21" class="footnote-ref" role="doc-noteref">21</a></sup> Further, some enslaved people (servi) were “so called from the fact that commanders are used to <em>sell</em> their captives, and by this means to <em>preserve</em> (servare) rather than kill them” ( <em>Justinian Institutes</em> , qtd. in<a href="#cameron1972">Cameron (1972), 5</a>).<sup id="fnref:22"><a href="#fn:22" class="footnote-ref" role="doc-noteref">22</a></sup> These potential associations are important because they complicate the idea that data straightforwardly means <em>given</em> and capta straightforwardly means <em>taken</em> as we think of those terms today. Moreover, these examples underscore the broader point that it can be a mistake to use a word’s denotation in its language of origin (or its historic denotation in English) as the primary criteria by which to judge its contemporary meaning or appeal.</p>
<p>Denotations, connotations, and rhetorical uses change over time, and this is good news. Such associations can change once again, and scholars of the humanities can help change them. As Helen Longino has argued, the notion of objectivity in scientific inquiry is bound up in two different senses of the term, the first emphasizing scientific realism, or the idea that science provides an “accurate description of the facts of the world as they are” <a class="footnote-ref" href="#longino1990"> [longino1990] </a>. The second, associated with modes of inquiry, stresses “reliance on nonarbitrary and nonsubjective criteria for developing, accepting, and rejecting the hypotheses and theories that make up the view” <a class="footnote-ref" href="#longino1990"> [longino1990] </a>. Further, scientists will often “speak of the objectivity of data” but objectivity in this sense should be taken as a claim of reliability<a class="footnote-ref" href="#longino1990"> [longino1990] </a>. What makes data objective is “the relationship of measurements to one another within a particular dimension or kind of scale” <a class="footnote-ref" href="#longino1990"> [longino1990] </a>. According to Longino, it should not be assumed that these measures “are real properties of real entities” or “that their measurements provide us with an unmediated view of the natural world” <a class="footnote-ref" href="#longino1990"> [longino1990] </a>. In other words, there is an important difference between an etymologically inspired belief that data are natural representations of the world — given rather than taken — and the idea that scientific observation generates objective information by providing epistemological order. The two positions are based on competing definitions of objectivity. Adopting the perspective that phenomena become data only when they are structured by an investigative agent invites an important debate about the objectivity of data once they are standardized and assembled, and that is a debate that I would be glad take part in.</p>
<p>I would not deny the merit of distinguishing between the idea of data-as-given and capta-as-taken. However, this distinction may be more a matter of emphasis than definition. Whatever the etymological roots and contemporary associations of data, we can take capta to emphasize that the measurements, readings, observations have been taken. As I have suggested, the wordcaptamay stir connotations of human bondage and violence. This association may be appropriate in some cases, especially in a moment when for-profit companies are covertly monetizing and selling data on the open market, often in direct conflict with the interests of its users. On the other hand, many data do not fit this description, and I think regarding all data as captive or hostage to their stewards is provocative, but ultimately misguided, especially where there is strong potential for unintended consequences.</p>
<p>As previously suggested, there are matters of utility to consider when deciding between data and capta as conceptual signposts. Many in digital humanists are likely using the termdataat least some of the time, either as a standalone key term, or in cognates or phrases like metadata, database, or data visualization. Going further down this path would allow digital humanists to compete more effectively for search engine keyword searches, which could make digital humanities scholarship more visible to audiences from the sciences. This would include direct searches, as well as related searches likedata analysis,data analytics,anddata usage,all of which are toprelated to datasearches on Google Trends. It would also encourage digital humanities to continue any previous efforts to engage directly with specialized topics such as data pedagogy, data literacy, open data, data management, and data curation.</p>
<p>Along these lines, thinking differently about the worddatamay lead to new exchanges of information among and between disciplines. Digital humanities, as a field, has a deeply interdisciplinary history, but a precondition to such interdisciplinarity is the premise that different disciplines can learn from one another. By engaging directly with concepts related to data, we have opportunity to shifts scholarly focus from the nature of data to strategies that promote engaging critically, theoretically, and computationally with data.</p>
<p>Catherine D’Ignazio and Lauren F. Klein take an approach of this sort in <em>Data Feminism</em> (2020). They define data as “information made tractable,” which I immediately associate with Marinus’ discussion ofordinatum. In various passages, D’Ignazio and Klein cite Donna Haraway, whose essay “Situated Knowledges: The Science Question in Feminism and the Privilege of Partial Perspective” (1988) called for “a doctrine of embodied objectivity that accommodates paradoxical and critical feminist science projects” <a class="footnote-ref" href="#haraway1988"> [haraway1988] </a>. Core to Haraway’s “feminist objectivity” is the principle of situated knowledges. Adapting and extending Haraway’s intervention, D’Ignazio and Klein argue that “one of the central tenets of feminist thinking is that all knowledge is situated” <a class="footnote-ref" href="#dignazio2020"> [dignazio2020] </a>. There is a more complicated discussion to be had about whether all knowledge is inherently situated or whether it is the work of feminism that situates it. Haraway seems to be arguing for the latter when she contrasts situated knowledges with “unlocatable, and so irresponsible, knowledge claims” <a class="footnote-ref" href="#haraway1988"> [haraway1988] </a>. Either way, all data have important contexts of creation and organization, and situating them (or emphasizing them as situated) includes critical examination of those contexts.<sup id="fnref:23"><a href="#fn:23" class="footnote-ref" role="doc-noteref">23</a></sup></p>
<p>D’Ignazio and Klein describe numerous practices related to examining the context of data. These practices begin with understanding the conditions of production associated with data, including information about those who constructed the data. “When approaching any new source of knowledge,” they write, “it’s essential to ask questions about the social, cultural, historical, institutional, and material conditions under which that knowledge was produced, as well as about the identities of the people who created it” <a class="footnote-ref" href="#dignazio2020"> [dignazio2020] </a>. Like Gitelman, D’Ignazio and Klein argue against the existence of anything that might be called raw data<a class="footnote-ref" href="#dignazio2020"> [dignazio2020] </a>. They advocate for considering the “functional limitations of the data” and “any associated ethical obligations” <a class="footnote-ref" href="#dignazio2020"> [dignazio2020] </a>; “exploring and analyzing what is missing from a dataset” <a class="footnote-ref" href="#dignazio2020"> [dignazio2020] </a>; attending to “power differentials” that have shaped and/or continue to be present in collected data<a class="footnote-ref" href="#dignazio2020"> [dignazio2020] </a>; and interrogating a dataset’s validity — that is, the degree to which it can be said to represent the concept being analyzed<a class="footnote-ref" href="#dignazio2020"> [dignazio2020] </a>. They emphasize the importance of context at the stages of data acquisition, data analysis, and “framing and communication of results” <a class="footnote-ref" href="#dignazio2020"> [dignazio2020] </a>. Their advocacy is empowered rather than hindered by their adoption of a precise vocabulary that practitioners in a range of disciplines can recognize and quickly understand.<sup id="fnref:24"><a href="#fn:24" class="footnote-ref" role="doc-noteref">24</a></sup></p>
<p>Similarly, Katherine Bode has argued in <em>A World of Fiction</em> (2019) for an intervention in computational literary studies based on the argument that “distant reading and macroanalysis construct and seek to extract meaning from models of literary systems that are essentially deficient,” which to say, “inadequate for representing the ways in which literary works existed and generated meaning in the past” <a class="footnote-ref" href="#bode2019"> [bode2019] </a>. Bode does not advocate that we reject data-driven literary history, as others have argued, nor does she endorse the adoption of “new, more elaborate forms of computational analysis” as a way around the conflict<a class="footnote-ref" href="#bode2019"> [bode2019] </a>.<sup id="fnref:25"><a href="#fn:25" class="footnote-ref" role="doc-noteref">25</a></sup> Instead, she anchors her intervention around the idea of “a scholarly edition of a literary system” — which engages explicitly with modeling practices — as “a mechanism through which to interrogate and refine conceptions of literary works and systems” <a class="footnote-ref" href="#bode2019"> [bode2019] </a>. As with D’Ignazio and Klein, Bode calls for increased contextualization, for “a data-rich model of a literary system is inevitably an argument shaped by not only the scholar’s perception of cultural artifacts and phenomena but the complex history by which those artifacts and phenomena are transmitted to and by us in the present” <a class="footnote-ref" href="#bode2019"> [bode2019] </a>. Bode’s use of the key phrase “data-rich” is important to her argument, in that she presents it as an alternative to labels likedistant reading,macroanalysis,andcomputational literary history<a class="footnote-ref" href="#bode2019"> [bode2019] </a>. It is structurally reminiscent ofdata-drivenbut the adjectivalrichhas a strong contrast to the past participledriven, both in terms of connotation and its symbolic rejection of that idea that data ostensibly occupy the driver’s seat. D’Ignazio &amp; Klein and Bode make very different interventions, but they share a common position that the termdatahas purchase, especially when that term is qualified with productively descriptive modifiers. Further, they base their work, in part, on the premise that data-driven inquiry can be executed more effectively if instilled with feminist and/or humanistic values.</p>
<p>If this premise is to be accepted, our conversations about how to improve upon computational inquiry can go further. Most immediately, there are implications for digital humanities pedagogy. As Ted Underwood has argued, “digital humanities classes, as currently defined, don’t really teach students how to use numbers&hellip;So it’s almost naive to discuss barriers to entry. There is no entrance to this field” <a class="footnote-ref" href="#underwood2018"> [underwood2018] </a>). Underwood argues that the cultural analytics subfield, in particular, operates primarily as a social network. In such a context competency and fluency come as a result of participants’ tacit knowledge and through person-to-person or small group interactions. In contrast, a well-defined curriculum with explicit pedagogies for teaching research design, data collection, data modeling, programming, and data analysis (including but not limited to quantitative analysis) would help create pathways to entry that are currently hidden for or unavailable to many people. Overt teaching of methods, Underwood argues, allows facility with methods to be more equally distributed. Discussing data literacy openly and critically strikes me as a key component of the curricular and pedagogical intervention Underwood has described.</p>
<p>There are wide-ranging benefits to speaking and writing about data, openly and often, with implications from the practical to the profound. As part of this strategy, we should ask our students to read about the history of the worddata, but to include how Euclid used the worddedoména, how Marinus described two different potential definitions for the word, and how natural philosophers of 17th-century England, in particular, appropriated dedoména’s Latin alternativedata. We should ask them to discuss why such thinkers saw similarities between the Greek notion of data and how they were collecting and structuring information they observed and recorded. We should speak openly about crucial concepts related to data-driven inquiry, including practices aimed at contextualizing and situating data and datasets. We should press the idea that social constructions foreground and pervade what many think of as neutral or natural measures of reality, and we should openly question paradigms of thought that emphasize data as self-evident or unproblematically objective. In short, with a sense of pride and purpose, let us say and write data, all the while remaining humanists.</p>
<ul>
<li id="oed2020a"> “almanac, n.”  _OED Online_ . (Oxford University Press, June 2020), accessed October 16, 2020.<a href="https://www.oed.com/view/Entry/5564">https://www.oed.com/view/Entry/5564</a>.
</li>
<li id="oed2020b"> “ephemeris, n.”  _OED Online_ . (Oxford University Press, June 2020), accessed October 16, 2020.<a href="https://www.oed.com/view/Entry/63209">https://www.oed.com/view/Entry/63209</a>.
</li>
<li id="austin1934">Austin, Roland G. “Zeno’s Game of Τάβλη (A. P. ix. 482),”  _The Journal of Hellenic Studies_ 54 (1934): 202–5.<a href="https://doi.org/10.2307/626864">https://doi.org/10.2307/626864</a>.
</li>
<li id="bode2019">Bode, Katherine. _A World of Fiction: Digital Collections and the Future of Literary History_ . Ann Arbor: U of Michigan P, 2019.
</li>
<li id="campbell-kelly2003">Campbell-Kelly, Martin, et al., _The History of Mathematical Tables: From Sumer to Spreadsheets_ , Oxford, Miss.: OUP, 2003.
</li>
<li id="chambers1728">Chambers, Ephraim. _Cyclopædia, or, An Universal Dictionary of Arts and Sciences_ (1728). ARTFL Reference Collection.<a href="https://artfl-project.uchicago.edu/content/chambers-cyclopaedia">https://artfl-project.uchicago.edu/content/chambers-cyclopaedia</a>.
</li>
<li id="dignazio2020">D’Ignazio, Catherine, and Lauren F. Klein, _Data Feminism_ . Cambridge, MIT Press, 2020.
</li>
<li id="oed2020c"> “data, n.”  _OED Online_ . (Oxford University Press, June 2020), accessed July 01, 2020.<a href="https://www.oed.com/view/Entry/296948">https://www.oed.com/view/Entry/296948</a>.
</li>
<li id="drucker2011">Drucker, Johanna. “Humanities Approaches to Graphical Display,”  _Digital Humanities Quarterly_ 5.1 (2011): n.p.<a href="http://www.digitalhumanities.org/dhq/vol/5/1/000091/000091.html">http://www.digitalhumanities.org//dhq/vol/5/1/000091/000091.html</a>.
</li>
<li id="drucker2014">—-. _Graphesis: Visual Forms of Knowledge Production_ . Cambridge: Harvard UP, 2014.
</li>
<li id="elizabeth1845">Elizabeth, Charlotte. _Judæa Capta: An Historical Sketch of the Destruction of Jerusalem by the Romans_ . London: W.H. Dalton, 1845.
</li>
<li id="encyclopaedia1816"> _Encyclopaedia Perthensis, Or, Universal Dictionary of the Arts, Sciences, Literature, Etc.: Intended to Supersede the Use of Other Books of Reference_ . Edinburgh: J. Brown, 1816:
</li>
<li id="evans2018">Evans, James, and J. Lennart Berggren, _Geminos’s Introduction to the Phenomena: A Translation and Study of a Hellenistic Survey of Astronomy_ . Princeton: Princeton UP, 2018.
</li>
<li id="furner2016">Furner, Jonathan. “ Data : the data,”  _Information Cultures in the Digital Age._ Eds. M. Kelly, J. Bielby, Wiesbaden: Springer, 2016. 287-306.<a href="https://doi.org/10.1007/978-3-658-14681-8_17">https://doi.org/10.1007/978-3-658-14681-8_17</a>.
</li>
<li id="gebru2020">Gebru, Timnit, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé III, and Kate Crawford. “Datasheets for Datasets.” ArXiv:1803.09010 [Cs], March 19, 2020.<a href="http://arxiv.org/abs/1803.09010">http://arxiv.org/abs/1803.09010</a>.
</li>
<li id="gitelman2013">Gitelman, Lisa. “Introduction,”  _ Raw Data is an Oxymoron_ , ed. by Lisa Gitelman. Cambridge: MIT Press, 2013. 1-14.
</li>
<li id="haraway1988">Haraway, Donna. “Situated Knowledges: The Science Question in Feminism and the Privilege of Partial Perspective,”  _Feminist Studies_ 14, no. 3 (1988): 575–99,<a href="https://doi.org/10.2307/3178066">https://doi.org/10.2307/3178066</a>.
</li>
<li id="hayakawa1990">Hayakawa, Samuel Ichiyé. _Language in Thought and Action_ . New York: Houghton Mifflin Harcourt, 1990.
</li>
<li id="heath1921">Heath, Sir Thomas. _A History of Greek Mathematics Volume 1: From Thales to Euclid_ . Oxford: Clarendon Press, 1921.
</li>
<li id="justinian1972"> _Justinian Institutes_ , Lib. 1, Tit. 3., “De Jure Personarum.” Qtd. in Archibald Alexander Cameron, _Protestantism and Its Relation to the Moral, Intellectual and Spiritual Developments of Modern Times_ . Ottawa: Joseph Loveday, 1972.
</li>
<li id="kelly-bootle1995">Kelly-Bootle, Stan. T _he Computer Contradictionary_ . Cambridge: MIT Press, 1995.
</li>
<li id="kitchin2014">Kitchin, Rob. “1. What Are Data?” section on “Conceptualizing Data,”  _The Data Revolution: Big Data, Open Data, Data Infrastructures and Their Consequences_ . Sage Publications e-book edition, 2014.
</li>
<li id="longino1990">Longino, Helen E. _Science as Social Knowledge: Values and Objectivity in Scientific Inquiry_ . Princeton: Princeton UP, 1990.
</li>
<li id="marinus1661">Marinus, “Euclides Data, a Commentary or Preface Written by the Phylosopher Marinus on Euclides Data,” in _Euclid’s Elements of Geometry_ . London: John Leeke & George Serle, 1661.
</li>
<li id="oed2020d">mathematics, n. _OED Online_ . (Oxford University Press, June 2020), accessed October 15, 2020.<a href="https://www.oed.com/view/Entry/114974">https://www.oed.com/view/Entry/114974</a>
</li>
<li id="oed2020e">phenomenon, n. _OED Online_ . (Oxford University Press, June 2020), accessed October 15, 2020.<a href="https://www.oed.com/view/Entry/142352">https://www.oed.com/view/Entry/142352</a>.
</li>
<li id="rosenberg2013">Rosenberg, Daniel. “Data Before the Fact,”  _ Raw Data is an Oxymoron,_ ed. by Lisa Gitelman (Cambridge: MIT Press, 2013): 15-40.
</li>
<li id="sandars1917">Sandars, Thomas Collett. _The Institutes of Justinian_ . New York: Longmans, 1917.
</li>
<li id="seco2010">Seco, Lucas, Francisco Mateo, and Giulio Maspero. _The Brill Dictionary of Gregory of Nyssa_ . BRILL, 2010.
</li>
<li id="shipley2001">Shipley, Joseph Twadell. _The Origins of English Words: A Discursive Dictionary of Indo-European Roots_ . Baltimore: Johns Hopkins UP, 2001.
</li>
<li id="sullivan1860">Sullivan, Robert. _A Dictionary of Derivations; Or, An Introduction to Etymology: On a New Plan._ Dublin: Marcus and John Sullivan, 1860.
</li>
<li id="oed2020f"> “table, n.”  _OED Online_ . (Oxford University Press, June 2020), accessed October 25, 2020.<a href="https://www.oed.com/view/Entry/196785">https://www.oed.com/view/Entry/196785</a>.
</li>
<li id="taisbak2003">Taisbak, Christian Marinus. _Dedomena: Euclid’s Data or The Importance of Being Given_ , The Greek Text translated and explained by Christian Maris Taisbak. Copenhagen: Museum Tusculanum Press, University of Copenhagen, 2003.
</li>
<li id="vico2000">Vico, Giambattista. _Universal Right_ . Trans. and ed. by Georgio Pinton & Margaret Diehl. Amsterdam: Ridopi, 2000.
</li>
<li id="underwood2018">Underwood, Ted. “A Broader Purpose,” Varieties of Digital Humanities Panel, MLA, Jan 5, 2018.<a href="https://tedunderwood.com/2018/01/04/a-broader-purpose/">https://tedunderwood.com/2018/01/04/a-broader-purpose/</a>.
</li>
<li id="vaan2008">Vaan, Michiel de. _Etymological Dictionary of Latin and the Other Italic Languages_ . 2008. Leiden, Boston: Brill, 2018.
</li>
<li id="wickham2014">Wickham, Hadley. “Tidy Data.”  _The Journal of Statistical Software_ 59 (August 2014): 1-23,<a href="https://doi.org/10.18637/jss.v059.i10">https://doi.org/10.18637/jss.v059.i10</a>.
</li>
</ul>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>I would like to thank Sam Cowling, John Ladd, Rebecca Lee, and Scott Weingart for reading drafts of this essay at various stages of the writing process, as well as the peer reviewers of this piece for generously providing constructive criticism and suggestions for revision.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>In making this claim, Drucker gestures at two well-known positions on the relationship between “theoretical claims” and “knowledge of the world.” However, these polarities can be divided into numerous subcategories, and many such positions do not fit neatly into the category of realism or constructivism. For a detailed summary of such responses, see Anjan Chakravartty, “Scientific Realism” , <em>The Stanford Encyclopedia of Philosophy</em> (Summer 2017 Edition), Ed. Edward N. Zalta,<a href="https://plato.stanford.edu/archives/sum2017/entries/scientific-realism">https://plato.stanford.edu/archives/sum2017/entries/scientific-realism</a>.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>In this essay, I have chosen to treat data and capta primarily as singular nouns when discussing either the termdataor the concept of data. When referring to that which is signified by the term data, I have maintained the norm of treating data as plural. I have italicized when making statements that refer directly to key terms but not when referring to corresponding concepts.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>See, for example, Stan Kelly-Bootle, <em>The Computer Contradictionary</em> (MIT Press, 1995); H.E. Jensen, “Editorial Note” (1950) in H Becker, <em>Through Values to Social Interpretation</em> (Duke UP, 1952): vii-xi.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>At the time of writing this essay, “Humanities Approaches to Graphical Display” had 399 citations on Google Scholar, and <em>Graphesis</em> had 348. Examples of syllabi raising this topic directly or assigning Drucker on this topic include:<a href="https://cms633.github.io/assets/cms-633-syllabus.pdf">https://cms633.github.io/assets/cms-633-syllabus.pdf</a>;<a href="http://benschmidt.org/HDA19/syllabus__syllabus.html">http://benschmidt.org/HDA19/syllabus__syllabus.html</a>;<a href="https://dhcu.ca/portfolio/syllabus/">https://dhcu.ca/portfolio/syllabus/</a>; and<a href="http://www.cs.cmu.edu/~dbamman/courses/76829/schedule.html">http://www.cs.cmu.edu/~dbamman/courses/76829/schedule.html</a>.&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>Shawn Graham, syllabus for graduate seminar in DH,<a href="https://dhcu.ca/portfolio/syllabus/">https://dhcu.ca/portfolio/syllabus/</a>. On the original syllabus, the wordcaptais hyperlinked to Drucker’s piece for <em>DHQ</em> .&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>It is conceivable that DH would begin publishing papers on captabases and metacapta, but many who would prefer the termcaptaare probably using data cognates and phrases out of convenience or preference. Kelly-Bootle satirically raises the possibility of terms like capta entry, capta processing, and captabase in <em>The Computer Contradictionary</em> (1995)<a class="footnote-ref" href="#kelly-bootle1995"> [kelly-bootle1995] </a>. When I attempt a Google search for metacapta, I get the response, “Did you mean metacarta, metacipta, metacapital, metacafe?” On Google Scholar, I get results for metacaptcha and metacapita. (Results collected in Summer 2020.)&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>The <em>OED</em> cites the earliest use ofmathematicsas “(a collective term for) geometry, arithmetic, and certain physical sciences involving geometrical reasoning, such as astronomy and optics” to Christopher Langton, circa 1545<a class="footnote-ref" href="#oed2020d"> [oed2020d] </a>For more on the unification of geometrical and numerical thought, see Patrick Suppes et. al., <em>Foundations of Measurement Volume 2</em> : <em>Geometrical, Threshold, and Probabilistic Representations</em> (Elsevier, 2014): 80-81.&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>English language translations of Euclid’s <em>Elements of Geometry</em> date as far back as 1570. See Adam Clarke, <em>The Bibliographical Miscellany</em> (London: W. Baynes, 1806): 77-78.&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>The <em>OED</em> credits William Batten’s <em>A MOST PLAINE and easie way for the finding of the Sunnes Amplitude and Azimuth, and thereby the Variation of the Compasse, by Logarithme</em> (1630) under a truncated title. Note, also, that the worddataappears in keyword searches of databases like Early English Book Online (EEBO) before these 1630 because editions in Latin, or English language editions with Latin quotations or epigraphs, were quite common. In Rosenberg’s words, “the Latin word data , as a conjugation of the verb dare , was in constant use during the seventeenth and eighteenth centuries&hellip;but data in Latin rarely translates to data in English” <a class="footnote-ref" href="#rosenberg2013"> [rosenberg2013] </a>.&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>The <em>OED</em> credits T. Urquhart’s <em>Trissotetras</em> (1645) as the earliest known example ofdataas a count noun, and credits Robert Morden’s <em>An Introduction to Astronomy, Geography, Navigation, and other mathematical sciences, made easie by the description and uses of the cœlestial and terrestial globes</em> (1702) as the earliest known example of data as a mass noun.&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p>See Michael Frede, “Stoic vs. Aristotelian Syllogistic.” In <em>Essays in Ancient Philosophy</em> (U of Minnesota P, 1987), pp. 99–124; John Spangler Kieffer, <em>Galen’s Institutio Logica: English Translation, Introduction, and Commentary</em> (JHU Press, 2020); and Lucas Francisco Mateo Seco and Giulio Maspero. <em>The Brill Dictionary of Gregory of Nyssa</em> (Brill, 2010).&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13">
<p>See David Dunér and Christer Ahlberger, <em>Cognitive History: Mind, Space, and Time</em> (Walter de Gruyter GmbH &amp; Co KG, 2019), p. 100.&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14">
<p>The <em>OED</em> ’s etymological entry forphenomenondescribes, “things that appear, appearances, phenomena, celestial phenomena (title of a work by Eudoxus, versified by Aratus), use as noun of neuter of  ϕαινόμενος  appearing, apparent (to the senses or mind), passive present participle of  ϕαίνειν  to show, cause to appear” <a class="footnote-ref" href="#oed2020d"> [oed2020d] </a>. For more on Eratosthenes’ <em>Geography</em> , see Eratosthenes, _Eratosthenes’ Geography _ (Princeton University Press, 2010).&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:15">
<p>See Hadley Wickham, “Tidy Data,”  <em>The Journal of Statistical Software</em> , vol. 59 (August 2014), pp. 1-23. In contrast to an observation, values are specific numbers or string, and a variable “contains all values that measure the same underlying attribute (like height, temperature, duration) across units” <a class="footnote-ref" href="#wickham2014"> [wickham2014] </a>.&#160;<a href="#fnref:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:16">
<p>See Peter H. Reill, <em>Vitalizing Nature in the Enlightenment</em> (University of California Press, 2005); Lorraine Daston and Peter Galison, <em>Objectivity</em> (Zone Books, 2010); Edward Dolnick, <em>The Clockwork Universe: Isaac Newton, The Royal Society, and The Birth of the Modern World</em> (Harper Collins, 2011).&#160;<a href="#fnref:16" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:17">
<p>For more on early modern associations between knowledge and sin, as well as associations between epistemological errors and sin, see Peter Harrison, <em>The Fall of Man and the Foundations of Science</em> (Cambridge University Press, 2007).&#160;<a href="#fnref:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:18">
<p>Taisbak translates πορίσαοθαι (porizesthai) cognate to the verbto provide,asavailablein this context<a class="footnote-ref" href="#taisbak2003"> [taisbak2003] </a>.&#160;<a href="#fnref:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:19">
<p>Taisbak adds thatporimonis “vital to the interpretation of the <em>Data</em> ,” but Euclid “uses the word as being self-explanatory. It seems to be imported from everyday market prose meaning procure, furnish, get (for oneself): any kind of purchase without specification of method and currency” <a class="footnote-ref" href="#taisbak2003"> [taisbak2003] </a>.&#160;<a href="#fnref:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:20">
<p>The wordplasticcomes from the Latin for “capable of shaping or molding” <a class="footnote-ref" href="#sullivan1860"> [sullivan1860] </a>whilestigmaconveys the “mark of a pointed instrument, puncture, tattoo-mark, or brand” <a class="footnote-ref" href="#sullivan1860"> [sullivan1860] </a>.Campusis Latin forfield,focusis Latin forhearth,andversusis Latin for a line of poetry<a class="footnote-ref" href="#sullivan1860"> [sullivan1860] </a>S. Astimuluswas a prod or a goad, which is to say a sharp stick used to urge livestock forward (Vaan 587).Sinistercomes from the Latin word forleft,and eventually came to mean evil or unlucky, but was purportedly used by the Romans “in the opposite sense,” such that amavis sinistraor “bird in the left hand&hellip;was esteemed a happy omen” <a class="footnote-ref" href="#encyclopaedia1816"> [encyclopaedia1816] </a>. If the frame is widened to include linguistic doublets and cognates, the list balloons to the point of absurdity. The wordsdose,antidote,anecdote,date,donatio,addendum,addition,edition,editor,mandate,andmandatoryall share the Greek or Latin root word for the verbto give,but they have a vast range of meanings and associations today<a class="footnote-ref" href="#shipley2001"> [shipley2001] </a>. Perhaps most ironically, the portmanteauanecdatareplaces part of the Greek root word forgivenwith the Latin word forgiven.See Anu Garg, “A.Word.A.Day” (<a href="http://www.wordsmith.org/words/anecdata.html">http://www.wordsmith.org/words/anecdata.html</a>) for a summary of the termanecdataand Shipley for a summary of howanecdoteoriginates with the Greekdidonai<a class="footnote-ref" href="#shipley2001"> [shipley2001] </a>.&#160;<a href="#fnref:20" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:21">
<p>Such phrases referred especiallyproperties taken in warand could describe compensation paid by “the vanquished&hellip;either in kind ( vectigal ), with money ( stipendium ), or with a tribute ( tributum )” <a class="footnote-ref" href="#vico2000"> [vico2000] </a>.&#160;<a href="#fnref:21" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:22">
<p>They are also said to bemancipia, because they were taken by hand (manu capta) from the enemy ( <em>Justinian Institutes</em> , qtd. in<a href="#cameron1972">Cameron (1972), 5</a>).&#160;<a href="#fnref:22" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:23">
<p>See also Jill Walker Rettberg, “Situated Data Analysis: A New Method for Analysing Encoded Power Relationships in Social Media Platforms and Apps,”  <em>Humanities and Social Sciences Communications</em> 7, no. 1 (June 17, 2020): 1–13.<a href="https://doi.org/10.1057/s41599-020-0495-3">https://doi.org/10.1057/s41599-020-0495-3</a>. Rettberg also frames “situated data analysis” as an extension of Haraway’s concept of “situated knowledge.”&#160;<a href="#fnref:23" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:24">
<p>For a further indication of such terminological compatibility, see Timnit Gebru et. al. “Datasheets for Datasets,”  <em>ArXiv:1803.09010 [Cs]</em> , March 19, 2020.<a href="http://arxiv.org/abs/1803.09010">http://arxiv.org/abs/1803.09010</a>. The authors note, “Despite the importance of data to machine learning, there is no standardized process for documenting machine learning datasets” <a class="footnote-ref" href="#gebru2020"> [gebru2020] </a>. They propose a standard to “encourage careful reflection on the process of creating, distributing, and maintaining a dataset, including any underlying assumptions, potential risks or harms, and implications of use” and to ensure that “dataset consumers&hellip;have the information they need to make informed decisions about using a dataset” <a class="footnote-ref" href="#gebru2020"> [gebru2020] </a>.&#160;<a href="#fnref:24" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:25">
<p>Bode does not eschew methodological rigor altogether. She writes, “Although it is a premise of the book that data- rich literary history should not focus on the methods of analysis used to the detriment of the object analyzed, in a field that attempts to understand literature and culture by applying techniques devised for other purposes a critical approach to methodology is essential” <a class="footnote-ref" href="#bode2019"> [bode2019] </a>. Her critique of methodological emphasis lies in a tendency of others to rely on “trialing the newest or most innovative digital methods” instead of those best suited to the subject matter and “to responding to the requirements of humanities inquiry” <a class="footnote-ref" href="#bode2019"> [bode2019] </a>.## Bibliography&#160;<a href="#fnref:25" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry><entry><title type="html">Interpretable Outputs: Criteria for Machine Learning in the Humanities</title><link href="https://startwords.cdh.princeton.edu/vol/15/2/000555/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://startwords.cdh.princeton.edu/vol/15/2/000555/</id><author><name>James Dobson</name></author><published>2020-06-25T00:00:00+00:00</published><updated>2023-08-04T13:01:02-04:00</updated><content type="html"><![CDATA[<p>Those disciplines and fields that understand themselves as primarily humanistic have long had a special interest in exploring nuance, uncertainty, and ambiguity in our objects. While data-driven research might frequently be assumed to be in opposition to such concepts, this is not the case. Data may, in fact, enable the discovery and foregrounding of ambiguity. Yet as computational methods, especially those designed to operate on large-scale textual archives, increasingly enter into the work of such humanists there is a risk that access to those ambiguous dimensions of data will be lost. This risk is as much present for the researcher using these tools to interpret their objects as it is for the skeptical reader or reviewer of such interpretations. Visually appealing graphic renderings of data, high classification accuracy and confidence scores, and impressive summary statistics can easily be rhetorically structured to appeal to preconceived notions and commonly accepted understandings. Effective and academically responsible argumentation and interpretation in the humanities requires not just openness, which is to say unfettered access to the methods and data used to generate visualizations and to demonstrate accuracy, but that the selected datasets and models are interpretable. When features are derived from text, as in the case of much of the machine learning taking place in the humanities, these features need to be exposed along with the data that helps readers understand their significance. Humanistic work that is primarily argumentative — it should be acknowledged that of course not all digital humanities work is argumentative — depends upon certain frameworks by which readers can evaluate claims. These frameworks generally operate within established interpretive communities of scholars working on similar problems or using similar approaches<a class="footnote-ref" href="#fish1980"> [fish1980] </a>. While data and statistics have had a minimal presence within the humanities prior to the twenty-first century, contemporary computational approaches, if presented properly, are highly compatible with existing humanistic argumentative frameworks<a class="footnote-ref" href="#dobson2019"> [dobson2019] </a>. While some computational fields have been undergoing a reproducibility crisis, the present discussion is not primarily an argument about access, an argument that would critique the use of so-called black box algorithms or advocate for the digital humanities to join in the movement toward open science, but rather this essay argues for the importance of listening to rather than hiding noise, for exposing complexities and ambiguities, and for interpreting the nuances within data derived from our objects. It will do so by explicating the role of hermeneutics in the digital humanities before turning to three critical sites within computational workflows that will serve as case studies: the creation of data objects, topic modeling methods, and classification algorithms. These three sites, with their increasing complexity, enable us to see key common limits to interpretability across the softwarestackof tools commonly used in the digital humanities today.</p>
<p>Joining the recent injunction “against cleaning” made by Katie Rawson and Trevor Muñoz, who argue that data cleaning, standardization, and “tidiness [privilege] the structure of a container, rather than the data inside it” <a class="footnote-ref" href="#rawson2019"> [rawson2019] </a>, this essay turns to basic computational objects used in text-based machine learning, data structures, model formats, and classification algorithms, as a case study for the preservation of nuance, diversity, and interpretability. Similar to the way in which Rawson and Muñoz call data cleaning a “diversity-hiding trick,” selecting certain models and data formats as containers can suppress evidence and minimize the presence of the unexpected, the minor, and the troubling, leading to a misrepresentation of the contents and unwarranted claims. The following critique also shares the ethical force of the request for researchers to establish and follow clear documentation procedures for producing and using machine learning datasets and models made by Margaret Mitchell, Timnit Gebru, and their co-authors<a class="footnote-ref" href="#mitchell2019"> [mitchell2019] </a><a class="footnote-ref" href="#gebru2018"> [gebru2018] </a>. The combination of prioritizing methodological transparency and foregrounding the complexity and diversity of data is necessary in order to preserve scholarly standards and to enable, within the computational environment, the dialogical forms of argumentation common to the humanities.</p>
<p>Data, if we construe data broadly to mean evidence presented to readers or listeners in support of an argument, has long existed within the argumentative apparatus used within humanistic fields. Forms of data include textual evidence put forward in a close reading or material culture gathered from an archive. Evidence is made explicit by authors in academic arguments. Evidence can be variously interpreted by readers and observers. It is another feature of the academic argumentation that frames and focalizes evidence. Warrants, to borrow a term from the philosophy of logic — in particular the work of Stephen Toulmin, function within argumentation as the implicit link between evidence and claims<a class="footnote-ref" href="#toulmin1969"> [toulmin1969] </a>. Strong arguments involve making explicit that which implicitly provides the foundation for the interpretive claim, stating the warrants alongside the arguments that leverage them. I invoke the language of formal argumentation in order to help frame some emerging problems within computational work within the digital humanities.</p>
<p>But we first need to disentangle explanation from interpretation. Recently, there has been a growing demand for explainable models within artificial intelligence research and especially within those social scientific fields making use of AI techniques. These efforts have primarily asserted that explanation and interpretation are the same<a class="footnote-ref" href="#miller2019"> [miller2019] </a>. Explanation, however, does not have the same meaning as interpretation because an explanation can still function without resolving uncertainty and ambiguity found within data while an interpretation can and frequently does take these up as the basis for exploration or argumentation<a class="footnote-ref" href="#burckhardt1968"> [burckhardt1968] </a>. Interpretation arises from an encounter with evidence. In the humanities, interpretation tends to foreground uncertainty and ambiguity within this encounter; the interpreter may locate these sites within the primary object or gesture toward other evidence. Interpretation thus needs to be in relation to presented evidence, whether this evidence is an image, a passage of text, or information derived from data.</p>
<p>Computational models tends to reduce uncertainty by only registering that which is classifiable according to the logical structuring of the model as a container. While Julia Flanders and Fotis Jannidis are concerned with text-markup models and frameworks for complex reference systems, their assertion that “most modeling systems at their core require uncertainty to be represented in the content (i.e. through qualifying notations) rather than in the modeling itself” applies equally to numerical models of textual sources and the typical ways in which these are used in the application of machine learning in the humanities<a class="footnote-ref" href="#flanders2019"> [flanders2019] </a>. Data-driven arguments in the humanities need to present interpretable outputs because in order to accept arguments as plausible, readers need to be able to understand if the claims are warranted by evidence. The rhetorical situation within existing communities of interpretation is structured according to the logics of plausibility and therefore interpretive arguments cannot only be evaluated according to the governing logic of the correct and rigorous interpretation of data. From the basis of a humanistic approach toward tools and data, interpretable outputs from computational models derived from textual sources are simultaneously qualitative and quantitative. This is to say that plausibility and the qualitative are frequently registered in quantitative features and attributes including edge cases, outliers, anomalies, ill-fitting values as well as cases of noise taken as signal and vice versa.</p>
<p>There are two major categories in which we might place most computational humanities work at present. The first category is more affiliated with the practices of literary criticism and it draws upon the long history of understanding criticism not as secondary to the creative work that it interprets but itself as a creative act<a class="footnote-ref" href="#hartman1980"> [hartman1980] </a>. We can understand this mode of criticism as involving a collaborative relation between critic and reader that is playful yet built on shared norms. Creative computational criticism does not explicitly depend upon scientific rigor associated with statistics and computation such as validity, falsifiability, and reproducibility. Rather, algorithmic transformations of text are framed as creative or even speculative renderings. Criticism in this vein can be argumentative but the grounds for the critical work are not subject to a mode of critique that would prove it wrong, obsolete, or invalid. Such forms of computation are primary playful; these readings transform and deform input text. This form of computational work might best be associated with the work of Stephen Ramsay and Nick Montfort, although many others frame their work in similar terms<a class="footnote-ref" href="#ramsay2014"> [ramsay2014] </a><a class="footnote-ref" href="#montfort2018"> [montfort2018] </a>. The second category involves the use of computation for research in order to make empirical claims about data. These practices and methods share a desire to demonstrate the significance of any findings with the social sciences. Researchers working in this area thus produce arguments that require the methods to be statistically sound. Interpretations of their results are warranted by a series of assumptions that can be tested and verified. These scholars make hermeneutical claims about computational models that are grounded in their analysis of data. While they might make use of similar algorithms as those that I have classed as creative computational critics, they are less interested in engaging deformations than refining models to produce more significant results.</p>
<p>Though it provides some level of access, the explicit presentation of data in support of an argument, a discovery, or a finding does not necessarily mean greater interpretability. One increasingly common example, machine learning classification data, are frequently presented in the form of accuracy scores and the confusion matrix. Parsing f-scores, and precision and recall values enables a certain degree of understanding of the overall performance of the classifier and the function of the workflow as a whole, but this leaves aside many other post-classification metrics that can be directly applied to the underlying data model. These data can tell us much about our input datasets and the criteria by which the classifier made its classifications. For applications in the humanities, in which there is a scholarly community that cares deeply about ambiguity, nuance, and the historicity of language, models that can provide extended interpretable output features, for example coefficients, probability values, and weights used in classification are necessary. The exposure of these extended interpretable features enables some shifting of scale — a shifting that matches the needs of humanistic interpretation and recognizes that while meaning can be made at the level of individual signifier, the word or token and its frequencies, it is in those larger units, for example the sentence, paragraph, chapter, and volume, that much of the activity of meaning making is to be found<a class="footnote-ref" href="#algee-hewitt2017"> [algee-hewitt2017] </a><a class="footnote-ref" href="#allison2017"> [allison2017] </a>. Just as decontextualized individual signifiers can all too easily hide ambiguous meanings and data diversity, the absence of interpretable features frequently leaves readers of computational work with many unknowns as lists are leveled, distributions smoothed, and complex geometries collapsed.</p>
<p>Tool criticism is an emergent category of analysis that provides a humanities-centric framework for understanding the use of computer-aided methods in both the humanities and the sciences. Tool criticism seeks to foreground the tools used in humanities research that make use of computation. In so doing, it foregrounds methodology for computational researchers and for their critics. Karin van Es, who has done a great deal to develop and mobilize this concept, asks researchers to understand the epistemic impact of their tools. Tool criticism framework requires access to tools and data and produces inquiry into the affordances of computational methods:</p>
<blockquote>
<p>Tool criticism is the critical inquiry of knowledge technologies considered or used for various purposes. It reviews the qualities of the tool in light of, for instance, research activities and reflects on how the tool (e.g. its data source, working mechanisms, anticipated use, interface, and embedded assumptions) affects the user, research process and output.<br>
<a class="footnote-ref" href="#vones2018"> [vones2018] </a>Understanding computational methods through the language of tool criticism helps foreground the need for a robust inspection of input data, the workflow, and algorithmically generated output data. In many ways, the preference for interpretable data is the output version of input solutions offered by scholars interested in better descriptions of input data. In Katherine Bode’s notion of the data-rich object, we find a strong argument for better descriptions of input data, especially when these data concern book history. The data-rich object, in Bode’s account, provides a conceptual framework through which we might bridge the gap between scholarly work in bibliographical studies and computational text analysis in literary studies<a class="footnote-ref" href="#bode2017"> [bode2017] </a>. While some shared repositories provide a set of reference texts and features extracted from these texts that have been designed for computational work (see the HathiTrust Extracted Features Dataset for an example of this type of repository) once these input objects enter into scholarly workflow, much information has been stripped from the input data objects making it much more difficult to do cross-study comparisons and to apply these insights to ongoing scholarly debates within literary critical communities<a class="footnote-ref" href="#capitanu2016"> [capitanu2016] </a>.</p>
</blockquote>
<p>In many computational fields it is a norm that researchers, especially academic researchers undergoing peer review and working with complex technological instruments, make their methods transparent and include their data, models, as well as the code necessary to implement their research. Academic researchers are making demands of each other that they open up the black box of their experimental apparatus, much in the way that critics and activists have made similar demands of corporate and government entities deploying proprietary and secretive algorithmically-driven decision-making tools. Theopen sciencemovement champions such a model of methodological transparency as a solution and remedy for what has come to be recognized as a replication crisis in several fields, most notably in the psychological and brain sciences. Open access and transparency into methodology, however, do not ensure that reviewers and others will be able to understand the data-driven claims and the argumentative grounds supporting such claims. Mike Ananny and Kate Crawford critique what they call the “transparency ideal” for its emphasis on openness as the single solution to accountability in technological systems (Ananny and Crawford 2018). Ananny and Crawford understand accountability in terms not just of the operation of a single element but rather through the social critique of systems and in particular the workings of power throughout the complex social systems in which algorithms and other computer-based technological tools are embedded. Johannes Paßmann and Asher Boersma argue in “Unknowing Algorithms: On Transparency of Unopenable Black Boxes” for another mode of interpretation applicable to computational models that might be considered supplementary or an alternative practice to the demands for openness driven by the transparency ideal, a practice “that is not so much concerned with positive knowledge, but that deals with skills which help dealing with those parts of an artefact that one still does not know” <a class="footnote-ref" href="#pasmann2017"> [pasmann2017] </a>. They cite situations like medical procedures in which an actor — an actor who importantly is not the primary actor but perhaps another physician — might gather unspecified and potentially useless data as a form of an unopenable black box; the collection of data and experience cannot be explained or defended in methodological terms but it might provide useful information for later procedures. This example provides them with a way of describing negative knowledge, an understanding of the known unknown. Paßmann and Boersma use such a scene to defend a notion of transparency that is distant yet still rooted in phenomenological experience. They offer, via Maurice Merleau-Ponty’s account of the way in which a woman wearing a hat equipped with an extended feather navigates space, a tool-based mode of exploratory analysis, “a carefully paced out unknowing” that can limn the space of known unknown<a class="footnote-ref" href="#pasmann2017"> [pasmann2017] </a>. This amounts to what they callfeather knowledge,an alternative to direct contact that might be capable of providing knowledge about the workings of computational models when such direct access is not available or not possible.</p>
<p>Paßmann and Boersma’s concept of distance and mediated access to black boxes should not be taken as an endorsement of a refusal of access but an interpretive strategy that enables researchers to probe and feel their way around propriety technology. There are situations within academic research in which some models might need to be protected, for example if they might leak identifiable training data about human subjects, but in the vast majority of cases, open access is essential to understand the meaning of the data and subsequent data-based claims. At the same time, feather knowledge might contribute to the understanding of computational models in ways that are fundamentally different and perhaps even more crucial than disembodied observation of models.</p>
<p>Even if it were possible to achieve the goals of the transparency ideal, certain methods require other forms of inspection than observation of function in order to be understood and explainable. Direct access to parameters and features supplied as training data for classification algorithms can enable queries to examine the distribution of certain words or phrases in order to understand if these are indeed representative of the modeled event or phenomenon. For example, counterfactual testing of a model and its output can be done simply by using the same model with other samples, with data known to the counterfactual investigator. While counterfactual testing fits into the general design of a tool or workflow, sometimes this might not provide enough useful information to evaluate the model and to expose the diversity of data within the model. Developing an explanation of how some computational functions work can involve treating the algorithm or model as something to be tricked through adversarial techniques. Adversarial techniques might involve observing reactions to unexpected input, for example, and rather than supplying expected textual features one might supply randomized data or even alternate numerical input. This method of probing the functioning of an algorithm can leak or reveal decision criteria or training features if these are not available. Needing to follow such procedures might be considered undesirable due to the unreliability of such methods of probing. By making use of highly interpretable data and algorithms, the digital humanities community can avoid needing to turn to adversarial techniques to expose nuanced and diverse data and aid in the verification and interpretation of computational models.</p>
<h2 id="data-objects">Data Objects</h2>
<p>It is the containers that reshape and normalize data, the data objects themselves, in which we find the largest potential risks to interpretability. Because data models and formats are the foundation for higher level transformations and operations, they are crucial to improving interpretability. In order to better understand this problem, I will turn now to examine the affordances and limitations in two alternative data models found in the popular and widely-used Scikit-learn package for the Python programming ecosystem. Scikit-learn bundles together a number of fast and reliable machine learning algorithms along with associated data models, tools for preprocessing text, and facilities for connecting these together in a reproducible workflow. While Scikit-learn provides the foundations for developing, using, and distributing complex models for a number of different applications and research domains, it is especially well suited to the analysis of textual sources and widely used within the computational humanities community. The major data models and algorithms implemented by Scikit-learn will provide case studies through which we can examine the stakes of the choice between these models for the interpretability of the models themselves.</p>
<p>The ability to inspect and interpret models is a crucial capability for understanding the meaning of both their output and the function of the models themselves. Models, after all, model something. The assumptions that inform the design and operation of models are sometimes explicitly coded but more often these assumptions are implicitly registered in the details of their construction and in their operation. These details can span a number of levels, from the aesthetic features and arbitrary choices made during the coding, to the selection of specific algorithms and input parameters. What makes “model work” in the humanities distinct is the critical interest in the possibilities opened up with and foreclosed by each of these levels of interpretation. To put it another way, modeling in the humanities is subject to forms of critique that aim at both explicit and implicit assumptions while in other disciplines the primary mode of critique operates according technical criteria. Because of this, it is necessary for humanists, in selecting their tools and data objects, to use those objects and methods that afford the greatest levels of inspection and interpretability.</p>
<p>We can understand the stakes of these differences in terms of inspection by examining the various attributes found in two Scikit-learn feature extraction tools that produce models of textual sources. These two tools perform the exact same function and are typically used for differing scales of analysis. Crucially, they make available dramatically different levels of inspection and interrogation. Feature extraction, for text analysis, involves the preprocessing and fragmentation of the text into a document-term matrix (dtm). Preprocessing, for most applications, means normalization of accented text, removal of stopwords, and selection criteria for inclusion (thresholds for feature count). The fragmentation of the text or document involves splitting or tokenizing input text into words or n-gram (multiword) phrases.</p>
<p>The conversion of a document (text) or a set of documents into a document-term matrix or other vector space model provides what we might call a remediated representation of the text. The vectorization of data, of course, involves more than just shortcuts for the transposition or manipulation of data. The ability to apply a transformation to an entire row, column, or matrix without iteration marks, as Adrian Mackenzie argues, an epistemic shift away from the tabular representation of data and toward new pliable movements through data<a class="footnote-ref" href="#mackenzie2017"> [mackenzie2017] </a>. This is one of the primary transformations of the text in most computational workflows used by humanists. Vectorizing texts produces an alternate representation, a data model of a text collection that makes it possible to compare one text to another and the entire collection to other collections, as well as making it possible to perform a simultaneous vector operation across the entire model. Word order is lost but depending on key choices made by the researcher there can still be much information available that may serve interpretive goals within the resulting model about the size and distribution of the vocabulary and its relative frequency.</p>
<p>There are several vectorizers included within the Scikit-learn package, the two that are most pertinent to this discussion are the HashingVectorizer and the CountVectorizer. The Scikit-learn documentation explains that there are three limitations orcons,as they put it, to using the HashingVectorizer to generate a document-term matrix instead of other alternatives including the CountVectorizer:</p>
<ul>
<li>there is no way to compute the inverse transform (from feature indices to string feature names) which can be a problem when trying to introspect which features are most important to a model.</li>
<li>there can be collisions: distinct tokens can be mapped to the same feature index. However in practice this is rarely an issue if n_features is large enough (e.g. 2 ** 18 for text classification problems).</li>
<li>no IDF weighting as this would render the transformer stateful.<a class="footnote-ref" href="#scikit-learn2020"> [scikit-learn2020] </a>).</li>
</ul>
<p>The second limitation is related to the typical use-case of the HashingVectorizer: it can vectorize very large numbers of documents and with a lower use of computational resources, primarily random access memory, but there is a chance of some data corruption (in the form of collisions) in which the same index in the document-term matrix is used for distinct features (words or phrases). The first limitation presents the largest problem for researchers wanting to examine or “introspect,” to use the language of the Scikit-learn developers, the document-term matrix and to use the feature index to extract meaningful features that have contributed to classification accuracy. The HashingVectorizer uses hashing, a non-cryptographic one-way encoding scheme that saves space and increases access to encoded data but comes at a cost to interpretability in that you can no longer match those features identified as statistically significant to their indexed names.</p>
<p>The short segment of Python code in Figure 1 demonstrates what we might want to call the different affordances of the CountVectorizer and the HashingVectorizer. While both will extract features and produce a document-term matrix usable for different tasks, the HashingVectorizer does not enable direct inspection of the vocabulary. The HashingVectorizer might enable scholars to vectorize large-scale archives into reasonable compact document-term matrices but it has important limitations that make it much more difficult to inspect and interpret. With the CountVectorizer-produced matrix, we can query the vocabulary with the column index number of a term of interest, for example, the termwomanin Figure 1, and then determine the number of times it appears in each text, leading to knowledge about the relative representation of this term across the collection. More importantly, the existence of the mapping between feature index (columns) and names (vocabulary) in the CountVectorizer makes it possible to determine, in the example of a classification task, the vocabulary terms that were the most important features for that task. Here, as in the other examples found in this essay,importanthas a technical meaning, the degree to which a particular feature is statistically influential to the model rather than the relative importance of these terms within a text. The sharing of a model produced with the HashingVectorizer might satisfy the demand for openness, which is to say transparency in relation to methods, but the model is not as interpretable as one produced with CountVectorizer. Without these crucial feature indexing elements, models become much harder to interpret. This may render such a model inappropriate for applications in the digital humanities.</p>




























<figure ><img loading="lazy" alt="" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Comparing Scikit-Learn’s HashingVectorizer and CountVectorizer using the latter to map feature indices back to vocabulary terms.
        </p>
    </figcaption>
</figure>
<p>Data models, especially vectorized data models derived from text, are quite varied. There are some data models that can be easily modified for exploratory counterfactual testing while others cannot. Vectorizing new texts with the intention of hypothesis testing, which would involve adding new rows to the matrix, can be accomplished when the document-text matrix has been produced with either all existing word/n-grams or a restricted and known set of excluded or stopwords. If new words found in these additional texts are added to the vocabulary then new columns will be added and zero-count values for existing documents can be added. If terms were excluded previously for not meeting a minimum threshold of appearances or other criteria, then the newly modified model may not be correct. Using methods to vectorize documents that preserve all detected vocabulary and recording selection criteria alongside the model are strategies that can enable such modes of counter factual understanding and increase interpretability. More complex, higher-level methods making use of a slightly different variety of vectors, such as the word embedding models found in word2vec or fastText, can sometimes function in similar ways. One form of counterfactual testing of word2vec-produced vectors would include aligning or fitting another, well-known model to the one on hand and querying the resulting model. Perhaps the more common method at present takes the original model as the initial vector for additional training with another set of inputs. It is important to keep in mind that this method is limited to the extracted and trained vocabulary of the initial model. Comparisons of the original and the resulting model can make visible previously unavailable aspects of the model<a class="footnote-ref" href="#hamilton2016"> [hamilton2016] </a>. Digital humanists need not only produce transparent and inspectable but also interpretable data models, models that in their containing function preserve as many points of access into the complexity and diversity of the data. Selecting interpretable data models is crucial because they are the foundation for transformations and operations. This is important because limits in interpretable data at this lower level are compounded by those found at the higher levels of many workflows.</p>
<h2 id="topic-models">Topic Models</h2>
<p>Topic modeling, one of the earliest computational models used for exploratory data analysis in the humanities, continues to be a dominant method. The topic word or feature lists — much more than the frequencies by which these topics are associated with documents — have attracted the attention of humanists. The result of an unsupervised model, thetopicsproduced as one of the outputs of a topic modeling algorithm are unnamed and presented to the interpreter as an interpretive object. There is much variability to found in the output of topic modeling, depending upon the construction of workflow including the algorithms chosen, the parameters used, and the preprocessing performed. Nan Da argues that this variability makes topic modeling “extremely sensitive to parametricization, prone to overfitting, and…fairly unstable” <a class="footnote-ref" href="#da2019"> [da2019] </a>. The presentation of individual topics from a topic model with a wordcloud makes topic models particularly difficult to interpret. The viewer has to first <em>infer</em> the relationship between the words, typically presented with different font sizes for the depicted words or less frequently with color-coding, to understand how to read these words in relation to each other. Frequently critics making use of topic models will label their model with what they take to be the meaning of the topic.</p>
<p>In David M. Blei’s review article that popularized topic modeling for the humanities, Blei displayed the partial inference of a 100-topic latent Dirichlet allocation (LDA) model from 17,000 articles published in <em>Science</em> . This method is one of several used in topic modeling and remains the most popular due to its widespread availability and simplicity. Blei’s presentation of these topics and his explanation of his inferring of the topics helped established a pattern of what we might think of as the display of data without data. The terms, of course, are data but the values attached to these terms and their relative importance to the topics are hidden. The way in which Blei assigns labels to his topics also leads to some additional confusion by introducing another explicitly subjective layer of interpretation. We understand that words listed first in each column of a table labeled “top 15 most frequent words from the most frequent topics” <a class="footnote-ref" href="#blei2012"> [blei2012] </a>might be the most important to determining the meaning of the list but how important really are these words to the topic? The meaning of important in a model like LDA means statistically influential or having a relatively high bearing on the statistically calculated outcome. The statistical importance of these terms is difficult to interpret without numerical values by which we can determine the relations among the items in these lists. While Evolution, Disease, and Computers are relatively straightforward labels of topics with the most frequently appearing words, the Genetics topic required moving down to the fourth most frequent term to find the correct label. The point here is not that these topics are mislabeled but that the method by which labels are determined is opaque — they are assigned in an ad-hoc manner by the operator — and the information needed to determine the relationships among these words — which is to say that other than ranked order there are no numerical values presented — is not available.<br>
Topic model displaying words without data and with operator-assignedtopicheadings<a class="footnote-ref" href="#blei2012"> [blei2012] </a>.“Genetics”“Evolution”“Disease”“Computers”humanevolutiondiseasecomputergenomeevolutionaryhostmodelsdnaspeciesbacteriainformationgeneticorganismsdiseasesdatagenesliferesistancecomputerssequenceoriginbacterialsystemgenomeevolutionaryhostmodelsgenebiologynewnetworkmoleculargroupsstrainssystemssequencingphylogeneticcontrolmodelmaplivinginfectiousparallelinformationdiversitymalariamethodsgeneticsgroupparasitenetworksmappingnewparasitessoftwareprojecttwounitednewsequencescommontuberculosissimulations<br>
Scikit-learn, which provides several algorithms used to generate topic models, makes available methods to access the data used to assign words to the topic groups with LDA models. These values can be queried with then_componentsattribute of Scikit-learn’s implementation of the LDA model. These “components” or “variational parameters for topic word distribution” can provide either what the package calls a pseudocount, which is the number of times that each of the vocabulary terms has been assigned to the topic, or a number that represents the normalized distribution of words within each topic, the: “Since the complete conditional for topic word distribution is a Dirichlet, <code>components_[i, j]</code> can be viewed as pseudocount that represents the number of times word <code>j</code> was assigned to topic <code>i</code> ” <a class="footnote-ref" href="#scikit-learn2020"> [scikit-learn2020] </a>. The pseudocount figure is especially useful in determining the relative significance of each term within the topic list or container.</p>
<p>There are several methods that use the output from standard LDA models to provide higher-level statistics and visualizations that enable additional degrees of inspection of both the topics and the individual features included within the topics. Despite a history of calls for greater clarity with the presentation of topic models in the humanities, these methods to enable greater inspection have not seen much use by humanists<a class="footnote-ref" href="#schmidt2012"> [schmidt2012] </a>. The LDAvis package for R and Python provides several metrics for examining and understanding the contribution of single terms to the topics. One of the metrics used by LDAvis is known as termite and uses seriation to insert extracted terms back into some greater textual context, i.e., the appearance of the word of interest prior to vectorization<a class="footnote-ref" href="#chuang2012"> [chuang2012] </a>. This restored context preserves word order and enhances the understanding of topics and words but it is only available if the pre-vectorized sources, in other words plain-text sources with the original word order intact, are available. The selection of interpretable data objects, in this case the presence of complete textual sources, with associated bibliographic information, rather than just the vectorized sources as might be distributed by scholars working with copywritten or otherwise restricted textual datasets, directly informs the interpretability of higher-level models.</p>
<h2 id="classification-algorithms">Classification Algorithms</h2>
<p>Humanists are eager to have interpretable output that can be parsed according to familiar interpretative and framing strategies. Classification algorithms that promise to reify through empirical methods longstanding categories of humanistic analysis operate from what might be thought of as the opposite interpretive position: while topic models demand hermeneutical interpretation in which featured words are read in relation to algorithmically proposedtopics,classification algorithms are generally used to produce numerical probabilities of the likelihood of individual texts as belonging to this or that category. Classification algorithms tend to draw their power through numerous features. These features are usually not inspected through the hermeneutical circle that would interpret features in relation to categories and categories in relation to features. There are numerous uses for classification algorithms in the humanities and literary studies, in particular. Major classification categories include genres, topoi, literary periodization, focalization, narrative structures, and authorial identity. Equipped with labeled training and testing data, humanists have developed models that can, with various degrees of accuracy, classify input texts or sections of texts into such categories.</p>
<p>What we might want to call thedata sciencemethod of reporting results frequently takes the form of displaying overall model accuracy along with what is known as the confusion matrix, a visualization that can help determine the number and nature of misclassifications. Much of the published work in computational literary studies provides such descriptions and visualizations of these models and their reported metrics as evidence of successful modeling. Yet we cannot assess the usefulness and ultimately the meaning of classification models without access to the features, weights, and parameters used in the classification task. The selection of classification algorithms is usually made according to the model with the highest reported accuracy. The scholarship concerning the evaluation of these algorithms highlights this dimension. Bei Yu evaluates and compares the two classification algorithms invoked in the present essay, Naïve Bayes and Support Vector Machines, for use in literary studies, but her evaluation criteria consists of performance metrics, the relative accuracy of these algorithms in correctly classifying (verification involves the comparison of machine and human assigned labels applied to the objects) a selection of Emily Dickinson’s poems as either erotic or non-erotic and chapters of early American novels as either sentimental or non-sentimental<a class="footnote-ref" href="#yu2008"> [yu2008] </a>. While Matthew L. Jockers and Daniela M. Witten’s comparison of algorithms and exploration of the results of different classification models used in authorship attribution includes a table of the fifty most important words and two-word phrases for one classifier, the ability of these algorithms and models to produce such data are not emphasized as much as the performance (accuracy) of the algorithms in correctly identifying the author of a text<a class="footnote-ref" href="#jockers2010"> [jockers2010] </a>. This preference for high accuracy on testing datasets, while the best practice in many fields, does not necessarily lead to the most interpretable model. In order to be appropriate for humanities scholarship, the selection criteria for a classification algorithm needs to depend as much on the ability to return interpretable features as its classification accuracy on known testing data. It is an issue of balance, to be sure, but one needs a model that can successfully discriminate among texts while also making available the criteria used to produce these decisions.</p>
<p>Different classification algorithms applied to the same exact data can produce very similar classification results and accuracies but the features used by these algorithms to determine class membership can be substantially different. This complexity has proved to be a significant problem for humanistic engagement. What might it mean that one algorithm assigns more importance to specific subset of an extracted feature set than another? If both of these algorithms can determine a significant distinction at the level of word frequencies or other extracted features between the classes that make up the provided labeled dataset, does that mean that the distinction within the dataset is real? And if these are to be determined with a different set of features, why do the individual word features matter? Handling these questions and objections is not straightforward. An algorithm tuned for high accuracy will find distinctive features that fit into the model of distinction prioritized by that particular algorithm.</p>
<p>The dataset of Text-Encoding Initiative (TEI) tagged texts from the Text Creation Partnership (TCP) from the Early English Books Online (EEBO) collection enables some trivial experimentation with different classifiers to examine the features used for classifying short lyrical objects. After extracting plain-text segments from the XML mark-up files that were tagged as these lyrical categories (ballad, prayer, song) and vectorized using CountVectorizer, the objects can be split into training and testing datasets for examining the ability of several classification algorithms to generate interpretable output. The Support Vector Machines (SVM) machine learning classifier on one iteration of this classification task produces a 92.3% overall accuracy in classification across the three classes: precision recall f1-score support ballad 0.80 0.41 0.54 116 prayer 0.99 0.95 0.97 1151 song 0.85 0.97 0.90 733 avg/total 0.92 0.92 0.92 2000 The confusion matrix for this model, using the testing dataset described above, demonstrates excellent classification accuracy for prayers and songs, some ability to classify ballads, and many misclassifications of ballads as songs (leading to the recall score of 41%).</p>




























<figure ><img loading="lazy" alt="" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>SVM Confusion Matrix
        </p>
    </figcaption>
</figure>
<p>At the present time, some classification algorithms such as the SVM classifier are much more preferable than others, for example the k-nearest neighbors (kNN) algorithm, which is well-known as simple to understand but difficult to interpret. The k-nearest neighbors algorithm can return similarities between objects but does not readily supply the criteria by which these data elements were determined to be similar. Additional probing of the data can return feature similarity between the neighbors discovered via the k-nearest neighbors algorithm but these are not initially provided as output. At present, the level of computational skill required and critical faculty for such probing is rather high. The ability to engage in data exploration is necessary for the evaluation of results presented as meaningful. Yet determining the meaning of textual data requires both computational knowledge that can determine significance within the model and domain-specific contextual knowledge that can be applied to the understanding of these features. If features presented as significant are in fact meaningful within the terms of the selected textual dataset, the information needs to framed in an understandable fashion.<br>
Most statistically important features from each class for the SVM classifierBalladPrayerSongpdf (3.644), page (2.346), ballad (2.279), tune (1.785), ſame (1.302), engliſh (1.113), pope (1.103), ſecond (1.091), dog (1.091), popery (1.08), young (1.063), did (1.023), tryal (1.019), ſaid (0.983), husband (0.982), england (0.97), tanner (0.952), cripple (0.929), finis (0.928), gillian (0.917), ſir (0.901), ſée (0.896), monſters (0.893), country (0.889), ile (0.873), trading (0.871), farewel (0.854), true (0.839), ſwéet (0.834), live (0.798)amen (5.614), prayer (4.456), iesu (2.414), spirit (2.091), unto (1.964), god (1.862), deus (1.823), thy (1.806), praier (1.742), sins (1.646), yn (1.624), point (1.604), bleſſed (1.529), ps (1.529), jesus (1.52), father (1.51), lord (1.488), oremus (1.482), speach (1.48), vnto (1.404), things (1.382), vpon (1.376), pſal (1.369), merciful (1.365), meate (1.359), com (1.356), qui (1.331), beseech (1.33), ein (1.328), vs (1.306)song (7.892), ij (2.749), repeat (2.355), psalme (2.263), voc (2.134), doth (2.113), ſupra (1.817), lawes (1.649), sing (1.611), prayſe (1.581), ſong (1.549), bassvs (1.482), cantvs (1.473), ii (1.43), mr (1.402), oh (1.383), chorus (1.382), heau (1.311), ſing (1.31), gods (1.305), ll (1.297), allison (1.259), heav (1.23), duplicate (1.211), foes (1.167), gate (1.142), st (1.138), ſtyll (1.137), tenor (1.12), eu (1.112)<br>
If we want to understand why the wordpdfwas the most statistically important feature for classification of ballads, we can explore the supplied data, provided we have used a data model that makes the vocabulary available for inspection. In Figure 3 we can see a sample function that queries the vocabulary for the column number for a word of interest, extracts total count, and then examines the rows belonging to the supplied classes. The SVM classifier learned from labeled training data thatpdfwas the most informative feature for those included texts marked ballads and the document-term matrix for this training data shows that by far the majority of the three hundred and eighty two appearances of the termpdfappeared in those documents. The wordpdf,the shortened name for a portable document file, was mistakenly included within the extracted tagged text in an earlier preprocessing stage. It functions as a metadata description, an indicator of the location of the material on scanned pages, and was recorded in the XML as a comment (an example of this comment appears as <code>&lt;! — PDF PAGE 1 — &gt;</code> ). Because this comment text appeared more frequently in those texts in the training dataset identified as ballads, it has become statistically important to the classifier in recognizing this class of lyrical text. This level of inspection, querying the labeled data object directly, enables checks for obvious errors in preprocessing or coding (as in this case) as well as a form of hypothesis testing directed at understanding the meaning of the extracted features. Attempting to explain the classification from a single term, as this example would imply, will most likely not be possible in most cases. But words or other features of interest can be queried, and the distribution of the vocabulary over the classes can help determine if the classification is in fact meaningful to the classes or an artifact of the particular training data selected or of the model and its parameters.</p>




























<figure ><img loading="lazy" alt="" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Code to extract token count for each class in training data
        </p>
    </figcaption>
</figure>
<p>Interpreting individual features becomes increasingly complex when comparing multiple classification algorithms. Workflows and pipelines used in many machine learning tasks iterate through different parameters and algorithms in search of the greatest accuracy score. As previously mentioned, the algorithms reporting highest accuracy might not necessarily be the most interpretable choices and the features selected might not be meaningful in the available interpretive contexts. The features determined as key for classification can be dramatically different even if the overall performance on the classification task is roughly similar. While this might suggest that we do not need to spend valuable time interpreting the individual features that were indicated as important, this is simply not true. The Naïve Bayes classification algorithm, when applied to the exact same training data and features as those used in the Support Vector Machines classification task displayed above, results in a slightly higher accuracy score, 94% total accuracy across the three classes. The following accuracy report was produced by Scikit-Learn’s multinomial Naïve Bayes classifier: precision recall f1-score support ballad 0.66 0.58 0.62 116 prayer 0.99 0.98 0.98 1151 song 0.91 0.94 0.92 733 avg/total 0.94 0.94 0.94 2000</p>




























<figure ><img loading="lazy" alt="" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Naïve Bayes Confusion Matrix
        </p>
    </figcaption>
</figure>
<p>The Scikit-learn multinomial implementation of Naïve Bayes uses a mechanism similar to the package’s SVM implementation to access and display what the algorithm deems the most important features for making classification decisions among input data. It is one of the few Naïve Bayes classifiers in Scikit-learn that can provide access to these data. For Hoyt Long and Richard Jean So, the probability values used by Naïve Bayes classifiers and the ability to learn the criteria used in misclassifications were key to their selection of this method for finding statistical patterns in English-language haiku, although they used a different implementation of Naïve Bayes<a class="footnote-ref" href="#long2016"> [long2016] </a>. Values for individual features are stored as log probability values (available via classifier’s <code>feature_log_prob_</code> or formerly the <code>coef_</code> property) for each trained class. In addition, the Naïve Bayes classifier has a property ( <code>feature_count_</code> ) that can be accessed to get raw word or feature counts or weighted counts for the training data as found within labeled data. Table 3 displays the probability values for the features within each class or category of labeled data. The multinomial Naïve Bayes classifier’s log probability values are negative numbers because these are result of the logarithm of numbers between 0 and 1 and values that are closer to zero have a greater probability value and thus are more statistically important to the classification. While this Naïve Bayes classifier has a similar and even slightly higher accuracy across the model, it is not able to classify ballads as well as the SVM classifier. One reason might be the significantly less important role features such aspdfandpageplay in determining classification of these texts. The Naïve Bayes classifier has trouble distinguishing between ballads and songs, although far fewer songs are misclassified as ballads than ballads misclassified as songs. There are many features shared between the classes and interpreting the significance of these terms collectively across the classes and within each class requires investigation of both quantitative and semantic meaning.<br>
Most important features within each class of lyrical text for the Naïve Bayes classifierBalladPrayerSongdid (-4.916), thy (-5.467), thou (-5.634), ſo (-5.747), man (-5.749), good (-5.78), ſhe (-5.895), like (-5.962), men (-5.996), let (-6.027), king (-6.086), doth (-6.089), love (-6.092), god (-6.13), make (-6.168), haue (-6.174), come (-6.258), ſhall (-6.258), day (-6.301), great (-6.329), lord (-6.349), thee (-6.351), doe (-6.385), tune (-6.411), true (-6.431), tis (-6.539), heart (-6.577), hath (-6.59), wife (-6.644), came (-6.65)thy (-3.224), thou (-4.004), thee (-4.052), lord (-4.347), vs (-4.589), god (-4.605), vnto (-5.185), let (-5.185), haue (-5.339), good (-5.391), life (-5.483), father (-5.486), holy (-5.488), thine (-5.525), hast (-5.545), amen (-5.622), christ (-5.682), grace (-5.717), art (-5.751), make (-5.761), shall (-5.809), vpon (-5.87), unto (-5.872), death (-5.883), things (-5.892), mercy (-5.892), great (-5.904), world (-5.954), come (-5.972), mee (-5.989)la (-3.76), thy (-4.688), repeat (-4.821), thou (-4.996), loue (-5.215), thee (-5.251), did (-5.279), ij (-5.305), doth (-5.391), let (-5.425), fa (-5.464), love (-5.569), lord (-5.638), like (-5.684), shall (-5.727), come (-5.743), song (-5.753), god (-5.758), haue (-5.85), make (-5.937), ſo (-5.945), hath (-5.955), man (-5.991), doe (-5.994), ii (-6.06), day (-6.062), men (-6.087), good (-6.089), great (-6.122), vs (-6.153)</p>
<h2 id="conclusion">Conclusion</h2>
<p>From a humanist perspective, we might want to think of data models created from textual sources as alternative representations of supplied texts, and transformations of these might be, as Katherine Bode argues, performative materializations of the text sources<a class="footnote-ref" href="#bode2020"> [bode2020] </a>. These might be creative performances, in line with the notion of computational transformation as deformance, or argumentative acts. Even as an act or performance, humanists would best serve their audience by selecting, as the basic building blocks of their work, data models and algorithms that enable the greatest degree of interpretability. Understanding the significance of a particular instantiation of the execution or performance requires an attentive act by the audience. To pay attention means to enter into collaborative meaning making with the critical/creative work. This collaborative meaning-making activity is licensed by access to a shared vocabulary and the potential space for playful manipulation. If argumentative claims are put forward, these are then evaluated and warranted by shared assumptions and the ability to test and verify that the data are indeed comprehensible according to the norms of the shared interpretative community. Inspection and interpretation thus function together to animate the relationship between researcher and reader. Making computational work interpretable is essential to preserving two distinct threads within the digital humanities: upholding the standards of responsible scholarship, as articulated by the move toward open and reproducible workflows, and enabling the shared meaning-making activity between critic and reader that characterizes much humanistic interpretation.</p>
<p>Throughout this essay we have seen the limits to interpretability found in the selection and use of common vector-based data models, topic modeling algorithms and parameters, and in classification algorithms. Computational workflows are composite and modular. Alternative procedures and choices exist at almost every level within a workflow. This is both an asset and a liability. As a mode of conducting research, this flexibility enables digital humanists to select and link together the best models and algorithms for their purposes but as abstraction and complexity increases so too do the risks to interpretability and access to underlying data diversity. The ability for humanists, in particular, to examine and interpret the texts and methods used to make a model is as important if not more important than the reporting of the overall best results of a model — we saw this in the topic modeling case study — or the accuracy of a particular classification model. It is for all these reasons that humanists making use of computational methods to conduct their research need to select and privilege those models and methods that most enable inspection, exploration, and interpretation of diverse data and parameters.</p>
<h2 id="acknowledgments">Acknowledgments</h2>
<p>Much of my thinking about interpretive issues related to computation in the humanities has been the result of conversations and debates with Aden Evens. I would also like to thank the many students who have taken my Cultural Analytics courses and worked with me as research assistants, especially Julianna Thomson and Catherine Parnell, for their probing questions and for putting so much of the theory into practice.</p>
<h2 id="bibliography">Bibliography</h2>
<ul>
<li id="algee-hewitt2017">Algee-Hewitt, Mark, Ryan Heuser, and Franco Moretti. “On Paragraphs: Scale, Themes, and Narrative Form.” In _Canon/Archive: Studies in Quantitative Formalism from the Stanford Literary Lab_ . Edited by Franco Moretti. New York: n+1 books, 2017, 65-94.
</li>
<li id="allison2017">Allison, Sarah, Marissa Gemma, Ryan Heuser, Franco Moretti, Amir Tevel, and Irena Yamboliev. “Style at the Scale of the Sentence.” In _Canon/Archive: Studies in Quantitative Formalism from the Stanford Literary Lab_ . Edited by Franco Moretti. New York: n+1 books, 2017, 33-63.
</li>
<li id="ananny2018">Ananny, Mike and Kate Crawford, “Seeing without Knowing: Limitations of the Transparency Ideal and Its Application to Algorithmic Accountability.”  _New Media & Society_ 20, no. 3 (March 2018): 973–89.
</li>
<li id="blei2012">Blei, David M. “Probabilistic Topic Models.”  _Communications of the ACM_ 55, no. 4 (April 1, 2012): 77-84.
</li>
<li id="bode2017">Bode, Katherine. “The Equivalence of Close and Distant Reading; or, Toward a New Object for Data-Rich Literary History.”  _Modern Language Quarterly_ 78, no. 1 (March 2017): 77–106.
</li>
<li id="bode2020">Bode, Katherine. “Data Beyond Representation: From Computational Modelling to Performative Materiality.” Presented at the Annual MLA Convention, Seattle, WA, January 2020.<a href="https://katherinebode.wordpress.com/home/mla-convention-2020/">https://katherinebode.wordpress.com/home/mla-convention-2020/</a>.
</li>
<li id="burckhardt1968">Burckhardt, Sigurd. “Notes on the Theory of Intrinsic Interpretation.” In _Shakespearean Meanings_ . Princeton, NJ: Princeton University Press, 1968.
</li>
<li id="capitanu2016">Capitanu, Boris, Ted Underwood, Peter Organisciak, Timothy Cole, Maria Janina Sarol, J. Stephen Downie _The HathiTrust Research Center Extracted Feature Dataset_ (1.0) [Dataset]. HathiTrust Research Center. (2016).
</li>
<li id="chuang2012">Chuang, Jason, Christopher D. Manning, and Jeffrey Heer, “Termite: Visualization Techniques for Assessing Textual Topic Models.” In _Proceedings of the International Working Conference on Advanced Visual Interfaces - AVI ’12_ . The International Working Conference. Capri Island, Italy: ACM Press, 2012, 74-78.
</li>
<li id="da2019">Da, Nan Z. “The Computational Case against Computational Literary Studies.”  _Critical Inquiry_ 45, no. 3 (March 2019): 601–39.
</li>
<li id="dobson2019">Dobson, James E. _Critical Digital Humanities: The Search for a Methodology._ Urbana: University of Illinois Press, 2019.
</li>
<li id="fish1980">Fish, Stanley. _Is There a Text in This Class? The Authority of Interpretive Communities._ Cambridge, MA: Harvard University Press, 1980.
</li>
<li id="flanders2019">Flanders, Julia and Fotis Jannidis. “Data Modeling in a Digital Humanities Context: An Introduction.” In _The Shape of Data in the Digital Humanities: Modeling Texts and Text-based Resources_ , edited by Julia Flanders and Fotis Jannidis. New York: Routledge, 2019, 3-25.
</li>
<li id="gebru2018">Gebru, Timnit, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna M. Wallach, Hal Daumé III, and Kate Crawford. “Datasheets for Datasets.”  _CoRR_ abs/1803.09010 (2018). http://arxiv.org/abs/1803.09010
</li>
<li id="hamilton2016">Hamilton, William L., Jure Leskovec, and Dan Jurafsky, “Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change,”  _ArXiv Preprint ArXiv:1605.09096_ , 2016,<a href="https://arxiv.org/abs/1605.09096">https://arxiv.org/abs/1605.09096</a>.
</li>
<li id="hartman1980">Hartman, Geoffrey H. _Criticism in the Wilderness: The Study of Literature Today_ . New Haven: Yale University Press, 1980.
</li>
<li id="jockers2010">Jockers Matthew L. and Daniela M. Witten. “A Comparative Study of Machine Learning Methods for Authorship Attribution.”  _Literary and Linguistic Computing_ 25, no. 2 (2010): 215–223.
</li>
<li id="long2016">Long, Hoyt, and Richard Jean So. “Literary Pattern Recognition: Modernism between Close Reading and Machine Learning.”  _Critical Inquiry_ 42, no. 2 (January 2016): 235–67.
</li>
<li id="mackenzie2017">Mackenzie, Adrian. _Machine Learners: Archaeology of a Data Practice_ . Cambridge: MIT Press, 2017.
</li>
<li id="miller2019">Miller, Tim. “Explanation in Artificial Intelligence: Insights from the Social Sciences.”  _Artificial Intelligence_ 267 (February 2019): 1–38.
</li>
<li id="mitchell2019">Mitchell, Margaret, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. “Model Cards for Model Reporting,”  _Proceedings of the Conference on Fairness, Accountability, and Transparency - FAT* ’19_ (2019): 220–229.<a href="https://doi.org/10.1145/3287560.3287596">https://doi.org/10.1145/3287560.3287596</a>.
</li>
<li id="montfort2018">Montfort, Nick. _The Truelist_ . Denver: Counterpath, 2018.
</li>
<li id="pasmann2017">Paßmann, Johannes and Asher Boersma, “Unknowing Algorithms: On Transparency of Unopenable Black Boxes.” In _The Datafied Society_ , edited by Mirko Tobias Schäfer and Karin van Es. Amsterdam: Amsterdam University Press, 2017, 139–46.
</li>
<li id="ramsay2014">Ramsay, Stephen. “The Hermeneutics of Screwing Around; or What You Do with a Million Books.” In _Pastplay: Teaching and Learning History with Technology_ , edited by Kevin Kee. Ann Arbor: University of Michigan Press, 2014.
</li>
<li id="rawson2019">Rawson Katie and Trevor Muñoz, “Against Cleaning.” In _Debates in the Digital Humanities 2019_ , edited by Matthew K Gold and Lauren F. Klein. Minneapolis: University of Minnesota Press, 2019, 279–92.
</li>
<li id="scikit-learn2020">Scikit-Learn. Documentation. (2020).<a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html">https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html</a>
</li>
<li id="schmidt2012">Schmidt, Benjamin. “Words Alone: Dismantling Topic Models in the Humanities.”  _Journal of Digital Humanities_ 2, no. 1 (Winter 2012).
</li>
<li id="toulmin1969">Toulmin, Stephen Edelston. _The Uses of Argument._ Cambridge University Press, 1969.
</li>
<li id="yu2008">Yu, Bei. “An Evaluation of Text Classification Methods for Literary Study.”  _Literary and Linguistic Computing_ 23, no. 3 (2008): 327–43.
</li>
<li id="vones2018">van Es, Karin, Maranke Wieringa, and Mirko Tobias Schäfer. “Tool Criticism: From Digital Methods to Digital Methodology.” In _Proceedings of the 2nd International Conference on Web Studies - WS.2 2018_ , 24–27. Paris, France: ACM Press, 2018.
</li>
</ul>
]]></content></entry></feed>