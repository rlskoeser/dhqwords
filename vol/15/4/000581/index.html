<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=theme-color content="#3D206C"><meta http-equiv=x-ua-compatible content="IE=edge"><link rel=apple-touch-icon sizes=180x180 href=/img/favicon/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/img/favicon/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/img/favicon/favicon-16x16.png><link rel=icon type=image/png sizes=384x384 href=/img/favicon/android-chrome-384x384.png><link rel=icon type=image/png sizes=192x192 href=/img/favicon/android-chrome-192x192.png><link rel=icon type=image/png sizes=150x150 href=/img/favicon/mstile-150x150.png><link rel="shortcut icon" href=/favicon.ico><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/img/favicon/safari-pinned-tab.svg color=#3d206c><link rel=schema.dc href=http://purl.org/DC/elements/1.0/><meta name=citation_public_url content="https://rlskoeser.github.io/dhqwords/vol/15/4/000581/"><meta name=citation_title content="Character Recognition Of Seventeenth-Century Spanish American Notary Records Using Deep Learning"><meta name=citation_date content="2021/12"><meta name=citation_author content="Nouf Alrasheed"><meta name=citation_author content="Praveen Rao"><meta name=citation_author content="Viviana Grieco"><meta name=citation_abstract content="I. INTRODUCTION 1 Notary records contain a wealth of information for understanding different aspects of the human experience. For that reason, historians specialized in different regions and time periods employ them in writing social, economic, political, and cultural histories. The seventeenth-century Spanish American notarial scripts housed in the National Archives of Argentina are among the most challenging collections, as they were written by multiple hands, for an audience of experts, and at a time the written Spanish language underwent significant transformations 2 3 ."><meta name=citation_journal_title content="DHQwords"><meta name=citation_issue content="15.4"><meta name=citation_publisher content="Center for Digital Humanities, Princeton University"><meta name=DC.rights content="http://creativecommons.org/licenses/by/4.0/"><meta name=author content="Nouf Alrasheed, Praveen Rao, Viviana Grieco"><meta name=generator content="Center for Digital Humanities, Princeton University"><meta name=dcterms.created content="2021-12"><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-regular.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-italic.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-regular.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-300.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-700.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-300italic.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-500italic.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-700italic.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-500.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/Source_Sans_Pro/SourceSans3VF-Roman.otf.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/Source_Sans_Pro/SourceSans3VF-Italic.otf.woff2 crossorigin><title>Character Recognition Of Seventeenth-Century Spanish American Notary Records Using Deep Learning</title><meta name=description content="DHQwords Issue 15.4, December 2021. an experiment republishing DHQ articles with Hugo & Startwords theme. "><meta property="og:title" content="Character Recognition Of Seventeenth-Century Spanish American Notary Records Using Deep Learning"><meta property="og:description" content="I. INTRODUCTION 1 Notary records contain a wealth of information for understanding different aspects of the human experience. For that reason, historians specialized in different regions and time periods employ them in writing social, economic, political, and cultural histories. The seventeenth-century Spanish American notarial scripts housed in the National Archives of Argentina are among the most challenging collections, as they were written by multiple hands, for an audience of experts, and at a time the written Spanish language underwent significant transformations 2 3 ."><meta property="og:type" content="article"><meta property="og:url" content="https://rlskoeser.github.io/dhqwords/vol/15/4/000581/"><meta property="og:image" content="https://rlskoeser.github.io/dhqwords/social.png"><meta property="article:section" content="vol"><meta property="article:published_time" content="2021-12-07T00:00:00+00:00"><meta property="article:modified_time" content="2021-12-07T00:00:00+00:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://rlskoeser.github.io/dhqwords/social.png"><meta name=twitter:title content="Character Recognition Of Seventeenth-Century Spanish American Notary Records Using Deep Learning"><meta name=twitter:description content="I. INTRODUCTION 1 Notary records contain a wealth of information for understanding different aspects of the human experience. For that reason, historians specialized in different regions and time periods employ them in writing social, economic, political, and cultural histories. The seventeenth-century Spanish American notarial scripts housed in the National Archives of Argentina are among the most challenging collections, as they were written by multiple hands, for an audience of experts, and at a time the written Spanish language underwent significant transformations 2 3 ."><link rel=stylesheet href=/dhqwords/style.css><link rel=stylesheet href=/dhqwords/print.css media=print><script src=/dhqwords/js/polyfills.js></script><script defer src=/dhqwords/js/bundle.js></script></head><body class=article><header><nav class=main aria-label=main><ul><li class=home><a href=/dhqwords/>dhq</a></li><li class=issues><a href=/dhqwords/vol/>volumes</a></li></ul></nav></header><main><article><div class=grid><header><p class=number><a href=/dhqwords/vol/15/4/>Issue 15.4</a></p><h1>Character Recognition Of Seventeenth-Century Spanish American Notary Records Using Deep Learning</h1><p><ul class=authors><li><address>Nouf Alrasheed</address></li><li><address>Praveen Rao</address></li><li><address>Viviana Grieco</address></li></ul></p><p><time class=pubdate datetime=2021-12>December 2021</time></p><ul class="categories tags"></ul><ul class=tags></ul><p class=formats></p></header><section class=print-only><a class=first-page-header href=/ aria-label=Startwords><img alt=Startwords src=/pdf-logotype.svg></a>
<a class=page-header href=/ aria-label=Startwords><img alt=Startwords src=/pdf-logo.svg></a>
<a href=http://doi.org/ rel=alternate class=page-doi>doi:</a></section><div class=text-container><h2 id=i-introduction-1>I. INTRODUCTION <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></h2><p>Notary records contain a wealth of information for understanding different aspects of the human experience. For that reason, historians specialized in different regions and time periods employ them in writing social, economic, political, and cultural histories. The seventeenth-century Spanish American notarial scripts housed in the National Archives of Argentina are among the most challenging collections, as they were written by multiple hands, for an audience of experts, and at a time the written Spanish language underwent significant transformations <sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup> <sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup> .<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup> Consequently, it takes years of training and practice in seventeenth-century Spanish paleography to become proficient in reading and analyzing these notarial scripts (Fig.1). On an average, it takes expert Spanish speaking readers about one hour to read a four to five pages long notarized deed. The task is even more daunting for non-native Spanish speakers.</p><figure><img loading=lazy alt="Screenshot of two different styles of handwriting" src=/dhqwords/vol/15/4/000581/resources/images/Figure_1.png sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/15/4/000581/resources/images/Figure_1_hu537a1537ddc18eeda4816caa697768b1_507212_500x0_resize_box_3.png 500w,
/dhqwords/vol/15/4/000581/resources/images/Figure_1_hu537a1537ddc18eeda4816caa697768b1_507212_800x0_resize_box_3.png 800w,/dhqwords/vol/15/4/000581/resources/images/Figure_1.png 716w" class=landscape><figcaption><p>Samples of different handwriting styles present in the collection of seventeenth-century notarial scripts used for this study</p></figcaption></figure><p>Digitization significantly aided in the preservation of these records and made them relatively more accessible primarily by enabling their duplication without damaging the originals. However, despite the quantity and variety of documents this collection compiles, these records are still waiting to be fully utilized in scholarly endeavors. To this day, researchers and students rely on traditional, time consuming, and expensive methods of archival research to access these documents and archival discovery primarily depends on the skill, patience and luck of the scholar. The development of a system capable of storing, reading, querying, and analyzing this historical collection is crucial as it will make these manuscripts accessible to a broader community of researchers without requiring extensive and expensive paleography training. Character recognition is the first step in the development of such a system.</p><p>Today, using optical character recognition (OCR), we can automatically convert printed or handwritten text into machine-readable, editable, and searchable text. For that reason, OCR is regarded as the heart of many document analysis systems. Unlike the recognition of printed text, historical handwritten text presents unique challenges. Written in cursive, historical scripts usually employ irregular characters and capitalization, abbreviations, archaic spelling, and linked words (Fig. 2). <sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup> Additionally, the scans contain different types of noise including discoloration, stains, as well as ink bleeds and smudges. In order to enable OCR tasks, researchers applied different methods including Support Vector Machine (SVM) <sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup>, K-NN <sup id=fnref:7><a href=#fn:7 class=footnote-ref role=doc-noteref>7</a></sup>, and deep learning <sup id=fnref:8><a href=#fn:8 class=footnote-ref role=doc-noteref>8</a></sup>.</p><figure><img loading=lazy alt="Screenshot of two different styles of handwriting, annotated" src=/dhqwords/vol/15/4/000581/resources/images/Figure2.png sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/15/4/000581/resources/images/Figure2_hu042aee0e0a1c5da8c55190d8d1e9d2db_858937_500x0_resize_box_3.png 500w,
/dhqwords/vol/15/4/000581/resources/images/Figure2_hu042aee0e0a1c5da8c55190d8d1e9d2db_858937_800x0_resize_box_3.png 800w,/dhqwords/vol/15/4/000581/resources/images/Figure2.png 1031w" class=landscape><figcaption><p>Example of some of the challenges present in the manuscripts utilized for this study. On average, one page has 24 lines. The variations showed in this short paragraph are standard throughout the deeds.</p></figcaption></figure><p>The challenges of converting Spanish American historical texts into machine-readable text have been pointed out by Hannah Alpert-Abrams. She used <a href=https://icebergnlp.github.io/>Ocular</a>, the OCR tool developed by Taylor Berg-Kirkpatrick, to machine read the <em>Primeros Libros</em> , a sixteenth-century, printed, bilingual (Spanish and Nahuatl) text <sup id=fnref:9><a href=#fn:9 class=footnote-ref role=doc-noteref>9</a></sup>. Her experiments delivered not only dirty OCR but also linguistic errors that could lead to inaccuracies in cultural translation. We experimented with <a href=https://transkribus.eu/Transkribus/>Transkribus</a>, another popular tool, which also yielded an inaccurate output for our collection of seventeenth-century, handwritten, Spanish American notarial records. However, in recent years, deep learning has achieved remarkable success for image understanding and classification, image segmentation, speech recognition, and natural language processing. In this work, we explore if deep learning, and more specifically CNNs, can enable accurate recognition of characters in Spanish American notarial scripts.</p><p>Deep learning techniques perform better and achieve higher accuracy when large labeled datasets are available (e.g., ImageNet, MS COCO) <sup id=fnref:10><a href=#fn:10 class=footnote-ref role=doc-noteref>10</a></sup> <sup id=fnref:11><a href=#fn:11 class=footnote-ref role=doc-noteref>11</a></sup>. However, there is no readily available labeled dataset for seventeenth-century historical texts. Although data labeling is an expensive, error-prone, and time-consuming task, with the help of professional historians and paleography experts, we manually prepared a labeled dataset. Additionally, historical texts have various kinds of noises and degradation and the uneven scanning quality of the images pose additional challenges for image preprocessing and cleaning as it has to be performed without losing any of the essential features that define each of the characters.</p><p>In this paper, we present an empirical study of how well state-of-the-art CNNs perform for the task of recognizing handwritten characters in seventeenth-century Spanish American notarial scripts. To the best of our knowledge, this is the first effort towards automatically recognizing characters in seventeenth-century Spanish American notarial scripts.</p><p><strong>The key contributions of our work are the following:</strong></p><ul><li>With the assistance of professional historians as well as labelers proficient in Spanish and trained in paleography, we prepared the training dataset in two steps. Firstly, we collected from our labelers 250 unique samples of each of the characters present on the manuscripts. Secondly, we augmented the dataset and generated additional characters by applying random distortions and rotations to the original ones. For quality control, before training CNNs on the generated dataset, our professional historians verified that the expanded characters resembled the original ones. There are certain characters that are rare in both, the Spanish language and the notarial scripts. For those characters we had fewer labels.</li><li>We selected four state-of-the-art CNNs, namely, Inception-v3 <sup id=fnref:12><a href=#fn:12 class=footnote-ref role=doc-noteref>12</a></sup> <sup id=fnref:13><a href=#fn:13 class=footnote-ref role=doc-noteref>13</a></sup>, ResNet-50 <sup id=fnref:14><a href=#fn:14 class=footnote-ref role=doc-noteref>14</a></sup>, VGG-16 <sup id=fnref:15><a href=#fn:15 class=footnote-ref role=doc-noteref>15</a></sup> and InceptionResNet-v2 <sup id=fnref:16><a href=#fn:16 class=footnote-ref role=doc-noteref>16</a></sup> to recognize high frequency characters as well as rare characters (i.e., x and z). We trained these networks by configuring the hyperparameters to achieve the best classification accuracy. Our experiments showed that ResNet-50 achieved the best classification accuracy of 97.08% whereas other networks achieved lower accuracy with VGG-16 being the poorest.</li><li>For broader use by the academic community and to foster new research in transcribing historical texts, our labeled dataset and software are publicly available on GitHub via <a href=https://github.com/UMKC-BigDataLab/DeepLearningSpanishAmerican>https://github.com/UMKC-BigDataLab/DeepLearningSpanishAmerican</a>.</li></ul><p>This paper is organized as follows. Section II discusses the relevant related works and the motivation for this study. Section III presents the methodology we employed and discusses our evaluation results. Finally, we conclude in Section IV and outline our future work.</p><h2 id=ii-related-work--motivation>II. RELATED WORK & MOTIVATION</h2><p>Deep learning approaches have been widely used for handwritten text recognition of many modern languages. CNNs <sup id=fnref:17><a href=#fn:17 class=footnote-ref role=doc-noteref>17</a></sup> are among the most popular deep learning methods and have a proven record of outstanding performance when applied to image recognition tasks. Additionally, CNNs have shown an outstanding success when applied to the MNIST dataset <sup id=fnref:18><a href=#fn:18 class=footnote-ref role=doc-noteref>18</a></sup>. Ashiquzzaman et al. proposed a CNN-based model using ReLU activation function and dropout as a regularization layer that has achieved 97.4% accuracy <sup id=fnref:19><a href=#fn:19 class=footnote-ref role=doc-noteref>19</a></sup>. Tsai investigated various convolutional neural network architectures for handwritten Japanese character recognition and created a model with a 96.1% recognition rate for character classification <sup id=fnref:20><a href=#fn:20 class=footnote-ref role=doc-noteref>20</a></sup>. Another CNN-based model has been proposed by Rabby et al. to classify Bangla handwriting characters. A 95.71% validation accuracy was achieved for the BanglaLekha-Isolated dataset <sup id=fnref:21><a href=#fn:21 class=footnote-ref role=doc-noteref>21</a></sup>.</p><p>However, applying these methods to historical documents present unique challenges due to the quality of the scanned images, writing style variations, and the lack of labeled data. Consequently, only a few studies have taken this path. For instance, Kölsch et al. used a Fully Convolutional Neural Network (FCNN)-based approach for historic German documents, which achieved 95.6% accuracy <sup id=fnref:22><a href=#fn:22 class=footnote-ref role=doc-noteref>22</a></sup>. Clanuwat et al. proposed a KuroNet model that jointly recognizes an entire page of text by using a residual U-Net architecture and predicts the location and identity of all characters on a given page. Additionally, their proposed system was able to successfully recognize a significant fraction of pre-modern Japanese documents <sup id=fnref:23><a href=#fn:23 class=footnote-ref role=doc-noteref>23</a></sup>.</p><p>Researchers tend to combine CNNs with recurrent neural networks (RNNs) to further improve accuracy. That was the case for Granell et al. who proposed a handwritten text recognition system to transcribe a corpus of Spanish medieval scrips based on a CNN and RNN <sup id=fnref1:8><a href=#fn:8 class=footnote-ref role=doc-noteref>8</a></sup>. The authors showed that deep learning approaches outperform the traditional machine learning models such as Hidden Markov Model-based systems. Dona Valy et al. evaluated different deep learning approaches for character recognition that have been constructed from Khmer palm leaf manuscripts <sup id=fnref:24><a href=#fn:24 class=footnote-ref role=doc-noteref>24</a></sup>. The authors showed that the combination of CNN and RNN-based architectures achieves better results with a 5.01% error rate. Finally, Chammas et al. presented a CRNN system for text- line recognition of historical documents <sup id=fnref1:7><a href=#fn:7 class=footnote-ref role=doc-noteref>7</a></sup>. They showed how to train the system with only 10% manually labeled text-line data from the READ 2017 dataset.</p><p>Next, we describe the salient features of four recent CNNs that we used in our study.</p><p>VGG</p><p>In 2014, Karen Simonyan and Andrew Zisserman (2014) proposed the VGGNet for the Large Scale Visual Recognition Challenge (ILSVRC2014). The key contribution from this model was to increase the depth of the architecture by using a 3x3 convolutional filters to achieve higher performance. The VGG model achieved 92.7% top-5 accuracy on the ImageNet dataset and won the ILSVRC2014 challenge. For our experimental study, we chose VGG-16 as a representative of the VGGNet due to its smaller number of parameters compared to VGG19.</p><p>Inception</p><p>Inception architecture was first proposed in 2014 by Szegedy et al. The authors claimed that deeper networks are more prone to overfitting and consume computational resources. They solved that challenge by moving from fully connected to sparsely connected architectures. They introduced the inception layer, which is a combination of three different convolutional layers (1x1 convolutional layer, 3x3 convolutional layer, and 5x5 convolutional layer) with a max pooling layer that operates at the same level. Their outputs are concatenated to be the input of the next layer. This architecture has been updated to increase the accuracy further and proved that any convolution with kernel size more substantial than 3x3 could be represented efficiently with a series of smaller convolutions. In our experimental study, we used Inception- V3 <sup id=fnref1:12><a href=#fn:12 class=footnote-ref role=doc-noteref>12</a></sup> <sup id=fnref1:16><a href=#fn:16 class=footnote-ref role=doc-noteref>16</a></sup>.</p><p>ResNet</p><p>He et al. introduced the deep residual neural network (ResNet) architecture and won the first place in the ILSVRC 2015 classification competition. ResNet introduces the idea of identity connections that skip one or more layers to train deeper neural networks. This resolved the vanishing gradient problem by allowing the gradients to flow directly through the skipped connections backward from later layers to the initial filter. For our experimental study, we used ResNet-50 as a representative <sup id=fnref1:14><a href=#fn:14 class=footnote-ref role=doc-noteref>14</a></sup>.</p><p>InceptionResNet</p><p>Inception-ResNet is a convolutional neural network proposed by Szegedy et al. in 2016. It was trained on more than one million images from the ImageNet database and achieved a 3.08% top-5 error on the test set of the ImageNet classification (CLS) challenge <sup id=fnref2:16><a href=#fn:16 class=footnote-ref role=doc-noteref>16</a></sup> The success of residual connections in training very deep architectures and the performance of the Inception-V3 inspired the authors to replace the Inception filter concatenation step with residual connections. This combination allows Inception to obtain all the advantages of the residual approach but with the preservation of its computational efficiency. We used InceptionResNet-v2 as a representative for our experimental study.</p><p>Despite these advances, to this day, there is a lack of end-to-end systems capable of managing and analyzing historical documents in general and those in Spanish in particular. This gap, coupled with the professional training needs of 21st century humanities scholars, draw our attention and drives our experimentation efforts to make these manuscripts accessible to a broader community of researchers without requiring extensive and expensive paleography training. Our effort is the first step in this direction and will open up a wide range of research opportunities for others in the academic community.</p><h2 id=iii-methodology>III. METHODOLOGY</h2><p>In this section, we present the methodology for conducting this empirical study. The overall steps are illustrated in Figure 3. There are four main stages: (a) pre-processing, (b) dataset preparation, (c) training and validation (to tune the hyperparameters), and (d) testing the accuracy of character recognition.</p><figure><img loading=lazy alt="Image of a flowchart" src=/dhqwords/vol/15/4/000581/resources/images/Figure3.png sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/15/4/000581/resources/images/Figure3_hud90fb3966c53353164fd0ccac5ab7365_3051747_500x0_resize_box_3.png 500w,
/dhqwords/vol/15/4/000581/resources/images/Figure3_hud90fb3966c53353164fd0ccac5ab7365_3051747_800x0_resize_box_3.png 800w,/dhqwords/vol/15/4/000581/resources/images/Figure3_hud90fb3966c53353164fd0ccac5ab7365_3051747_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/4/000581/resources/images/Figure3.png 1483w" class=landscape><figcaption><p>Overall steps involved in character recognition of seventeenth-century Spanish American notarial scripts</p></figcaption></figure><h2 id=a-pre-processing>a) Pre-processing:</h2><p>The manuscripts scans contained noise including spurious ink markings, ink smudges and bleeds. Such noise affects the feature extraction as well as the classification. Before constructing the character dataset, we reduced the noise on the manuscripts images (Figure 4). The following preprocessing techniques allowed us to clean the images without affecting the quality of the written content. Firstly, we converted the original manuscript images into grayscale. Secondly, we applied a median filter to soften the images backgrounds and remove background noise. Finally, we applied image binarization to convert the grayscale images into black and white ones as this technique significantly reduces the information contained in an image and increases the training speed <sup id=fnref:25><a href=#fn:25 class=footnote-ref role=doc-noteref>25</a></sup>.</p><figure><img loading=lazy alt="Two images of handwriting" src=/dhqwords/vol/15/4/000581/resources/images/Figure4.png sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/15/4/000581/resources/images/Figure4_hu6bf271ac2052d8a55a04d760e3210a21_406367_500x0_resize_box_3.png 500w,
/dhqwords/vol/15/4/000581/resources/images/Figure4_hu6bf271ac2052d8a55a04d760e3210a21_406367_800x0_resize_box_3.png 800w,/dhqwords/vol/15/4/000581/resources/images/Figure4_hu6bf271ac2052d8a55a04d760e3210a21_406367_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/4/000581/resources/images/Figure4_hu6bf271ac2052d8a55a04d760e3210a21_406367_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/4/000581/resources/images/Figure4.png 1508w" class=landscape><figcaption><p>An example of an original scan and its cleaned version.</p></figcaption></figure><h2 id=b-dataset-preparation>b) Dataset preparation:</h2><p>We constructed the character dataset from clean images. <a href=http://www.colabeler.com/>Colabeler</a> tool was used to annotate and label the characters. The annotations were exported in JSON format along with each character label and its corresponding coordinates. We ran a Python script to crop and save every character in .png format. As it was difficult to keep each character within square dimensions while annotating them, each one of them was padded with white pixels and resized to fixed dimensions for training purposes. We considered 24 characters that comprise most of the Spanish alphabet: a, b, c, c, d, e, f, g, h, i, j, l, m, n, o, p, q, r, s, t, u/v, x, y, and z. Note that the &ldquo;u&rdquo; and the &ldquo;v&rdquo; were interchangeable in seventeenth-century Spanish, and thus, the alphabets provided by most paleography manuals list them as a single character. We followed this standard and treated them as single letter (u/v). Additionally, characters such as &ldquo;k&rdquo; and &ldquo;w&rdquo; are infrequently used in modern Spanish and were so uncommon in seventeenth-century notarial scripts that paleography textbooks do not even list them on their sample alphabets.<sup id=fnref:26><a href=#fn:26 class=footnote-ref role=doc-noteref>26</a></sup> To build our sample dataset, we selected the hand of Nicolas de Valdibia y Brizuela who, by 1650, acted as an interim notary in Buenos Aires, Argentina. As opposed to those who held permanent positions, interim notaries did not receive extensive training nor were they skilled scribes. Thus, the experiments we report in this paper were based on very irregular and, therefore, hard to read scripts, as shown in Figure 2 & Figure 4.</p><p>Our labeling team labeled 250 unique samples for every character. This resulted in a total of 6,000 original images. As deep learning models perform well on large labeled datasets, the dataset was augmented by applying random distortion and rotation of +5 and -5 degrees without affecting the shape and/or the direction of each character. A few examples are shown in Figure 5.</p><figure><img loading=lazy alt="Two images of handwriting" src=/dhqwords/vol/15/4/000581/resources/images/Figure5.png sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/15/4/000581/resources/images/Figure5_hu6a769b6c412eec0c3c1232cefd8de7c9_110619_500x0_resize_box_3.png 500w,
/dhqwords/vol/15/4/000581/resources/images/Figure5_hu6a769b6c412eec0c3c1232cefd8de7c9_110619_800x0_resize_box_3.png 800w,/dhqwords/vol/15/4/000581/resources/images/Figure5.png 1000w" class=landscape><figcaption><p>Example of the original characters b; d and p b) Example of augmented characters after applying random distortion and rotation</p></figcaption></figure><p>For quality control, before training CNNs on the generated dataset, the professional historians and paleography experts in our team verified that the expanded characters resembled the original ones. Our dataset contained 1,000 samples for each character out of which 250 samples were manually labeled, and 750 samples were generated. This resulted in a total of 24,000 images.</p><h2 id=c-training-and-validation>c) Training and validation:</h2><p>We conducted all our experiments on a GeForce RTX 2080 Ti GPU with 12GB GPU memory. Our software was implemented using Keras <sup id=fnref:27><a href=#fn:27 class=footnote-ref role=doc-noteref>27</a></sup> with TensorFlow backend <sup id=fnref:28><a href=#fn:28 class=footnote-ref role=doc-noteref>28</a></sup>, TensorBoard,<sup id=fnref:29><a href=#fn:29 class=footnote-ref role=doc-noteref>29</a></sup> and OpenCV <sup id=fnref:30><a href=#fn:30 class=footnote-ref role=doc-noteref>30</a></sup>.</p><p>We trained the state-of-art CNNs using 24,000 images (1,000 images per character) in our labeled dataset prepared from our seventeenth-century Spanish American notary scripts. For each character, we split the training set into an 80-20 split. The samples not used for training were part of the testing set. The testing set contained 50 samples per character and were preprocessed in the same way as the training images. Data augmentation was applied to the training set to avoid overfitting. As we mentioned earlier, most of the paleography manuals list the u and the v as a single character as they were interchangeable in seventeenth-century Spanish. We followed the same standard and treated them as a single letter (u/v).</p><h2 id=d-test-of-the-accuracy-of-character-recognition>d) Test of the accuracy of character recognition:</h2><p>In this research, the character recognition accuracy was used as the primary metric to evaluate the performance of different CNNs. An empirical tuning approach has been followed to tune the hyperparameters to obtain higher character recognition accuracy. To ensure a fair comparison, we set a total of 150 epochs to train the models, and used the Adam optimizer <sup id=fnref:31><a href=#fn:31 class=footnote-ref role=doc-noteref>31</a></sup>. Adam is a gradient descent optimization algorithm that is popularly used in training deep learning models. (Using gradient descent, it is possible to find local minima of functions during optimization.) The performance of the models with different hyperparameters values is shown in Table I and Table II.</p><p>Table I shows the recognition accuracy when we set the dropout value to 0.6. ResNet-50 achieved the highest accuracy of 97.02%, followed by InceptionResnet-v2, Inception-v3, and VGG-16 with a recognition accuracy of 96.33%, 93.83%, and 96.33%, respectively. The performance of most of the networks improved when the batch size was increased to 64 except for Inception-v3, which achieved a better recognition accuracy when the batch size was 32.</p><p>The recognition accuracy results obtained from using 0.5 dropout rate are presented in Table II. The recognition accuracy decreased for most of the networks when we changed the dropout rate from 0.6 to 0.5. VGG-16 was the only model where it performed better on a 0.5 dropout rate.<br>Recognition accuracy obtained with 0.6 dropout rate; best accuracy is shown in blue and the worst in red Models Batch Size 32 Batch Size 64 VGG-16 62.50% 69.33% Inception-V3 94.91% 93.83% ResNet-50 95.50% 97.08% InceptionResnet-V2 96.00% 96.33% Recognition accuracy obtained with 0.5 dropout rate; best accuracy is shown in blue and the worst in red Models Batch Size 32 Batch Size 64 VGG-16 70.33% 70.91% Inception-V3 93.83% 96.33% ResNet-50 96.92% 87.58% InceptionResnet-V2 96.66% 94.08%<br>Table III shows the accuracy breakdown of each character obtained from two different experiments. We labeled the best results in bold and worse results in italics. As shown in the table, VGG-16 fails in recognizing non-confusing characters such as &ldquo;o&rdquo; and &ldquo;u/v&rdquo;. However, it performs well on few characters such as m and y.<br>Recognition accuracy per character. Best results are highlighted in bold and the worst results are highlighted in italics   Batch Size = 32 & Dropout rate = 0.5       Batch Size = 64 & Dropout rate = 0.5       VGG Inception ResNet Inception VGG Inception ResNet Inception 16 v3 50 Resnet v2 16 v3 50 Resnet v2 A <em>82%</em> 94% 94% <strong>96%</strong> 92% <strong>94%</strong> <em>90%</em> 92% B <em>82%</em> <strong>100%</strong> <strong>100%</strong> 96% <em>86%</em> <strong>98%</strong> <strong>98%</strong> 92% C <em>62%</em> 92% <strong>94%</strong> <strong>94%</strong> <em>42%</em> <strong>94%</strong> 86% 92% c¸ <em>76%</em> <strong>100%</strong> 98% <strong>100%</strong> <em>74%</em> <strong>100%</strong> 98% 98% D <em>74%</em> 92% <strong>96%</strong> <strong>96%</strong> <em>74%</em> 94% 76% <strong>96%</strong> E <em>36%</em> 84% <strong>90%</strong> 84% <em>46%</em> <strong>90%</strong> 76% 86% F <em>38%</em> 86% 88% <strong>92%</strong> <em>60%</em> 92% 92% <strong>94%</strong> G <em>22%</em> 94% 96% <strong>98%</strong> <em>32%</em> <strong>88%</strong> 68% 68% H <em>74%</em> 98% <strong>100%</strong> 98% <em>68%</em> <strong>100%</strong> 78% <strong>100%</strong> I <em>64%</em> <strong>100%</strong> 96% <strong>100%</strong> 64% 96% <em>44%</em> <strong>100%</strong> J <em>88%</em> <strong>100%</strong> 98% 96% <em>86%</em> <strong>100%</strong> 96% <strong>100%</strong> L <em>76%</em> <strong>100%</strong> <strong>100%</strong> <strong>100%</strong> <em>70%</em> <strong>100%</strong> 80% <strong>100%</strong> M 98% <strong>100%</strong> <strong>100%</strong> <em>96%</em> <em>96%</em> <strong>100%</strong> 98% <strong>100%</strong> N <em>90%</em> 94% <strong>96%</strong> <strong>96%</strong> 76% <strong>92%</strong> <em>56%</em> 78% O <em>82%</em> <strong>100%</strong> <strong>100%</strong> <strong>100%</strong> <em>82%</em> <strong>100%</strong> <strong>100%</strong> <strong>100%</strong> P <em>88%</em> 98% 98% <strong>100%</strong> <em>94%</em> <strong>98%</strong> <strong>98%</strong> <strong>98%</strong> Q <em>74%</em> <strong>100%</strong> <strong>100%</strong> <strong>100%</strong> <em>78%</em> <strong>100%</strong> <strong>100%</strong> 98% R <em>58%</em> 92% 96% <strong>98%</strong> <em>44%</em> 94% <strong>96%</strong> 90% S <em>74%</em> <strong>94%</strong> <strong>94%</strong> 92% <em>58%</em> <strong>96%</strong> 94% 90% T <em>64%</em> 96% <strong>100%</strong> <strong>100%</strong> <em>82%</em> <strong>96%</strong> 94% <strong>96%</strong> u/v <em>68%</em> <strong>100%</strong> <strong>100%</strong> <strong>100%</strong> <em>64%</em> <strong>100%</strong> <strong>100%</strong> <strong>100%</strong> X <em>52%</em> 62% <strong>92%</strong> 90% <em>66%</em> <strong>90%</strong> 84% <strong>90%</strong> Y <em>94%</em> <strong>100%</strong> <strong>100%</strong> <strong>100%</strong> <em>88%</em> <strong>100%</strong> <strong>100%</strong> <strong>100%</strong> Z <em>72%</em> <strong>100%</strong> <strong>100%</strong> 98% <em>80%</em> <strong>100%</strong> <strong>100%</strong> <strong>100%</strong><br>Overall, VGG-16 performed the worst for most of our character datasets. Figure 6 gives the graphs of accuracy and loss values for the training set with respect to the number of epochs. It shows that VGG-16 accuracy could have been improved if it trained with more epochs.</p><figure><img loading=lazy alt="Images of eight line charts" src=/dhqwords/vol/15/4/000581/resources/images/Figure6.png sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/15/4/000581/resources/images/Figure6_hubfb39c0a3ee62c7a2b5bb6eecc93dd13_1254918_500x0_resize_box_3.png 500w,
/dhqwords/vol/15/4/000581/resources/images/Figure6_hubfb39c0a3ee62c7a2b5bb6eecc93dd13_1254918_800x0_resize_box_3.png 800w,/dhqwords/vol/15/4/000581/resources/images/Figure6_hubfb39c0a3ee62c7a2b5bb6eecc93dd13_1254918_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/4/000581/resources/images/Figure6_hubfb39c0a3ee62c7a2b5bb6eecc93dd13_1254918_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/4/000581/resources/images/Figure6_hubfb39c0a3ee62c7a2b5bb6eecc93dd13_1254918_1800x0_resize_box_3.png 1800w,/dhqwords/vol/15/4/000581/resources/images/Figure6.png 2424w" class=portrait><figcaption><p>The accuracy (left) and loss (right) curves on the training set of the CNN models</p></figcaption></figure><p>To further understand why some models achieved low accuracy, we generated confusion matrices for all the models. Confusion matrices help us study the miss-classified characters. As seen in Figure 7, confusion matrices confirm that most of the confusions occur between the characters that are written similarly. For instance, the character n is confused with r as shown in Figure 7(a), and g is confused with q as shown in Figure 7(b). The results are not surprising as these characters generally confuse non-expert human readers and, occasionally, trained paleographers. Figure 8 shows samples of these characters. However, as shown on Table III, the recognition accuracy remains overall strong.</p><figure><img loading=lazy alt="Two images of matrices" src=/dhqwords/vol/15/4/000581/resources/images/Figure7.png sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/15/4/000581/resources/images/Figure7_hudcd3a90dcb78b56bf1458c977feca5f9_197489_500x0_resize_box_3.png 500w,
/dhqwords/vol/15/4/000581/resources/images/Figure7_hudcd3a90dcb78b56bf1458c977feca5f9_197489_800x0_resize_box_3.png 800w,/dhqwords/vol/15/4/000581/resources/images/Figure7.png 1090w" class=landscape><figcaption><p>(a) ResNet-50 Confusion Matrix (b) InceptionReset-v2 Confusion Matrix Confusion matrices of selected models to show the miss-classified characters</p></figcaption></figure><figure><img loading=lazy alt="Two images of handwriting" src=/dhqwords/vol/15/4/000581/resources/images/Figure8.png sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/15/4/000581/resources/images/Figure8_hu2fd093c8ef39ed43c156b8696be8d9a6_61657_500x0_resize_box_3.png 500w,
/dhqwords/vol/15/4/000581/resources/images/Figure8_hu2fd093c8ef39ed43c156b8696be8d9a6_61657_800x0_resize_box_3.png 800w,/dhqwords/vol/15/4/000581/resources/images/Figure8.png 1000w" class=landscape><figcaption><p>a) Example of the shape similarities between the characters r and n. b) Example of the shape similarities between the characters g, q, and y</p></figcaption></figure><h2 id=iv-conclusion--future-work>IV. CONCLUSION & FUTURE WORK</h2><p>Historical handwritten character recognition is a challenging pattern recognition problem due to the inconsistency of the handwritten scripts and the lack of accurate labeled data. In this paper, we presented an empirical study on how state-of- the-art CNNs (developed for image classification) perform for the task of recognizing handwritten characters in seventeenth-century Spanish American notarial scripts. The labeled dataset employed in this study was carefully curated with the help of paleography experts and professional historians. Data augmentation was employed to increase the number of training samples. We observed that ResNet-50 achieved the promising accuracy of 97.08% compared to InceptionResnet- V2, Inception-V3, and VGG-16, which achieved 96.66%, 96.33%, and 70.91%, respectively.</p><p>Our study demonstrates that recent CNNs are promising to detect characters in seventeenth-century Spanish notarial scripts. Our future work will test the performance of deep learning-based OCR models such as Keras-OCR, YOLO-OCR, Tesseract and Kraken for the detection and recognition of handwritten words on these manuscripts. Accurate word recognition will be a necessary step in the development of a tool for reading, querying, and analyzing this historical collection. We plan model the content of these manuscripts in a form that would make information retrieval faster and better.</p><p>The labeled dataset and software used in this study are publicly available on GitHub via <a href=https://github.com/UMKC-BigDataLab/DeepLearningSpanishAmerican>https://github.com/UMKC-BigDataLab/DeepLearningSpanishAmerican</a>.</p><h2 id=acknowledgements>ACKNOWLEDGEMENTS</h2><p>This research was supported by the NEH Digital Humanities Advancement Grant (HAA-271747-20), UMKC’s Missouri Institute for Energy and Defense (MIDE), UMKC’s Funding for Excellence Program, and a Collaborative Data Science Grant from UMKC’s Institute for Data Education, Analytics and Science (IDEAS). The authors would like thank Ryan Rowland, Maha Alrasheed, and Vania Todorova for labeling data as well as the <em>Archivo General de la República Argentina</em> for granting their permission to use in this study their digitized collection of notary records. Dr. Martin L. E. Wasserman contributed to this project with his expertise in Spanish paleography. The first author (N. A.) would like to thank UMKC’s Women’s Council Graduate Assistance Fund (GAF) as well as the University of Tabuk in Saudi Arabia for sponsoring her scholarship.</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>Viviana Grieco’s research focuses on Colonial Latin American history and has received extensive paleography training in Argentina and in Spain. She leads our labeling team which counts on the expertise of Martin Wasserman and David Freeman, historians who have employed in their research the collection of notary records used in this study. For more information about our research team visit <a href=https://www.umkc.edu/mide/NEH-Project/>https://www.umkc.edu/mide/NEH-Project/</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>Silva Prada, N. “Paleografías americanas,” <a href=https://www.openedition.org/21549>https://www.openedition.org/21549</a>, 2001.r&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>Wasserman, M.L.E., “La escritura paleográfica Iberoamericana: letras procesales y encadenadas,” in <em>Introducción a la Paleografía. Herramientas para la Lectura y Anáisis de Documentos Antiguos</em> , ed. Rosana Vassallo (La Plata: Facultad de Humanidades y Ciencias de la Educación, 2018)&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p>The speed and volume of the documentary production forced the scribes to link words and use an increasing number of abbreviations. The office of the public notary in seventeenth century Buenos Aires had a high turnover rate, which explains the large number of interim notaries as well as the variety of hands present in this collection.&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5><p>The Document Analysis Group at the Universitat Autònoma de Barcelona has been developing a digital library for the sixteenth-century <em>Llibres d’Esposalles</em> (marriage records). These handwritten marriage records are quite challenging although each marriage license follows a regular formula and the scripts are more consistent than those for the seventeenth-century notary records, <a href=http://dag.cvc.uab.es/the-esposalles-database/>http://dag.cvc.uab.es/the-esposalles-database/</a>&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:6><p>Vellingiriraj E., Balamurugan M., and Balasubramanie P., “Information extraction and text mining of ancient vattezhuthu characters in historical documents using image zoning,” in 2016 International Conference on Asian Language Processing (IALP). IEEE, 2016, pp. 37–40.&#160;<a href=#fnref:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:7><p>Chammas E., Mokbel C., and Likforman-Sulem L., “Handwriting recognition of historical documents with few labeled data,” in 2018 13th IAPR International Workshop on Document Analysis Systems (DAS). IEEE, 2018, pp. 43–48.&#160;<a href=#fnref:7 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:7 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:8><p>Granell E., Chammas E., Likforman-Sulem L., Martíınez-Hinarejos C.-D., Mokbel C., and Cîrstea B.-I., “Transcription of Spanish historical handwritten documents with deep neural networks,” <em>Journal of Imaging</em> , vol. 4, no. 1, p. 15, 2018.&#160;<a href=#fnref:8 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:8 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:9><p>Alpert-Abrams, H., “Machine Reading the <em>Primeros Libros</em> ,” <em>DHQ</em> 10, no. 4, 2016.&#160;<a href=#fnref:9 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:10><p>Deng J., Dong W., Socher R., Kai Li L. Li, and Li Fei-Fei, “Imagenet: A large-scale hierarchical image database,” in 2009 IEEE Conference on Computer Vision and Pattern Recognition, 2009, pp. 248–255.&#160;<a href=#fnref:10 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:11><p>Lin, T-Y., Maire M., S. Belongie, J. Hays, Perona P., Ramanan D., Dollár P., and. Zitnick C. L, “Microsoft coco: Common objects in con- text,” in Computer Vision – ECCV 2014, D. Fleet, T. Pajdla, B. Schiele, and T. Tuytelaars, Eds. Cham: Springer International Publishing, 2014, pp. 740–755.&#160;<a href=#fnref:11 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:12><p>Szegedy C., Liu W., Jia Y., Sermanet P., Reed S., Anguelov D., Erhan D., Vanhoucke V., and Rabinovich A., “Going deeper with convolutions,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2015, pp. 1–9.&#160;<a href=#fnref:12 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:12 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:13><p>Szegedy C., Vanhoucke V., Ioffe S., Shlens J., and. Wojna Z, “Rethinking the inception architecture for computer vision,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 2818–2826.&#160;<a href=#fnref:13 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:14><p>He K., Zhang X., Ren S., and Sun J., “Deep residual learning for image recognition,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770–778.&#160;<a href=#fnref:14 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:14 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:15><p>Simonyan K. and Zisserman A., “Very deep convolutional networks for large-scale image recognition,” arXiv preprint arXiv:1409.1556, 2014.&#160;<a href=#fnref:15 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:16><p>Szegedy C., Ioffe S., Vanhoucke V., and Alemi A. A., “Inception-v4, inception-resnet and the impact of residual connections on learning,” in Thirty-first AAAI conference on artificial intelligence, 2017.&#160;<a href=#fnref:16 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:16 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref2:16 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:17><p>Krizhevsky A., Sutskever I., and Hinton G. E., “Imagenet classification with deep convolutional neural networks,” in <em>Advances in neural infor- mation processing systems</em> , 2012, pp. 1097–1105.&#160;<a href=#fnref:17 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:18><p>LeCun Y., Bottou L., Bengio Y., and Haffner P., “Gradient-based learning applied to document recognition,” Proceedings of the IEEE, vol. 86, no. 11, pp. 2278–2324, 1998.&#160;<a href=#fnref:18 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:19><p>Ashiquzzaman A. and Tushar A. K., “Handwritten Arabic numeral recognition using deep learning neural networks,” in 2017 IEEE International Conference on Imaging, Vision & Pattern Recognition (icIVPR). IEEE, 2017, pp. 1–4.&#160;<a href=#fnref:19 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:20><p>Tsai C., “Recognizing handwritten Japanese characters using deep convolutional neural networks,” 2016.&#160;<a href=#fnref:20 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:21><p>Rabby A. S. A., Haque S., Islam S., Abujar S., and Hossain S. A., “Bornonet: Bangla handwritten characters recognition using convolutional neural network,” Procedia computer science, vol. 143, pp. 528– 535, 2018.&#160;<a href=#fnref:21 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:22><p>Kölsch A., Mishra A., Varshneya S., Afzal M.Z., and Liwicki M., “Recognizing challenging handwritten annotations with fully convolutional networks,” in 2018 16th International Conference on Frontiers in Handwriting Recognition (ICFHR). IEEE, 2018, pp. 25–31.&#160;<a href=#fnref:22 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:23><p>Clanuwat T., Lamb A., and. Kitamoto A, “Kuronet: Pre-modern Japanese Kuzushiji character recognition with deep learning,” arXiv preprint arXiv:1910.09433, 2019.&#160;<a href=#fnref:23 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:24><p>Valy D., Verleysen M., Chhun S., and Burie J.-C., “Character and text recognition of Khmer historical palm leaf manuscripts,” in 2018 16th International Conference on Frontiers in Handwriting Recognition (ICFHR). IEEE, 2018, pp. 13–18.&#160;<a href=#fnref:24 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:25><p>Chamchong R. and Fung C. C., “Comparing background elimination approaches for processing of ancient Thai manuscripts on palm leaves,” in 2009 International Conference on Machine Learning and Cybernetics, vol. 6. IEEE, 2009, pp. 3436–3441.&#160;<a href=#fnref:25 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:26><p>N. Silva-Prada, Manual de paleografía y diplomática hispanoamericana, siglos XVI, XVII y XVIII. Libros de texto, manuales de prácticas y antologías. Universidad Autónoma Metropolitana, Unidad Iztapalapa, 2001. <a href=https://paleografi.hypotheses.org/el-manual-de-silva-prada>https://paleografi.hypotheses.org/el-manual-de-silva-prada</a>&#160;<a href=#fnref:26 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:27><p>Chollet F. et al., “Keras,” <a href=https://github.com/fchollet/keras>https://github.com/fchollet/keras</a>, 2015.&#160;<a href=#fnref:27 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:28><p>Abadi M., Agarwal A., Barham P., Brevdo E., Chen Z., Citro C., Corrado G. S., Davis A., Dean J., Devin M., Ghemawat S., Goodfellow I., Harp A., Irving G., Isard M., Jia Y., Jozefowicz R., Kaiser L., Kudlur M., Levenberg J., Mané D., Monga R., Moore S., Murray D., Olah C., Schuster M., Shlens J., Steiner B., Sutskever I., Talwar K., Tucker P., Vanhoucke V., Vasudevan V., Viégas F., Vinyals O., Warden P., Wattenberg M., Wicke M., Yu Y., and Zheng, X. “TensorFlow: Large-scale machine learning on heterogeneous systems,” 2015, software available <a href=https://www.tensorflow.org/>https://www.tensorflow.org/</a>&#160;<a href=#fnref:28 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:29><p>Tensorflow, “tensorflow/tensorboard,” Apr 2020. Available: <a href=https://github.com/tensorflow/tensorboard>https://github.com/tensorflow/tensorboard</a>&#160;<a href=#fnref:29 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:30><p>Bradski G., “The OpenCV Library,” <em>Dr. Dobb’s Journal of Software Tools</em> , 2000.&#160;<a href=#fnref:30 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:31><p>Kingma, Diederik P. and Ba, Jimmy, “Adam: A Method for Stochastic Optimization,” arxiv:1412.6980, published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015.&#160;<a href=#fnref:31 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div></div></article></main><footer><nav aria-label="footer links"><ul><li><a class=highlight-focus href=/dhqwords/tags/>Tags</a></li><li><a class=highlight-focus href=/dhqwords/about/>About</a></li><li><a class=highlight-focus href=/dhqwords/categories/>Keywords</a></li></ul></nav><div class=icons><a class="license highlight-focus" rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons Attribution No-Derivatives 4.0 International License" src=/dhqwords/img/logos/license.svg width=120 height=42></a></div></footer></body></html>