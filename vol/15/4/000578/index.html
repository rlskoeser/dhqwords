<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=theme-color content="#3D206C"><meta http-equiv=x-ua-compatible content="IE=edge"><link rel=apple-touch-icon sizes=180x180 href=/img/favicon/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/img/favicon/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/img/favicon/favicon-16x16.png><link rel=icon type=image/png sizes=384x384 href=/img/favicon/android-chrome-384x384.png><link rel=icon type=image/png sizes=192x192 href=/img/favicon/android-chrome-192x192.png><link rel=icon type=image/png sizes=150x150 href=/img/favicon/mstile-150x150.png><link rel="shortcut icon" href=/favicon.ico><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/img/favicon/safari-pinned-tab.svg color=#3d206c><link rel=schema.dc href=http://purl.org/DC/elements/1.0/><meta name=citation_public_url content="https://rlskoeser.github.io/dhqwords/vol/15/4/000578/"><meta name=citation_title content="Compounded Mediation: A Data Archaeology of the Newspaper Navigator Dataset"><meta name=citation_date content="2021/12"><meta name=citation_author content="Benjamin Lee"><meta name=citation_abstract content="I. An Introduction to the Newspaper Navigator Dataset In partnership with LC Labs, the National Digital Newspaper Program, and IT Design &amp;amp;amp; Development at the Library of Congress, as well as Professor Daniel Weld at the University of Washington, I constructed the Newspaper Navigator dataset as the first phase of my Library of Congress Innovator in Residence project, _ Newspaper Navigator_ .1 The project has its origins in Chronicling America , a database of digitized historic American newspapers created and maintained by the National Digital Newspaper Program, itself a partnership between the Library of Congress and the National Endowment for the Humanities."><meta name=citation_journal_title content="DHQwords"><meta name=citation_issue content="15.4"><meta name=citation_publisher content="Center for Digital Humanities, Princeton University"><meta name=DC.rights content="http://creativecommons.org/licenses/by/4.0/"><meta name=author content="Benjamin Lee"><meta name=generator content="Center for Digital Humanities, Princeton University"><meta name=dcterms.created content="2021-12"><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-regular.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-italic.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-regular.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-300.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-700.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-300italic.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-500italic.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-700italic.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-500.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/Source_Sans_Pro/SourceSans3VF-Roman.otf.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/Source_Sans_Pro/SourceSans3VF-Italic.otf.woff2 crossorigin><title>Compounded Mediation: A Data Archaeology of the Newspaper Navigator Dataset</title><meta name=description content="DHQwords Issue 15.4, December 2021. an experiment republishing DHQ articles with Hugo & Startwords theme. "><meta property="og:title" content="Compounded Mediation: A Data Archaeology of the Newspaper Navigator Dataset"><meta property="og:description" content="I. An Introduction to the Newspaper Navigator Dataset In partnership with LC Labs, the National Digital Newspaper Program, and IT Design & Development at the Library of Congress, as well as Professor Daniel Weld at the University of Washington, I constructed the Newspaper Navigator dataset as the first phase of my Library of Congress Innovator in Residence project, _ Newspaper Navigator_ .1 The project has its origins in Chronicling America , a database of digitized historic American newspapers created and maintained by the National Digital Newspaper Program, itself a partnership between the Library of Congress and the National Endowment for the Humanities."><meta property="og:type" content="article"><meta property="og:url" content="https://rlskoeser.github.io/dhqwords/vol/15/4/000578/"><meta property="og:image" content="https://rlskoeser.github.io/dhqwords/social.png"><meta property="article:section" content="vol"><meta property="article:published_time" content="2021-12-07T00:00:00+00:00"><meta property="article:modified_time" content="2021-12-07T00:00:00+00:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://rlskoeser.github.io/dhqwords/social.png"><meta name=twitter:title content="Compounded Mediation: A Data Archaeology of the Newspaper Navigator Dataset"><meta name=twitter:description content="I. An Introduction to the Newspaper Navigator Dataset In partnership with LC Labs, the National Digital Newspaper Program, and IT Design & Development at the Library of Congress, as well as Professor Daniel Weld at the University of Washington, I constructed the Newspaper Navigator dataset as the first phase of my Library of Congress Innovator in Residence project, _ Newspaper Navigator_ .1 The project has its origins in Chronicling America , a database of digitized historic American newspapers created and maintained by the National Digital Newspaper Program, itself a partnership between the Library of Congress and the National Endowment for the Humanities."><link rel=stylesheet href=/dhqwords/style.css><link rel=stylesheet href=/dhqwords/print.css media=print><script src=/dhqwords/js/polyfills.js></script><script defer src=/dhqwords/js/bundle.js></script></head><body class=article><header><nav class=main aria-label=main><ul><li class=home><a href=/dhqwords/>dhq</a></li><li class=issues><a href=/dhqwords/vol/>volumes</a></li></ul></nav></header><main><article><div class=grid><header><p class=number><a href=/dhqwords/vol/15/4/>Issue 15.4</a></p><h1>Compounded Mediation: A Data Archaeology of the Newspaper Navigator Dataset</h1><p><ul class=authors><li><address>Benjamin Lee</address></li></ul></p><p><time class=pubdate datetime=2021-12>December 2021</time></p><ul class="categories tags"><li><span class=tag>machine learning</span></li><li><span class=tag>archaeology</span></li><li><span class=tag>digital</span></li><li><span class=tag>cultural heritage</span></li><li><span class=tag>databases</span></li></ul><ul class=tags></ul><p class=formats></p></header><section class=print-only><a class=first-page-header href=/ aria-label=Startwords><img alt=Startwords src=/pdf-logotype.svg></a>
<a class=page-header href=/ aria-label=Startwords><img alt=Startwords src=/pdf-logo.svg></a>
<a href=http://doi.org/ rel=alternate class=page-doi>doi:</a></section><div class=text-container><h2 id=i-an-introduction-to-the-newspaper-navigator-dataset>I. An Introduction to the Newspaper Navigator Dataset</h2><p>In partnership with LC Labs, the National Digital Newspaper Program, and IT Design & Development at the Library of Congress, as well as Professor Daniel Weld at the University of Washington, I constructed the <em>Newspaper Navigator</em> dataset as the first phase of my Library of Congress Innovator in Residence project, _ Newspaper Navigator_ .<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> The project has its origins in <em>Chronicling America</em> , a database of digitized historic American newspapers created and maintained by the National Digital Newspaper Program, itself a partnership between the Library of Congress and the National Endowment for the Humanities. Content in <em>Chronicling America</em> is contributed by state partners of the National Digital Newspaper Program who have applied for and received awards from the Division of Preservation and Access at the National Endowment for the Humanities <sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>. At the time of the construction of the <em>Newspaper Navigator</em> dataset in March, 2020, <em>Chronicling America</em> contained approximately 16.3 million digitized historic newspaper pages published between 1789 and 1963, covering 47 states as well as Washington, D.C. and Puerto Rico. The technical specifications of the National Digital Newspaper Program require that each digitized page in <em>Chronicling America</em> comprises the following digital artifacts <sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>:</p><p>A page image in two raster formats:<br>Grayscale, scanned for maximum resolution possible between 300-400 DPI, relative to the original material, uncompressed TIFF 6.0 Same image, compressed as JPEG2000</p><pre><code>Optical character recognition (OCR) text and associated bounding boxes for words (one file per page image)  PDF Image with Hidden Text, i.e., with text and image correlated  Structural metadata (a) to relate pages to title, date, and edition; (b) to sequence pages within issue or section; and (c) to identify associated image and OCR files  Technical metadata to support the functions of a trusted repository  
</code></pre><p>Additional artifacts and metadata are contributed for each digitized newspaper issue and microfilm reel. All digitized pages are in the public domain and are available online via a public search user interface,<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup> making <em>Chronicling America</em> an immensely rich resource for the American public.</p><p>The central goal of _Newspaper Navigator _ is to re-imagine how the American public explores <em>Chronicling America</em> by utilizing emerging machine learning techniques to extract, categorize, and search over the visual content and headlines in <em>Chronicling America</em> ’s 16.3 million pages of digitized historic newspapers. <em>Newspaper Navigator</em> was both inspired and directly enabled by the Library of Congress’s <em>Beyond Words</em> crowdsourcing initiative <sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup>. Launched by LC Labs in 2017, <em>Beyond Words</em> engages the American public by asking volunteers to identify and draw boxes around photographs, illustrations, maps, comics, and editorial cartoons on World War I-era pages in <em>Chronicling America</em> , note the visual content categories, and transcribe the relevant textual information such as titles and captions.<sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup> The thousands of annotations created by _Beyond Words _ volunteers are in the public domain and available for download online. _Newspaper Navigator _ directly builds on <em>Beyond Words</em> by utilizing these annotations, as well as additional annotations of headlines and advertisements, to train a machine learning model to detect visual content in historic newspapers.<sup id=fnref:7><a href=#fn:7 class=footnote-ref role=doc-noteref>7</a></sup> Because <em>Beyond Words</em> volunteers were asked to draw bounding boxes to include any relevant textual content, such as a photograph’s title, this machine learning model learns during training to include relevant textual content when predicting bounding boxes.<sup id=fnref:8><a href=#fn:8 class=footnote-ref role=doc-noteref>8</a></sup> Furthermore, in the _Transcribe _ step of <em>Beyond Words</em> , the system provided the OCR with each bounding box as an initial transcription for the volunteer to correct; inspired by this, the <em>Newspaper Navigator</em> pipeline automatedly extracts the OCR falling within each predicted bounding box in order to provide noisy textual metadata for each image. In the case of headlines, this method enables the headline text to be directly extracted from the bounding box predictions. Lastly, the pipeline generates image embeddings for the extracted visual content using an image classification model trained on ImageNet.<sup id=fnref:9><a href=#fn:9 class=footnote-ref role=doc-noteref>9</a></sup> A diagram of the full <em>Newspaper Navigator</em> pipeline can be found in Figure 1.</p><figure><img loading=lazy alt="A diagram of newspaper screenshots" src=/dhqwords/vol/15/4/000578/resources/images/image1.png sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/15/4/000578/resources/images/image1_hu029dda16cde63a400ee842ebc3c8ad63_247356_500x0_resize_box_3.png 500w,
/dhqwords/vol/15/4/000578/resources/images/image1_hu029dda16cde63a400ee842ebc3c8ad63_247356_800x0_resize_box_3.png 800w,/dhqwords/vol/15/4/000578/resources/images/image1.png 960w" class=landscape><figcaption><p>A diagram showing the <em>Newspaper Navigator</em> pipeline, which processed over 16.3 million historic newspaper pages in <em>Chronicling America</em> , resulting in the <em>Newspaper Navigator</em> dataset.</p></figcaption></figure><p>Over the course of 19 days from late March to early April of 2020, the <em>Newspaper Navigator</em> pipeline processed 16.3 million pages in <em>Chronicling America</em> ; the resulting _Newspaper Navigator _ dataset was publicly released in May, 2020. The full dataset, as well as all code written for this project, are available online and have been placed in the public domain for unrestricted re-use.<sup id=fnref:10><a href=#fn:10 class=footnote-ref role=doc-noteref>10</a></sup> Currently, the <em>Newspaper Navigator</em> dataset can be queried using HTTPS and Amazon S3 requests. Furthermore, hundreds of pre-packaged datasets have been made available for download, along with associated metadata. These pre-packaged datasets consist of different types of visual content for each year, from 1850 to 1963, allowing users to download, for example, all of the maps from 1863 or all of the photographs from 1910. For more information on the technical aspects of the pipeline and the construction of the <em>Newspaper Navigator</em> dataset, I refer the reader to <sup id=fnref:11><a href=#fn:11 class=footnote-ref role=doc-noteref>11</a></sup></p><h2 id=ii-why-a-data-archaeology>II. Why a Data Archaeology?</h2><p>As machine learning and artificial intelligence play increasing roles in digitization and digital content stewardship, the Libraries, Archives, and Museums (LAM) community has repeatedly emphasized the importance of ensuring that these emerging methodologies are incorporated ethically and responsibly. Indeed, a major theme that emerged from the “Machine Learning + Libraries Summit” hosted by LC Labs in September, 2019, was that “there is much more “human” in machine learning than the name conveys” and that transparency and communication are first steps toward addressing the “human subjectivities, biases, and distortions” embedded within machine learning systems <sup id=fnref:12><a href=#fn:12 class=footnote-ref role=doc-noteref>12</a></sup>. This data archaeology has been written in support of this call for transparency and responsible stewardship, which is echoed in the Library of Congress’s Digital Strategy, as well as the recommendations in Ryan Cordell’s report to the Library of Congress “ML + Libraries: A Report on the State of the Field,” Thomas Padilla’s OCLC position paper “Responsible Operations: Science, Machine Learning, and AI in Libraries” , and the University of Nebraska-Lincoln’s report on machine learning to the Library of Congress <sup id=fnref:13><a href=#fn:13 class=footnote-ref role=doc-noteref>13</a></sup>; <sup id=fnref:14><a href=#fn:14 class=footnote-ref role=doc-noteref>14</a></sup>; <sup id=fnref:15><a href=#fn:15 class=footnote-ref role=doc-noteref>15</a></sup>; <sup id=fnref:16><a href=#fn:16 class=footnote-ref role=doc-noteref>16</a></sup>. I write this data archaeology from my perspective of having created the dataset, and although I am not without my own biases, I have attempted to represent my work as honestly as possible. Accordingly, I seek not only to document the construction of the _Newspaper Navigator _ dataset through the lens of data stewardship but also to critically examine the dataset’s limitations. In doing so, I advocate for the importance of autoethnographic approaches to documenting a cultural heritage dataset’s construction from a humanistic perspective.</p><p>This article draws inspiration from recent works in media and data archaeology, including Paul Fyfe’s “An Archaeology of Victorian Newspapers” ; Bonnie Mak’s “Archaeology of a Digitization” ; Kate Crawford and Trevor Paglen’s “Excavating AI: The Politics of Images in Machine Learning Training Sets” ; and, most directly, Ryan Cordell’s “Qi-jtb the Raven: Taking Dirty OCR Seriously,” in which Cordell traces the digitization of a single issue of the <em>Lewisburg Chronicle</em> from its selection by the Pennsylvania Digital Newspaper Project to its ingestion into the _Chronicling America _ online database, with a focus on the distortive effects of OCR <sup id=fnref:17><a href=#fn:17 class=footnote-ref role=doc-noteref>17</a></sup>; <sup id=fnref:18><a href=#fn:18 class=footnote-ref role=doc-noteref>18</a></sup>; <sup id=fnref:19><a href=#fn:19 class=footnote-ref role=doc-noteref>19</a></sup>; <sup id=fnref:20><a href=#fn:20 class=footnote-ref role=doc-noteref>20</a></sup>. As argued by Trevor Owens and Thomas Padilla, it is essential to “document how digitization practices and how the affordances of particular sources … produce unevenness in the discoverability and usability of collections” <sup id=fnref:21><a href=#fn:21 class=footnote-ref role=doc-noteref>21</a></sup>. Recent works within the machine learning literature have analogously emphasized the importance of documenting the collection and curation efforts underpinning community datasets and machine learning models. Reporting mechanisms include “Datasheets for Datasets,” “Dataset Nutrition Labels,” “Data Statements for NLP,” “Model Cards for Model Reporting,” and “Algorithmic Impact Assessments” <sup id=fnref:22><a href=#fn:22 class=footnote-ref role=doc-noteref>22</a></sup>; <sup id=fnref:23><a href=#fn:23 class=footnote-ref role=doc-noteref>23</a></sup>; <sup id=fnref:24><a href=#fn:24 class=footnote-ref role=doc-noteref>24</a></sup>; <sup id=fnref:25><a href=#fn:25 class=footnote-ref role=doc-noteref>25</a></sup>; <sup id=fnref:26><a href=#fn:26 class=footnote-ref role=doc-noteref>26</a></sup>. This case study adopts a similar framing in stressing the importance of reporting mechanisms, with a particular focus on the data archaeology in the context of cultural heritage datasets.</p><p>In the following sections, I trace the digitization process and data flow for <em>Newspaper Navigator</em> , beginning with the physical artifact of the newspaper itself and ending with the machine learning predictions that constitute the _Newspaper Navigator _ dataset, reflecting on each step through the lens of discoverability and erasure. In particular, I study four different _Chronicling America _ Black newspaper pages published in 1910, each depicting the same photograph of W.E.B. Du Bois, as the pages move through the <em>Chronicling America</em> and <em>Newspaper Navigator</em> pipelines. All four pages reproduce the same article by Franklin F. Johnson, a reporter from _The Baltimore Afro-American _ <sup id=fnref:27><a href=#fn:27 class=footnote-ref role=doc-noteref>27</a></sup>; the headline is as follows:</p><p>NEW MOVEMENT</p><p>BEGINS WORK</p><p>Plan and Scope of the Asso-</p><p>ciation Briefly Told.</p><p>Will Publish the Crisis.</p><p>Review of Causes Which Led to the</p><p>Organization of the Association in</p><p>New York and What Its Policy Will</p><p>Be-Career and Work of Professor</p><p>W.E.B. Du Bois</p><p>The article describes the creation of the National Association for the Advancement of Colored People (NAACP), details W.E.B. Du Bois’s background, and announces the launch of <em>The Crisis</em> , the official magazine of the NAACP, with Du Bois as Editor-in-Chief. The four pages comprise the front page of the October 14th, 1910, issue of the _Iowa State Bystander _ <sup id=fnref:28><a href=#fn:28 class=footnote-ref role=doc-noteref>28</a></sup>; the 16th page of the October 15th, 1910, issue of _Franklin’s Paper the Statesman _ <sup id=fnref:29><a href=#fn:29 class=footnote-ref role=doc-noteref>29</a></sup>; and the 2nd and 3rd pages of the October 15th, 1910, and November 26th, 1910, issues of <em>The Broad Ax</em> , respectively <sup id=fnref:30><a href=#fn:30 class=footnote-ref role=doc-noteref>30</a></sup>; <sup id=fnref:31><a href=#fn:31 class=footnote-ref role=doc-noteref>31</a></sup>. All four digitized pages are reproduced in the Appendix.</p><h2 id=iii--_chronicling-america_--a-genealogy-of-collecting-microfilming-and-digitizing>III. <em>Chronicling America</em> : A Genealogy of Collecting, Microfilming, and Digitizing</h2><p>Any examination of _Newspaper Navigator _ must begin with the genealogy of collecting, microfilming, and digitizing that dictates which newspapers have been ingested into the _Chronicling America _ database. The question of what to digitize is, in practice, answered and realized incrementally over decades, beginning at its most fundamental level with the question of which newspapers have survived and which have been reduced to lacunae in the historical record <sup id=fnref:32><a href=#fn:32 class=footnote-ref role=doc-noteref>32</a></sup>. <sup id=fnref:33><a href=#fn:33 class=footnote-ref role=doc-noteref>33</a></sup> Historic newspapers present challenges for digitization in part due to the ephemerality of the physical printed newspaper itself: many newspapers were microfilmed and immediately discarded due to a fear that the physical pages would deteriorate.<sup id=fnref:34><a href=#fn:34 class=footnote-ref role=doc-noteref>34</a></sup> Indeed, almost all of the pages included in <em>Chronicling America</em> have been digitized directly from microfilm. In the next section, I will examine the microfilm imaging process in more detail; however, in most cases, librarians selected newspapers for collecting and microfilming decades before the National Digital Newspaper Program was launched in 2004. These selections were informed by a range of factors including historical significance - itself a subjective, nebulous, and ever-evolving notion that has historically served as the basis for perpetuating oppression within the historical record. In “Chronicling White America,” Benjamin Fagan highlights the paucity of Black newspapers in <em>Chronicling America</em> , in particular in relation to pre-Civil War era newspapers [Fagan 2016]. It is imperative to remember that this paucity can directly be traced back decades to the collecting and preserving stages.<sup id=fnref:35><a href=#fn:35 class=footnote-ref role=doc-noteref>35</a></sup></p><p>In regard to collecting, the newspaper page is both an informational object (i.e., the newspaper page as defined by its content) and a material object (i.e., the specific printed copy of the newspaper page) <sup id=fnref:36><a href=#fn:36 class=footnote-ref role=doc-noteref>36</a></sup>. At some point in time, librarians accessioned a specific copy of each printed page and microfilmed it or contracted out the microfilming. The materiality of that specific printed page is a confluence of unique ink smudges, rips, creases, and page alignment, much of which is captured in the microfilm imaging process. Though we may not make much of a crease or a smudge on a digitized page when we find it in the <em>Chronicling America</em> database, it can very well take on a life of its own with a machine learning algorithm in <em>Newspaper Navigator</em> . The machine learning algorithm might deem two newspaper photographs as similar simply due to the presence of creases or smudges, even if the photographs are easily discernible to the naked eye, or the smudges are of entirely different origin (i.e., a printing imperfection versus a smudge from a dirty hand).</p><p>It is only by foregrounding these subtleties of the collection, preservation, and microfilming processes that we can understand the selection process for <em>Chronicling America</em> in its proper context. The grant-seeking process dictates selection criteria for _Chronicling America _ by which state-level institutions including state libraries, historical societies, and universities apply for two years of grant funding from the National Digital Newspaper Program via the Division of Preservation and Access at the National Endowment for the Humanities. With the awarding of a grant, a state-level awardee then digitizes approximately 100,000 newspaper pages published in their state for inclusion in <em>Chronicling America</em> <sup id=fnref1:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>; <sup id=fnref:37><a href=#fn:37 class=footnote-ref role=doc-noteref>37</a></sup>. The grant-seeking and awarding process is nuanced, but salient points include that state-level applicants must assemble an advisory board including scholars, teachers, librarians, and archivists to aid in the selection of newspapers, and grants are reviewed by National Endowment for the Humanities staff, as well as peer reviewers.<sup id=fnref:38><a href=#fn:38 class=footnote-ref role=doc-noteref>38</a></sup></p><p>Regarding selection criteria for newspaper titles, the National Digital Newspaper Program defines the following factors for state-level awardees to consider for content selection after a newspaper is determined to be in the public domain <sup id=fnref:39><a href=#fn:39 class=footnote-ref role=doc-noteref>39</a></sup>:</p><ul><li>image quality in the selection of microfilm</li><li>research value</li><li>geographic representation</li><li>temporal coverage</li><li>bibliographic completeness of microfilm copy</li><li>diversity (i.e., “newspaper titles that document a significant minority community at the state or regional level”)</li><li>whether the title is orphaned (i.e., whether the newspaper has “ceased publication and lack[s] active ownership” [Chronicling America no date])</li><li>whether the title has already been digitized.</li></ul><p>Though factors such as research value are considered by each state awardee’s advisory board, as well as by the National Endowment for the Humanities and peer review experts, the titles included in <em>Chronicling America</em> are largely dictated by which exist on microfilm and are of sufficient image quality within a state-level grantee’s collection. Thus, the significance of the collection and microfilming practices of decades prior cannot be understated.</p><p>I also highlight that assessing microfilmed titles based on image quality is a complex procedure in its own right. The National Digital Newspaper Program has made publicly available a number of resources devoted specifically to this task, including documents and video tutorials <sup id=fnref:40><a href=#fn:40 class=footnote-ref role=doc-noteref>40</a></sup>; <sup id=fnref:41><a href=#fn:41 class=footnote-ref role=doc-noteref>41</a></sup>. They articulate factors such as the microfilm generation (archive master, print master, or review copy), the material (polyester or acetate), the reduction ratio, and the physical condition. The detailed resources made available by the National Digital Newspaper Program, the Library of Congress, and the National Endowment for the Humanities for navigating this process are testaments to the multidimensional complexity of the selection process for _Chronicling America _ <sup id=fnref:42><a href=#fn:42 class=footnote-ref role=doc-noteref>42</a></sup>; <sup id=fnref1:39><a href=#fn:39 class=footnote-ref role=doc-noteref>39</a></sup>; <sup id=fnref1:37><a href=#fn:37 class=footnote-ref role=doc-noteref>37</a></sup>.</p><p>We have not yet investigated the topic of digitization, and we have already encountered a profusion of factors from collection to digitization that mediate which artifacts appear in <em>Chronicling America</em> and thus <em>Newspaper Navigator</em> . Let us now examine the microfilm itself.</p><h2 id=iv-the-microfilm>IV. The Microfilm</h2><p>In “What Computational Archival Science Can Learn from Art History and Material Culture Studies,” Lyneise Williams shares a powerful anecdote of coming across a physical copy of a 1927 issue of the French sports newspaper <em>Match L’Intran</em> that featured accomplished Black Panamanian boxer, Alfonso Teofilo Brown, on the front cover <sup id=fnref:43><a href=#fn:43 class=footnote-ref role=doc-noteref>43</a></sup>. Williams describes Brown as “glowing. He looked like a 1920s film star rather than a boxer” <sup id=fnref1:43><a href=#fn:43 class=footnote-ref role=doc-noteref>43</a></sup>. Curious to learn more about the printing process, Williams discovered that the issue of <em>Match L’Intran</em> was produced using rotogravure, a specific printing process that could “capture details in dark tones” <sup id=fnref2:43><a href=#fn:43 class=footnote-ref role=doc-noteref>43</a></sup>. However, when Williams found a version of the same newspaper cover that had been digitized from microfilm, it was apparent that the microfilming process had washed out the detail of the rotogravure, reducing Brown to a “flat black, cartoonish form” <sup id=fnref3:43><a href=#fn:43 class=footnote-ref role=doc-noteref>43</a></sup>. Williams relays the anecdote to articulate that the microfilming process itself is thus a form of erasure for communities of color <sup id=fnref4:43><a href=#fn:43 class=footnote-ref role=doc-noteref>43</a></sup>.</p><p>The grayscale saturation of photographs induced by microfilming is widely documented and recognizable to most researchers who have ever worked with the medium <sup id=fnref:44><a href=#fn:44 class=footnote-ref role=doc-noteref>44</a></sup>; however, Lyneise Williams’s article affords us a lens into what precisely is lost amongst the distortive effects of the microfilming process. This erasure via microfilming can be seen in _Chronicling America _ directly. In Figure 2, I show the same photograph of W.E.B. Du Bois as it appears in 4 different <em>Chronicling America</em> newspaper pages published during October and November of 1910 and digitized from microfilm <sup id=fnref1:28><a href=#fn:28 class=footnote-ref role=doc-noteref>28</a></sup>; <sup id=fnref1:29><a href=#fn:29 class=footnote-ref role=doc-noteref>29</a></sup>; <sup id=fnref1:30><a href=#fn:30 class=footnote-ref role=doc-noteref>30</a></sup>; <sup id=fnref1:31><a href=#fn:31 class=footnote-ref role=doc-noteref>31</a></sup>. The phenomenon described by Williams is immediately recognizable in these four images: Du Bois’s facial features are distorted by the grayscale saturation. In the case of the <em>Iowa State Bystander</em> , Du Bois has been rendered into a silhouette.</p><p>Moreover, each digitized reproduction reveals unique visual qualities, varying in contrast, sharpness, and noise - a testament to the confluence of mediating conditions from printing through digitization that have rendered each newspaper photograph in digital form. Even in the case of the two images reproduced in the <em>The Broad Ax</em> , which were digitized from the very same microfilm reel (reel #00280761059) by the University of Illinois at Urbana-Champaign Library, variations are still apparent. To understand how these subtle differences between images are amplified through digitization, we now turn to optical character recognition.</p><figure><img loading=lazy alt="Images of W.E.B. Du Bois" src=/dhqwords/vol/15/4/000578/resources/images/image2.jpg sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/15/4/000578/resources/images/image2_hucec43a56b45cae513ff69f00ca438498_51528_500x0_resize_q75_box.jpg 500w,
/dhqwords/vol/15/4/000578/resources/images/image2_hucec43a56b45cae513ff69f00ca438498_51528_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/15/4/000578/resources/images/image2.jpg 960w" class=landscape><figcaption><p>The same image of W.E.B. Du Bois reproduced in 4 different digitized Black newspapers in <em>Chronicling America</em> from 1910. Note that the combined effects of printing, microfilming, and digitizing have led to different visual effects in each image, ranging from contrast to sharpness.</p></figcaption></figure><h2 id=v-ocr>V. OCR</h2><p>Optical character recognition, commonly called OCR, refers to machine learning algorithms that are trained to read images of typewritten text and output machine-readable text, thereby providing the bridge between an image of typewritten text and the transcribed text itself. Because OCR algorithms are “trained and evaluated using labeled data: examples with ground-truth classification labels that have been assigned by another means,” the algorithms are considered a form of supervised learning in the machine learning literature <sup id=fnref:45><a href=#fn:45 class=footnote-ref role=doc-noteref>45</a></sup>. OCR engines are remarkably powerful in their ability to improve access to historic texts. Indeed, OCR is a crucial form of metadata for <em>Chronicling America</em> , enabling keyword search in the search portal and making possible scholarship with the newspaper text at large scales.<sup id=fnref:46><a href=#fn:46 class=footnote-ref role=doc-noteref>46</a></sup> However, OCR is not perfect. Although humans are able to discern an E from an R on a digitized page even if the type has been smudged, an OCR engine is not always able to do so: its performance is dependent on factors ranging from the sharpness of text in an image to printing imperfections to the specific typography on the page.</p><p>In Figure 3, I show the same four images shown in Figure 2, along with OCR transcriptions of the captions provided by <em>Chronicling America</em> . All four transcriptions fail to reproduce the true caption with 100% accuracy, differing from one another by at least one character. Consequently, a keyword search of W. E. B. Du Bois over the raw text would not register the caption for any of the four photographs (the <em>Chronicling America</em> search portal utilizes a form of relevance search to alleviate this problem). These examples reveal how sensitive OCR engines are to slight perturbations, or “noise,” in the digitized images, from ink smudges to text sharpness to page contrast. Though the NDNP awardees who contributed these pages may have utilized different OCR engines or chosen different OCR settings, the OCR for the two image captions from _The Broad Ax _ that have been digitized from the very same microfilm reel was in all likelihood generated using the same OCR engine and settings. Put succinctly, OCR engines amplify the noise from both the material page and the digitization pipeline.<sup id=fnref:47><a href=#fn:47 class=footnote-ref role=doc-noteref>47</a></sup></p><figure><img loading=lazy alt="four images of W.E.B. Du Bois" src=/dhqwords/vol/15/4/000578/resources/images/image3.jpg sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/15/4/000578/resources/images/image3_hucec43a56b45cae513ff69f00ca438498_55732_500x0_resize_q75_box.jpg 500w,
/dhqwords/vol/15/4/000578/resources/images/image3_hucec43a56b45cae513ff69f00ca438498_55732_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/15/4/000578/resources/images/image3.jpg 960w" class=landscape><figcaption><p>The OCR transcriptions of the caption “W. E. B. DU BOIS, PH. D.” appearing in the image of W.E.B. Du Bois reproduced in 4 different digitized Black newspapers in <em>Chronicling America</em> . These OCR transcriptions are provided by <em>Chronicling America</em> .</p></figcaption></figure><p>Though OCR engines have become standard components of digitization pipelines, it is important to remember that OCR engines are themselves machine learning models that have been trained on sets of transcribed typewritten pages. Like any machine learning model, OCR predictions are thus subject to biases encoded not only in the OCR engine’s architecture but also in the training data itself. Though it is often called algorithmic bias, this bias is undeniably human, in that the construction of training data machine learning models are imprinted with countless human decisions and judgment calls. For example, if an OCR engine is trained on transcriptions that consistently misspell a word, the OCR engine will amplify this misspelling across all transcriptions of processed pages.<sup id=fnref:48><a href=#fn:48 class=footnote-ref role=doc-noteref>48</a></sup> A recurring theme of algorithmic bias is that it is a force for marginalization, especially in the context of how we navigate information digitally. In <em>Algorithms of Oppression</em> , Safiya Noble describes how Google’s search engine consistently marginalizes women and people of color by displaying search results that reinforce racism <sup id=fnref:49><a href=#fn:49 class=footnote-ref role=doc-noteref>49</a></sup>. This bias is not restricted to Google: in <em>Masked by Trust: Bias in Library Discovery</em> , Matthew Reidsma articulates how library search engines suffer from similar biases <sup id=fnref:50><a href=#fn:50 class=footnote-ref role=doc-noteref>50</a></sup>. Despite the fact that knowledge of algorithmic bias in relation to search engines and image recognition tools is becoming increasingly widespread among the cultural heritage community, the errors introduced by OCR engines are often accepted as inevitable without critical inquiry from this perspective. However, algorithmic bias is a useful framework for examining OCR engines <sup id=fnref:51><a href=#fn:51 class=footnote-ref role=doc-noteref>51</a></sup>.</p><p>Perhaps the most significant challenge to studying OCR engines is that the best-performing and most widely-used OCR engines are proprietary. Though ABBYY FineReader and Google Cloud Vision API offer high performance, the systems fundamentally are black boxes: we have no access to the underlying algorithms or the training data. The ability to audit a system is crucial to developing an understanding of how it works and the biases it encodes. The fact that many OCR engines are opaque prevents us from disentangling whether poor performance on a particular page is due to algorithmic limitations or due to a lack of relevant training data. The distinction is significant: the former may reflect an algorithmic upper bound, whereas the latter reflects decisions made by humans.</p><p>Indeed, algorithmic bias distorts and occludes the historical record, as it is made discoverable through OCR. Discrepancies in OCR performance for different languages and scripts is a consequence of human prioritization, from the collection of training data and lexicons to the development of the algorithms themselves. As articulated by Hannah Alpert-Abrams in “Machine Reading the <em>Primeros Libros</em> ,” “the machine-recognition of printed characters is a historically charged event, in which the system and its data conspire to embed cultural biases in the output, or to affix them as supplementary information hidden behind the screen” <sup id=fnref1:51><a href=#fn:51 class=footnote-ref role=doc-noteref>51</a></sup>. Alpert-Abrams’s work reveals how the OCR inaccuracies for indigenous languages recorded in colonial scripts perpetuate colonialism. For other languages such as Ladino, typically typeset in Rashi script, the lack of high-performing OCR has presented consistent challenges for digitization and scholarship.</p><p>In the case of <em>Chronicling America</em> , the National Digital Newspaper Program is exemplary in its efforts to support OCR for non-English languages. In the Notice of Funding Opportunity for the National Digital Newspaper Program produced by the Division of Preservation of Access at the National Endowment for the Humanities, OCR performance in different languages is explicitly addressed: “Applicants proposing to digitize titles in languages other than English must include staff with the relevant language expertise to review the quality of the converted content and related metadata” <sup id=fnref2:37><a href=#fn:37 class=footnote-ref role=doc-noteref>37</a></sup>. I have included this discussion of OCR and algorithmic bias to offer a broader provocation regarding machine learning and digitization: how much text in digitized sources has been transmuted by this effect and thus effectively erased due to inaccessibility when using search and discovery platforms?</p><h2 id=vi-the-visual-content-recognition-model>VI. The Visual Content Recognition Model</h2><p>I will now turn to the <em>Newspaper Navigator</em> pipeline itself, in particular the visual content recognition model. Trained on annotations from the _Beyond Words _ crowdsourcing initiative, as well as additional annotations of headlines and advertisements, the visual content recognition model detects photographs, illustrations, maps, comics, editorial cartoons, headlines, and advertisements on historic newspaper pages.</p><p>As described in the previous section, examining training data is an essential component of auditing any machine learning model, from understanding how the dataset was constructed to uncovering any biases in the composition of the dataset itself. For the visual content recognition model, this examination begins with <em>Beyond Words</em> . Launched in 2017 by LC Labs, _Beyond Words _ has collected to-date over 10,000 verified annotations of visual content in World War 1-era newspaper pages from <em>Chronicling America</em> . The _Beyond Words _ workflow consists of the three steps listed below:</p><p>A Mark step, in which volunteers are asked to draw bounding boxes around visual content on the page <sup id=fnref:52><a href=#fn:52 class=footnote-ref role=doc-noteref>52</a></sup>. The instructions read as follows:</p><blockquote><p>In the Mark step, your task is to identify and select pictures in newspaper pages. For our project, pictures means illustrations, photographs, comics, and cartoons. You&rsquo;ll use the marking tool to draw a box around the picture using your mouse. After you have marked all pictures on the newspaper page, click the ‘DONE’ button. Skip the page altogether by clicking the Skip this page button. If no illustrations, photographs, or cartoons appear on the page, click the DONE button. Not sure if a picture should be marked? Select the Done for now, more left to mark button so another volunteer can help finish that page. Please do not select pictures within advertisements.</p></blockquote><p>A Transcribe step, in which volunteers are asked to transcribe the caption of the highlighted visual content, as well as note the artist and visual content category (Photograph, Illustration, Map, Comics/Cartoon, Editorial Cartoon) <sup id=fnref:53><a href=#fn:53 class=footnote-ref role=doc-noteref>53</a></sup>. The transcription is pre-populated with the OCR falling within the bounding box in question. The instructions for this step state:</p><blockquote><p>Most pictures have captions or descriptions. Enter the text exactly as you see it. Include capitalization and punctuation, but remove hyphenation that breaks words at the end of the line. Use new lines to separate different parts of captions and descriptions. You can zoom in for better looks at the page. You can also select View the original page in the upper right corner of the screen to view the original high resolution image of the newspaper.</p></blockquote><p>An example of this step can be seen in Figure 4.</p><p>A Verify step, in which volunteers are asked to select the best caption for an identified region of visual content from at least two examples; alternatively, a volunteer can add another caption <sup id=fnref:54><a href=#fn:54 class=footnote-ref role=doc-noteref>54</a></sup>. The instructions state:</p><blockquote><p>Choose the transcription that most accurately captures the text as written. If multiple transcriptions appear valid, choose the first one. If the selected region isn&rsquo;t appropriate for the prompt, click Bad region.</p></blockquote><figure><img loading=lazy alt="screenshot of a newspaper article and a text box where the article has been transcribed" src=/dhqwords/vol/15/4/000578/resources/images/image4.png sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/15/4/000578/resources/images/image4_huac20f953a7605e7c7f1c5edc8f444540_1116229_500x0_resize_box_3.png 500w,
/dhqwords/vol/15/4/000578/resources/images/image4_huac20f953a7605e7c7f1c5edc8f444540_1116229_800x0_resize_box_3.png 800w,/dhqwords/vol/15/4/000578/resources/images/image4_huac20f953a7605e7c7f1c5edc8f444540_1116229_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/4/000578/resources/images/image4_huac20f953a7605e7c7f1c5edc8f444540_1116229_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/4/000578/resources/images/image4.png 1584w" class=landscape><figcaption><p>A screenshot showing an example of the Transcribe step of the _Beyond Words _ workflow. Note that the photograph caption is pre-populated using the OCR falling within the bounding box [^lclabs2017b].</p></figcaption></figure><p>For the purposes of <em>Newspaper Navigator</em> , only the bounding boxes from the Mark step and the category labels from the Transcribe step were utilized as training data; however, understanding the full workflow is essential because annotations are considered verified only if they have passed through the full workflow.</p><p>A number of factors contribute to which _Chronicling America _ pages were processed by volunteers in <em>Beyond Words</em> . First, the temporal restriction to World War 1-era pages affects the ability of the visual content recognition model to generalize: after all, if the model is trained on World War 1-era pages, how well should we expect it to perform on 19th century pages? I will return to this question later in the section. Moreover, _Beyond Words _ volunteers could select either an entirely random page or a random page from a specific state, an important affordance from an engagement perspective, as volunteers could explore the local histories of states in which they are interested. But this affordance is also imprinted on the training data, as certain states - and thus, certain newspapers - appear at a higher frequency than if the World War-1 era <em>Chronicling America</em> pages had been drawn randomly from this temporal range in <em>Chronicling America</em> .</p><p>Furthermore, it should be noted that the Mark and Transcribe steps - specifically, drawing bounding boxes and labeling the visual content category - are complex tasks. Because newspaper pages are remarkably heterogenous, ambiguities and edge-cases abound. Should a photo collage be marked as one unit or segmented into constituent parts? What precisely is the distinction between an editorial cartoon and an illustration? How much relevant textual content should be included in a bounding box? Naturally, volunteers did not always agree on these choices. In this regard, the notion of a ground-truth, a set of perfect annotations against which we can assess performance, is itself called into question. Moreover, with thousands of annotations, mistakes in the form of missed visual content, as well as misclassifications, are inevitable.<sup id=fnref:55><a href=#fn:55 class=footnote-ref role=doc-noteref>55</a></sup> These ambiguities and errors are natural components of <em>any</em> training dataset and must be taken into account when analyzing a machine learning model’s predictions.</p><p>A breakdown of <em>Beyond Words</em> annotations included in the training data can be found in the second column of Table 1. I downloaded these 6,732 publicly-accessible annotations as a JSON file on December 1, 2019. Table 1 reveals an imbalance between the number of examples for each category; in the language of machine learning, this is called <em>class imbalance</em> . While the discrepancy between maps and photographs is to be expected, the fact that so few maps were included was concerning from a machine learning standpoint: a machine learning algorithm’s ability to generalize to new data is dependent on having many diverse training examples. To address this concern, I searched _Chronicling America _ and identified 134 pages published between January 1st, 1914, and December 31st, 1918, that contain maps. I then annotated these pages myself.</p><p>In addition, during the development of the <em>Newspaper Navigator</em> pipeline, I realized the value in training the visual content recognition model to identify headlines and advertisements. Consequently, I added annotations of headlines and advertisements for all 3,559 pages included in the training data. The statistics for this augmented set of annotations can be found in the third column of Table 1. Though I attempted to use a consistent approach to annotating the headlines and advertisements, my interpretation of what constitutes a headline is certainly not unimpeachable: I am not a trained scholar of periodicals or of print culture; even if I were, the task itself is inevitably subjective. Furthermore, I made decisions to annotate large grids of classified ads as a single ad to expedite the annotation process. Whether this was a correct judgment call can be debated. Lastly, annotating all 3,559 pages for headlines and advertisements required a significant amount of time, and there are inevitably mistakes and inconsistencies embedded within the annotations. My own decisions in terms of how to annotate, as well as my mistakes and inconsistencies, are embedded within the visual content recognition model through training. For those interested in examining the training data directly, the data can be found in the GitHub repository for this project <sup id=fnref:56><a href=#fn:56 class=footnote-ref role=doc-noteref>56</a></sup>.<br>A breakdown of _Beyond Words _ annotations included in the training data for the visual content recognition model, as well as all annotations constituting the training data. Category Beyond Words Annotations Total Annotations Photograph 4,193 4,254 Illustration 1,028 1,048 Map 79 215 Comic/Cartoon 1,139 1,150 Editorial Cartoon 293 293 Headline - 27,868 Advertisement - 13,581 <em>Total</em> 6,732 48,409<br>Beyond the construction of the training data, I made manifold decisions regarding the selection of the correct model architecture and the training of the model. Because this discussion surrounding these choices is quite technical, I refer the reader to <sup id=fnref1:11><a href=#fn:11 class=footnote-ref role=doc-noteref>11</a></sup> for an in-depth examination. However, I will state that the choice of model, the number of iterations for which the model was trained, and the choice of model parameters are all of significant import for the resulting trained model and consequently, the _Newspaper Navigator _ dataset.</p><p>I will now turn to the visual content recognition model’s outputs in relation to the _Newspaper Navigator _ pipeline. The model itself consumes a lower-resolution version of a _Chronicling America _ page as input and then outputs a JSON file containing predictions, each of which consists of bounding box coordinates,<sup id=fnref:57><a href=#fn:57 class=footnote-ref role=doc-noteref>57</a></sup> the predicted class (i.e., photograph, map, etc.), and a confidence score generated by the machine learning model.<sup id=fnref:58><a href=#fn:58 class=footnote-ref role=doc-noteref>58</a></sup> Cropping out and saving the visual content required extra code to be written. Because the high-resolution images of the <em>Chronicling America</em> pages, in addition to the METS/ALTO OCR, amount to many tens of terabytes of data, questions of data storage became major considerations in the pipeline. I chose to save the extracted visual content as lower-resolution JPEG images in order to reduce the upload time and lessen the storage burden. Though the <em>Newspaper Navigator</em> dataset retains identifiers to all high-resolution pages in _Chronicling America, _ the images in the _Newspaper Navigator _ dataset are altered by the downsampling procedure. This downsampling procedure should be free of any significant biasing effects.</p><p>For visual content recognition, “Newspaper Navigator” utilized an object detection model, which is a type of widely-used computer vision technique for identifying objects in images. The performance for computer vision techniques is regularly measured using metrics such as average precision. For “Newspaper Navigator” , the model’s performance on a specific page, as measured by average precision, is dependent on a confluence of factors. These factors include the page’s layout, artifacts and distortions introduced in the microfilming and digitization process, and - most importantly - the composition of the training data. Thus, each image is seen differently by the visual content recognition model. In Figure 5, I show the four images of W.E.B. Du Bois, as identified by the visual content recognition model and saved in the _Newspaper Navigator _ dataset. Each image is cropped slightly differently. In the case of the image from the <em>Iowa State Bystander</em> , extra text is included, while in the case of the images from <em>The Broad Ax</em> , the captions are partially cut off. The loss in image quality is due to the aforementioned downsampling performed by the pipeline. This downsampling leads to artifacts such as the dots appearing on Du Bois’s face in the image from the <em>Iowa State Bystander</em> , as well as the streaks in the image from <em>Franklin’s Paper the Statesman</em> , that are not present in Figure 2.</p><p>Returning to the question of the visual content recognition model’s performance on pages published outside of the temporal range of the training data (1914-1918), it is possible to provide a quantitative answer by measuring average precision on test sets of annotated pages from different periods of time. In <sup id=fnref2:11><a href=#fn:11 class=footnote-ref role=doc-noteref>11</a></sup>, I describe this analysis in detail and demonstrate that the performance declines for pages published between 1875 and 1900 and further declines for pages published between 1850 and 1875. This confirms that the composition of the training data directly manifests in the model’s performance. While it is certainly the case that the <em>Newspaper Navigator</em> dataset can still be used for scholarship related to 19th century newspapers in <em>Chronicling America</em> , any scholarship with the 19th century visual content in the _Newspaper Navigator _ dataset must consider how the dataset may skew what visual content is represented.</p><figure><img loading=lazy alt="four images of W.E.B. Du Bois" src=/dhqwords/vol/15/4/000578/resources/images/image5.jpg sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/15/4/000578/resources/images/image5_hucec43a56b45cae513ff69f00ca438498_53115_500x0_resize_q75_box.jpg 500w,
/dhqwords/vol/15/4/000578/resources/images/image5_hucec43a56b45cae513ff69f00ca438498_53115_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/15/4/000578/resources/images/image5.jpg 960w" class=landscape><figcaption><p>The four images of W.E.B. Du Bois, as identified by the visual content recognition model and included in the _Newspaper Navigator _ dataset [^navigator1910a]; [^navigator1910c]; [^navigator1910e]; [^navigator1910g].</p></figcaption></figure><p>Let me conclude this section with a discussion of the act of visual content extraction itself in relation to digitization. While this extraction enables a wide range of affordances for searching <em>Chronicling America</em> , it is also an act of decontextualization: visual content no longer appears in relation to the <em>mise-en-page</em> . In the Appendix, the full pages containing the photographs of W.E.B. Du Bois are reproduced, showing each photograph in context. Only by examining the full pages does it become clear that the article featuring W.E.B. Du Bois was printed with a second article in the <em>Iowa State Bystander</em> and <em>The Broad Ax</em> , the headline of which reads: “ANTI-LYNCHING SOCIETY ORGANIZED IN BOSTON — Afro-American Women Unite For Active Campaign Against Injustice.” Furthermore, upon examination, the _Iowa State Bystander _ front page features the article on <em>The Crisis</em> and W.E.B. Du Bois as the most prominent article of the issue. Though links between the extracted visual content and the original <em>Chronicling America</em> pages are always retained, this decontextualization inevitably transmutes <em>how</em> we perceive and interact with the visual content in <em>Chronicling America</em> . Indeed, all uses of machine learning for metadata enhancement are a form of decontextualization, centering the user’s discovery and analysis of content around the metadata itself.</p><h2 id=vii-prediction-uncertainty>VII. Prediction Uncertainty</h2><p>Perhaps the most fundamental question to ask of the _Newspaper Navigator _ dataset is: How many photographs does the dataset contain? Because the dataset has been constructed using a machine learning model, predictions are ultimately probabilistic in nature, quantified by the confidence score returned by the model. This begs the question of what counts as an identified unit of visual content: a user is much more inclined to tally a prediction of a map if it has an associated confidence score of 99% rather than 1%. However, choosing this cut is fundamentally a subjective decision, informed by the user’s end goals with the dataset. In the language of machine learning, picking a stringent confidence cut (i.e., only counting predictions with high confidence scores) emphasizes <em>precision</em> : a prediction of a photograph likely corresponds to a true photograph, but the predictions will suffer from false negatives. Conversely, picking a loose confidence cut (i.e., counting predictions with low confidence scores) emphasizes <em>recall</em> : most true photographs are identified as such, but the predictions will suffer from many false positives. In this regard, the total number of images in the _Newspaper Navigator _ dataset is dependent on one’s desired tradeoff between precision and recall. In Table 2, I show the dynamic range of the dataset size, as induced by three different cuts on confidence score: 90%, 70%, and 50%. Figure 6 shows the effects of different cuts on confidence score for the page featuring W.E.B. Du Bois in the November 26,1910, issue of <em>The Broad Ax</em> .<br>The number of occurrences of each category of visual content in the <em>Newspaper Navigator</em> dataset with confidence scores above the listed thresholds (0.9, 0.7, 0.5). Category ≥ 90% ≥ 70% ≥ 50% Photograph 1.59 x 106 2.63 x 106 3.29 x 106 Illustration 8.15 x 105 2.52 x 106 4.36 x 106 Map 2.07 x 105 4.59 x 105 7.54 x 105 Comic/Cartoon 5.35 x 105 1.23 x 106 2.06 x 106 Editorial Cartoon 2.09 x 105 6.67 x 105 1.27 x 106 Headline 3.44 x 107 5.37 x 107 6.95 x 107 Advertisement 6.42 x 107 9.48 x 107 1.17 x 108 <em>Total</em> 1.02 x 108 1.56 x 108 1.98 x 108<br><figure><img loading=lazy alt="screenshot of four newspaper pages" src=/dhqwords/vol/15/4/000578/resources/images/Figure_6.jpg sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/15/4/000578/resources/images/Figure_6_hub72a7f3cb3fab45d8318dc0ebca11125_612503_500x0_resize_q75_box.jpg 500w,
/dhqwords/vol/15/4/000578/resources/images/Figure_6_hub72a7f3cb3fab45d8318dc0ebca11125_612503_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/15/4/000578/resources/images/Figure_6.jpg 960w" class=portrait><figcaption><p>The same page of _The Broad Ax _ from November 26, 1910, along with predictions from the visual content recognition model, thresholded on confidence score at 5%, 50%, 70%, and 90% [^navigator1910g]; [^navigator1910h]. Note that red corresponds to a prediction of photograph, cyan corresponds to a prediction of headline, and blue corresponds to a prediction of advertisement.</p></figcaption></figure></p><p>Rather than pre-selecting a confidence score threshold, the _Newspaper Navigator _ dataset contains all predictions with confidence scores greater than 5%,<sup id=fnref:59><a href=#fn:59 class=footnote-ref role=doc-noteref>59</a></sup> allowing the user to define their own confidence cut when querying the dataset. However, the website for the <em>Newspaper Navigator</em> dataset also includes hundreds of pre-packaged datasets in order to make it easier for users to work with the dataset. In particular, users can download zip files containing all of the visual content of a specific type with confidence scores greater than or equal to 90%, for any year from 1850 to 1963. I made this choice of 90% as the threshold cut for these pre-packaged datasets based on heuristic evidence from inspecting sample pre-packaged datasets by eye. However, as articulated above, based on different use cases, this cut of 90% may be too restrictive or permissive: relevant visual content may be absent from the pre-packaged dataset or lost in a sea of other examples. In Figure 7, I show the visual content recognition model’s confidence scores for the four images of W.E.B. Du Bois described throughout this data archaeology. The effect of a cut on confidence score can be seen here: selecting a cut of 95% would exclude the image from “Franklin’s Paper the Statesman” . I raise this point to emphasize that even this seemingly innocuous choice of 90% for the pre-packaged datasets alters the discovery process and thus can have an impact on scholarship.</p><figure><img loading=lazy alt="Four images of W.E.B. Du Bois" src=/dhqwords/vol/15/4/000578/resources/images/image8.jpg sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/15/4/000578/resources/images/image8_hucec43a56b45cae513ff69f00ca438498_55279_500x0_resize_q75_box.jpg 500w,
/dhqwords/vol/15/4/000578/resources/images/image8_hucec43a56b45cae513ff69f00ca438498_55279_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/15/4/000578/resources/images/image8.jpg 960w" class=landscape><figcaption><p>The visual content recognition model’s confidence score for each of the four images of W.E.B. Du Bois. Note how the model assigns a different confidence score to each identified image [^navigator1910b]; [^navigator1910d]; [^navigator1910f]; [^navigator1910h].</p></figcaption></figure><p>Just as the bounding box predictions themselves are affected by the training data, as well as newspaper page layout, date of publication, and noise from the digitization pipeline, so too are the confidence scores. In particular, the visual content recognition model suffers from high-confidence misclassifications, for example, crossword puzzles that are identified as maps with confidence scores greater than 90%. High-confidence misclassifications pose challenges for machine learning writ large, and the field of explainable artificial intelligence is largely devoted to developing tools for understanding this type of misclassification <sup id=fnref:60><a href=#fn:60 class=footnote-ref role=doc-noteref>60</a></sup>. However, these high-confidence misclassifications can often be traced back to the composition of the training set. For example, the fact that the visual content recognition model sometimes identifies crossword puzzles as maps with high confidence is likely due to the fact that the training data did not contain enough labeled examples of maps and crossword puzzles for the visual content recognition model to differentiate them with high accuracy.</p><p>The questions surrounding confidence scores and probabilistic descriptions of items is by no means restricted to the _Newspaper Navigator _ dataset. I echo Thomas Padilla’s assertion that “attempts to use algorithmic methods to describe collections must embrace the reality that, like human descriptions of collections, machine descriptions come with varying measure of certainty” <sup id=fnref1:15><a href=#fn:15 class=footnote-ref role=doc-noteref>15</a></sup>. Machine-generated metadata such as OCR are also fundamentally probabilistic in nature; this fact is not immediately apparent to end users of cultural heritage collections because cuts on confidence score are typically chosen before surfacing the metadata. Effectively communicating confidence scores, probabilistic descriptions, and the decisions surrounding them to end users remains a challenge for content stewards.</p><h2 id=viii-ocr-extraction>VIII. OCR Extraction</h2><p>In the _Newspaper Navigator _ pipeline, a textual description of each prediction is obtained by extracting the OCR within each predicted bounding box. The resulting textual description is thus dependent on not only the OCR provided by _Chronicling America _ but also the exact coordinates of the bounding box: if the coordinates of a word in the localized OCR extend beyond the bounds of the box, the word is excluded. I experimented with utilizing tolerance limits to allow words that extend just beyond the bounds of the boxes to be included, but doing so ultimately introduces false positives as well, as words from neighboring articles or visual content were inevitably included some fraction of the time. Once again, the tradeoff between false positives and false negatives is manifest.</p><p>In Figure 8, I show the textual descriptions of the four images of W.E.B. Du Bois, as identified by the <em>Newspaper Navigator</em> pipeline. Significantly, in the _Newspaper Navigator _ dataset, the OCR is stored as a list of words, with line breaks removed; these lists are what appear in Figure 8. These four examples provide intuition as to how the captions are altered. While the examples from the _Iowa State Bystander _ and <em>Franklin’s Paper the Statesman</em> both have very similar captions as shown in Figure 3, the captions for both of the examples from <em>The Broad Ax</em> are unrecognizable. Because the bounding boxes have clipped the caption, none of the characters from the proper OCR captions from Figure 3 are present. Furthermore, the captions contain OCR noise due to the OCR engine attempting to read text from the photographs. Consequently, the mentions of W.E.B. Du Bois are erased from the textual descriptions in the _Newspaper Navigator _ dataset. The visual content in the _Newspaper Navigator _ dataset is thus decontextualized not only in the sense that the visual content is extracted from the newspaper pages but also in the sense that the OCR extraction method further alters the textual descriptions. While the images from the <em>Iowa State Bystander</em> and <em>Franklin’s Paper the Statesman</em> are still recoverable with fuzzy keyword search, the two images from _The Broad Ax _ are impossible to retrieve with _any _ form of keyword search, revealing another instance in which employing automated techniques for collections processing affects discoverability.</p><figure><img loading=lazy alt="four images of W.E.B. Du Bois" src=/dhqwords/vol/15/4/000578/resources/images/image9.jpg sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/15/4/000578/resources/images/image9_hu1756b1b6b0725208e81b779d62f11eee_60709_500x0_resize_q75_box.jpg 500w,
/dhqwords/vol/15/4/000578/resources/images/image9_hu1756b1b6b0725208e81b779d62f11eee_60709_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/15/4/000578/resources/images/image9.jpg 960w" class=landscape><figcaption><p>The textual descriptions of each image, as extracted from the OCR and saved in the _Newspaper Navigator _ dataset [^navigator1910b]; [^navigator1910d]; [^navigator1910f]; [^navigator1910h].</p></figcaption></figure><p>Fortunately, visual content can still be recovered using similarity search over the images themselves; these methods are discussed in detail in the next section. However, in the case of headlines, the errors introduced by OCR engines and the subsequent OCR extraction have no recourse, as similarity search for images of headlines would only capture similar typography and text layout.<sup id=fnref:61><a href=#fn:61 class=footnote-ref role=doc-noteref>61</a></sup></p><p>To illustrate the effects of this OCR extraction on headlines, I reproduce in Table 3 the extracted OCR as it appears in the _Newspaper Navigator _ dataset for Franklin F. Johnson’s headline:</p><p>NEW MOVEMENT</p><p>BEGINS WORK</p><p>Plan and Scope of the Asso-</p><p>ciation Briefly Told.</p><p>Will Publish the Crisis.</p><p>Review of Causes Which Led to the</p><p>Organization of the Association in</p><p>New York and What Its Policy Will</p><p>Be-Career and Work of Professor</p><p>W.E.B. Du Bois<br>The extracted OCR associated with each of the four photographs of W.E.B. Du Bois <sup id=fnref:62><a href=#fn:62 class=footnote-ref role=doc-noteref>62</a></sup>; <sup id=fnref:63><a href=#fn:63 class=footnote-ref role=doc-noteref>63</a></sup>; <sup id=fnref:64><a href=#fn:64 class=footnote-ref role=doc-noteref>64</a></sup>; <sup id=fnref:65><a href=#fn:65 class=footnote-ref role=doc-noteref>65</a></sup>. _Iowa State Bystander _ (14 Oct. 1910) <em>Franklin’s Paper the Statesman</em> (15 Oct. 1910) _The Broad Ax _ (15 Oct. 1910) _The Broad Ax _ (26 Nov. 1910) 98.72% 99.57% 99.76% 99.70% [NEW , MOVEMENT , BEGINS , WORK , and , Plan , Scope , of , the , Asso\u00ad , ciation , Briefly , Told. , WILL , PUBLISH , THE , CRISIS. , Review , of , Causae , Which , Lad , to , the , Organisation , of , the , Auooiation , In , Naw , York , and , JWhat , It* , Polioy , Will , Ba\u2014Career , and , Wark , of , Profeasor] [NEW , MOVEMENT , BEGINS , WORK , Plan , and , Scope , of , the , Asso , ciation , Briefly , Told. , WILL , PUBLISH , THE , CRISIS.] [NEW , MOVEMENT , BEGINS , WORK , Plan , and , Sep , if , the , Asso , ciation , Briefly , Told. , WILL , PUBLISH , THE , CRISIS, , Be , Career , nnd , Work , of , Professor , W. , E. , B. , Du , Bois. , Review , of , Causes , Which , Led , to , the , Oraanteallon , of , th. , A.Me!.!?n , i , i , New , York , and , What , IU , Policy , Will] [NEW , MOVEMENT , BEGINS , WORK , Plan , and , Scope , of , the , Asso , ciation , Briefly , Told. , WILL , PUBLISH , THE , CRISIS. , Review , of , Causes , Which , Lad , to , tha , Organization , of , the" , Association , In , New , York , and , What , Its , Policy , Will]<br>The full pages are reproduced in the appendix for reference. Notably, all four extracted headlines contain OCR errors, as well as missing words due to the OCR extraction. The visual content recognition model consistently fails to include the last line of the headline, “W.E.B. Du Bois,” revealing another case in which Du Bois’s name is rendered inaccessible by keyword search in the _Newspaper Navigator _ dataset.</p><h2 id=ix-image-embeddings>IX. Image Embeddings</h2><p>An image embedding canonically refers to a low-dimensional representation of an image, often a list of a few hundred or a few thousand numbers, that captures much of the image’s semantic content. Image embeddings are typically generated by feeding an image into a pre-trained neural image classification model (i.e., a model that takes in an image and outputs a label of dog or cat) and extracting a representation of the image from one of the model’s hidden layers, often the penultimate layer.<sup id=fnref:66><a href=#fn:66 class=footnote-ref role=doc-noteref>66</a></sup> Image embeddings are valuable for three reasons:</p><p>Image embeddings are remarkably adept at capturing semantic similarity between images. For example, images of dogs tend to be clustered together in embedding space, with images of bicycles in another cluster and images of buildings in yet another. These clusters can be fine-grained: sometimes, the red bicycles are grouped closer together than the blue bicycles. Image embeddings can be constructed by feeding images into an image classification model already trained on another dataset (such as ImageNet), meaning that generating image embeddings is a useful method for comparing images without having to construct training data by labeling images. Image embeddings are low-dimensional and thus much smaller in size than the images themselves (i.e., on the order of kilobytes instead of megabytes). As a result, image embeddings are much less computationally expensive to compare to one another when conducting similarity search, clustering, or related tasks. In short, image embeddings speed up image comparison.</p><p>Utilizing image embeddings to visualize and explore large collections of images has become an increasingly common approach among cultural heritage practitioners. Projects and institutions that have utilized image embeddings for visualizing cultural heritage collections include the Yale Digital Humanities Lab’s PixPlot interface <sup id=fnref:67><a href=#fn:67 class=footnote-ref role=doc-noteref>67</a></sup>, the National Neighbors project <sup id=fnref:68><a href=#fn:68 class=footnote-ref role=doc-noteref>68</a></sup>, Google Arts and Culture <sup id=fnref:69><a href=#fn:69 class=footnote-ref role=doc-noteref>69</a></sup>, The Norwegian National Museum’s Principal Components project <sup id=fnref:70><a href=#fn:70 class=footnote-ref role=doc-noteref>70</a></sup>, the State Library of New South Wales’s Aero Project <sup id=fnref:71><a href=#fn:71 class=footnote-ref role=doc-noteref>71</a></sup>, the Royal Photographic Society <sup id=fnref:72><a href=#fn:72 class=footnote-ref role=doc-noteref>72</a></sup>, The American Museum of Natural History <sup id=fnref:73><a href=#fn:73 class=footnote-ref role=doc-noteref>73</a></sup>, and The National Library of the Netherlands <sup id=fnref:74><a href=#fn:74 class=footnote-ref role=doc-noteref>74</a></sup>; <sup id=fnref:75><a href=#fn:75 class=footnote-ref role=doc-noteref>75</a></sup>. These visualizations provide insights into broader themes in the collections, thereby allowing curators, researchers, and the public to explore collections at a scale previously only possible by organizing images by color or other low-level features.<sup id=fnref:76><a href=#fn:76 class=footnote-ref role=doc-noteref>76</a></sup> In this regard, image embeddings provide new affordances for searching over images that complement canonical faceted and keyword search.</p><p>Because these image embeddings enable these visualization approaches and open the door to similarity search and recommendation, I opted to include image embeddings as part of the <em>Newspaper Navigator</em> pipeline. Indeed, these image embeddings power the similarity search functionality in the <em>Newspaper Navigator</em> user interface and, in this regard, are crucial to the broader vision of the project <sup id=fnref:77><a href=#fn:77 class=footnote-ref role=doc-noteref>77</a></sup>.<sup id=fnref:78><a href=#fn:78 class=footnote-ref role=doc-noteref>78</a></sup> To generate the embeddings, I utilized ResNet-18 and ResNet-50, two variants of a prominent deep learning architecture for image classification, both of which had already been pre-trained on ImageNet <sup id=fnref:79><a href=#fn:79 class=footnote-ref role=doc-noteref>79</a></sup>.</p><p>ImageNet is perhaps the most well-known image dataset in the history of machine learning. Constructed by scraping publicly available images from the internet and recruiting Amazon Mechanical Turk workers to annotate the images, ImageNet contains approximately 14 million images across 20,000 categories <sup id=fnref:80><a href=#fn:80 class=footnote-ref role=doc-noteref>80</a></sup>; <sup id=fnref:81><a href=#fn:81 class=footnote-ref role=doc-noteref>81</a></sup>. Kate Crawford and Trevor Paglen’s essay “Excavating AI: The Politics of Images in Machine Learning Training Sets” offers a history and incisive critique of the classification schema of ImageNet; here, I will summarize the most salient critiques. First, many of the categories in the taxonomy utilized are themselves marginalizing <sup id=fnref1:19><a href=#fn:19 class=footnote-ref role=doc-noteref>19</a></sup>. Though many of the classes relating to people were removed in 2019, ImageNet had previously bifurcated the Natural Object <code>></code> Body <code>></code> Adult Body category into Male Body and Female Body subcategories. Second, ethnic classes were included, implying that 1) classification into rigid categories of ethnicity is possible and appropriate and 2) a machine learning system could learn how to classify ethnicity from these images. Diving deeper, the classifications become horrifying in their supposed granularity: until 2019, an image of a woman in a bikini was accompanied with the tags “slattern, slut, slovenly woman, trollop” <sup id=fnref2:19><a href=#fn:19 class=footnote-ref role=doc-noteref>19</a></sup>. Though many embedding models are pre-trained on subsets of ImageNet categories included in the ImageNet Large Scale Visual Recognition Challenge that elide these particularly troubling classifications, these classifications nonetheless necessitate a reckoning with our use of ImageNet writ large, especially in regard to how the semantics of ImageNet are projected onto any image embedding generated with such a model <sup id=fnref:82><a href=#fn:82 class=footnote-ref role=doc-noteref>82</a></sup>. <sup id=fnref:83><a href=#fn:83 class=footnote-ref role=doc-noteref>83</a></sup></p><p>However, questions probing the data in ImageNet fail to critique the ethically questionable practices on which ImageNet is built. Though the researchers responsible for the dataset scraped all 14 million images from public URLs, ImageNet does not provide any guarantees on image copyright, as only the URLs are provided in the database: “The images in their original resolutions may be subject to copyright, so we do not make them publicly available on our server” <sup id=fnref:84><a href=#fn:84 class=footnote-ref role=doc-noteref>84</a></sup>. It is highly unlikely that a photographer with an image in the dataset could have known that a photograph could be used this way, much less actively consent to the image’s inclusion, as is the case with subjects in the photographs. Furthermore, the labels themselves were collected using Amazon’s Mechanical Turk platform, which has been repeatedly criticized for its exploitative labor practices: as of 2017, workers earned a median wage of approximately $2 an hour on the platform <sup id=fnref:85><a href=#fn:85 class=footnote-ref role=doc-noteref>85</a></sup>. Scholars including Natalia Cecire, Bonnie Mak, and Paul Fyfe have highlighted how outsourced marginalized labor underpins digitization efforts, and the reliance on Mechanical Turk for the production of ImageNet further entrenches the digitization and discovery process within a system of labor exploitation <sup id=fnref:86><a href=#fn:86 class=footnote-ref role=doc-noteref>86</a></sup>; <sup id=fnref1:18><a href=#fn:18 class=footnote-ref role=doc-noteref>18</a></sup>; <sup id=fnref1:17><a href=#fn:17 class=footnote-ref role=doc-noteref>17</a></sup>. As cultural heritage practitioners and humanities researchers, we must acknowledge these exploitative practices, and we must reckon with how we perpetuate them through the use of ImageNet as a training source for image search and discovery.</p><p>In offering these critiques, my intention is not to dismiss ImageNet in a wholesale manner. Certainly, the benefits of utilizing ImageNet are manifold, as evidenced by widespread community adoption, as well as new affordances for searching cultural heritage collections enabled by the dataset that are shaping the contours of digital scholarship. In the case of my own scholarship with Newspaper Navigator, I have elected to utilize machine learning models pre-trained on ImageNet precisely for these reasons. I offer these provocations instead to question how we can do better as a community, not only in imagining alternatives but in bringing them to fruition. Classification is an act of interpretive reduction, whether by human or machine, and thus manifests all too often as an act of oppression.<sup id=fnref:87><a href=#fn:87 class=footnote-ref role=doc-noteref>87</a></sup> And yet, the structure imposed by classification constitutes the very basis for search and discovery systems. The salient question is thus not how we dispense of these systems but rather how we progressively realize a more inclusive vision of these systems, from the labor practices behind their construction to the very classification taxonomies themselves.</p><p>How, then, do image embeddings derived from ImageNet mediate our interactions with the photographs in <em>Newspaper Navigator</em> ? Figure 9 shows a visualization of 1,000 photographs from the _Newspaper Navigator _ dataset published during the year 1910. This visualization was created using the ResNet-50 image embeddings, as well as a dimensionality reduction algorithm known as T-SNE <sup id=fnref:88><a href=#fn:88 class=footnote-ref role=doc-noteref>88</a></sup>. With T-SNE, a cluster of photographs indicates that the photographs are likely semantically similar, but the size of the cluster and distances from other clusters bear no meaning <sup id=fnref:89><a href=#fn:89 class=footnote-ref role=doc-noteref>89</a></sup>. With this in mind, we can examine the clusters. Despite the fact that the high-contrast, grayscale photographs in <em>Newspaper Navigator</em> are markedly different, or “out-of-sample,” in comparison to the clear, color images in ImageNet, the clusters nonetheless capture semantic similarity. In Figure 9, we observe the clustering of photographs depicting crowds of people, as well as photographs depicting ships and the sea. This visualization technique with the image embeddings is thus powerful in helping to navigate large collections of photographs by their semantic content.</p><figure><img loading=lazy alt="image of a network graph" src=/dhqwords/vol/15/4/000578/resources/images/image10.jpg sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/15/4/000578/resources/images/image10_hudceb583da96818911575874c6008651a_61476_500x0_resize_q75_box.jpg 500w,
/dhqwords/vol/15/4/000578/resources/images/image10_hudceb583da96818911575874c6008651a_61476_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/15/4/000578/resources/images/image10.jpg 960w" class=landscape><figcaption><p>A visualization of 1,000 photographs from the year 1910 in the _Newspaper Navigator _ dataset, generated using the <em>Newspaper Navigator</em> ResNet-50 image embeddings.</p></figcaption></figure><p>What about the photographs of W.E.B. Du Bois? In Figure 10, I show the clusters containing these four photographs. This visualization affords us a lens into the limitations of image embeddings. First, it is evident that image embeddings are directly impacted by the distortions of the digitization process: while the three photographs from _Franklin’s Paper the Statesman _ and <em>The Broad Ax</em> are clustered together with other portraits, the photograph from the <em>Iowa State Bystander</em> is located in an entirely different cluster - a consequence of the fact that the <em>Iowa State Bystander</em> photograph is saturated and that W.E.B. Du Bois’s facial features are obscured (notably, neighboring photographs suffer from similar distortions). A search engine powered with these image embeddings would in all likelihood return the three photographs from <em>Franklin’s Paper the Statesman</em> and <em>The Broad Ax</em> together, but the fourth photograph would effectively be lost. This algorithmic mediation is particularly troubling because, as described in Section IV, the microfilming digitization process causes newspaper photographs of darker-skinned people to lose contrast. While this loss in image quality is marginalizing in its own right, image embeddings perpetuate this marginalization: digitized newspaper portraits of darker-skinned individuals are more likely to suffer from saturated facial features, in turn resulting in these photographs being lost during the discovery and retrieval process, as is the case with the saturated _Iowa State Bystander _ photograph of W.E.B. Du Bois in Figure 10. Understanding these limitations of image embeddings are particularly relevant in the case of <em>Newspaper Navigator</em> , as these image embeddings power the visual similarity search affordance within the publicly-deployed _Newspaper Navigator _ search application <sup id=fnref1:77><a href=#fn:77 class=footnote-ref role=doc-noteref>77</a></sup>. Though machine learning methods are often offered as panaceas for automation, this algorithmic erasure reminds us that traditional methods of scholarship and historiography, such as detailed analyses and close readings of Black newspapers in <em>Chronicling America</em> , are more important than ever to counter algorithmic bias.</p><figure><img loading=lazy alt="image of a network graph focused on W.E.B. Du Bois" src=/dhqwords/vol/15/4/000578/resources/images/image11.jpg sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/15/4/000578/resources/images/image11_hu1545f165794f728ff2a9b20755b773b4_69931_500x0_resize_q75_box.jpg 500w,
/dhqwords/vol/15/4/000578/resources/images/image11_hu1545f165794f728ff2a9b20755b773b4_69931_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/15/4/000578/resources/images/image11.jpg 960w" class=landscape><figcaption><p>The same visualization as in Figure 9, this time showing the locations of the four photographs of W.E.B. Du Bois.</p></figcaption></figure><h2 id=x-environmental-impact>X. Environmental Impact</h2><p>Any examination of a dataset whose construction required large-scale computing would be remiss in not investigating the environmental impact of the computation itself. The carbon emissions generated from training a state-of-the-art machine learning model such as BERT is comparable to a single flight across the United States; however, factoring in experimentation and tuning, the carbon emissions can quickly amount to the carbon emissions of a car over its entire lifetime, including fuel <sup id=fnref:90><a href=#fn:90 class=footnote-ref role=doc-noteref>90</a></sup>. OpenAI’s GPT-3 model required several thousand petaflop/s-days to train; without specific numbers, the carbon emissions are not possible to calculate exactly, but they are nonetheless substantial <sup id=fnref:91><a href=#fn:91 class=footnote-ref role=doc-noteref>91</a></sup>. In response, machine learning researchers have recommended ideas such as Green AI, with the goal of encouraging the community to value computational efficiency and not just accuracy <sup id=fnref:92><a href=#fn:92 class=footnote-ref role=doc-noteref>92</a></sup>.</p><p>In the case of <em>Newspaper Navigator</em> , most of the compute time was devoted to processing all 16.3 million <em>Chronicling America</em> pages with the visual content recognition model, as opposed to training the model itself. In Tables 4 and 5, I report details on training the model and running the pipeline, as well as the carbon emissions generated by each step, computed using the Machine Learning Impact Calculator <sup id=fnref:93><a href=#fn:93 class=footnote-ref role=doc-noteref>93</a></sup>. In total, approximately 380 kg CO2 were emitted during the construction of the _Newspaper Navigator _ dataset, including development, experimentation, training, pipeline processing, and post-processing. It should be noted that this number is an estimate, as the statistics for experimentation and post-processing are difficult to quantify exactly. Nonetheless, this is approximately equivalent to the carbon emissions incurred by a single person flying from Washington, D.C. to Boston <sup id=fnref:94><a href=#fn:94 class=footnote-ref role=doc-noteref>94</a></sup>. I include these numbers in the hope that cultural heritage practitioners will consider the environmental impact of utilizing machine learning and artificial intelligence for digital content stewardship. Doing so is essential to the data archaeology: given that climate change will disproportionately affect cultural heritage institutions in regions unable to develop proper infrastructure to withstand rapid temperature fluctuations and unprecedented flooding, even the environmental impacts of utilizing machine learning within digital content stewardship has the capacity to contribute to erasure and marginalization.<br>Carbon emissions from the GPU usage for <em>Newspaper Navigator</em> , broken down by project component. Note that all computation was done on Amazon AWS g4dn instances in the zone us-east-2. The carbon emissions were calculated using the Machine Learning Impact Calculator <sup id=fnref1:93><a href=#fn:93 class=footnote-ref role=doc-noteref>93</a></sup>. Activity # of NVIDIA T4 GPUs GPU Hours (each) Carbon Emissions Training 1 19 0.96 kg CO2 Pipeline Processing 8 456 144.56 kg CO2 Experimentation for Training and Pipeline Processing (estimate) 8 24 7.66 kg CO2 <em>Total</em> - - 153.18 kg CO2 Carbon emissions from the CPU usage for <em>Newspaper Navigator</em> , broken down by project component. Note that all computation was done on Amazon AWS g4dn instances in the zone us-east-2. The CPU processors are all 2nd generation Intel Xeon Scalable Processors (Cascade Lake) <sup id=fnref:95><a href=#fn:95 class=footnote-ref role=doc-noteref>95</a></sup>. The 48-core processor outputs approximately 350 W; the 4-core processor outputs approximately 104 W <sup id=fnref:96><a href=#fn:96 class=footnote-ref role=doc-noteref>96</a></sup>; <sup id=fnref:97><a href=#fn:97 class=footnote-ref role=doc-noteref>97</a></sup>. The carbon emissions were calculated using the Machine Learning Impact Calculator <sup id=fnref2:93><a href=#fn:93 class=footnote-ref role=doc-noteref>93</a></sup>. Note that the energy consumption by RAM is not factored in, but it is insignificant in comparison to the CPU and GPU energy consumption. Activity CPU Processor (#) # Processor CPU Cores CPU Hours (each) Carbon Emissions Training 1 4 CPUs 19 1.13 kg CO2 Pipeline Processing 2 48 CPUs 456 181.9 kg CO2 Experimentation for Training and Pipeline Processing ( <em>estimate</em> ) 2 48 CPUs 24 9.57 kg CO2 Extra Computation (dataset post-processing, etc., <em>estimate</em> ) 1 48 CPUs 168 33.52 kg CO2 <em>Total</em> - - - 226.12 kg CO2</p><h2 id=xi-conclusion>XI. Conclusion</h2><p>In this data archaeology, I have traced four <em>Chronicling America</em> pages reproducing the same photograph of W.E.B. Du Bois as they have traveled through the <em>Chronicling America</em> and <em>Newspaper Navigator</em> pipelines. The excavated genealogy of digital artifacts has revealed the imprintings of the complex interactions between humans and machines. Indeed, the journey of each newspaper page through the _Chronicling America _ and <em>Newspaper Navigator</em> pipelines is one of refraction, mediation, and decontextualization that is compounded upon with each step. Decisions made decades ago when microfilming a newspaper page inevitably affect how the machine learning models employed for OCR, visual content extraction, and image embedding generation ultimately process the pages, render them as digital artifacts in the _Newspaper Navigator _ dataset, and mediate their discoverability.</p><p>As articulated by Trevor Owens in <em>The Theory and Craft of Digital Preservation</em> , machine learning and artificial intelligence are the “underlying sciences for digital preservation” <sup id=fnref1:36><a href=#fn:36 class=footnote-ref role=doc-noteref>36</a></sup>. Though machine learning techniques provide us with new affordances for searching and studying cultural heritage materials, they have the power to perpetuate and amplify the marginalization and erasure of entire communities within the archive. This erasure, coupled with the labor practices involved in creating training data as well as the environmental impact of training and deploying machine learning models in large-scale digitization pipelines, necessitates that we continue to examine the broader socio-technical ecosystems in which we participate. In doing so, we can work toward a more inclusive vision of the digital collection and the ways in which we render its contents discoverable.</p><p>How, then, is <em>Newspaper Navigator</em> situated within this vision? In reimagining how we search over the visual content in <em>Chronicling America</em> , one explicit goal of the project is to engage the public with the rich history preserved within historic American periodicals and thus build on <em>Chronicling America</em> as a free-to-use, public domain resource for scholars, educators, students, journalists, genealogists, and beyond <sup id=fnref:98><a href=#fn:98 class=footnote-ref role=doc-noteref>98</a></sup>. <sup id=fnref:99><a href=#fn:99 class=footnote-ref role=doc-noteref>99</a></sup>. With <em>Newspaper Navigator</em> , it is my belief that the new modes of interacting with <em>Chronicling America</em> have the capacity to not only enable a breadth of new scholarship but also foster engagement in and reckoning with America’s multilayered history of oppression. In documenting the different components of the project with this data archaeology and corresponding technical paper <sup id=fnref3:11><a href=#fn:11 class=footnote-ref role=doc-noteref>11</a></sup>, as well as releasing the full dataset and all code into the public domain, I have intended to be as transparent as possible with the tools and methodologies employed. _Newspaper Navigator _ is not without its shortcomings, but my hope is that the project contributes to this vision of the digital collection through transparency and inclusivity, as well as the scholarship and pedagogy that it has enabled.</p><p>I offer this case study not only to contextualize the _Newspaper Navigator _ dataset but also to advocate for the autoethnographic data archaeology as a valuable apparatus for reflecting on a cultural heritage dataset from a humanistic perspective. Though the digital humanities community has yet to adopt the data archaeology as standard practice when creating and releasing cultural heritage datasets, doing so has the capacity to improve accountability and context surrounding applications of machine learning for both practitioners and end users. Given the manifold ways in which machine learning mediates access to the archive and perpetuates erasure, reflecting critically on these systems is not only urgent but essential for transparency and inclusivity.</p><h2 id=sources-of-funding>Sources of Funding</h2><p>This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant DGE-1762114, as well as the Library of Congress Innovator in Residence Position.</p><h2 id=acknowledgments>Acknowledgments</h2><p>I would like to thank Eileen Jakeway, Jaime Mears, Laurie Allen, Meghan Ferriter, Robin Butterhof, and Nathan Yarasavage at the Library of Congress, as well as Molly Hardy and Joshua Ortiz Baco at the National Endowment for the Humanities, for their thoughtful and enlightening feedback on drafts of this article. I am grateful to my Ph.D. advisor, Daniel Weld, at the University of Washington, for his support, guidance, and invaluable advice with <em>Newspaper Navigator</em> . In addition, I would like to thank Kurtis Heimerl and Esther Jang at the University of Washington for the opportunity to formulate and write early sections of this data archaeology as part of this Spring’s CSE 599: “Computing for Social Good” course.</p><p>Lastly, I would like to thank the following people who have shaped <em>Newspaper Navigator</em> : Kate Zwaard, Leah Weinryb Grohsgal, Abbey Potter, Chris Adams, Tong Wang, John Foley, Brian Foo, Trevor Owens, Mark Sweeney, and the entire National Digital Newspaper Program staff at the Library of Congress; Devin Naar, Stephen Portillo, Daniel Gordon, and Tim Dettmers at the University of Washington; Michael Haley Goldman, Robert Ehrenreich, Eric Schmalz, and Elliott Wrenn at the United States Holocaust Memorial Museum; Jim Casey at The Pennsylvania State University; Sarah Salter at Texas A&amp;M University-Corpus Christi; and Gabriel Pizzorno at Harvard University.</p><h2 id=appendix>Appendix:</h2><figure><img loading=lazy alt="screenshot of a newspaper page" src=/dhqwords/vol/15/4/000578/resources/images/image12.png sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/15/4/000578/resources/images/image12_hu25bbdc990c9bbce604c766ed0c52ee30_765549_500x0_resize_box_3.png 500w,
/dhqwords/vol/15/4/000578/resources/images/image12_hu25bbdc990c9bbce604c766ed0c52ee30_765549_800x0_resize_box_3.png 800w,/dhqwords/vol/15/4/000578/resources/images/image12.png 945w" class=landscape><figcaption><p>_ Iowa state bystander_ . [volume] (Des Moines, Iowa), 14 Oct. 1910. Chronicling America: Historic American Newspapers. Lib. of Congress. <a href=https://chroniclingamerica.loc.gov/lccn/sn83025186/1910-10-14/ed-1/seq-1/>https://chroniclingamerica.loc.gov/lccn/sn83025186/1910-10-14/ed-1/seq-1/</a></p></figcaption></figure><figure><img loading=lazy alt="screenshot of a newspaper page" src=/dhqwords/vol/15/4/000578/resources/images/image13.png sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/15/4/000578/resources/images/image13_huff8071bbd709c78e084077f74a266d27_650561_500x0_resize_box_3.png 500w,
/dhqwords/vol/15/4/000578/resources/images/image13_huff8071bbd709c78e084077f74a266d27_650561_800x0_resize_box_3.png 800w,/dhqwords/vol/15/4/000578/resources/images/image13.png 945w" class=landscape><figcaption><p><em>Franklin&rsquo;s paper the statesman</em> . (Denver, Colo.), 15 Oct. 1910. Chronicling America: Historic American Newspapers. Lib. of Congress. <a href=https://chroniclingamerica.loc.gov/lccn/sn91052311/1910-10-15/ed-1/seq-16/>https://chroniclingamerica.loc.gov/lccn/sn91052311/1910-10-15/ed-1/seq-16/</a></p></figcaption></figure><figure><img loading=lazy alt="screenshot of a newspaper page" src=/dhqwords/vol/15/4/000578/resources/images/image14.png sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/15/4/000578/resources/images/image14_hufe57ca21519b4e1ae0671c5f93816be0_697409_500x0_resize_box_3.png 500w,
/dhqwords/vol/15/4/000578/resources/images/image14_hufe57ca21519b4e1ae0671c5f93816be0_697409_800x0_resize_box_3.png 800w,/dhqwords/vol/15/4/000578/resources/images/image14.png 945w" class=landscape><figcaption><p><em>The broad ax.</em> [volume] (Salt Lake City, Utah), 15 Oct. 1910. Chronicling America: Historic American Newspapers. Lib. of Congress. <a href=https://chroniclingamerica.loc.gov/lccn/sn84024055/1910-10-15/ed-1/seq-2/>https://chroniclingamerica.loc.gov/lccn/sn84024055/1910-10-15/ed-1/seq-2/</a></p></figcaption></figure><figure><img loading=lazy alt="screenshot of a newspaper page" src=/dhqwords/vol/15/4/000578/resources/images/image15.png sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/15/4/000578/resources/images/image15_hue2722590f04e9a16cf78d0443474a88c_671619_500x0_resize_box_3.png 500w,
/dhqwords/vol/15/4/000578/resources/images/image15_hue2722590f04e9a16cf78d0443474a88c_671619_800x0_resize_box_3.png 800w,/dhqwords/vol/15/4/000578/resources/images/image15.png 945w" class=landscape><figcaption><p><em>The broad ax.</em> [volume] (Salt Lake City, Utah), 26 Nov. 1910. Chronicling America: Historic American Newspapers. Lib. of Congress. <a href=https://chroniclingamerica.loc.gov/lccn/sn84024055/1910-11-26/ed-1/seq-3/>https://chroniclingamerica.loc.gov/lccn/sn84024055/1910-11-26/ed-1/seq-3/</a></p></figcaption></figure><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>More on the organizational considerations surrounding _Newspaper Navigator _ can be found in <sup id=fnref1:99><a href=#fn:99 class=footnote-ref role=doc-noteref>99</a></sup>.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>Mears, J. <em>National Digital Newspaper Program Impact Study 2004-2014</em> , National Endowment for the Humanities (2014). Available at: <a href=https://www.neh.gov/divisions/preservation/featured-project/neh-releases-national-digital-newspaper-program-impact-study>https://www.neh.gov/divisions/preservation/featured-project/neh-releases-national-digital-newspaper-program-impact-study</a>. (Accessed 29 May 2020).&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>The National Digital Newspaper Program (NDNP) Technical Guidelines for Applicants 2020-22 Awards (2020). Available at: <a href=https://www.loc.gov/ndnp/guidelines/>https://www.loc.gov/ndnp/guidelines/</a> (Accessed 28 June 2020).&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p>The public search interface is available at: <a href=https://chroniclingamerica.loc.gov/>https://chroniclingamerica.loc.gov/</a>&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5><p>Ferriter, M. “Introducing Beyond Words | The Signal,” (2017). Available at: <a href=https://doi.org/blogs.loc.gov/thesignal/2017/09/introducing-beyond-words/>//blogs.loc.gov/thesignal/2017/09/introducing-beyond-words/</a>. (Accessed: 13 July 2020).&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:6><p>For more information on the <em>Beyond Words</em> workflow, see <sup id=fnref:100><a href=#fn:100 class=footnote-ref role=doc-noteref>100</a></sup>, as well as <sup id=fnref4:11><a href=#fn:11 class=footnote-ref role=doc-noteref>11</a></sup>.&#160;<a href=#fnref:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:7><p>In particular, the annotations were used to finetune an object detection model that had been pre-trained on Common Objects in Context, a common dataset for benchmarking object detection algorithms.&#160;<a href=#fnref:7 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:8><p>A screenshot of the workflow can be found later in this article in Figure 4.&#160;<a href=#fnref:8 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:9><p>For those who are not familiar with image embeddings, a detailed description is provided in Section IX.&#160;<a href=#fnref:9 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:10><p>For the dataset, see: <a href=https://news-navigator.labs.loc.gov>https://news-navigator.labs.loc.gov</a>; for the code, see <a href=https://github.com/LibraryOfCongress/newspaper-navigator>https://github.com/LibraryOfCongress/newspaper-navigator</a>.&#160;<a href=#fnref:10 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:11><p>Lee, B., Mears, J., Jakeway, E., Ferriter, M., Adams, C., Yarasavage, N., Thomas, D., Zwaard, K., and Weld, D. “The Newspaper Navigator Dataset: Extracting And Analyzing Visual Content from 16 Million Historic Newspaper Pages in Chronicling America,” <a href=https://dl.acm.org/doi/proceedings/10.1145/3340531>CIKM &lsquo;20: Proceedings of the 29th ACM International Conference on Information & Knowledge Management</a> _, _ pp. 3055–3062 (2020). Available at: <a href=https://doi.org/10.1145/3340531.3412767>https://doi.org/10.1145/3340531.3412767</a>.&#160;<a href=#fnref:11 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:11 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref2:11 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref3:11 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref4:11 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:12><p>LC Labs and Digital Strategy Directorate, “Machine Learning + Libraries Summit Event Summary” (2020). Available at: <a href=https://labs.loc.gov/static/labs/meta/ML-Event-Summary-Final-2020-02-13.pdf>https://labs.loc.gov/static/labs/meta/ML-Event-Summary-Final-2020-02-13.pdf</a>.&#160;<a href=#fnref:12 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:13><pre><code>“Digital Strategy | Library of Congress,”  Library of Congress (2019). Available at: [https://www.loc.gov/digital-strategy/](https://www.loc.gov/digital-strategy/) (Accessed: 30 May 2020).  
</code></pre>&#160;<a href=#fnref:13 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></li><li id=fn:14><p>Cordell, R. “Machine Learning + Libraries: A Report on the State of the Field” (2020). Available at: <a href="https://labs.loc.gov/static/labs/work/reports/Cordell-LOC-ML-report.pdf?loclr=blogsig">https://labs.loc.gov/static/labs/work/reports/Cordell-LOC-ML-report.pdf?loclr=blogsig</a>.&#160;<a href=#fnref:14 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:15><p>Padilla, T. <em>Responsible Operations: Data Science, Machine Learning, and AI in Libraries</em> (2019). Available at: <a href=https://doi.org/10.25333/xk7z-9g97>https://doi.org/10.25333/xk7z-9g97</a>.&#160;<a href=#fnref:15 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:15 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:16><p>Lorang, E., Soh, L., Liu, Y., and Pack, C. “Digital Libraries, Intelligent Data Analytics, and Augmented Description: A Demonstration Project” (2020). Available at: <a href=https://digitalcommons.unl.edu/libraryscience/396/>https://digitalcommons.unl.edu/libraryscience/396/</a>.&#160;<a href=#fnref:16 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:17><p>Fyfe, P. “An Archaeology of Victorian Newspapers,” <em>Victorian Periodicals Review</em> 49:4, pp. 546–77 (2016). Available at: <a href=https://doi.org/10.1353/vpr.2016.0039>https://doi.org/10.1353/vpr.2016.0039</a>.&#160;<a href=#fnref:17 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:17 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:18><p>Mak, B. “Archaeology of a Digitization,” <em>Journal of the Association for Information Science and Technology</em> 65:8, pp. 1515–26 (2014). Available at: <a href=https://doi.org/10.1002/asi.23061>https://doi.org/10.1002/asi.23061</a>.&#160;<a href=#fnref:18 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:18 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:19><p>Crawford, K., and Paglen, T. “Excavating AI: The Politics of Training Sets for Machine Learning” (2019). Available at: <a href=https://excavating.ai>https://excavating.ai</a> (Accessed: 19 September 2019).&#160;<a href=#fnref:19 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:19 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref2:19 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:20><p>Cordell, R. “‘Q i-Jtb the Raven’: Taking Dirty OCR Seriously,” <em>Book History</em> 20:1, pp. 188–225 (2017). Available at: <a href=https://doi.org/10.1353/bh.2017.0006>https://doi.org/10.1353/bh.2017.0006</a>.&#160;<a href=#fnref:20 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:20 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref2:20 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:21><p>Owens, T., and Padilla, T. “Digital Sources and Digital Archives: Historical Evidence in the Digital Age,” _International Journal of Digital Humanities _ (2020). Available at:<a href=https://doi.org/10.1007/s42803-020-00028-7>https://doi.org/10.1007/s42803-020-00028-7</a> <a href=https://doi.org/10.1007/s42803-020-00028-7>https://doi.org/10.1007/s42803-020-00028-7</a>.&#160;<a href=#fnref:21 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:22><p>Gebru, T., Morgenstern, J., Vecchione, B., Vaughan, J., Wallach, H., Daumé III, H., and Crawford, K. “Datasheets for Datasets.” <em>ArXiv:1803.09010 [Cs]</em> , March 19, 2020. <a href=http://arxiv.org/abs/1803.09010>http://arxiv.org/abs/1803.09010</a> (Accessed: July 29 2021).&#160;<a href=#fnref:22 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:23><p>Holland, S., Hosny, A., Newman, S., Joseph, J., and Chmielinski, K. “The Dataset Nutrition Label: A Framework To Drive Higher Data Quality Standards.” <em>ArXiv:1805.03677 [Cs]</em> , May 9, 2018. <a href=http://arxiv.org/abs/1805.03677>http://arxiv.org/abs/1805.03677</a> (Accessed 29 July 2021).&#160;<a href=#fnref:23 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:24><p>Bender, E., and Friedman, B. “Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science.” <em>Transactions of the Association for Computational Linguistics</em> 6 (2018): 587–604. <a href=https://doi.org/10.1162/tacl_a_00041>https://doi.org/10.1162/tacl_a_00041</a> (Accessed 29 July 2021).&#160;<a href=#fnref:24 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:25><p>Mitchell, M., Wu, S., Zaldivar, A., Barnes, P., Vasserman, L., Hutchinson, B., Spitzer, E., Raji, I., and Gebru, T. “Model Cards for Model Reporting.” <em>Proceedings of the Conference on Fairness, Accountability, and Transparency</em> , January 29, 2019, 220–29. <a href=https://doi.org/10.1145/3287560.3287596>https://doi.org/10.1145/3287560.3287596</a>.&#160;<a href=#fnref:25 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:26><p>Reisman, D., Schultz, J., Crawford, K., Whittaker, M. <em>Algorithmic Impact Assessments: A Practical Framework for Public Agency Accountability</em> (2018). Available at: <a href=https://ainowinstitute.org/aiareport2018.pdf>https://ainowinstitute.org/aiareport2018.pdf</a>.&#160;<a href=#fnref:26 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:27><p>Farrar, H. <em>The Baltimore Afro-American, 1892-1950</em> . Greenwood Publishing Group (1998).&#160;<a href=#fnref:27 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:28><p>Iowa state bystander. [volume] (Des Moines, Iowa), 14 Oct. 1910. <em>Chronicling America: Historic American Newspapers</em> . Library of Congress. Available at: <a href=https://chroniclingamerica.loc.gov/lccn/sn83025186/1910-10-14/ed-1/seq-1/>https://chroniclingamerica.loc.gov/lccn/sn83025186/1910-10-14/ed-1/seq-1/</a>&#160;<a href=#fnref:28 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:28 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:29><p>Franklin&rsquo;s paper the statesman. (Denver, Colo.), 15 Oct. 1910. <em>Chronicling America: Historic American Newspapers</em> . Library of Congress. Available at: <a href=https://chroniclingamerica.loc.gov/lccn/sn91052311/1910-10-15/ed-1/seq-16/>https://chroniclingamerica.loc.gov/lccn/sn91052311/1910-10-15/ed-1/seq-16/</a>&#160;<a href=#fnref:29 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:29 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:30><p>The broad ax. [volume] (Salt Lake City, Utah), 15 Oct. 1910. <em>Chronicling America: Historic American Newspapers</em> . Library of Congress. Available at: <a href=https://chroniclingamerica.loc.gov/lccn/sn84024055/1910-10-15/ed-1/seq-2/>https://chroniclingamerica.loc.gov/lccn/sn84024055/1910-10-15/ed-1/seq-2/</a>&#160;<a href=#fnref:30 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:30 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:31><p>The broad ax. [volume] (Salt Lake City, Utah), 26 Nov. 1910. <em>Chronicling America: Historic American Newspapers</em> . Library of Congress. Available at: <a href=https://chroniclingamerica.loc.gov/lccn/sn84024055/1910-11-26/ed-1/seq-3/>https://chroniclingamerica.loc.gov/lccn/sn84024055/1910-11-26/ed-1/seq-3/</a>&#160;<a href=#fnref:31 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:31 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:32><p>Hardy, M., and DiCuirci, L. “Critical Cataloging and the Serials Archive: The Digital Making of ‘Mill Girls in Nineteenth-Century Print,’” Archive Journal, Available at: <a href="http://www.archivejournal.net/?p=8073">http://www.archivejournal.net/?p=8073</a>.&#160;<a href=#fnref:32 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:32 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:33><p>Indeed, compiling bibliographies of serials published after 1820 remains an immensely difficult task <sup id=fnref1:32><a href=#fn:32 class=footnote-ref role=doc-noteref>32</a></sup>.&#160;<a href=#fnref:33 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:34><p>The extent to which newspaper microfilming was driven by credible fear of deterioration versus other factors, such as microfilm marketing, is an important question that is rightly debated. For more on this topic, see <sup id=fnref1:44><a href=#fn:44 class=footnote-ref role=doc-noteref>44</a></sup>.&#160;<a href=#fnref:34 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:35><p>For example, a 2017 article describing the West Virginia University Libraries’ West Virginia & Regional History Center and its participation in the National Digital Newspaper Program states: “ By August 2017, all known issues of West Virginia’s African-American newspapers from the 19th and early 20th centuries will have been digitized ” <sup id=fnref:101><a href=#fn:101 class=footnote-ref role=doc-noteref>101</a></sup>. The article describes Curator Stewart Plein’s efforts to locate surviving copies of three Black West Virginia newspapers in order to digitize and include them in <em>Chronicling America</em> .&#160;<a href=#fnref:35 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:36><p>Owens, T. <em>The Theory and Craft of Digital Preservation</em> . Johns Hopkins University Press, Baltimore (2018).&#160;<a href=#fnref:36 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:36 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:37><p>Division of Preservation and Access (NEH), “Notice of Funding Opportunity, National Digital Newspaper Program” (2020). Available at: <a href=https://www.neh.gov/sites/default/files/inline-files/National-Digital-Newspaper-Program-NOFO-January-2020.pdf>https://www.neh.gov/sites/default/files/inline-files/National-Digital-Newspaper-Program-NOFO-January-2020.pdf</a>. (Accessed 28 June 2020).&#160;<a href=#fnref:37 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:37 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref2:37 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:38><p>For a thorough case study of this process, I direct the reader to “Qi-jtb the Raven,” in which Ryan Cordell walks through an example with the Pennsylvania Digital Newspaper Program <sup id=fnref1:20><a href=#fn:20 class=footnote-ref role=doc-noteref>20</a></sup>.&#160;<a href=#fnref:38 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:39><pre><code>“Content Selection - National Digital Newspaper Program (Library of Congress)” (2020). Available at: [https://www.loc.gov/ndnp/guidelines/selection.html](https://www.loc.gov/ndnp/guidelines/selection.html) (Accessed 3 July 2020).   
</code></pre>&#160;<a href=#fnref:39 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:39 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></li><li id=fn:40><p>Barrall, K. and Guenther, C. “Microfilm Selection for Digitization,” (2005). Available at: <a href=https://www.loc.gov/ndnp/guidelines/NEH_MicrofilmSelectionNDNP.pdf>https://www.loc.gov/ndnp/guidelines/NEH_MicrofilmSelectionNDNP.pdf</a>.&#160;<a href=#fnref:40 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:41><pre><code>“Meta | Morphosis: Tutorials,”  National Digital Newspaper Program and the University of Kentucky Libraries. Available at: [https://www.uky.edu/Libraries/NDNP/metamorphosis/tutorials.html](https://www.uky.edu/Libraries/NDNP/metamorphosis/tutorials.html) (Accessed 3 July 2020).  
</code></pre>&#160;<a href=#fnref:41 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></li><li id=fn:42><pre><code>“About the Program - National Digital Newspaper Program (Library of Congress),”  (2019). Available at: [https://www.loc.gov/ndnp/about.html](https://www.loc.gov/ndnp/about.html) (Accessed 3 July 2020).   
</code></pre>&#160;<a href=#fnref:42 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></li><li id=fn:43><p>Williams, L. “What Computational Archival Science Can Learn from Art History and Material Culture Studies,” in <em>2019 IEEE International Conference on Big Data (Big Data)</em> , 2019, pp. 3153–55. Available at: <a href=https://doi.org/10.1109/BigData47090.2019.9006527>https://doi.org/10.1109/BigData47090.2019.9006527</a>.&#160;<a href=#fnref:43 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:43 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref2:43 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref3:43 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref4:43 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:44><p>Baker, N. <em>Double Fold: Libraries and the Assault on Paper</em> . Random House (2001).&#160;<a href=#fnref:44 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:44 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:45><p>Lee, B. “Machine Learning, Template Matching, and the International Tracing Service Digital Archive: Automating the Retrieval of Death Certificate Reference Cards from 40 Million Document Scans,” <em>Digital Scholarship in the Humanities</em> 34:3, pp. 513-535 (2019). <a href=https://doi.org/10.1093/llc/fqy063>Available at:</a> <a href=https://doi.org/10.1093/llc/fqy063>https://doi.org/10.1093/llc/fqy063</a>.&#160;<a href=#fnref:45 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:45 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:46><p>For exemplary research collaborations that utilize the _Chronicling America _ bulk OCR, see the Viral Text Project and the Oceanic Exchanges Project <sup id=fnref2:20><a href=#fn:20 class=footnote-ref role=doc-noteref>20</a></sup>; <sup id=fnref:102><a href=#fn:102 class=footnote-ref role=doc-noteref>102</a></sup>.&#160;<a href=#fnref:46 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:47><p>For other examinations of how OCR mediates our interactions with digital archives, see <sup id=fnref:103><a href=#fn:103 class=footnote-ref role=doc-noteref>103</a></sup>; <sup id=fnref:104><a href=#fn:104 class=footnote-ref role=doc-noteref>104</a></sup>; <sup id=fnref:105><a href=#fn:105 class=footnote-ref role=doc-noteref>105</a></sup>; <sup id=fnref:106><a href=#fn:106 class=footnote-ref role=doc-noteref>106</a></sup>; <sup id=fnref:107><a href=#fn:107 class=footnote-ref role=doc-noteref>107</a></sup>.&#160;<a href=#fnref:47 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:48><p>For a concrete example of a similar phenomenon in the image domain, see <sup id=fnref1:45><a href=#fn:45 class=footnote-ref role=doc-noteref>45</a></sup>, in which a machine learning algorithm was trained to classify digitized images but consistently misclassified images that had been misoriented 180 degrees in the scanning bed - a consequence of the classifier not having seen enough instances of these misoriented scans during training.&#160;<a href=#fnref:48 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:49><p>Noble, S. <em>Algorithms of Oppression: How Search Engines Reinforce Racism</em> . NYU Press, New York (2018).&#160;<a href=#fnref:49 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:50><p>Reidsma, M. <em>Masked by Trust: Bias in Library Discovery.</em> Litwin Books, Sacramento (2019).&#160;<a href=#fnref:50 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:51><p>Alpert-Abrams, H. “Machine Reading the Primeros Libros,” <em>Digital Humanities Quarterly</em> 10:4 (2016).&#160;<a href=#fnref:51 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:51 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:52><p>LC Labs, “Beyond Words: Mark” Available at: <a href=http://beyondwords.labs.loc.gov/#/mark>http://beyondwords.labs.loc.gov/#/mark</a> (Accessed 5 June, 2020).&#160;<a href=#fnref:52 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:53><p>LC Labs, “Beyond Words: Transcribe,” Available at: <a href=http://beyondwords.labs.loc.gov/#/transcribe>http://beyondwords.labs.loc.gov/#/transcribe</a> (Accessed 5 June, 2020).&#160;<a href=#fnref:53 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:54><p>LC Labs, “Beyond Words: Veriffy,” Available at: <a href=http://beyondwords.labs.loc.gov/#/verify>http://beyondwords.labs.loc.gov/#/verify</a> (Accessed 5 June, 2020).&#160;<a href=#fnref:54 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:55><p>It should be noted that _Beyond Words _ was introduced by LC Labs as an experiment, with no interventions in workflow or community management.&#160;<a href=#fnref:55 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:56><p>Lee, B. <em>LibraryOfCongress/Newspaper-Navigator</em> , GitHub Repository ( Library of Congress, 2020).<a href=https://github.com/LibraryOfCongress/newspaper-navigator> Available at:</a> <a href=https://github.com/LibraryOfCongress/newspaper-navigator>https://github.com/LibraryOfCongress/newspaper-navigator</a>.&#160;<a href=#fnref:56 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:57><p>Bounding box coordinates refer to the positions of the corners of the predicted bounding box, relative to the image coordinates.&#160;<a href=#fnref:57 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:58><p>The confidence score is examined in more detail in the next section.&#160;<a href=#fnref:58 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:59><p>This modest cut is provided to remove the large number of predictions with confidence scores between 0% and 5%, which have high false-positive rates, and thus reduce the size of the _Newspaper Navigator _ dataset.&#160;<a href=#fnref:59 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:60><p>Weld, D., and Bansal, G. 2019. The challenge of crafting intelligible intelligence. Commun. ACM 62: 6, pp. 70–79 (2019). Available at: <a href=https://doi.org/10.1145/3282486>https://doi.org/10.1145/3282486</a>.&#160;<a href=#fnref:60 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:61><p>The _ Newspaper Navigator _ dataset does not retain the cropped images of headlines, as the textual content is more salient than visual snippets in the case of headlines.&#160;<a href=#fnref:61 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:62><p>[Newspaper Navigator 1910b] _Newspaper Navigator _ metadata for the <em>Iowa State Bystander</em> (14 October 1910). From the Library of Congress, Newspaper Navigator dataset: Extracted Visual Content from Chronicling America. Available at: <a href=https://news-navigator.labs.loc.gov/data/iahi_ames_ver01/data/sn83025186/00202198417/1910101401/1015.json>https://news-navigator.labs.loc.gov/data/iahi_ames_ver01/data/sn83025186/00202198417/1910101401/1015.json</a>.&#160;<a href=#fnref:62 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:63><pre><code>_Newspaper Navigator _  metadata for  _Franklin’s Paper the Statesman_  (15 October 1910). From the Library of Congress, Newspaper Navigator dataset: Extracted Visual Content from Chronicling America. Available at: [https://news-navigator.labs.loc.gov/data/cohi_abbeyville_ver01/data/sn91052311/00279550730/1910101501/2272](https://news-navigator.labs.loc.gov/data/cohi_abbeyville_ver01/data/sn91052311/00279550730/1910101501/2272.json)  [.json](https://news-navigator.labs.loc.gov/data/cohi_abbeyville_ver01/data/sn91052311/00279550730/1910101501/2272.json)    
</code></pre>&#160;<a href=#fnref:63 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></li><li id=fn:64><pre><code>_Newspaper Navigator _ metadata for  _The Broad Ax _ (15 October 1910). From the Library of Congress, Newspaper Navigator dataset: Extracted Visual Content from Chronicling America. Available at: [https://news-navigator.labs.loc.gov/data/iune_charlie_ver01/data/sn84024055/00280761059/1910101501/0538.json](https://news-navigator.labs.loc.gov/data/iune_charlie_ver01/data/sn84024055/00280761059/1910101501/0538.json)    
</code></pre>&#160;<a href=#fnref:64 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></li><li id=fn:65><pre><code>_Newspaper Navigator _ metadata for The Broad Ax (26 November 1910). From the Library of Congress, Newspaper Navigator dataset: Extracted Visual Content from Chronicling America. Available at: [https://news-navigator.labs.loc.gov/data/iune_charlie_ver01/data/sn84024055/00280761059/1910112601/0564.json](https://news-navigator.labs.loc.gov/data/iune_charlie_ver01/data/sn84024055/00280761059/1910112601/0564.json)    
</code></pre>&#160;<a href=#fnref:65 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></li><li id=fn:66><p>If these words are unfamiliar, the three takeaways listed are more important.&#160;<a href=#fnref:66 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:67><pre><code>“Yale Digital Humanities Lab - PixPlot”  (2020). Available at: [https://dhlab.yale.edu/projects/pixplot/](https://dhlab.yale.edu/projects/pixplot/) (Accessed 11 June 2020).  </code></pre>&#160;<a href=#fnref:67 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></li><li id=fn:68><p>Lincoln, M., Levin, G., Conell, S., and Huang, L. (2019) “National Neighbors: Distant Viewing the National Gallery of Art&rsquo;s Collection of Collections” (2019) Available at: <a href=https://nga-neighbors.library.cmu.edu/>https://nga-neighbors.library.cmu.edu</a>. (Accessed: 30 May 2020).&#160;<a href=#fnref:68 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:69><pre><code>“Google Arts &amp; Culture Experiments - t-SNE Map Experiment”  (2018). Available at: [https://artsexperiments.withgoogle.com/tsnemap/](https://artsexperiments.withgoogle.com/tsnemap/) (Accessed: 11 June 2020).  
</code></pre>&#160;<a href=#fnref:69 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></li><li id=fn:70><pre><code>“Project: «Principal Components»,”  Nasjonalmuseet (2018). Available at: [https://www.nasjonalmuseet.no/en/about-the-national-museum/collection-management — -behind-the-scenes/digital-collection-management/project-principal-components/](https://www.nasjonalmuseet.no/en/about-the-national-museum/collection-management---behind-the-scenes/digital-collection-management/project-principal-components/) (Accessed 11 June 2020).  
</code></pre>&#160;<a href=#fnref:70 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></li><li id=fn:71><p>Giraldo, M. “Building Aereo,” DX Lab | State Library of NSW (2020). Available at: <a href=https://dxlab.sl.nsw.gov.au/blog/building-aereo>https://dxlab.sl.nsw.gov.au/blog/building-aereo</a> (Accessed: 2 July 2020).&#160;<a href=#fnref:71 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:72><p>Vane, O. “Visualising the Royal Photographic Society Collection: Part 2 • V&amp;A Blog,” <em>V&amp;A Blog</em> (2018). Available at: <a href=https://www.vam.ac.uk/blog/digital/visualising-the-royal-photographic-society-collection-part-2>https://www.vam.ac.uk/blog/digital/visualising-the-royal-photographic-society-collection-part-2</a>.&#160;<a href=#fnref:72 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:73><p>Foo, B. “AMNH Photographic Collection,” (2020). Available at: <a href=https://amnh-sciviz.github.io/image-collection/about.html>https://amnh-sciviz.github.io/image-collection/about.html</a> (Accessed: 11 June 2020).&#160;<a href=#fnref:73 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:74><p>Lonij, J., and Wevers, M. (2017) SIAMESE. KB Lab: The Hague (2017). Available at: <a href=http://lab.kb.nl/tool/siamese>http://lab.kb.nl/tool/siamese</a>.&#160;<a href=#fnref:74 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:75><p>Wevers, M., and Smits, T. “The Visual Digital Turn: Using Neural Networks to Study Historical Images,” <em>Digital Scholarship in the Humanities</em> 35:1, pp. 194-207 (2020). Available at: <a href=https://doi.org/10.1093/llc/fqy085>https://doi.org/10.1093/llc/fqy085</a>.&#160;<a href=#fnref:75 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:76><p>For an introduction to some of these methods with lower-level features, see <sup id=fnref:108><a href=#fn:108 class=footnote-ref role=doc-noteref>108</a></sup>.&#160;<a href=#fnref:76 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:77><p>Lee, B., and Weld, D. “Newspaper Navigator: Open Faceted Search for 1.5 Million Images,” <a href=https://dl.acm.org/doi/proceedings/10.1145/3379350>UIST &lsquo;20 Adjunct: Adjunct Publication of the 33rd Annual ACM Symposium on User Interface Software and Technology</a>, pp. 120-122 (2020). Available at: <a href=https://doi.org/10.1145/3379350.3416143>https://doi.org/10.1145/3379350.3416143</a>.&#160;<a href=#fnref:77 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:77 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:78><p>The search application can be found at: <a href=https://news-navigator.labs.loc.gov/search>https://news-navigator.labs.loc.gov/search</a>.&#160;<a href=#fnref:78 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:79><p>He, K., Zhang, X., Ren, S., and Sun, J. “Deep Residual Learning for Image Recognition,” in <em>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em> , 2016, pp. 770–78,<a href=https://doi.org/10.1109/CVPR.2016.90> Available at:</a> <a href=https://doi.org/10.1109/CVPR.2016.90>https://doi.org/10.1109/CVPR.2016.90</a>.&#160;<a href=#fnref:79 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:80><p>Deng, J., Dong, W., Socher, R., Li, L., Li, K., and Fei-Fei, L. “ImageNet: A Large-Scale Hierarchical Image Database,” in <em>2009 IEEE Conference on Computer Vision and Pattern Recognition</em> (2009), pp. 248–55,<a href=https://doi.org/10.1109/CVPR.2009.5206848> Available at:</a> <a href=https://doi.org/10.1109/CVPR.2009.5206848>https://doi.org/10.1109/CVPR.2009.5206848</a>.&#160;<a href=#fnref:80 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:81><pre><code>“ImageNet, ” Available at: [http://image-net.org/index](http://image-net.org/index) (Accessed: 8 June 2020).  
</code></pre>&#160;<a href=#fnref:81 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></li><li id=fn:82><p>Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A., and Fei-Fei, L. “ImageNet Large Scale Visual Recognition Challenge,” <em>International Journal of Computer Vision</em> 115:3, pp. 211-252 (2015). Available at:<a href=https://doi.org/10.1007/s11263-015-0816-y>https://doi.org/10.1007/s11263-015-0816-y</a> <a href=https://doi.org/10.1007/s11263-015-0816-y>https://doi.org/10.1007/s11263-015-0816-y</a>.&#160;<a href=#fnref:82 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:83><p>The specific categories used in the challenge can be found at: <a href=http://image-net.org/challenges/LSVRC/2010/browse-synsets>http://image-net.org/challenges/LSVRC/2010/browse-synsets</a>.&#160;<a href=#fnref:83 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:84><pre><code>“What about the images?” Available at: [http://image-net.org/download-faq](http://image-net.org/download-faq) (Accessed: 8 June 2020).  
</code></pre>&#160;<a href=#fnref:84 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></li><li id=fn:85><p>Hara, K. et al. “A Data-Driven Analysis of Workers’ Earnings on Amazon Mechanical Turk,” in <em>Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems</em> , CHI ’18 (Montreal QC, Canada: Association for Computing Machinery, 2018), pp. 1–14. Available at: <a href=https://doi.org/10.1145/3173574.3174023>https://doi.org/10.1145/3173574.3174023</a>.&#160;<a href=#fnref:85 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:86><p>Cecire, N. “Works Cited: The Visible Hand,” _ Works Cited_ (blog) (2011). Available at: <a href=http://nataliacecire.blogspot.com/2011/05/visible-hand.html>http://nataliacecire.blogspot.com/2011/05/visible-hand.html</a>.&#160;<a href=#fnref:86 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:87><p>For more reading on this topic, see <sup id=fnref:109><a href=#fn:109 class=footnote-ref role=doc-noteref>109</a></sup>.&#160;<a href=#fnref:87 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:88><p>van der Maaten, L., and Hinton, G. “Visualizing Data Using T-SNE,” <em>Journal of Machine Learning Research</em> 9, pp. 2579-2605 (2008). Available at: <a href=http://www.jmlr.org/papers/v9/vandermaaten08a.html>http://www.jmlr.org/papers/v9/vandermaaten08a.html</a>.&#160;<a href=#fnref:88 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:89><p>Wattenberg, M., Viégas, F., and Johnson, I. “How to Use T-SNE Effectively,” <em>Distill</em> 1:10 (2016). Available at: <a href=https://doi.org/10.23915/distill.00002>https://doi.org/10.23915/distill.00002</a>.&#160;<a href=#fnref:89 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:90><p>Strubell, E., Ganesh, A., and McCallum, A. “Energy and Policy Considerations for Deep Learning in NLP,” ArXiv:1906.02243 [Cs] (2019). Available at: <a href=http://arxiv.org/abs/1906.02243>http://arxiv.org/abs/1906.02243</a>.&#160;<a href=#fnref:90 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:91><p>Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei D. “Language Models Are Few-Shot Learners,” <em>ArXiv:2005.14165 [Cs]</em> (2020),<a href=http://arxiv.org/abs/2005.14165> Available at:</a> <a href=http://arxiv.org/abs/2005.14165>http://arxiv.org/abs/2005.14165</a> (Accessed: 6 June 2020).&#160;<a href=#fnref:91 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:92><p>Schwartz, R., Dodge, J., Smith, N., and Etzioni, O. “Green AI,” ArXiv:1907.10597 [Cs, Stat], (2019). Available at:<a href=http://arxiv.org/abs/1907.10597>http://arxiv.org/abs/1907.10597</a> <a href=http://arxiv.org/abs/1907.10597>http://arxiv.org/abs/1907.10597</a>.&#160;<a href=#fnref:92 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:93><p>Lacoste, A., Luccioni, A., Schmidt, V., and Dandres, T. “Quantifying the Carbon Emissions of Machine Learning,” <em>ArXiv:1910.09700 [Cs]</em> (2019). Available at: <a href=http://arxiv.org/abs/1910.09700>http://arxiv.org/abs/1910.09700</a>.&#160;<a href=#fnref:93 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:93 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref2:93 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:94><pre><code>“Carbon Footprint Calculator,”   [Available at: ](https://calculator.carbonfootprint.com/calculator.aspx?lang=en-GB&amp;tab=3)  [https://calculator.carbonfootprint.com/calculator.aspx?lang=en-GB&amp;tab=3](https://calculator.carbonfootprint.com/calculator.aspx?lang=en-GB&amp;tab=3). (Accessed: 6 June 2020).  
</code></pre>&#160;<a href=#fnref:94 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></li><li id=fn:95><pre><code>“Amazon EC2 Instance Types - Amazon Web Services,”  (2020) Amazon Web Services, Inc.[ Available at: ](https://aws.amazon.com/ec2/instance-types/)  [https://aws.amazon.com/ec2/instance-types/](https://aws.amazon.com/ec2/instance-types/). (Accessed: 5 June 2020).  
</code></pre>&#160;<a href=#fnref:95 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></li><li id=fn:96><pre><code>“Intel® Xeon® Platinum 9242 Processor (71.5M Cache, 2.30 GHz) Product Specifications,”  Available at: [https://ark.intel.com/content/www/us/en/ark/products/194145/intel-xeon-platinum-9242-processor-71-5m-cache-2-30-ghz.html](https://ark.intel.com/content/www/us/en/ark/products/194145/intel-xeon-platinum-9242-processor-71-5m-cache-2-30-ghz.html) (Accessed: 5 June 2020).  
</code></pre>&#160;<a href=#fnref:96 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></li><li id=fn:97><pre><code>“Intel® Xeon® Platinum 8256 Processor (16.5M Cache, 3.80 GHz) Product Specifications,”  Available at: [https://ark.intel.com/content/www/us/en/ark/products/192467/intel-xeon-platinum-8256-processor-16-5m-cache-3-80-ghz.html](https://ark.intel.com/content/www/us/en/ark/products/192467/intel-xeon-platinum-8256-processor-16-5m-cache-3-80-ghz.html) (Accessed: June 5, 2020).  
</code></pre>&#160;<a href=#fnref:97 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></li><li id=fn:98><p>Lee, B., Berson, I., and Berson, M. “Machine Learning and the Social Studies,” <em>Social Education</em> 85:2, pp. 88-92 (2021). Available at: <a href=https://www.socialstudies.org/social-education/85/2/machine-learning-and-social-studies>https://www.socialstudies.org/social-education/85/2/machine-learning-and-social-studies</a>.&#160;<a href=#fnref:98 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:99><p>Lee, B., Mears, J., Jakeway, E., Ferriter, M., and Potter, A. “Newspaper Navigator: Putting Machine Learning in the Hands of Library Users,” <em>EuropeanaTech Insight</em> 16 (2021). Available at: <a href=https://pro.europeana.eu/page/issue-16-newspapers>https://pro.europeana.eu/page/issue-16-newspapers</a>.&#160;<a href=#fnref:99 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:99 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:100><p>LC Labs, Beyond Words | Experiments. Available at: <a href=https://labs.loc.gov/work/experiments/beyond-words/>https://labs.loc.gov/work/experiments/beyond-words/</a> (Accessed 5 June, 2020).&#160;<a href=#fnref:100 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:101><p>Maxwell, M. “WVU Today | WVRHC Seeking Copies of Rare African-American Newspapers” (2017). Available at: <a href=https://wvutoday.wvu.edu/stories/2017/01/19/wvrhc-seeking-copies-of-rare-african-american-newspapers>https://wvutoday.wvu.edu/stories/2017/01/19/wvrhc-seeking-copies-of-rare-african-american-newspapers</a>. (Accessed 11 July 2020).&#160;<a href=#fnref:101 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:102><p>Oceanic Exchanges Project Team. Oceanic Exchanges: Tracing Global Information Networks In Historical Newspaper Repositories, 1840-1914 (2017). Available at: 10.17605/OSF.IO/WA94S.&#160;<a href=#fnref:102 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:103><p>Hitchcock, T. “Confronting the Digital,” <em>Cultural and Social History</em> 10:1. pp. 9–23 (2013). Available at: <a href=https://doi.org/10.2752/147800413X13515292098070>https://doi.org/10.2752/147800413X13515292098070</a>.&#160;<a href=#fnref:103 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:104><p>Milligan, I. “Illusionary Order: Online Databases, Optical Character Recognition, and Canadian History, 1997–2010,” <em>Canadian Historical Review</em> 94:4, pp. 540–69 (2013). Available at: <a href=https://doi.org/10.3138/chr.694>https://doi.org/10.3138/chr.694</a>.&#160;<a href=#fnref:104 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:105><p>Strange, C., McNamara, D., Wodak, J., and Wood, I. “Mining for the Meanings of a Murder: The Impact of OCR Quality on the Use of Digitized Historical Newspapers,” <em>Digital Humanities Quarterly</em> 8:1 (2014). Available at: <a href=/dhqwords/vol/8/1/000168/>http://www.digitalhumanities.org/dhq/vol/8/1/000168/000168.html</a>.&#160;<a href=#fnref:105 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:106><p>Traub, M., van Ossenbruggen, J., and Hardman, L. “Impact Analysis of OCR Quality on Research Tasks in Digital Archives,” in <em>Research and Advanced Technology for Digital Libraries</em> , ed. Sarantos Kapidakis, Cezary Mazurek, and Marcin Werla (Cham: Springer International Publishing, 2015), 252–263.&#160;<a href=#fnref:106 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:107><p>Wright, R. “Typewriting Mass Observation Online: Media Imprints on the Digital Archive,” <em>History Workshop Journal</em> 87, pp. 118–38 (2019). Available at: <a href=https://doi.org/10.1093/hwj/dbz005>https://doi.org/10.1093/hwj/dbz005</a>.&#160;<a href=#fnref:107 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:108><p>Manovich, L. “How to Compare One Million Images?,” in <em>Understanding Digital Humanities</em> , ed. David M. Berry (London: Palgrave Macmillan UK, 2012), pp. 249–78. Available at: <a href=https://doi.org/10.1057/9780230371934_14>https://doi.org/10.1057/9780230371934_14</a>.&#160;<a href=#fnref:108 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:109><p>Bowker, G., and Star, S. <em>Sorting Things Out: Classification and Its Consequences</em> . MIT Press, Cambridge (2000).&#160;<a href=#fnref:109 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div></div></article></main><footer><nav aria-label="footer links"><ul><li><a class=highlight-focus href=/dhqwords/tags/>Tags</a></li><li><a class=highlight-focus href=/dhqwords/about/>About</a></li><li><a class=highlight-focus href=/dhqwords/categories/>Keywords</a></li></ul></nav><div class=icons><a class="license highlight-focus" rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons Attribution No-Derivatives 4.0 International License" src=/dhqwords/img/logos/license.svg width=120 height=42></a></div></footer></body></html>