<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://gohugo.io/" version="0.116.0">Hugo</generator><link href="https://rlskoeser.github.io/dhqwords/vol/15/4/" rel="alternate" type="text/html" title="html"/><link href="https://rlskoeser.github.io/dhqwords/vol/15/4/index.xml" rel="alternate" type="application/rss+xml" title="rss"/><link href="https://rlskoeser.github.io/dhqwords/vol/15/4/atom.xml" rel="self" type="application/atom+xml" title="Atom"/><updated>2023-08-07T14:26:47+00:00</updated><rights>This work is licensed under a Creative Commons Attribution-NoDerivatives 4.0 International License.</rights><id>https://rlskoeser.github.io/dhqwords/vol/15/4/</id><entry><title type="html">Character Recognition Of Seventeenth-Century Spanish American Notary Records Using Deep Learning</title><link href="https://rlskoeser.github.io/dhqwords/vol/15/4/000581/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://rlskoeser.github.io/dhqwords/vol/15/4/000581/</id><author><name>Nouf Alrasheed</name></author><author><name>Praveen Rao</name></author><author><name>Viviana Grieco</name></author><published>2021-12-07T00:00:00+00:00</published><updated>2021-12-07T00:00:00+00:00</updated><content type="html"><![CDATA[<h2 id="i-introduction-1">I. INTRODUCTION <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></h2>
<p>Notary records contain a wealth of information for understanding different aspects of the human experience. For that reason, historians specialized in different regions and time periods employ them in writing social, economic, political, and cultural histories. The seventeenth-century Spanish American notarial scripts housed in the National Archives of Argentina are among the most challenging collections, as they were written by multiple hands, for an audience of experts, and at a time the written Spanish language underwent significant transformations <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>  <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup> .<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>  Consequently, it takes years of training and practice in seventeenth-century Spanish paleography to become proficient in reading and analyzing these notarial scripts (Fig.1). On an average, it takes expert Spanish speaking readers about one hour to read a four to five pages long notarized deed. The task is even more daunting for non-native Spanish speakers.</p>




























<figure ><img loading="lazy" alt="Screenshot of two different styles of handwriting" src="/dhqwords/vol/15/4/000581/resources/images/Figure_1.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/4/000581/resources/images/Figure_1_hu537a1537ddc18eeda4816caa697768b1_507212_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/4/000581/resources/images/Figure_1_hu537a1537ddc18eeda4816caa697768b1_507212_800x0_resize_box_3.png 800w,/dhqwords/vol/15/4/000581/resources/images/Figure_1.png 716w" 
     class="landscape"
     ><figcaption>
        <p>Samples of different handwriting styles present in the collection of seventeenth-century notarial scripts used for this study
        </p>
    </figcaption>
</figure>
<p>Digitization significantly aided in the preservation of these records and made them relatively more accessible primarily by enabling their duplication without damaging the originals. However, despite the quantity and variety of documents this collection compiles, these records are still waiting to be fully utilized in scholarly endeavors. To this day, researchers and students rely on traditional, time consuming, and expensive methods of archival research to access these documents and archival discovery primarily depends on the skill, patience and luck of the scholar. The development of a system capable of storing, reading, querying, and analyzing this historical collection is crucial as it will make these manuscripts accessible to a broader community of researchers without requiring extensive and expensive paleography training. Character recognition is the first step in the development of such a system.</p>
<p>Today, using optical character recognition (OCR), we can automatically convert printed or handwritten text into machine-readable, editable, and searchable text. For that reason, OCR is regarded as the heart of many document analysis systems. Unlike the recognition of printed text, historical handwritten text presents unique challenges. Written in cursive, historical scripts usually employ irregular characters and capitalization, abbreviations, archaic spelling, and linked words (Fig. 2). <sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>  Additionally, the scans contain different types of noise including discoloration, stains, as well as ink bleeds and smudges. In order to enable OCR tasks, researchers applied different methods including Support Vector Machine (SVM) <sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>, K-NN <sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>, and deep learning <sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>.</p>




























<figure ><img loading="lazy" alt="Screenshot of two different styles of handwriting, annotated" src="/dhqwords/vol/15/4/000581/resources/images/Figure2.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/4/000581/resources/images/Figure2_hu042aee0e0a1c5da8c55190d8d1e9d2db_858937_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/4/000581/resources/images/Figure2_hu042aee0e0a1c5da8c55190d8d1e9d2db_858937_800x0_resize_box_3.png 800w,/dhqwords/vol/15/4/000581/resources/images/Figure2.png 1031w" 
     class="landscape"
     ><figcaption>
        <p>Example of some of the challenges present in the manuscripts utilized for this study. On average, one page has 24 lines. The variations showed in this short paragraph are standard throughout the deeds.
        </p>
    </figcaption>
</figure>
<p>The challenges of converting Spanish American historical texts into machine-readable text have been pointed out by Hannah Alpert-Abrams. She used <a href="https://icebergnlp.github.io/">Ocular</a>, the OCR tool developed by Taylor Berg-Kirkpatrick, to machine read the  <em>Primeros Libros</em> , a sixteenth-century, printed, bilingual (Spanish and Nahuatl) text <sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup>. Her experiments delivered not only dirty OCR but also linguistic errors that could lead to inaccuracies in cultural translation. We experimented with <a href="https://transkribus.eu/Transkribus/">Transkribus</a>, another popular tool, which also yielded an inaccurate output for our collection of seventeenth-century, handwritten, Spanish American notarial records. However, in recent years, deep learning has achieved remarkable success for image understanding and classification, image segmentation, speech recognition, and natural language processing. In this work, we explore if deep learning, and more specifically CNNs, can enable accurate recognition of characters in Spanish American notarial scripts.</p>
<p>Deep learning techniques perform better and achieve higher accuracy when large labeled datasets are available (e.g., ImageNet, MS COCO) <sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup>  <sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>. However, there is no readily available labeled dataset for seventeenth-century historical texts. Although data labeling is an expensive, error-prone, and time-consuming task, with the help of professional historians and paleography experts, we manually prepared a labeled dataset. Additionally, historical texts have various kinds of noises and degradation and the uneven scanning quality of the images pose additional challenges for image preprocessing and cleaning as it has to be performed without losing any of the essential features that define each of the characters.</p>
<p>In this paper, we present an empirical study of how well state-of-the-art CNNs perform for the task of recognizing handwritten characters in seventeenth-century Spanish American notarial scripts. To the best of our knowledge, this is the first effort towards automatically recognizing characters in seventeenth-century Spanish American notarial scripts.</p>
<p><strong>The key contributions of our work are the following:</strong></p>
<ul>
<li>With the assistance of professional historians as well as labelers proficient in Spanish and trained in paleography, we prepared the training dataset in two steps. Firstly, we collected from our labelers 250 unique samples of each of the characters present on the manuscripts. Secondly, we augmented the dataset and generated additional characters by applying random distortions and rotations to the original ones. For quality control, before training CNNs on the generated dataset, our professional historians verified that the expanded characters resembled the original ones. There are certain characters that are rare in both, the Spanish language and the notarial scripts. For those characters we had fewer labels.</li>
<li>We selected four state-of-the-art CNNs, namely, Inception-v3 <sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>  <sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup>, ResNet-50 <sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup>, VGG-16 <sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup> and InceptionResNet-v2 <sup id="fnref:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup> to recognize high frequency characters as well as rare characters (i.e., x and z). We trained these networks by configuring the hyperparameters to achieve the best classification accuracy. Our experiments showed that ResNet-50 achieved the best classification accuracy of 97.08% whereas other networks achieved lower accuracy with VGG-16 being the poorest.</li>
<li>For broader use by the academic community and to foster new research in transcribing historical texts, our labeled dataset and software are publicly available on GitHub via <a href="https://github.com/UMKC-BigDataLab/DeepLearningSpanishAmerican">https://github.com/UMKC-BigDataLab/DeepLearningSpanishAmerican</a>.</li>
</ul>
<p>This paper is organized as follows. Section II discusses the relevant related works and the motivation for this study. Section III presents the methodology we employed and discusses our evaluation results. Finally, we conclude in Section IV and outline our future work.</p>
<h2 id="ii-related-work--motivation">II. RELATED WORK &amp; MOTIVATION</h2>
<p>Deep learning approaches have been widely used for handwritten text recognition of many modern languages. CNNs <sup id="fnref:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup> are among the most popular deep learning methods and have a proven record of outstanding performance when applied to image recognition tasks. Additionally, CNNs have shown an outstanding success when applied to the MNIST dataset <sup id="fnref:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup>. Ashiquzzaman et al. proposed a CNN-based model using ReLU activation function and dropout as a regularization layer that has achieved 97.4% accuracy <sup id="fnref:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup>. Tsai investigated various convolutional neural network architectures for handwritten Japanese character recognition and created a model with a 96.1% recognition rate for character classification <sup id="fnref:20"><a href="#fn:20" class="footnote-ref" role="doc-noteref">20</a></sup>. Another CNN-based model has been proposed by Rabby et al. to classify Bangla handwriting characters. A 95.71% validation accuracy was achieved for the BanglaLekha-Isolated dataset <sup id="fnref:21"><a href="#fn:21" class="footnote-ref" role="doc-noteref">21</a></sup>.</p>
<p>However, applying these methods to historical documents present unique challenges due to the quality of the scanned images, writing style variations, and the lack of labeled data. Consequently, only a few studies have taken this path. For instance, Kölsch et al. used a Fully Convolutional Neural Network (FCNN)-based approach for historic German documents, which achieved 95.6% accuracy <sup id="fnref:22"><a href="#fn:22" class="footnote-ref" role="doc-noteref">22</a></sup>. Clanuwat et al. proposed a KuroNet model that jointly recognizes an entire page of text by using a residual U-Net architecture and predicts the location and identity of all characters on a given page. Additionally, their proposed system was able to successfully recognize a significant fraction of pre-modern Japanese documents <sup id="fnref:23"><a href="#fn:23" class="footnote-ref" role="doc-noteref">23</a></sup>.</p>
<p>Researchers tend to combine CNNs with recurrent neural networks (RNNs) to further improve accuracy. That was the case for Granell et al. who proposed a handwritten text recognition system to transcribe a corpus of Spanish medieval scrips based on a CNN and RNN <sup id="fnref1:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>. The authors showed that deep learning approaches outperform the traditional machine learning models such as Hidden Markov Model-based systems. Dona Valy et al. evaluated different deep learning approaches for character recognition that have been constructed from Khmer palm leaf manuscripts <sup id="fnref:24"><a href="#fn:24" class="footnote-ref" role="doc-noteref">24</a></sup>. The authors showed that the combination of CNN and RNN-based architectures achieves better results with a 5.01% error rate. Finally, Chammas et al. presented a CRNN system for text- line recognition of historical documents <sup id="fnref1:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>. They showed how to train the system with only 10% manually labeled text-line data from the READ 2017 dataset.</p>
<p>Next, we describe the salient features of four recent CNNs that we used in our study.</p>
<p>VGG</p>
<p>In 2014, Karen Simonyan and Andrew Zisserman (2014) proposed the VGGNet for the Large Scale Visual Recognition Challenge (ILSVRC2014). The key contribution from this model was to increase the depth of the architecture by using a 3x3 convolutional filters to achieve higher performance. The VGG model achieved 92.7% top-5 accuracy on the ImageNet dataset and won the ILSVRC2014 challenge. For our experimental study, we chose VGG-16 as a representative of the VGGNet due to its smaller number of parameters compared to VGG19.</p>
<p>Inception</p>
<p>Inception architecture was first proposed in 2014 by Szegedy et al. The authors claimed that deeper networks are more prone to overfitting and consume computational resources. They solved that challenge by moving from fully connected to sparsely connected architectures. They introduced the inception layer, which is a combination of three different convolutional layers (1x1 convolutional layer, 3x3 convolutional layer, and 5x5 convolutional layer) with a max pooling layer that operates at the same level. Their outputs are concatenated to be the input of the next layer. This architecture has been updated to increase the accuracy further and proved that any convolution with kernel size more substantial than 3x3 could be represented efficiently with a series of smaller convolutions. In our experimental study, we used Inception- V3 <sup id="fnref1:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>  <sup id="fnref1:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup>.</p>
<p>ResNet</p>
<p>He et al. introduced the deep residual neural network (ResNet) architecture and won the first place in the ILSVRC 2015 classification competition. ResNet introduces the idea of identity connections that skip one or more layers to train deeper neural networks. This resolved the vanishing gradient problem by allowing the gradients to flow directly through the skipped connections backward from later layers to the initial filter. For our experimental study, we used ResNet-50 as a representative <sup id="fnref1:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup>.</p>
<p>InceptionResNet</p>
<p>Inception-ResNet is a convolutional neural network proposed by Szegedy et al. in 2016. It was trained on more than one million images from the ImageNet database and achieved a 3.08% top-5 error on the test set of the ImageNet classification (CLS) challenge <sup id="fnref2:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup> The success of residual connections in training very deep architectures and the performance of the Inception-V3 inspired the authors to replace the Inception filter concatenation step with residual connections. This combination allows Inception to obtain all the advantages of the residual approach but with the preservation of its computational efficiency. We used InceptionResNet-v2 as a representative for our experimental study.</p>
<p>Despite these advances, to this day, there is a lack of end-to-end systems capable of managing and analyzing historical documents in general and those in Spanish in particular. This gap, coupled with the professional training needs of 21st century humanities scholars, draw our attention and drives our experimentation efforts to make these manuscripts accessible to a broader community of researchers without requiring extensive and expensive paleography training. Our effort is the first step in this direction and will open up a wide range of research opportunities for others in the academic community.</p>
<h2 id="iii-methodology">III. METHODOLOGY</h2>
<p>In this section, we present the methodology for conducting this empirical study. The overall steps are illustrated in Figure 3. There are four main stages: (a) pre-processing, (b) dataset preparation, (c) training and validation (to tune the hyperparameters), and (d) testing the accuracy of character recognition.</p>




























<figure ><img loading="lazy" alt="Image of a flowchart" src="/dhqwords/vol/15/4/000581/resources/images/Figure3.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/4/000581/resources/images/Figure3_hud90fb3966c53353164fd0ccac5ab7365_3051747_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/4/000581/resources/images/Figure3_hud90fb3966c53353164fd0ccac5ab7365_3051747_800x0_resize_box_3.png 800w,/dhqwords/vol/15/4/000581/resources/images/Figure3_hud90fb3966c53353164fd0ccac5ab7365_3051747_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/4/000581/resources/images/Figure3.png 1483w" 
     class="landscape"
     ><figcaption>
        <p>Overall steps involved in character recognition of seventeenth-century Spanish American notarial scripts
        </p>
    </figcaption>
</figure>
<h2 id="a-pre-processing">a) Pre-processing:</h2>
<p>The manuscripts scans contained noise including spurious ink markings, ink smudges and bleeds. Such noise affects the feature extraction as well as the classification. Before constructing the character dataset, we reduced the noise on the manuscripts images (Figure 4). The following preprocessing techniques allowed us to clean the images without affecting the quality of the written content. Firstly, we converted the original manuscript images into grayscale. Secondly, we applied a median filter to soften the images backgrounds and remove background noise. Finally, we applied image binarization to convert the grayscale images into black and white ones as this technique significantly reduces the information contained in an image and increases the training speed <sup id="fnref:25"><a href="#fn:25" class="footnote-ref" role="doc-noteref">25</a></sup>.</p>




























<figure ><img loading="lazy" alt="Two images of handwriting" src="/dhqwords/vol/15/4/000581/resources/images/Figure4.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/4/000581/resources/images/Figure4_hu6bf271ac2052d8a55a04d760e3210a21_406367_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/4/000581/resources/images/Figure4_hu6bf271ac2052d8a55a04d760e3210a21_406367_800x0_resize_box_3.png 800w,/dhqwords/vol/15/4/000581/resources/images/Figure4_hu6bf271ac2052d8a55a04d760e3210a21_406367_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/4/000581/resources/images/Figure4_hu6bf271ac2052d8a55a04d760e3210a21_406367_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/4/000581/resources/images/Figure4.png 1508w" 
     class="landscape"
     ><figcaption>
        <p>An example of an original scan and its cleaned version.
        </p>
    </figcaption>
</figure>
<h2 id="b-dataset-preparation">b) Dataset preparation:</h2>
<p>We constructed the character dataset from clean images. <a href="http://www.colabeler.com/">Colabeler</a> tool was used to annotate and label the characters. The annotations were exported in JSON format along with each character label and its corresponding coordinates. We ran a Python script to crop and save every character in .png format. As it was difficult to keep each character within square dimensions while annotating them, each one of them was padded with white pixels and resized to fixed dimensions for training purposes. We considered 24 characters that comprise most of the Spanish alphabet: a, b, c, c, d, e, f, g, h, i, j, l, m, n, o, p, q, r, s, t, u/v, x, y, and z. Note that the &ldquo;u&rdquo; and the &ldquo;v&rdquo; were interchangeable in seventeenth-century Spanish, and thus, the alphabets provided by most paleography manuals list them as a single character. We followed this standard and treated them as single letter (u/v). Additionally, characters such as &ldquo;k&rdquo; and &ldquo;w&rdquo; are infrequently used in modern Spanish and were so uncommon in seventeenth-century notarial scripts that paleography textbooks do not even list them on their sample alphabets.<sup id="fnref:26"><a href="#fn:26" class="footnote-ref" role="doc-noteref">26</a></sup>  To build our sample dataset, we selected the hand of Nicolas de Valdibia y Brizuela who, by 1650, acted as an interim notary in Buenos Aires, Argentina. As opposed to those who held permanent positions, interim notaries did not receive extensive training nor were they skilled scribes. Thus, the experiments we report in this paper were based on very irregular and, therefore, hard to read scripts, as shown in Figure 2 &amp; Figure 4.</p>
<p>Our labeling team labeled 250 unique samples for every character. This resulted in a total of 6,000 original images. As deep learning models perform well on large labeled datasets, the dataset was augmented by applying random distortion and rotation of +5 and -5 degrees without affecting the shape and/or the direction of each character. A few examples are shown in Figure 5.</p>




























<figure ><img loading="lazy" alt="Two images of handwriting" src="/dhqwords/vol/15/4/000581/resources/images/Figure5.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/4/000581/resources/images/Figure5_hu6a769b6c412eec0c3c1232cefd8de7c9_110619_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/4/000581/resources/images/Figure5_hu6a769b6c412eec0c3c1232cefd8de7c9_110619_800x0_resize_box_3.png 800w,/dhqwords/vol/15/4/000581/resources/images/Figure5.png 1000w" 
     class="landscape"
     ><figcaption>
        <p>Example of the original characters b; d and p b) Example of augmented characters after applying random distortion and rotation
        </p>
    </figcaption>
</figure>
<p>For quality control, before training CNNs on the generated dataset, the professional historians and paleography experts in our team verified that the expanded characters resembled the original ones. Our dataset contained 1,000 samples for each character out of which 250 samples were manually labeled, and 750 samples were generated. This resulted in a total of 24,000 images.</p>
<h2 id="c-training-and-validation">c) Training and validation:</h2>
<p>We conducted all our experiments on a GeForce RTX 2080 Ti GPU with 12GB GPU memory. Our software was implemented using Keras <sup id="fnref:27"><a href="#fn:27" class="footnote-ref" role="doc-noteref">27</a></sup> with TensorFlow backend <sup id="fnref:28"><a href="#fn:28" class="footnote-ref" role="doc-noteref">28</a></sup>, TensorBoard,<sup id="fnref:29"><a href="#fn:29" class="footnote-ref" role="doc-noteref">29</a></sup>  and OpenCV <sup id="fnref:30"><a href="#fn:30" class="footnote-ref" role="doc-noteref">30</a></sup>.</p>
<p>We trained the state-of-art CNNs using 24,000 images (1,000 images per character) in our labeled dataset prepared from our seventeenth-century Spanish American notary scripts. For each character, we split the training set into an 80-20 split. The samples not used for training were part of the testing set. The testing set contained 50 samples per character and were preprocessed in the same way as the training images. Data augmentation was applied to the training set to avoid overfitting. As we mentioned earlier, most of the paleography manuals list the u and the v as a single character as they were interchangeable in seventeenth-century Spanish. We followed the same standard and treated them as a single letter (u/v).</p>
<h2 id="d-test-of-the-accuracy-of-character-recognition">d) Test of the accuracy of character recognition:</h2>
<p>In this research, the character recognition accuracy was used as the primary metric to evaluate the performance of different CNNs. An empirical tuning approach has been followed to tune the hyperparameters to obtain higher character recognition accuracy. To ensure a fair comparison, we set a total of 150 epochs to train the models, and used the Adam optimizer <sup id="fnref:31"><a href="#fn:31" class="footnote-ref" role="doc-noteref">31</a></sup>. Adam is a gradient descent optimization algorithm that is popularly used in training deep learning models. (Using gradient descent, it is possible to find local minima of functions during optimization.) The performance of the models with different hyperparameters values is shown in Table I and Table II.</p>
<p>Table I shows the recognition accuracy when we set the dropout value to 0.6. ResNet-50 achieved the highest accuracy of 97.02%, followed by InceptionResnet-v2, Inception-v3, and VGG-16 with a recognition accuracy of 96.33%, 93.83%, and 96.33%, respectively. The performance of most of the networks improved when the batch size was increased to 64 except for Inception-v3, which achieved a better recognition accuracy when the batch size was 32.</p>
<p>The recognition accuracy results obtained from using 0.5 dropout rate are presented in Table II. The recognition accuracy decreased for most of the networks when we changed the dropout rate from 0.6 to 0.5. VGG-16 was the only model where it performed better on a 0.5 dropout rate.<br>
Recognition accuracy obtained with 0.6 dropout rate; best accuracy is shown in blue and the worst in red    Models   Batch Size 32    Batch Size 64       VGG-16  62.50%  69.33%      Inception-V3  94.91%  93.83%      ResNet-50  95.50%  97.08%      InceptionResnet-V2  96.00%  96.33%        Recognition accuracy obtained with 0.5 dropout rate; best accuracy is shown in blue and the worst in red    Models   Batch Size 32    Batch Size 64       VGG-16  70.33%   70.91%      Inception-V3  93.83%  96.33%      ResNet-50  96.92%  87.58%      InceptionResnet-V2  96.66%  94.08%   <br>
Table III shows the accuracy breakdown of each character obtained from two different experiments. We labeled the best results in bold and worse results in italics. As shown in the table, VGG-16 fails in recognizing non-confusing characters such as &ldquo;o&rdquo; and &ldquo;u/v&rdquo;. However, it performs well on few characters such as m and y.<br>
Recognition accuracy per character. Best results are highlighted in bold and the worst results are highlighted in italics         Batch Size = 32 &amp; Dropout rate = 0.5                 Batch Size = 64 &amp; Dropout rate = 0.5                       VGG  Inception  ResNet  Inception  VGG  Inception  ResNet  Inception        16  v3  50   Resnet v2  16  v3  50   Resnet v2      A   <em>82%</em>   94%  94%   <strong>96%</strong>   92%   <strong>94%</strong>    <em>90%</em>   92%      B   <em>82%</em>    <strong>100%</strong>    <strong>100%</strong>   96%   <em>86%</em>    <strong>98%</strong>    <strong>98%</strong>   92%      C   <em>62%</em>   92%   <strong>94%</strong>    <strong>94%</strong>    <em>42%</em>    <strong>94%</strong>   86%  92%      c¸   <em>76%</em>    <strong>100%</strong>   98%   <strong>100%</strong>    <em>74%</em>    <strong>100%</strong>   98%  98%      D   <em>74%</em>   92%   <strong>96%</strong>    <strong>96%</strong>    <em>74%</em>   94%  76%   <strong>96%</strong>       E   <em>36%</em>   84%   <strong>90%</strong>   84%   <em>46%</em>    <strong>90%</strong>   76%  86%      F   <em>38%</em>   86%  88%   <strong>92%</strong>    <em>60%</em>   92%  92%   <strong>94%</strong>       G   <em>22%</em>   94%  96%   <strong>98%</strong>    <em>32%</em>    <strong>88%</strong>   68%  68%      H   <em>74%</em>   98%   <strong>100%</strong>   98%   <em>68%</em>    <strong>100%</strong>   78%   <strong>100%</strong>       I   <em>64%</em>    <strong>100%</strong>   96%   <strong>100%</strong>   64%  96%   <em>44%</em>    <strong>100%</strong>       J   <em>88%</em>    <strong>100%</strong>   98%  96%   <em>86%</em>    <strong>100%</strong>   96%   <strong>100%</strong>       L   <em>76%</em>    <strong>100%</strong>    <strong>100%</strong>    <strong>100%</strong>    <em>70%</em>    <strong>100%</strong>   80%   <strong>100%</strong>       M  98%   <strong>100%</strong>    <strong>100%</strong>    <em>96%</em>    <em>96%</em>    <strong>100%</strong>   98%   <strong>100%</strong>       N   <em>90%</em>   94%   <strong>96%</strong>    <strong>96%</strong>   76%   <strong>92%</strong>    <em>56%</em>   78%      O   <em>82%</em>    <strong>100%</strong>    <strong>100%</strong>    <strong>100%</strong>    <em>82%</em>    <strong>100%</strong>    <strong>100%</strong>    <strong>100%</strong>       P   <em>88%</em>   98%  98%   <strong>100%</strong>    <em>94%</em>    <strong>98%</strong>    <strong>98%</strong>    <strong>98%</strong>       Q   <em>74%</em>    <strong>100%</strong>    <strong>100%</strong>    <strong>100%</strong>    <em>78%</em>    <strong>100%</strong>    <strong>100%</strong>   98%      R   <em>58%</em>   92%  96%   <strong>98%</strong>    <em>44%</em>   94%   <strong>96%</strong>   90%      S   <em>74%</em>    <strong>94%</strong>    <strong>94%</strong>   92%   <em>58%</em>    <strong>96%</strong>   94%  90%      T   <em>64%</em>   96%   <strong>100%</strong>    <strong>100%</strong>    <em>82%</em>    <strong>96%</strong>   94%   <strong>96%</strong>       u/v   <em>68%</em>    <strong>100%</strong>    <strong>100%</strong>    <strong>100%</strong>    <em>64%</em>    <strong>100%</strong>    <strong>100%</strong>    <strong>100%</strong>       X   <em>52%</em>   62%   <strong>92%</strong>   90%   <em>66%</em>    <strong>90%</strong>   84%   <strong>90%</strong>       Y   <em>94%</em>    <strong>100%</strong>    <strong>100%</strong>    <strong>100%</strong>    <em>88%</em>    <strong>100%</strong>    <strong>100%</strong>    <strong>100%</strong>       Z   <em>72%</em>    <strong>100%</strong>    <strong>100%</strong>   98%   <em>80%</em>    <strong>100%</strong>    <strong>100%</strong>    <strong>100%</strong>    <br>
Overall, VGG-16 performed the worst for most of our character datasets. Figure 6 gives the graphs of accuracy and loss values for the training set with respect to the number of epochs. It shows that VGG-16 accuracy could have been improved if it trained with more epochs.</p>




























<figure ><img loading="lazy" alt="Images of eight line charts" src="/dhqwords/vol/15/4/000581/resources/images/Figure6.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/4/000581/resources/images/Figure6_hubfb39c0a3ee62c7a2b5bb6eecc93dd13_1254918_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/4/000581/resources/images/Figure6_hubfb39c0a3ee62c7a2b5bb6eecc93dd13_1254918_800x0_resize_box_3.png 800w,/dhqwords/vol/15/4/000581/resources/images/Figure6_hubfb39c0a3ee62c7a2b5bb6eecc93dd13_1254918_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/4/000581/resources/images/Figure6_hubfb39c0a3ee62c7a2b5bb6eecc93dd13_1254918_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/4/000581/resources/images/Figure6_hubfb39c0a3ee62c7a2b5bb6eecc93dd13_1254918_1800x0_resize_box_3.png 1800w,/dhqwords/vol/15/4/000581/resources/images/Figure6.png 2424w" 
     class="portrait"
     ><figcaption>
        <p>The accuracy (left) and loss (right) curves on the training set of the CNN models
        </p>
    </figcaption>
</figure>
<p>To further understand why some models achieved low accuracy, we generated confusion matrices for all the models. Confusion matrices help us study the miss-classified characters. As seen in Figure 7, confusion matrices confirm that most of the confusions occur between the characters that are written similarly. For instance, the character n is confused with r as shown in Figure 7(a), and g is confused with q as shown in Figure 7(b). The results are not surprising as these characters generally confuse non-expert human readers and, occasionally, trained paleographers. Figure 8 shows samples of these characters. However, as shown on Table III, the recognition accuracy remains overall strong.</p>




























<figure ><img loading="lazy" alt="Two images of matrices" src="/dhqwords/vol/15/4/000581/resources/images/Figure7.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/4/000581/resources/images/Figure7_hudcd3a90dcb78b56bf1458c977feca5f9_197489_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/4/000581/resources/images/Figure7_hudcd3a90dcb78b56bf1458c977feca5f9_197489_800x0_resize_box_3.png 800w,/dhqwords/vol/15/4/000581/resources/images/Figure7.png 1090w" 
     class="landscape"
     ><figcaption>
        <p>(a) ResNet-50 Confusion Matrix (b) InceptionReset-v2 Confusion Matrix Confusion matrices of selected models to show the miss-classified characters
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="Two images of handwriting" src="/dhqwords/vol/15/4/000581/resources/images/Figure8.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/4/000581/resources/images/Figure8_hu2fd093c8ef39ed43c156b8696be8d9a6_61657_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/4/000581/resources/images/Figure8_hu2fd093c8ef39ed43c156b8696be8d9a6_61657_800x0_resize_box_3.png 800w,/dhqwords/vol/15/4/000581/resources/images/Figure8.png 1000w" 
     class="landscape"
     ><figcaption>
        <p>a) Example of the shape similarities between the characters r and n. b) Example of the shape similarities between the characters g, q, and y
        </p>
    </figcaption>
</figure>
<h2 id="iv-conclusion--future-work">IV. CONCLUSION &amp; FUTURE WORK</h2>
<p>Historical handwritten character recognition is a challenging pattern recognition problem due to the inconsistency of the handwritten scripts and the lack of accurate labeled data. In this paper, we presented an empirical study on how state-of- the-art CNNs (developed for image classification) perform for the task of recognizing handwritten characters in seventeenth-century Spanish American notarial scripts. The labeled dataset employed in this study was carefully curated with the help of paleography experts and professional historians. Data augmentation was employed to increase the number of training samples. We observed that ResNet-50 achieved the promising accuracy of 97.08% compared to InceptionResnet- V2, Inception-V3, and VGG-16, which achieved 96.66%, 96.33%, and 70.91%, respectively.</p>
<p>Our study demonstrates that recent CNNs are promising to detect characters in seventeenth-century Spanish notarial scripts. Our future work will test the performance of deep learning-based OCR models such as Keras-OCR, YOLO-OCR, Tesseract and Kraken for the detection and recognition of handwritten words on these manuscripts. Accurate word recognition will be a necessary step in the development of a tool for reading, querying, and analyzing this historical collection. We plan model the content of these manuscripts in a form that would make information retrieval faster and better.</p>
<p>The labeled dataset and software used in this study are publicly available on GitHub via <a href="https://github.com/UMKC-BigDataLab/DeepLearningSpanishAmerican">https://github.com/UMKC-BigDataLab/DeepLearningSpanishAmerican</a>.</p>
<h2 id="acknowledgements">ACKNOWLEDGEMENTS</h2>
<p>This research was supported by the NEH Digital Humanities Advancement Grant (HAA-271747-20), UMKC’s Missouri Institute for Energy and Defense (MIDE), UMKC’s Funding for Excellence Program, and a Collaborative Data Science Grant from UMKC’s Institute for Data Education, Analytics and Science (IDEAS). The authors would like thank Ryan Rowland, Maha Alrasheed, and Vania Todorova for labeling data as well as the  <em>Archivo General de la República Argentina</em>  for granting their permission to use in this study their digitized collection of notary records. Dr. Martin L. E. Wasserman contributed to this project with his expertise in Spanish paleography. The first author (N. A.) would like to thank UMKC’s Women’s Council Graduate Assistance Fund (GAF) as well as the University of Tabuk in Saudi Arabia for sponsoring her scholarship.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Viviana Grieco’s research focuses on Colonial Latin American history and has received extensive paleography training in Argentina and in Spain. She leads our labeling team which counts on the expertise of Martin Wasserman and David Freeman, historians who have employed in their research the collection of notary records used in this study. For more information about our research team visit <a href="https://www.umkc.edu/mide/NEH-Project/">https://www.umkc.edu/mide/NEH-Project/</a>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Silva Prada, N.  “Paleografías americanas,”   <a href="https://www.openedition.org/21549">https://www.openedition.org/21549</a>, 2001.r&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Wasserman, M.L.E.,  “La escritura paleográfica Iberoamericana: letras procesales y encadenadas,”  in  <em>Introducción a la Paleografía. Herramientas para la Lectura y Anáisis de Documentos Antiguos</em> , ed. Rosana Vassallo (La Plata: Facultad de Humanidades y Ciencias de la Educación, 2018)&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>The speed and volume of the documentary production forced the scribes to link words and use an increasing number of abbreviations. The office of the public notary in seventeenth century Buenos Aires had a high turnover rate, which explains the large number of interim notaries as well as the variety of hands present in this collection.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>The Document Analysis Group at the Universitat Autònoma de Barcelona has been developing a digital library for the sixteenth-century  <em>Llibres d’Esposalles</em>  (marriage records). These handwritten marriage records are quite challenging although each marriage license follows a regular formula and the scripts are more consistent than those for the seventeenth-century notary records, <a href="http://dag.cvc.uab.es/the-esposalles-database/">http://dag.cvc.uab.es/the-esposalles-database/</a>&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>Vellingiriraj E., Balamurugan M., and Balasubramanie P.,  “Information extraction and text mining of ancient vattezhuthu characters in historical documents using image zoning,”  in 2016 International Conference on Asian Language Processing (IALP). IEEE, 2016, pp. 37–40.&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>Chammas E., Mokbel C., and Likforman-Sulem L.,  “Handwriting recognition of historical documents with few labeled data,”  in 2018 13th IAPR International Workshop on Document Analysis Systems (DAS). IEEE, 2018, pp. 43–48.&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>Granell E., Chammas E., Likforman-Sulem L., Martíınez-Hinarejos C.-D., Mokbel C., and Cîrstea B.-I.,  “Transcription of Spanish historical handwritten documents with deep neural networks,”    <em>Journal of Imaging</em> , vol. 4, no. 1, p. 15, 2018.&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>Alpert-Abrams, H.,  “Machine Reading the  <em>Primeros Libros</em> ,”    <em>DHQ</em>  10, no. 4, 2016.&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>Deng J., Dong W., Socher R., Kai Li L. Li, and Li Fei-Fei,  “Imagenet: A large-scale hierarchical image database,”  in 2009 IEEE Conference on Computer Vision and Pattern Recognition, 2009, pp. 248–255.&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>Lin, T-Y., Maire M., S. Belongie, J. Hays, Perona P., Ramanan D., Dollár P., and. Zitnick C. L,  “Microsoft coco: Common objects in con- text,”  in Computer Vision – ECCV 2014, D. Fleet, T. Pajdla, B. Schiele, and T. Tuytelaars, Eds. Cham: Springer International Publishing, 2014, pp. 740–755.&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p>Szegedy C., Liu W., Jia Y., Sermanet P., Reed S., Anguelov D., Erhan D., Vanhoucke V., and Rabinovich A.,  “Going deeper with convolutions,”  in Proceedings of the IEEE conference on computer vision and pattern recognition, 2015, pp. 1–9.&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13">
<p>Szegedy C., Vanhoucke V., Ioffe S., Shlens J., and. Wojna Z,  “Rethinking the inception architecture for computer vision,”  in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 2818–2826.&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14">
<p>He K., Zhang X., Ren S., and Sun J.,  “Deep residual learning for image recognition,”  in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770–778.&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:15">
<p>Simonyan K. and Zisserman A.,  “Very deep convolutional networks for large-scale image recognition,”  arXiv preprint arXiv:1409.1556, 2014.&#160;<a href="#fnref:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:16">
<p>Szegedy C., Ioffe S., Vanhoucke V., and Alemi A. A.,  “Inception-v4, inception-resnet and the impact of residual connections on learning,”  in Thirty-first AAAI conference on artificial intelligence, 2017.&#160;<a href="#fnref:16" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:16" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:16" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:17">
<p>Krizhevsky A., Sutskever I., and Hinton G. E.,  “Imagenet classification with deep convolutional neural networks,”  in  <em>Advances in neural infor- mation processing systems</em> , 2012, pp. 1097–1105.&#160;<a href="#fnref:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:18">
<p>LeCun Y., Bottou L., Bengio Y., and Haffner P.,  “Gradient-based learning applied to document recognition,”  Proceedings of the IEEE, vol. 86, no. 11, pp. 2278–2324, 1998.&#160;<a href="#fnref:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:19">
<p>Ashiquzzaman A. and Tushar A. K.,  “Handwritten Arabic numeral recognition using deep learning neural networks,”  in 2017 IEEE International Conference on Imaging, Vision &amp; Pattern Recognition (icIVPR). IEEE, 2017, pp. 1–4.&#160;<a href="#fnref:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:20">
<p>Tsai C.,  “Recognizing handwritten Japanese characters using deep convolutional neural networks,”  2016.&#160;<a href="#fnref:20" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:21">
<p>Rabby A. S. A., Haque S., Islam S., Abujar S., and Hossain S. A.,  “Bornonet: Bangla handwritten characters recognition using convolutional neural network,”  Procedia computer science, vol. 143, pp. 528– 535, 2018.&#160;<a href="#fnref:21" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:22">
<p>Kölsch A., Mishra A., Varshneya S., Afzal M.Z., and Liwicki M.,  “Recognizing challenging handwritten annotations with fully convolutional networks,”  in 2018 16th International Conference on Frontiers in Handwriting Recognition (ICFHR). IEEE, 2018, pp. 25–31.&#160;<a href="#fnref:22" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:23">
<p>Clanuwat T., Lamb A., and. Kitamoto A,  “Kuronet: Pre-modern Japanese Kuzushiji character recognition with deep learning,”  arXiv preprint arXiv:1910.09433, 2019.&#160;<a href="#fnref:23" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:24">
<p>Valy D., Verleysen M., Chhun S., and Burie J.-C.,  “Character and text recognition of Khmer historical palm leaf manuscripts,”  in 2018 16th International Conference on Frontiers in Handwriting Recognition (ICFHR). IEEE, 2018, pp. 13–18.&#160;<a href="#fnref:24" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:25">
<p>Chamchong R. and Fung C. C.,  “Comparing background elimination approaches for processing of ancient Thai manuscripts on palm leaves,”  in 2009 International Conference on Machine Learning and Cybernetics, vol. 6. IEEE, 2009, pp. 3436–3441.&#160;<a href="#fnref:25" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:26">
<p>N. Silva-Prada, Manual de paleografía y diplomática hispanoamericana, siglos XVI, XVII y XVIII. Libros de texto, manuales de prácticas y antologías. Universidad Autónoma Metropolitana, Unidad Iztapalapa, 2001. <a href="https://paleografi.hypotheses.org/el-manual-de-silva-prada">https://paleografi.hypotheses.org/el-manual-de-silva-prada</a>&#160;<a href="#fnref:26" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:27">
<p>Chollet F. et al.,  “Keras,”   <a href="https://github.com/fchollet/keras">https://github.com/fchollet/keras</a>, 2015.&#160;<a href="#fnref:27" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:28">
<p>Abadi M., Agarwal A., Barham P., Brevdo E., Chen Z., Citro C., Corrado G. S., Davis A., Dean J., Devin M., Ghemawat S., Goodfellow I., Harp A., Irving G., Isard M., Jia Y., Jozefowicz R., Kaiser L., Kudlur M., Levenberg J., Mané D., Monga R., Moore S., Murray D., Olah C., Schuster M., Shlens J., Steiner B., Sutskever I., Talwar K., Tucker P., Vanhoucke V., Vasudevan V., Viégas F., Vinyals O., Warden P., Wattenberg M., Wicke M., Yu Y., and Zheng, X.  “TensorFlow: Large-scale machine learning on heterogeneous systems,”  2015, software available <a href="https://www.tensorflow.org/">https://www.tensorflow.org/</a>&#160;<a href="#fnref:28" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:29">
<p>Tensorflow,  “tensorflow/tensorboard,”  Apr 2020. Available: <a href="https://github.com/tensorflow/tensorboard">https://github.com/tensorflow/tensorboard</a>&#160;<a href="#fnref:29" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:30">
<p>Bradski G.,  “The OpenCV Library,”    <em>Dr. Dobb’s Journal of Software Tools</em> , 2000.&#160;<a href="#fnref:30" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:31">
<p>Kingma, Diederik P. and Ba, Jimmy,  “Adam: A Method for Stochastic Optimization,”  arxiv:1412.6980, published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015.&#160;<a href="#fnref:31" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry><entry><title type="html">Classifying and Contextualizing Edits in Variants with Coleto: Three Versions of Andy Weir’s The Martian</title><link href="https://rlskoeser.github.io/dhqwords/vol/15/4/000579/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://rlskoeser.github.io/dhqwords/vol/15/4/000579/</id><author><name>Erik Ketzan</name></author><author><name>Christof Schöch</name></author><published>2021-12-07T00:00:00+00:00</published><updated>2021-12-07T00:00:00+00:00</updated><content type="html"><![CDATA[<h2 id="1-introduction">1. Introduction</h2>
<p>This paper began with the desire to combine close and distant reading to compare three versions (or variants) of Andy Weir’s science fiction novel,  _The Martian _ (2011, 2014, 2016). We wished to formally describe the changes to the texts made by two professional edits, then interpret how these thousands of textual changes, some big but mostly small, alter readings of the texts. While a number of automatic collation and variant comparison tools exist, none of them easily provide information on the textual changes that we were most interested in when analyzing  <em>The Martian</em> , including: (1) a visualization of edits across the course of the entire texts; (2) all of the edits aligned and isolated from their original textual contexts; and (3) automatic classification of these thousands of edits. As no available tool performs all of these functions, we created one, and now present Coleto, a Python suite which inputs two variant text files (in English, German, or French) and creates a number of outputs: a visualization of edits across the progression of the texts, which indicates sections of the texts that contain greater or fewer edits; a .tsv table containing all of the textual changes presented side-by-side and categorized by type of edit; and visualizations of frequencies of edit types.<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></p>
<p>Interested users, even those with minimal coding experience, may use Coleto to quickly gain a variety of information about their variant texts. Comparing variant texts has a long scholarly history, yet to perform genetic/textual criticism requires a great amount of manual work: scholars place the variants side by side and read them through closely, pen or highlighter in hand. Coleto can reduce the massive effort of manual comparison and provide faster and easier access to all of the edits in variant texts, classified and presented in easy-to-use formats. With Coleto, philologists and scholars of genetic/textual criticism may gain speed, new information and new precision, at worst, and new insights, at best.</p>
<p>Coleto’s primary methodological improvement to collation is our suggested classification of edits in variants. Collation tools have previously provided information on insertion, deletion, and (sometimes) transposition, but the study of some variant texts may be improved by additional rule-based classification: expansion major/minor, condensation major/minor, as well as common orthographic features including punctuation (e.g. commas), hyphenation (e.g.  <em>large-ish</em>  →  <em>largish</em> ), whitespace (e.g.  <em>in to</em>  →  <em>into</em> ), capitalization (e.g.  <em>earth</em>  →  <em>Earth</em> ), and italics (e.g.  <em>again</em>  →  <em>again</em> ). We also defined another type of edit, numbers, for edits common in  <em>The Martian</em> , e.g.  <em>4</em>  →  <em>four</em> , or abbreviations involving numbers, e.g.  <em>80 km</em>  →  <em>80 kilometers</em> .</p>
<h2 id="2-use-case--_the-martian_">2. Use case:  <em>The Martian</em></h2>
<p><em>The Martian</em>  by Andy Weir is a best-selling science fiction novel about an astronaut named Mark Watney who is stranded alone on Mars. Using science and ingenuity, Watney secures food, water, oxygen, and shelter, establishes communication with NASA back on Earth, and finally reaches a spaceship that enables his rescue. The text of  <em>The Martian</em>  exists in three complete variants. Weir originally self-published  <em>The Martian</em>  on his personal website in 2011 (hereafter,  <em>Martian1</em>  or  <em>M1</em> ), then began selling it on Amazon.com in 2012, for 99 cents, where it sold 35,000 copies in three months <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. A major New York publisher, Crown, no doubt impressed by this self-publishing success, bought the rights from Weir, edited the text, and re-released it in 2014 (hereafter,  <em>Martian2</em>  or  <em>M2</em> ). Describing this edit from  <em>Martian1</em>  to  <em>Martian2</em>  in interviews, Weir has said,  “The editing process was pretty smooth. It was not a lot of changes at all”   <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>, as well as, conversely,  “&hellip;there were a lot of edits and changes&hellip; No significant plot changes, nothing like that, but a lot of the wording. It’s much more polished”   <sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>. A third variant was published by Crown in 2016,  <em>The Martian: Classroom Edition</em> , which removes the text’s extensive profanity to market the novel to educational audiences (hereafter,  _Martian3 _ or  <em>M3</em> ). Explaining this decision, Weir stated,  “I got a lot of emails from science teachers who said, ‘Man I’d love to use your book as a teaching aid, but there’s so much profanity in it that we can’t really do that’”   <sup id="fnref1:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>.</p>
<p>Our experiments first seek to formally describe the textual changes in the professional edits to  <em>The Martian</em> , especially as Weir’s inconsistent public statements on the first edit make it unclear how extensive this editing process was. Figure 1 summarizes the textual variation between  <em>Martian1</em> ,  <em>Martian2</em> , and Martian3, as performed by the statistics feature of Wdiff <sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>.<sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup></p>




























<figure ><img loading="lazy" alt="Image of two flow charts describing the first and second edit" src="/dhqwords/vol/15/4/000579/resources/images/image1.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/4/000579/resources/images/image1_hub2f5186be4af1f261c56d453ce1074f2_94226_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/15/4/000579/resources/images/image1_hub2f5186be4af1f261c56d453ce1074f2_94226_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/15/4/000579/resources/images/image1_hub2f5186be4af1f261c56d453ce1074f2_94226_1200x0_resize_q75_box.jpg 1200w,/dhqwords/vol/15/4/000579/resources/images/image1.jpg 1346w" 
     class="landscape"
     ><figcaption>
        <p>Edits from <em>Martian1</em> to _Martian2 _ and <em>Martian2</em> to <em>Martian3</em> as grouped by the statistics feature of Wdiff.[^3]
        </p>
    </figcaption>
</figure>
<p>Quantitatively, Wdiff’s collation statistics lends weight to Weir’s statement that there were  “not a lot of changes at all”  in the first edit to  <em>The Martian</em> , as 92.4% of his self-published version is shared with the Crown Publishers edition. But 7.6% is not an insubstantial textual change, as changing even a few words in a literary text can potentially alter its interpretation. The basic collation provided by Wdiff thus leaves many questions about both edits to  <em>The Martian</em>  unanswered. Are there passages or sections of  <em>The Martian</em>  that received especially high amounts of edits? How many edits comprise the 6,319 words transformed to 9,205 words in the first edit, or the words changed in the second edit? Are the edits mostly minor corrections to e.g. punctuation, or more substantial changes such as added paragraphs? Are the edits long or short? How many of these edits can be automatically classified, and classified how? Did  <em>Martian3</em> , the  “Classroom Edition”  (intended to remove profanity for student readers) alter any text  <em>besides</em>  the profanity? And moving on to interpretation, how do the hundreds of minor textual changes, such as capitalization and the rendering of numbers and scientific abbreviations, alter readings of the texts? To begin to answer these, we apply Coleto to the texts.</p>
<h2 id="3-related-work">3. Related Work</h2>
<p>A number of tools exist to assist in comparison of textual variants. Wdiff compares text files on a word-per-word basis and provides statistics of insertions, deletions, and changes <sup id="fnref1:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>. Archival and philological websites allow parallel visual inspection of variants with annotations such as linguistic information or highlighting <sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>. The Versioning Machine is a long-running open-source tool that displays variant texts and manuscript images side-by-side, with a number of changes highlighted <sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>. Juxta and its web-based interface Juxta Commons provided user-friendly collation and visualization of changes in plain text files <sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup>. Juxta Commons as a web service was shut down in September 2020, although its source code remains online.<sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup>  CollateX is advanced software for textual collation providing users with alignment tables as well as a visualization of variant changes as a word-level graph <sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>. TRAViz provides sophisticated collation and aims to improve on CollateX’s visualization methodology <sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>  <sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup>. Elisa Nury has introduced the PyCoviz tool to visualize collation based on CollateX <sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup>. Other tools not specifically focused on variant comparison could also be useful for variant comparison tasks, for instance TRACER, a suite for detection of text re-use <sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup>.</p>
<p>Different users and their research tasks will be drawn to one or more of these collation tools over the other. While existing collation suites generally approach collation visualization through side-by-side comparison of text chunks or sentence-level comparison, Coleto takes a different approach: to identify and separate all of the edits in variant texts, classified by rule-based methods and presented in output documents and visualizations. This assists our close and distant reading of  <em>The Martian</em> , as we wished to interpret the cumulative effect of hundreds of minor textual changes. As digital humanities collation tools can and do build upon one another, Coleto’s open-source code and methodology for classifying edits can be incorporated into any other collation suite.</p>
<p>In addition to work in digital genetic criticism (e.g. <sup id="fnref:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup>  <sup id="fnref:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup>), previous studies which apply digital methods to the interpretation of variants of relatively recent fiction include Yufang Ho’s comparison of the 1966 and revised 1977 versions of John Fowles’s novel  <em>The Magus</em>   <sup id="fnref:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup>. Martin Paul Eve examined differences in the U.S. and U.K. editions of David Mitchell’s  <em>Cloud Atlas</em> , including collation and comparison of a single character’s narrative, arguing that  “textual variance [is] an element of  <em>Cloud Atlas</em>  that can and should be  <em>read</em> ”   <sup id="fnref:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup>, see also <sup id="fnref:20"><a href="#fn:20" class="footnote-ref" role="doc-noteref">20</a></sup>), an assertion that we follow. In addition, Thomas Crombez and Edith Cassiers have analyzed Luc Perceval’s adaptation of Dostoyevsky’s  <em>The Brothers Karamazov</em>  into a theater play, based on extensive materials from the director’s Dropbox account and using Neil Fraser’s  “diff_match_patch”  library to great effect <sup id="fnref:21"><a href="#fn:21" class="footnote-ref" role="doc-noteref">21</a></sup>.</p>
<h2 id="4-advancing-the-classification-of-edits-in-variant-texts">4. Advancing the classification of edits in variant texts</h2>
<p>Existing schemas to classify edits in variant texts have so far been limited. The established methodology for classifying edits in scholarly editing is based on a distinction between accidentals and substantives, as defined by the widely-used Greg-Bowers tradition, which is also included, for instance, in the MLA Committee on Scholarly Editions’  <em>Guidelines for Editors of Scholarly Editions</em>   <sup id="fnref:22"><a href="#fn:22" class="footnote-ref" role="doc-noteref">22</a></sup>.<sup id="fnref:23"><a href="#fn:23" class="footnote-ref" role="doc-noteref">23</a></sup>  Importantly for digital texts, there is no widely-applicable or widely-followed typology of edits in digital scholarly editing and collation, with different materials calling for different typologies <sup id="fnref:24"><a href="#fn:24" class="footnote-ref" role="doc-noteref">24</a></sup>. We therefore suggest the following categories of edits in variant texts, as integrated in Coleto (Table 1).<br>
Coleto’s classification schema for types of edits in variants of text.    Script-Identifiable Edits  Semantically Open Edits         <strong>Numbers</strong> , e.g.  <em>4</em>  →  <em>four</em> , or abbreviations involving numbers, e.g.  <em>80 km → 80 kilometers</em>      <strong>Hyphenation</strong> , e.g.  <em>large-ish</em>  →  <em>largish</em>    <strong>Punctuation</strong> , e.g. commas   ** Whitespace** , e.g.  <em>in to</em>  →  <em>into</em>    ** Capitalization** , e.g.  <em>earth</em>  →  <em>Earth</em>    <strong>Italics</strong> , e.g.  <em>again</em>  →  <em>again</em>        ** Insertion**    ** Deletion**    ** Expansion (Minor)**    <strong>Expansion (Major)</strong>    <strong>Condensation (Minor)</strong>    ** Condensation (Major)**      <br>
Script-identifiable edits are those that can be confidently classified into types based on formal linguistic features alone; these include changes related to capitalization, whitespace, hyphenation, spelling of numbers, and changes to scientific abbreviations. For the rest of the edits that cannot be easily defined in a rule-based way, we classify them based on the nature and extent of the edit, under the proposed term, Semantically Open Edits: the well-established categories in existing collation tools of deletion and insertion, but supplemented by expansion and condensation categories. For the latter two, a distinction between small and large edits is beneficial, but the choice is (perhaps inevitably) an arbitrary one. For the following experiments, we defined major edits as having a Levenshtein distance greater than 5,<sup id="fnref:25"><a href="#fn:25" class="footnote-ref" role="doc-noteref">25</a></sup>  but users of Coleto may select their own preferred Levenshtein value in the configuration file.</p>
<p>The Gothenburg model of textual collation is described as Tokenization → Normalization → Alignment → Analysis/Feedback → Visualization (Birnbaum and Spadini 2020).<sup id="fnref:26"><a href="#fn:26" class="footnote-ref" role="doc-noteref">26</a></sup>  Coleto’s primary contribution to general collation methodology is to the Analysis step of the Gothenburg model (Figure 2).</p>




























<figure ><img loading="lazy" alt="Image of two flow charts describing Coleto&#39;s edits" src="/dhqwords/vol/15/4/000579/resources/images/image2.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/4/000579/resources/images/image2_hu910f08171952ecf35d934d13ccc39f83_67537_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/15/4/000579/resources/images/image2_hu910f08171952ecf35d934d13ccc39f83_67537_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/15/4/000579/resources/images/image2_hu910f08171952ecf35d934d13ccc39f83_67537_1200x0_resize_q75_box.jpg 1200w,/dhqwords/vol/15/4/000579/resources/images/image2.jpg 1495w" 
     class="landscape"
     ><figcaption>
        <p>Coleto’s features within the Gothenburg model of collation.
        </p>
    </figcaption>
</figure>
<h2 id="5-applying-coleto-to--_the-martian_">5. Applying Coleto to  <em>The Martian</em></h2>
<p>As its target audience is humanities researchers, Coleto is designed for ease of use, so users need only download the code from GitHub, place two variant texts in .txt format in the Data folder, choose configurations in the config.yaml file, and run the run_coleto Python script. Coleto then performs the following tasks: (1) Preprocessing of texts: splits texts into one sentence per line to prepare for alignment; (2) Align texts using Wdiff; (3) Perform analysis: rule-based detection and classification of edits; (4) Generate overview table; and (5) Generate statistics and visualizations.</p>
<p>Coleto also generates a number of outputs, the first being a data table in the .tsv format of all the edits in the variants, aligned in one convenient place (Table 2).<br>
Sample of table generated by Coleto comparing edits from  <em>Martian1</em>  to  <em>Martian2</em> .    itemid  version1  version2  category  main-type  lev-dist  lev-dist-class  lendiff-chars      9298-1  495  494  script-identifiable  numbers  1  minor  0      9299-1  5  five  script-identifiable  numbers  4  minor  3      9299-2  Tau Event  dust storm  other  tbc  9  major  1      9307-1  Adicalia    other  deletion  8  major  -8      9308-1  3000m  3000 meters  script-identifiable  numbers  6  major  6      9308-2  500m  500 meters  script-identifiable  numbers  6  major  6   <br>
Coleto also creates three visualizations. A bar chart visualizes frequencies of Semantically Open Edits, or edits as classified by the statistics feature of Wdiff (insertion and deletion) alongside our suggested Levenshtein-based edit schema for Wdiff’s changes, which we subdivide into Expansion Major/Minor and Condensation Major/Minor (Figure 3).</p>




























<figure ><img loading="lazy" alt="Image of a bar chart" src="/dhqwords/vol/15/4/000579/resources/images/image3.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/4/000579/resources/images/image3_hu0e8f63ab1eda8b6dc8cefe3da421998e_70086_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/15/4/000579/resources/images/image3_hu0e8f63ab1eda8b6dc8cefe3da421998e_70086_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/15/4/000579/resources/images/image3.jpg 1141w" 
     class="landscape"
     ><figcaption>
        <p>Coleto’s visualization of edits between <em>Martian1</em> and <em>Martian2</em> , with Insertion and Deletion as classified by the statistics feature of Wdiff, alongside our Levenshtein-based edit schema for Expansion and Condensation.
        </p>
    </figcaption>
</figure>
<p>This first visualization provides researchers with a bird’s-eye view of changes between two variant texts. And if a researcher wishes to examine, for instance, all of the Expansion Major edits, these are tagged in the .tsv table generated by Coleto. The next visualization is of our suggested, more narrowly defined, script-identifiable edits. We have so far implemented: numbers, punctuation, capitalization, hyphenation, whitespace, and italics (Figure 4).</p>




























<figure ><img loading="lazy" alt="Image of a bar chart" src="/dhqwords/vol/15/4/000579/resources/images/image4.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/4/000579/resources/images/image4_hu3614f11e408af314c1fb135f99a5b587_61603_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/15/4/000579/resources/images/image4_hu3614f11e408af314c1fb135f99a5b587_61603_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/15/4/000579/resources/images/image4.jpg 1176w" 
     class="landscape"
     ><figcaption>
        <p>Coleto’s visualization of script-identifiable edits between <em>Martian1</em> and _ Martian2_ .
        </p>
    </figcaption>
</figure>
<p>Figure 4 displays that 2,226 script-identifiable minor textual edits were detected in the editing process from  _M1 _ to  <em>M2</em> , which provide evidence for a commentator to reconcile author Andy Weir’s conflicting statements on the first edit:  “It was not a lot of changes at all,”  and  “&hellip;there were a lot of edits and changes&hellip; No significant plot changes, nothing like that, but a lot of the wording.”  While changes to hyphenation, capitalization, etc. may be of interest to some research tasks, they are undoubtedly of little interest to others, and the assignment of edits into finer classifications assists researchers in isolating the specific edits they wish to examine.</p>
<p>The third visualization generated by Coleto renders all of the edits across the progression of the texts (Figure 5). Script-identifiable and “other edits” are visualized as different series, with the value of each series calculated by the sum of the absolute Levenshtein distances for each sentence of the text, with Savitzky-Golay smoothing applied.<sup id="fnref:27"><a href="#fn:27" class="footnote-ref" role="doc-noteref">27</a></sup></p>




























<figure ><img loading="lazy" alt="Image of a bar chart" src="/dhqwords/vol/15/4/000579/resources/images/image5.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/4/000579/resources/images/image5_hu8b660e69f920a4dae4741326a1b4fc40_67617_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/15/4/000579/resources/images/image5_hu8b660e69f920a4dae4741326a1b4fc40_67617_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/15/4/000579/resources/images/image5.jpg 1123w" 
     class="landscape"
     ><figcaption>
        <p>Visualization by Coleto of edits between <em>Martian1</em> and <em>Martian2</em> across the progression of the texts (Script-identifiable Edits in red, Semantically Open Edits in blue).
        </p>
    </figcaption>
</figure>
<p>This visualization assists the researcher in obtaining a sense of how edits are distributed across a text, with notable spikes highlighting passages or sections which feature especially high amounts of edits, which may warrant closer inspection.</p>
<h2 id="6-investigating--_the-martian_--with-coleto">6. Investigating  <em>The Martian</em>  with Coleto</h2>
<p>The data and visualizations generated by Coleto may assist in a variety of formal, hermeneutic, and mixed-method studies. In this section, we return to our original goal of investigating textual issues in variants of  <em>The Martian</em> , which can also illustrate Coleto’s benefits for researchers.</p>
<h2 id="61-quantifying-and-interpreting-reductions-in-profanity">6.1 Quantifying and interpreting reductions in profanity</h2>
<p>Profanity is a key stylistic feature of _ The Martian_ , as exemplified by the novel’s opening lines in  <em>M1</em>  and  <em>M2</em> :  “I’m pretty much fucked. That’s my considered opinion. Fucked.”  Profanity may be the most important stylistic feature in the textual history of  <em>The Martian</em> , as the  <em>raison d&rsquo;être</em>  for an entire variant,  <em>M3</em> , the  “Classroom Edition,”  was to remove the novel’s copious profanity for educational audiences. Profanity is also a touchstone for issues of censorship and commercialization in discourse around the film adaptation of  <em>The Martian</em> . While the film only explicitly includes two verbal instances of  <em>fuck</em> , the offending word is also written on screen and alluded to a number of other times, as Jacob Brogan writes:  “[ <em>fuck</em>  is] often suggested in the way that sex might be in a different sort of all-ages movie.”   <sup id="fnref:28"><a href="#fn:28" class="footnote-ref" role="doc-noteref">28</a></sup>. Brogan reads  <em>The Martian</em>  film’s limited use of  <em>fuck</em>  as a metafictional device, the film  “pointedly nodding to its own limitations [&hellip; and] making reference to one of the most bizarre guidelines of the MPAA’s PG-13 rating, the principle that a film can include only one, and in some very rare cases two, (non-sexual) uses of the word.”  Brogan also suggests Watney’s frequent use of  <em>fuck</em>  is emblematic of the character’s psychology:  “Indeed, Watney’s access to the one thing he’s not supposed to say — and his willingness to keep saying it — indexes his indomitability.”  As another impression,  <em>The Martian</em>  foregrounds the use of conversational American English to render almost all of its narrative, a tone underscored by its frequent profanity. An improved formal understanding of profanity in  <em>The Martian</em>  texts could thus support a variety of research questions.</p>
<p>While locating profanity in texts is a matter of simple query, Coleto’s generated tables bring additional speed and convenience to comparing the profanity in  <em>M2</em>  and  <em>M3</em>  (Table 3), while the generated visualizations provide additional context (Figure 6).</p>




























<figure ><img loading="lazy" alt="Image of a bar chart" src="/dhqwords/vol/15/4/000579/resources/images/image6.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/4/000579/resources/images/image6_hue6e0a9e5b4ded18596c6c739c8c8f15f_69981_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/15/4/000579/resources/images/image6_hue6e0a9e5b4ded18596c6c739c8c8f15f_69981_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/15/4/000579/resources/images/image6.jpg 1112w" 
     class="landscape"
     ><figcaption>
        <p>Visualization by Coleto of edits between <em>Martian2</em> and <em>Martian3</em> across the progression of the texts (Script-identifiable Edits in red, Semantically Open Edits in blue).
        </p>
    </figcaption>
</figure>
<p>Note that the y-axis is much smaller here than in Figure 5, as the edit from  _Martian2 _ to  <em>Martian3</em>  (removing and replacing profanity) was much smaller than the edit from  <em>Martian1</em>  to  <em>Martian2</em> . The large spike around sentence 3600 in Figure 6 demonstrates how Coleto highlights areas of large edits which scholars may wish to investigate further. In this case, however, the explanation is mundane: small textual variation surrounding a particularly long string of code that NASA sends to Watney on Mars.<br>
Table 3. Selection of table generated by Coleto comparing edits from  <em>Martian2</em>  to  <em>Martian3</em> .    itemid  version1  version2  category  main-type  lev-dist  lev-dist-class  lendiff-chars      1-1  fucked.  screwed.  other  tbc  5  minor  1      3-1  Fucked.    other  deletion  7  major  -7      4-1    Screwed.  other  insertion  8  major  8      5-1  two months  month  other  condensation  5  minor  -5      48-1  lets  let  other  tbc  1  minor  -1      56-1  shit  crap  other  tbc  4  minor  0      95-1  fucking    other  deletion  7  major  -7      185-1  fucked.  screwed.  other  tbc  5  minor  1      241-1  piss.  pee.  other  tbc  3  minor  -1      280-1  damned  stupid  other  tbc  5  minor  0      309-1  botanist, damn it.  botanist!  other  condensation  10  major  -9   <br>
Manually reviewing the separated edits in Coleto’s generated .tsv table reveals that over 290 of the edits to  <em>Martian3</em>  remove or soften profanity, depending on how profanity is defined,<sup id="fnref:29"><a href="#fn:29" class="footnote-ref" role="doc-noteref">29</a></sup>  in ways presumably acceptable to school boards:  <em>fuck</em>  →  <em>crap</em> ,  _ass _ →  <em>butt</em> ,  <em>piss</em>  →  <em>pee</em> ,  <em>shit</em>  →  <em>crap</em> , etc. As some examples beyond corporeal profanity, Jesus Christ is changed to a secular Wow,  Thank God is removed, and a line involving the epithet  <em>gay</em>  is also altered:</p>
<blockquote>
</blockquote>
<p>[08:31] JPL: Good, keep us posted on any mechanical or electronic problems. By the way, the name of the probe we&rsquo;re sending is Iris. Named after the Greek goddess who traveled the heavens with the speed of wind. She&rsquo;s also the goddess of rainbows.</p>
<p>[08:47] WATNEY: Gay probe coming to save me. Got it.<br>
(<sup id="fnref:30"><a href="#fn:30" class="footnote-ref" role="doc-noteref">30</a></sup> and <sup id="fnref:31"><a href="#fn:31" class="footnote-ref" role="doc-noteref">31</a></sup>)</p>
<p>In  <em>Martian3</em> ,  “Gay probe”  is altered to  “Pride parade probe.”  These changes to  <em>Martian3</em>  were previously noted by Susan Ohanian (2016), who criticized the spirit of the edit and noted inconsistencies, such as another instance of  _Jesus Christ _ that remains in the text <sup id="fnref:32"><a href="#fn:32" class="footnote-ref" role="doc-noteref">32</a></sup>.</p>
<p>None of the bowdlerization in  <em>Martian3</em>  is surprising, as the express purpose of the text was to remove or reduce profanity and other potentially offensive content. But we wished to also know whether  <em>Martian3</em>  altered any text  <em>besides</em>  potentially offensive content — perhaps Weir or his editors used the occasion of a new edition to make some final corrections? — and the table generated by Coleto can quickly answer this question, as well. By manual inspection and sifting of the dataframe, it emerges that  <em>Martian3</em>  also makes factual corrections to scientific details, as well as occasional replacement of continuous verbs and other minor stylistic features (Table 4).<br>
Selection of edits from  <em>Martian2</em>  to  <em>Martian3</em> , including manually added comments.    M2  M3  Comment  Comment2  Levenshtein  Automatic Classification      eighteen  twenty-two  science?    9        most  part  science?    3        sixty seconds  five minutes  science?    10        thirty-two-minute  twenty-two-minute  science?    3        two months  month  science?    5  condensation      grumbled, crossing  grumbled and crossed    passive  7        said, rubbing  rubbed    passive  9  condensation   <br>
By separating and classifying edits, Coleto can also serve as an exploratory research tool by assisting in the spotting of non-obvious trends. For instance, manually inspecting the data table of changes from the earlier  <em>Martian1</em>  to  <em>Martian2</em> , we stumbled upon the discovery that profanity was cut in this first edit, as well. This was a surprise, given the large frequency and foregroundedness of profanity in  <em>Martian2</em> . It emerges that in the first edit of  <em>The Martian</em> , from self-published novel to major publishing house release,  <em>fuck</em>  and  <em>shit</em>  (and their word forms) were substantially reduced, by about 33% and 15%, respectively. Numerous other words and phrases were also softened with lesser profanity, more varied profanity, or non-profanity (e.g. the shit hits the fan → all hell breaks loose) in the first edit.</p>
<p><em>The Martian</em>  thus has the dubious distinction of having been bowdlerized three times: in  <em>M2</em> , which reduced profanity significantly, the thorough scrubbing of profanity in  <em>M3</em> , the  “Classroom Edition,”  and finally in the film adaptation, which abided by a (reported) film board’s code of two  <em>fuck</em> s allowed, but winked at the audience through additional implied profanity. This textual history should inform readings of profanity in any variant or adaptation of  <em>The Martian</em> , and perhaps any discussion of the protagonist Mark Watney’s nonconformist characterization.</p>
<h2 id="62-interpreting-changes-to-numbers-and-abbreviations">6.2 Interpreting changes to numbers and abbreviations</h2>
<p>When comparing variants manually, or even with existing collation tools, it can be difficult to gain a sense of the frequency and distribution of edits, especially very minor stylistic edits. For instance, in the first edit of  <em>The Martian</em> , it is readily apparent that numerous minor stylistic/orthographic changes have been made to the way scientific abbreviations and numbers are written, such as  <em>L</em>  →  <em>liter</em>  and  <em>8</em>  →  <em>eight,</em>  but existing variant analysis tools provide no simple way to quantify these and gain a sense of this type of edit’s distribution.</p>
<p>From the script-identifiable edits of  <em>The Martian</em> , Coleto classifies 868 edits as the rendering of numbers and scientific values in the first edit, from  <em>M1</em>  to  <em>M2</em> . While individual instances of such numerical changes may seem interpretively insignificant, hundreds of them scattered across the text have a  <em>cumulative</em>  effect upon readings of the text. In protagonist Mark Watney’s narration in  <em>Martian1</em> , the use of cardinal numbers and short scientific abbreviations support the fiction that Watney is a scientist working in dangerous conditions, too focused on surviving the life-threatening conditions on Mars to bother writing kilometers or forty-one out in full. Plausibility is a central concern of science fiction <sup id="fnref:33"><a href="#fn:33" class="footnote-ref" role="doc-noteref">33</a></sup>, and when such abbreviations and numerals are expanded in  <em>Martian2</em> , this may increase readability, but weakens the stylistic realism of Watney’s voice.</p>
<p>In  <em>Martian1</em> , however, Weir did not only write  <em>L</em>  for  <em>liter</em>  or  <em>8</em>  for  <em>eight</em>  in the voice of astronaut Mark Watney, but also in the third-person narration of the NASA scientists back on Earth, which has a jarring effect on readability (Table 5):<br>
Selection of passages containing edits classified as numbers by Coleto.      Martian1  Martian2      Watney’s first-person narration in personal journal style  That extra 18kwh of storage will be tough. I&rsquo;ll have to take 2 of the Hab&rsquo;s 9kwh fuel cells and load them on to the rover or trailer. <sup id="fnref1:30"><a href="#fn:30" class="footnote-ref" role="doc-noteref">30</a></sup>  That extra 18 kilowatt-hours of storage will be tough. I&rsquo;ll have to take two of the Hab&rsquo;s 9-kilowatt-hour fuel cells and load them onto the rover or trailer. <sup id="fnref1:31"><a href="#fn:31" class="footnote-ref" role="doc-noteref">31</a></sup>      Character dialogue in the context of third-person narration (NASA scientists)    What&rsquo;s the biggest gap in coverage we have on Watney right now?  Um, Mindy said. Once every 41 hours, we&rsquo;ll have a 17 minute gap. The orbits work out that way.  <sup id="fnref2:30"><a href="#fn:30" class="footnote-ref" role="doc-noteref">30</a></sup>      What&rsquo;s the biggest gap in coverage we have on Watney right now?  Um, Mindy said. Once every forty-one hours, we&rsquo;ll have a seventeen-minute gap. The orbits work out that way.   <sup id="fnref2:31"><a href="#fn:31" class="footnote-ref" role="doc-noteref">31</a></sup></p>
<h2 id="63-formal-identification-of-fixed-scientific-details">6.3 Formal identification of fixed scientific details</h2>
<p>In addition to plausibility, scientific rigor is another central concern of science fiction <sup id="fnref1:33"><a href="#fn:33" class="footnote-ref" role="doc-noteref">33</a></sup>, especially the subgenre of hard science fiction, and Weir pays careful attention to the accuracy of scientific and mathematical explanations throughout  <em>The Martian</em> . Weir went as far as to code a computer program that plotted the trajectory of the fictional  <em>Hermes</em>  spacecraft to ensure scientific accuracy in  <em>The Martian</em> . Weir’s dedication to scientific accuracy was so great that a NASA scientist later reviewed Weir’s trajectory calculations, and concluded that Weir had gotten them right <sup id="fnref:34"><a href="#fn:34" class="footnote-ref" role="doc-noteref">34</a></sup>.</p>
<p>In the process of writing and publishing  <em>Martian1</em>  online, readers occasionally pointed out corrections to mathematics and science presented in the novel, which Weir then incorporated <sup id="fnref:35"><a href="#fn:35" class="footnote-ref" role="doc-noteref">35</a></sup>. While locating these scientific corrections in the variants of  <em>The Martian</em>  would certainly be possible manually, it would be a time-consuming task of scanning the text for single-character changes to numerical digits. The side-by-side data table generated by Coleto, however, greatly speeds up this manual search (Table 6).<br>
Examples of scientific corrections in the edit from  <em>Martian1</em>  to  <em>Martian2</em> , as identified by Coleto’s  <em>numbers</em>  edit classification.    Martian1  Martian2      The answer is a cool  <strong>1000</strong>   The answer is about  <strong>1100</strong>       The atmosphere is  <strong>98%</strong>  CO2  The atmosphere is  <strong>95</strong>  percent CO2      It&rsquo;s a closed system.   It&rsquo;s a closed system. Okay, technically I&rsquo;m lying.  The plants aren&rsquo;t entirely water-neutral. They strip the hydrogen from some of it (releasing the oxygen) and use it to make the complex hydrocarbons  that are the plant itself.</p>
<h2 id="64-interpreting-the-altered-epilogue">6.4 Interpreting the altered epilogue</h2>
<p>The most substantial spike in Coleto’s visualization of edits between  <em>M1</em>  and  <em>M2</em>  in Figure 5, above, occurs at the very end of the novel: the removal of the 263-word epilogue at the end of  <em>Martian1</em>  and a 255-word addition at the end of  <em>Martian2</em> , which substantially alters the novel’s closure. In both  <em>M1</em>  and  <em>M2</em> , Watney, finally safely aboard the rescue spacecraft, is happy to be alive and states,  “This is the happiest day of my life.”  In  <em>Martian1</em> , however, a brief epilogue follows this. Some time after his rescue in space, Watney, now safely back on Earth, is employed by NASA to train the crew of an upcoming space mission. Watney sits upon a bench, killing time, when a young boy recognizes him as the famous astronaut. Watney tolerates the boy’s attention, until the boy poses a question:</p>
<blockquote>
</blockquote>
<p>So Mr. Watney, the boy said, If you could go to Mars again, like, if there was another mission and they wanted you to go, would you go?</p>
<p>Watney scowled at him. You out of your fucking mind?</p>
<p>Ok time to go, the mom said, quickly herding the boy away. They receded in to [sic] the crowded sidewalk.</p>
<p>Watney snorted in their direction. Then he closed his eyes and felt the sun on his face. It was a nice, boring afternoon.<br>
<sup id="fnref3:30"><a href="#fn:30" class="footnote-ref" role="doc-noteref">30</a></sup></p>
<p>This is a typical epilogue in fiction, as defined by the Russian Formalist Boris Eikhenbaum: it sets the perspective by a shift in time and provides some sort of after-history of the major characters <sup id="fnref:36"><a href="#fn:36" class="footnote-ref" role="doc-noteref">36</a></sup>. This epilogue of  <em>M1</em>  may be read to evince the lack of internal growth by Watney, who remains emotionally disengaged and contemptuous of playing the role of hero. Watney’s harrowing adventures on Mars have not altered his nonconformist characterization, a nonconformity encapsulated throughout the text by — and on the final page of the novel, reiterated by — the most foregrounded stylistic feature in the novel: profanity. The epilogue of  <em>M1</em>  also shuts the door on potential sequels, which could be read as authorial and textual nonconformity with commercial expectations for science fiction novels.</p>
<p><em>Martian2</em> , however, cuts this pessimistic epilogue and inserts a substantial amount of optimistic text just before the novel’s end. In  <em>M2</em> ’s new ending, Watney expresses gracious appreciation for all the parties involved in his rescue — his crewmates, NASA, the Chinese space program, the billions of people who hoped for his survival — and affirms a widespread faith in human nature:</p>
<blockquote>
</blockquote>
<p>The cost for my survival must have been hundreds of millions of dollars. All to save one dorky botanist. Why bother?</p>
<p>Well, okay. I know the answer to that. Part of it might be what I represent: progress, science, and the interplanetary future we’ve dreamed of for centuries. But really, they did it because every human being has a basic instinct to help each other out. It might not seem that way sometimes, but it’s true.<br>
<sup id="fnref3:31"><a href="#fn:31" class="footnote-ref" role="doc-noteref">31</a></sup></p>
<p>This greatly alters the tone of the novel’s ending, revises Watney’s growth and characterization, and now leaves open the possibility of sequels. The correlation between reduced profanity in  <em>M2</em>  and Watney’s character is notable: as the text itself becomes more conformist, so does its protagonist.</p>
<h2 id="7-conclusion-and-future-work">7. Conclusion and future work</h2>
<p>A traditional scholar who sat down to compare the first two variants of  <em>The Martian</em>  would possibly, but possibly not, have noticed the reduced profanity, although the changed epilogue is impossible to miss. If such a scholar were sufficiently rigorous and willing to invest the necessary time, they would undoubtedly locate the corrected science and note the changes to abbreviations, numerals, and other minor stylistic aspects, as well. Coleto, like any collation tool, does not seek to replace such diligent genetic criticism, but to supplement it with speedier information and new precision, as well as provide a method that scales to longer texts which may challenge the time and patience of scholars performing manual comparison of variant texts.</p>
<p>Coleto could naturally be adapted per research topic, and may hopefully be useful for critical interest in, inter alia, variants of fiction, the role of the editor, and text re-use. Some tantalizing use cases for comparing variants of fiction come from the rapidly emerging global phenomenon of digital self-published best-sellers,<sup id="fnref:37"><a href="#fn:37" class="footnote-ref" role="doc-noteref">37</a></sup>  in which, like  <em>The Martian</em> , texts often exist in a self-published variant as well as a later professionally-edited variant.</p>
<p>A significant weakness of Coleto is its inability to detect transpositions, as it relies on Wdiff for collation. For instance, Wdiff does not detect if a sentence is removed, but then replaced a few pages later in the novel — Coleto would classify this as an unrelated deletion and insertion. While Coleto could be integrated with transposition-detecting software in the future, for now, a pragmatic research suggestion would be to use Coleto in tandem with CollateX <sup id="fnref1:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>, as described e.g. in <sup id="fnref:38"><a href="#fn:38" class="footnote-ref" role="doc-noteref">38</a></sup>. Comparing  <em>Martian1</em>  and  <em>Martian2</em>  in CollateX results in 98 transpositions detected. Of these, 10 involve punctuation and should be considered artefacts of the method. Another 33 represent transpositions of a single word, showing stylistic preferences on the word-order level. Yet another 21 concern multi-word expressions which change the overall construction of a sentence or paragraph more substantially. Finally, 34 concern segments of more than 5 words, typically in the range of a short phrase to several sentences. In all cases, the scope of movement is rather small, with transpositions remaining very localized phenomena. This inspection of transpositions in this test case shows that, quantitatively and above all qualitatively, transpositions were not a major part of the first edit to  <em>The Martian</em> , but naturally this step might play a greater role in variants of other texts.</p>
<p>For further work on Coleto, we plan to pursue the quantitative investigation into the semantics of edits by using word embedding models, thereby quantifying the amount of actual semantic distance between two tokens in an edit, rather than the surface-level Levenshtein distance. Some notable challenges we expect are defining edits concerning spelling mistakes (where misspelled words may not be in a model’s vocabulary) and edits involving different numbers of words (where it is not obvious which words establish semantic distance).</p>
<h2 id="acknowledgements">Acknowledgements</h2>
<p>Thanks to Martin Paul Eve, the audience of our talk at Digital Humanities 2017 in Montreal, and the anonymous reviewers of this journal for excellent feedback on various versions of this project.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Coleto is available on GitHub: <a href="https://github.com/dh-trier/coleto">https://github.com/dh-trier/coleto</a>, <a href="https://doi.org/10.5281/zenodo.4569328">https://doi.org/10.5281/zenodo.4569328</a>.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Alter, A. (2017).  “Andy Weir’s Best Seller ‘The Martian’ Gets a Classroom-Friendly Makeover.”    <em>The New York Times</em> , February 24, 2017, <a href="https://www.nytimes.com/2017/02/24/business/andy-weirs-best-seller-the-martian-gets-a-classroom-friendly-makeover.html">https://www.nytimes.com/2017/02/24/business/andy-weirs-best-seller-the-martian-gets-a-classroom-friendly-makeover.html</a>.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Savage, A. (2015).  “Adam Savage Interviews &lsquo;The Martian&rsquo; Author Andy Weir - The Talking Room.”  [Online Video]. 11 June 2015. Available from: <a href="https://www.youtube.com/watch?v=5SemyzKgaUU">https://www.youtube.com/watch?v=5SemyzKgaUU</a>. [Accessed: 29 December 2017].&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Debic, B. (2014).  “The Martian | Andy Weir | Talks at Google.”  [Online Video]. 26 February 2014. Available from: <a href="https://www.youtube.com/watch?v=gMfuLtjgzA8">https://www.youtube.com/watch?v=gMfuLtjgzA8</a>. [Accessed: 29 December 2017].&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>von Gagern. (2014). GNU Wdiff. <a href="https://www.gnu.org/software/wdiff/">https://www.gnu.org/software/wdiff/</a>&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>Wdiff, <a href="https://www.gnu.org/software/wdiff/">https://www.gnu.org/software/wdiff/</a>&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>van Dalen-Oskam. (2015).  “In Praise of the Variant Analysis Tool: A Computational Approach to Medieval Literature.”  In  <em>Texts, Transmissions, Receptions: Modern Approaches to Narratives</em> . Edited by André Lardinois, Sophie Levie, Hans Hoeken and Christoph Lüthy. Leiden: Brill, pp. 35-54.&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>Schreibman et al. (2003). The Versioning Machine. <a href="http://v-machine.org">http://v-machine.org</a> (accessed January 5, 2021).&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>Wheeles D., Jensen K. (2013).  “Juxta Commons.”  In  <em>Proceedings of the Digital Humanities 2013</em> . University of Nebraska-Lincoln, 17 July 2013. <a href="http://dh2013.unl.edu/abstracts/ab-142.html">http://dh2013.unl.edu/abstracts/ab-142.html</a>&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p><a href="https://github.com/performant-software/juxta-service">https://github.com/performant-software/juxta-service</a>&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>Dekker, R. and Middell, G. (2011).  “Computer-Supported Collation with CollateX: Managing Textual Variance in an Environment with Varying Requirements.”    <em>Supporting Digital Humanities 2011</em> . University of Copenhagen, Denmark. 17-18 November 2011.&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p>Jänicke, S., and Wrisley, D.J. (2017).  “Visualizing Mouvance: Toward a visual analysis of variant medieval text traditions.”  Digital Scholarship in the Humanities, Volume 32, Issue suppl_2, Pages ii106–ii123, <a href="https://doi.org/10.1093/llc/fqx033">https://doi.org/10.1093/llc/fqx033</a>&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13">
<p>Jänicke S., Geßner A., Franzini G., Terras M., Mahony S., Scheuermann G. (2015).  “TRAViz: A Visualization for Variant Graphs.”  In Digital Scholarship in the Humanities , 30(suppl 1): i83–99.&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14">
<p>Nury, E. (2019).  “Visualizing Collation Results.”    <em>Varia</em>  14. <a href="https://journals.openedition.org/variants/950">https://journals.openedition.org/variants/950</a>&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:15">
<p>Büchler, M., Burns, P. R., Müller, M., Franzini, E., Franzini, G. (2014)  “Towards a Historical Text Re-use Detection.”  In: Biemann, C. and Mehler, A. (eds.)  <em>Text Mining, Theory and Applications of Natural Language Processing</em> . Springer International Publishing Switzerland.&#160;<a href="#fnref:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:16">
<p>van Hulle, D. (2008).  <em>Manuscript Genetics, Joyce’s Know-How, Beckett’s Nohow</em> . Gainesville: University Press of Florida.&#160;<a href="#fnref:16" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:17">
<p>Ferrer, D. (2011).  <em>Logiques du brouillon: Modèles pour une critique génétique</em> . Paris: Seuil.&#160;<a href="#fnref:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:18">
<p>Ho, Y. (2011).  <em>Corpus Stylistics in Principles and Practice: A Stylistic Exploration of John Fowles’ The Magus</em> . New York: Continuum.&#160;<a href="#fnref:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:19">
<p>Eve, M. P. (2016).  “‘You have to keep track of your changes’: The Version Variants and Publishing History of David Mitchell’s  <em>Cloud Atlas</em> .”    _Open Library of Humanities _ 2:2, <a href="https://olh.openlibhums.org/article/10.16995/olh.82/">https://olh.openlibhums.org/article/10.16995/olh.82/</a>&#160;<a href="#fnref:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:20">
<p>Eve, M. P. (2019).  <em>Close Reading With Computers</em> . Stanford: Stanford University Press.&#160;<a href="#fnref:20" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:21">
<p>Crombez, Th., Cassiers, E. (2017).  “Postdramatic methods of adaptation in the age of digital collaborative writing” .  <em>Digital Scholarship in the Humanities</em>  32.1, 17-35.&#160;<a href="#fnref:21" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:22">
<p>Modern Language Association (2011).  “Reports from the MLA Committee on Scholarly Editions, Guidelines for Editors of Scholarly Editions.”   <a href="https://www.mla.org/Resources/Research/Surveys-Reports-and-Other-Documents/Publishing-and-Scholarship/Reports-from-the-MLA-Committee-on-Scholarly-Editions/Guidelines-for-Editors-of-Scholarly-Editions">https://www.mla.org/Resources/Research/Surveys-Reports-and-Other-Documents/Publishing-and-Scholarship/Reports-from-the-MLA-Committee-on-Scholarly-Editions/Guidelines-for-Editors-of-Scholarly-Editions</a>&#160;<a href="#fnref:22" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:23">
<p>Greg-Bowers was originally designed for the disciplinary goal of interpreting authorial intention, and although authorial intention as the dominant paradigm of textual criticism and scholarly editing was disrupted in the 1980s, notably with Jerome J. McGann’s  <em>A Critique of Modern Textual Criticism</em>  (1983), and scholars have continued to evolve the goals of textual criticism and scholarly editing, the fundamental methodological categories of the Greg-Bowers tradition have persisted. Scholars are not unanimous in supporting Greg-Bowers, for instance G. Thomas Tanselle, who found the terms accidentals and substantives to be  “misleading and often untenable in their implication of a firm distinction in all cases”   <sup id="fnref:39"><a href="#fn:39" class="footnote-ref" role="doc-noteref">39</a></sup>.&#160;<a href="#fnref:23" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:24">
<p>TEI-L (2016). Types of Edits. TEI-List. <a href="http://tei-l.970651.n3.nabble.com/Types-of-edits-tp4028495.html">http://tei-l.970651.n3.nabble.com/Types-of-edits-tp4028495.html</a>&#160;<a href="#fnref:24" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:25">
<p>Levenshtein distance is a metric for quantifying the difference between two strings: this counts each character-level addition, deletion or transformation required to turn one string into another string. Each individual edit may concern several directly adjacent words, which is why many edits involve Levenshtein distances higher than 5. In the edit from  <em>M1 _ to _ M2</em> , 2634 (or 49.7%) of the edits are minor and 2664 (or 50.3%) are major, according to our definition. The median Levenshtein difference we observed was 4. For a discussion of this and other methods of approximate string matching, see <sup id="fnref:40"><a href="#fn:40" class="footnote-ref" role="doc-noteref">40</a></sup>.&#160;<a href="#fnref:25" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:26">
<p>The Interedition Development Group, “The Gothenburg Model”, <a href="https://collatex.net/doc/#gothenburg-model">https://collatex.net/doc/#gothenburg-model</a>&#160;<a href="#fnref:26" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:27">
<p>Savitzky-Golay filtering is a smoothing algorithm relying on a least-squares based, best-fit method, originally developed to filter out noise-induced variation in data while producing minimal distortions of the actual signal <sup id="fnref:41"><a href="#fn:41" class="footnote-ref" role="doc-noteref">41</a></sup>.&#160;<a href="#fnref:27" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:28">
<p>Brogan, J. (2015).  “Ridley Scott’s  <em>The Martian</em>  Has Far Less Profanity Than the Book — but Its F-Bombs Are Perfect.”    <em>Slate</em> , October 6, 2015. <a href="https://slate.com/culture/2015/10/profanity-in-ridley-scotts-the-martian-the-film-drops-so-many-fewer-f-bombs-than-andy-weir-s-book.html">https://slate.com/culture/2015/10/profanity-in-ridley-scotts-the-martian-the-film-drops-so-many-fewer-f-bombs-than-andy-weir-s-book.html</a>&#160;<a href="#fnref:28" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:29">
<p>The definition of profanity varies by authority. In Tony McEnery’s computational linguistic investigation of profanity,  <em>Swearing in English</em> , McEnery queries a list of  “bad language words”  or BLWs  “partly guided by claims within the literature, partly by my own intuition.”   <sup id="fnref:42"><a href="#fn:42" class="footnote-ref" role="doc-noteref">42</a></sup>.&#160;<a href="#fnref:29" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:30">
<p>Weir, A. (2011).  <em>The Martian</em> . Self-published.&#160;<a href="#fnref:30" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:30" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:30" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref3:30" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:31">
<p>Weir, A. (2014).  <em>The Martian</em> . New York: Crown Publishing Group.&#160;<a href="#fnref:31" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:31" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:31" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref3:31" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:32">
<p>Ohanian, S. (2016).  “Classroom Edition: Dumb-Ass Fiddling with a Smart-Ass Book” .  <em>Schools Matter</em> . <a href="http://www.schoolsmatter.info/2016/07/classroom-edition-dumb-ass-fiddling.html">http://www.schoolsmatter.info/2016/07/classroom-edition-dumb-ass-fiddling.html</a>&#160;<a href="#fnref:32" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:33">
<p>Stockwell, P. (2000).  <em>The Poetics of Science Fiction</em> . Oxon: Routledge.&#160;<a href="#fnref:33" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:33" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:34">
<p>Burke, L. (2015).  “An Examination of  “The Martian”  Trajectory.”    <em>NASA</em> , 1-13.&#160;<a href="#fnref:34" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:35">
<p>Dickerson, K. (2015).  “Some of the trickiest science in &lsquo;The Martian&rsquo; came from the book&rsquo;s biggest fans.”    <em>Business Insider</em> , October 8, 2015. <a href="https://www.businessinsider.com/andy-weir-the-martian-science-crowdsourcing-2015-10?r=US&amp;IR=T">https://www.businessinsider.com/andy-weir-the-martian-science-crowdsourcing-2015-10?r=US&amp;IR=T</a>&#160;<a href="#fnref:35" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:36">
<p>Torgovnick, M. (1981).  <em>Closure in the Novel</em> . Princeton: Princeton University Press.&#160;<a href="#fnref:36" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:37">
<p>Examples of this phenomenon include  <em>Fifty Shades of Grey</em>  (originally published as fan-fiction and later at author E.L. James’ website) and  <em>Wu Kong</em>  (a Chinese Internet novel turned big-budget action film). Some 40 self-published authors on Amazon had sold over a million copies of their e-books by 2016 <sup id="fnref:43"><a href="#fn:43" class="footnote-ref" role="doc-noteref">43</a></sup>, while Hollywood is reportedly snapping up self-published authors in 2017 <sup id="fnref:44"><a href="#fn:44" class="footnote-ref" role="doc-noteref">44</a></sup>.&#160;<a href="#fnref:37" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:38">
<p>Schöch, C. (2016).  “Detecting Transpositions when Comparing Text Versions using CollateX.”    <em>The Dragonfly’s Gaze</em> . <a href="http://dragonfly.hypotheses.org/954">http://dragonfly.hypotheses.org/954</a>&#160;<a href="#fnref:38" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:39">
<p>Greetham, D.C. (1992).  <em>Textual Scholarship: An Introduction</em> . New York: Garland.&#160;<a href="#fnref:39" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:40">
<p>Navarro, G. (2001).  “A guided tour to approximate string matching.”    <em>ACM Computing Surveys</em> . 33(1): 31–88. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier">doi</a>:<a href="https://dx.doi.org/10.1145%2F375360.375365">10.1145/375360.375365</a>.&#160;<a href="#fnref:40" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:41">
<p>Savitzky, A. and Golay, M.J.E. (1964).  “Smoothing and Differentiation of Data by Simplified Least Squares Procedures” .  <em>Analytical Chemistry</em>  38(8): 1727-1639. DOI: 10.1021/ac60214a047&#160;<a href="#fnref:41" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:42">
<p>McEnery, T. (2006).  <em>Swearing in English: Bad language, purity, and power from 1586 to the presen</em> t. London: Routledge.&#160;<a href="#fnref:42" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:43">
<p>Alter, A. (2016).  “Meredith Wild, a Self-Publisher Making an Imprint.”  _ The New York Times_ , January 30, 2016, <a href="https://www.nytimes.com/2016/01/31/business/media/meredith-wild-a-self-publisher-making-an-imprint.html">https://www.nytimes.com/2016/01/31/business/media/meredith-wild-a-self-publisher-making-an-imprint.html</a>.&#160;<a href="#fnref:43" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:44">
<p>Kean, D. (2017).  “‘Show me the money!’: the self-published authors being snapped up by Hollywood,”    <em>The Guardian</em> , May 15, 2017, <a href="https://www.theguardian.com/books/2017/may/15/self-published-authors-hollywood-andy-weir-the-martian-el-james">https://www.theguardian.com/books/2017/may/15/self-published-authors-hollywood-andy-weir-the-martian-el-james</a>&#160;<a href="#fnref:44" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry><entry><title type="html">Compounded Mediation: A Data Archaeology of the Newspaper Navigator Dataset</title><link href="https://rlskoeser.github.io/dhqwords/vol/15/4/000578/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://rlskoeser.github.io/dhqwords/vol/15/4/000578/</id><author><name>Benjamin Lee</name></author><published>2021-12-07T00:00:00+00:00</published><updated>2021-12-07T00:00:00+00:00</updated><content type="html"><![CDATA[<h2 id="i-an-introduction-to-the-newspaper-navigator-dataset">I. An Introduction to the Newspaper Navigator Dataset</h2>
<p>In partnership with LC Labs, the National Digital Newspaper Program, and IT Design &amp; Development at the Library of Congress, as well as Professor Daniel Weld at the University of Washington, I constructed the  <em>Newspaper Navigator</em>  dataset as the first phase of my Library of Congress Innovator in Residence project, _ Newspaper Navigator_ .<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>  The project has its origins in  <em>Chronicling America</em> , a database of digitized historic American newspapers created and maintained by the National Digital Newspaper Program, itself a partnership between the Library of Congress and the National Endowment for the Humanities. Content in  <em>Chronicling America</em> is contributed by state partners of the National Digital Newspaper Program who have applied for and received awards from the Division of Preservation and Access at the National Endowment for the Humanities <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. At the time of the construction of the  <em>Newspaper Navigator</em>  dataset in March, 2020,  <em>Chronicling America</em>  contained approximately 16.3 million digitized historic newspaper pages published between 1789 and 1963, covering 47 states as well as Washington, D.C. and Puerto Rico. The technical specifications of the National Digital Newspaper Program require that each digitized page in  <em>Chronicling America</em>  comprises the following digital artifacts <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>:</p>
<p>A page image in two raster formats:<br>
Grayscale, scanned for maximum resolution possible between 300-400 DPI, relative to the original material, uncompressed TIFF 6.0   Same image, compressed as JPEG2000</p>
<pre><code>Optical character recognition (OCR) text and associated bounding boxes for words (one file per page image)  PDF Image with Hidden Text, i.e., with text and image correlated  Structural metadata (a) to relate pages to title, date, and edition; (b) to sequence pages within issue or section; and (c) to identify associated image and OCR files  Technical metadata to support the functions of a trusted repository  
</code></pre>
<p>Additional artifacts and metadata are contributed for each digitized newspaper issue and microfilm reel. All digitized pages are in the public domain and are available online via a public search user interface,<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>  making  <em>Chronicling America</em>  an immensely rich resource for the American public.</p>
<p>The central goal of  _Newspaper Navigator _ is to re-imagine how the American public explores  <em>Chronicling America</em>  by utilizing emerging machine learning techniques to extract, categorize, and search over the visual content and headlines in  <em>Chronicling America</em> ’s 16.3 million pages of digitized historic newspapers.  <em>Newspaper Navigator</em>  was both inspired and directly enabled by the Library of Congress’s  <em>Beyond Words</em>  crowdsourcing initiative <sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>. Launched by LC Labs in 2017,  <em>Beyond Words</em>  engages the American public by asking volunteers to identify and draw boxes around photographs, illustrations, maps, comics, and editorial cartoons on World War I-era pages in  <em>Chronicling America</em> , note the visual content categories, and transcribe the relevant textual information such as titles and captions.<sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>  The thousands of annotations created by  _Beyond Words _ volunteers are in the public domain and available for download online.  _Newspaper Navigator _ directly builds on  <em>Beyond Words</em>  by utilizing these annotations, as well as additional annotations of headlines and advertisements, to train a machine learning model to detect visual content in historic newspapers.<sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>  Because  <em>Beyond Words</em>  volunteers were asked to draw bounding boxes to include any relevant textual content, such as a photograph’s title, this machine learning model learns during training to include relevant textual content when predicting bounding boxes.<sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>  Furthermore, in the  _Transcribe _ step of  <em>Beyond Words</em> , the system provided the OCR with each bounding box as an initial transcription for the volunteer to correct; inspired by this, the  <em>Newspaper Navigator</em>  pipeline automatedly extracts the OCR falling within each predicted bounding box in order to provide noisy textual metadata for each image. In the case of headlines, this method enables the headline text to be directly extracted from the bounding box predictions. Lastly, the pipeline generates image embeddings for the extracted visual content using an image classification model trained on ImageNet.<sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup>  A diagram of the full  <em>Newspaper Navigator</em>  pipeline can be found in Figure 1.</p>




























<figure ><img loading="lazy" alt="A diagram of newspaper screenshots" src="/dhqwords/vol/15/4/000578/resources/images/image1.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/4/000578/resources/images/image1_hu029dda16cde63a400ee842ebc3c8ad63_247356_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/4/000578/resources/images/image1_hu029dda16cde63a400ee842ebc3c8ad63_247356_800x0_resize_box_3.png 800w,/dhqwords/vol/15/4/000578/resources/images/image1.png 960w" 
     class="landscape"
     ><figcaption>
        <p>A diagram showing the <em>Newspaper Navigator</em> pipeline, which processed over 16.3 million historic newspaper pages in <em>Chronicling America</em> , resulting in the <em>Newspaper Navigator</em> dataset.
        </p>
    </figcaption>
</figure>
<p>Over the course of 19 days from late March to early April of 2020, the  <em>Newspaper Navigator</em>  pipeline processed 16.3 million pages in  <em>Chronicling America</em> ; the resulting  _Newspaper Navigator _ dataset was publicly released in May, 2020. The full dataset, as well as all code written for this project, are available online and have been placed in the public domain for unrestricted re-use.<sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup>  Currently, the  <em>Newspaper Navigator</em>  dataset can be queried using HTTPS and Amazon S3 requests. Furthermore, hundreds of pre-packaged datasets have been made available for download, along with associated metadata. These pre-packaged datasets consist of different types of visual content for each year, from 1850 to 1963, allowing users to download, for example, all of the maps from 1863 or all of the photographs from 1910. For more information on the technical aspects of the pipeline and the construction of the  <em>Newspaper Navigator</em>  dataset, I refer the reader to <sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup></p>
<h2 id="ii-why-a-data-archaeology">II. Why a Data Archaeology?</h2>
<p>As machine learning and artificial intelligence play increasing roles in digitization and digital content stewardship, the Libraries, Archives, and Museums (LAM) community has repeatedly emphasized the importance of ensuring that these emerging methodologies are incorporated ethically and responsibly. Indeed, a major theme that emerged from the  “Machine Learning + Libraries Summit”  hosted by LC Labs in September, 2019, was that  “there is much more  “human”  in machine learning than the name conveys”  and that transparency and communication are first steps toward addressing the  “human subjectivities, biases, and distortions”  embedded within machine learning systems <sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>. This data archaeology has been written in support of this call for transparency and responsible stewardship, which is echoed in the Library of Congress’s Digital Strategy, as well as the recommendations in Ryan Cordell’s report to the Library of Congress  “ML + Libraries: A Report on the State of the Field,”  Thomas Padilla’s OCLC position paper  “Responsible Operations: Science, Machine Learning, and AI in Libraries” , and the University of Nebraska-Lincoln’s report on machine learning to the Library of Congress <sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup>; <sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup>; <sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup>; <sup id="fnref:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup>. I write this data archaeology from my perspective of having created the dataset, and although I am not without my own biases, I have attempted to represent my work as honestly as possible. Accordingly, I seek not only to document the construction of the  _Newspaper Navigator _ dataset through the lens of data stewardship but also to critically examine the dataset’s limitations. In doing so, I advocate for the importance of autoethnographic approaches to documenting a cultural heritage dataset’s construction from a humanistic perspective.</p>
<p>This article draws inspiration from recent works in media and data archaeology, including Paul Fyfe’s  “An Archaeology of Victorian Newspapers” ; Bonnie Mak’s  “Archaeology of a Digitization” ; Kate Crawford and Trevor Paglen’s  “Excavating AI: The Politics of Images in Machine Learning Training Sets” ; and, most directly, Ryan Cordell’s  “Qi-jtb the Raven: Taking Dirty OCR Seriously,”  in which Cordell traces the digitization of a single issue of the  <em>Lewisburg Chronicle</em>  from its selection by the Pennsylvania Digital Newspaper Project to its ingestion into the  _Chronicling America _ online database, with a focus on the distortive effects of OCR <sup id="fnref:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup>; <sup id="fnref:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup>; <sup id="fnref:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup>; <sup id="fnref:20"><a href="#fn:20" class="footnote-ref" role="doc-noteref">20</a></sup>. As argued by Trevor Owens and Thomas Padilla, it is essential to  “document how digitization practices and how the affordances of particular sources … produce unevenness in the discoverability and usability of collections”   <sup id="fnref:21"><a href="#fn:21" class="footnote-ref" role="doc-noteref">21</a></sup>. Recent works within the machine learning literature have analogously emphasized the importance of documenting the collection and curation efforts underpinning community datasets and machine learning models. Reporting mechanisms include  “Datasheets for Datasets,”    “Dataset Nutrition Labels,”    “Data Statements for NLP,”    “Model Cards for Model Reporting,”  and  “Algorithmic Impact Assessments”   <sup id="fnref:22"><a href="#fn:22" class="footnote-ref" role="doc-noteref">22</a></sup>; <sup id="fnref:23"><a href="#fn:23" class="footnote-ref" role="doc-noteref">23</a></sup>; <sup id="fnref:24"><a href="#fn:24" class="footnote-ref" role="doc-noteref">24</a></sup>; <sup id="fnref:25"><a href="#fn:25" class="footnote-ref" role="doc-noteref">25</a></sup>; <sup id="fnref:26"><a href="#fn:26" class="footnote-ref" role="doc-noteref">26</a></sup>. This case study adopts a similar framing in stressing the importance of reporting mechanisms, with a particular focus on the data archaeology in the context of cultural heritage datasets.</p>
<p>In the following sections, I trace the digitization process and data flow for  <em>Newspaper Navigator</em> , beginning with the physical artifact of the newspaper itself and ending with the machine learning predictions that constitute the  _Newspaper Navigator _ dataset, reflecting on each step through the lens of discoverability and erasure. In particular, I study four different  _Chronicling America _ Black newspaper pages published in 1910, each depicting the same photograph of W.E.B. Du Bois, as the pages move through the  <em>Chronicling America</em>  and  <em>Newspaper Navigator</em>  pipelines. All four pages reproduce the same article by Franklin F. Johnson, a reporter from  _The Baltimore Afro-American _   <sup id="fnref:27"><a href="#fn:27" class="footnote-ref" role="doc-noteref">27</a></sup>; the headline is as follows:</p>
<p>NEW MOVEMENT</p>
<p>BEGINS WORK</p>
<p>Plan and Scope of the Asso-</p>
<p>ciation Briefly Told.</p>
<p>Will Publish the Crisis.</p>
<p>Review of Causes Which Led to the</p>
<p>Organization of the Association in</p>
<p>New York and What Its Policy Will</p>
<p>Be-Career and Work of Professor</p>
<p>W.E.B. Du Bois</p>
<p>The article describes the creation of the National Association for the Advancement of Colored People (NAACP), details W.E.B. Du Bois’s background, and announces the launch of  <em>The Crisis</em> , the official magazine of the NAACP, with Du Bois as Editor-in-Chief. The four pages comprise the front page of the October 14th, 1910, issue of the  _Iowa State Bystander _   <sup id="fnref:28"><a href="#fn:28" class="footnote-ref" role="doc-noteref">28</a></sup>; the 16th page of the October 15th, 1910, issue of  _Franklin’s Paper the Statesman _   <sup id="fnref:29"><a href="#fn:29" class="footnote-ref" role="doc-noteref">29</a></sup>; and the 2nd and 3rd pages of the October 15th, 1910, and November 26th, 1910, issues of  <em>The Broad Ax</em> , respectively <sup id="fnref:30"><a href="#fn:30" class="footnote-ref" role="doc-noteref">30</a></sup>; <sup id="fnref:31"><a href="#fn:31" class="footnote-ref" role="doc-noteref">31</a></sup>. All four digitized pages are reproduced in the Appendix.</p>
<h2 id="iii--_chronicling-america_--a-genealogy-of-collecting-microfilming-and-digitizing">III.  <em>Chronicling America</em> : A Genealogy of Collecting, Microfilming, and Digitizing</h2>
<p>Any examination of  _Newspaper Navigator _ must begin with the genealogy of collecting, microfilming, and digitizing that dictates which newspapers have been ingested into the  _Chronicling America _ database. The question of what to digitize is, in practice, answered and realized incrementally over decades, beginning at its most fundamental level with the question of which newspapers have survived and which have been reduced to lacunae in the historical record <sup id="fnref:32"><a href="#fn:32" class="footnote-ref" role="doc-noteref">32</a></sup>. <sup id="fnref:33"><a href="#fn:33" class="footnote-ref" role="doc-noteref">33</a></sup>  Historic newspapers present challenges for digitization in part due to the ephemerality of the physical printed newspaper itself: many newspapers were microfilmed and immediately discarded due to a fear that the physical pages would deteriorate.<sup id="fnref:34"><a href="#fn:34" class="footnote-ref" role="doc-noteref">34</a></sup>  Indeed, almost all of the pages included in  <em>Chronicling America</em>  have been digitized directly from microfilm. In the next section, I will examine the microfilm imaging process in more detail; however, in most cases, librarians selected newspapers for collecting and microfilming decades before the National Digital Newspaper Program was launched in 2004. These selections were informed by a range of factors including historical significance - itself a subjective, nebulous, and ever-evolving notion that has historically served as the basis for perpetuating oppression within the historical record. In “Chronicling White America,” Benjamin Fagan highlights the paucity of Black newspapers in  <em>Chronicling America</em> , in particular in relation to pre-Civil War era newspapers [Fagan 2016]. It is imperative to remember that this paucity can directly be traced back decades to the collecting and preserving stages.<sup id="fnref:35"><a href="#fn:35" class="footnote-ref" role="doc-noteref">35</a></sup></p>
<p>In regard to collecting, the newspaper page is both an informational object (i.e., the newspaper page as defined by its content) and a material object (i.e., the specific printed copy of the newspaper page) <sup id="fnref:36"><a href="#fn:36" class="footnote-ref" role="doc-noteref">36</a></sup>. At some point in time, librarians accessioned a specific copy of each printed page and microfilmed it or contracted out the microfilming. The materiality of that specific printed page is a confluence of unique ink smudges, rips, creases, and page alignment, much of which is captured in the microfilm imaging process. Though we may not make much of a crease or a smudge on a digitized page when we find it in the  <em>Chronicling America</em>  database, it can very well take on a life of its own with a machine learning algorithm in  <em>Newspaper Navigator</em> . The machine learning algorithm might deem two newspaper photographs as similar simply due to the presence of creases or smudges, even if the photographs are easily discernible to the naked eye, or the smudges are of entirely different origin (i.e., a printing imperfection versus a smudge from a dirty hand).</p>
<p>It is only by foregrounding these subtleties of the collection, preservation, and microfilming processes that we can understand the selection process for  <em>Chronicling America</em>  in its proper context. The grant-seeking process dictates selection criteria for  _Chronicling America _ by which state-level institutions including state libraries, historical societies, and universities apply for two years of grant funding from the National Digital Newspaper Program via the Division of Preservation and Access at the National Endowment for the Humanities. With the awarding of a grant, a state-level awardee then digitizes approximately 100,000 newspaper pages published in their state for inclusion in  <em>Chronicling America</em>   <sup id="fnref1:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>; <sup id="fnref:37"><a href="#fn:37" class="footnote-ref" role="doc-noteref">37</a></sup>. The grant-seeking and awarding process is nuanced, but salient points include that state-level applicants must assemble an advisory board including scholars, teachers, librarians, and archivists to aid in the selection of newspapers, and grants are reviewed by National Endowment for the Humanities staff, as well as peer reviewers.<sup id="fnref:38"><a href="#fn:38" class="footnote-ref" role="doc-noteref">38</a></sup></p>
<p>Regarding selection criteria for newspaper titles, the National Digital Newspaper Program defines the following factors for state-level awardees to consider for content selection after a newspaper is determined to be in the public domain <sup id="fnref:39"><a href="#fn:39" class="footnote-ref" role="doc-noteref">39</a></sup>:</p>
<ul>
<li>image quality in the selection of microfilm</li>
<li>research value</li>
<li>geographic representation</li>
<li>temporal coverage</li>
<li>bibliographic completeness of microfilm copy</li>
<li>diversity (i.e., “newspaper titles that document a significant minority community at the state or regional level”)</li>
<li>whether the title is orphaned (i.e., whether the newspaper has “ceased publication and lack[s] active ownership” [Chronicling America no date])</li>
<li>whether the title has already been digitized.</li>
</ul>
<p>Though factors such as research value are considered by each state awardee’s advisory board, as well as by the National Endowment for the Humanities and peer review experts, the titles included in  <em>Chronicling America</em>  are largely dictated by which exist on microfilm and are of sufficient image quality within a state-level grantee’s collection. Thus, the significance of the collection and microfilming practices of decades prior cannot be understated.</p>
<p>I also highlight that assessing microfilmed titles based on image quality is a complex procedure in its own right. The National Digital Newspaper Program has made publicly available a number of resources devoted specifically to this task, including documents and video tutorials <sup id="fnref:40"><a href="#fn:40" class="footnote-ref" role="doc-noteref">40</a></sup>; <sup id="fnref:41"><a href="#fn:41" class="footnote-ref" role="doc-noteref">41</a></sup>. They articulate factors such as the microfilm generation (archive master, print master, or review copy), the material (polyester or acetate), the reduction ratio, and the physical condition. The detailed resources made available by the National Digital Newspaper Program, the Library of Congress, and the National Endowment for the Humanities for navigating this process are testaments to the multidimensional complexity of the selection process for  _Chronicling America _   <sup id="fnref:42"><a href="#fn:42" class="footnote-ref" role="doc-noteref">42</a></sup>; <sup id="fnref1:39"><a href="#fn:39" class="footnote-ref" role="doc-noteref">39</a></sup>; <sup id="fnref1:37"><a href="#fn:37" class="footnote-ref" role="doc-noteref">37</a></sup>.</p>
<p>We have not yet investigated the topic of digitization, and we have already encountered a profusion of factors from collection to digitization that mediate which artifacts appear in  <em>Chronicling America</em>  and thus  <em>Newspaper Navigator</em> . Let us now examine the microfilm itself.</p>
<h2 id="iv-the-microfilm">IV. The Microfilm</h2>
<p>In  “What Computational Archival Science Can Learn from Art History and Material Culture Studies,”  Lyneise Williams shares a powerful anecdote of coming across a physical copy of a 1927 issue of the French sports newspaper  <em>Match L’Intran</em>  that featured accomplished Black Panamanian boxer, Alfonso Teofilo Brown, on the front cover <sup id="fnref:43"><a href="#fn:43" class="footnote-ref" role="doc-noteref">43</a></sup>. Williams describes Brown as  “glowing. He looked like a 1920s film star rather than a boxer”   <sup id="fnref1:43"><a href="#fn:43" class="footnote-ref" role="doc-noteref">43</a></sup>. Curious to learn more about the printing process, Williams discovered that the issue of  <em>Match L’Intran</em>  was produced using rotogravure, a specific printing process that could  “capture details in dark tones”   <sup id="fnref2:43"><a href="#fn:43" class="footnote-ref" role="doc-noteref">43</a></sup>. However, when Williams found a version of the same newspaper cover that had been digitized from microfilm, it was apparent that the microfilming process had washed out the detail of the rotogravure, reducing Brown to a  “flat black, cartoonish form”   <sup id="fnref3:43"><a href="#fn:43" class="footnote-ref" role="doc-noteref">43</a></sup>. Williams relays the anecdote to articulate that the microfilming process itself is thus a form of erasure for communities of color <sup id="fnref4:43"><a href="#fn:43" class="footnote-ref" role="doc-noteref">43</a></sup>.</p>
<p>The grayscale saturation of photographs induced by microfilming is widely documented and recognizable to most researchers who have ever worked with the medium <sup id="fnref:44"><a href="#fn:44" class="footnote-ref" role="doc-noteref">44</a></sup>; however, Lyneise Williams’s article affords us a lens into what precisely is lost amongst the distortive effects of the microfilming process. This erasure via microfilming can be seen in  _Chronicling America _ directly. In Figure 2, I show the same photograph of W.E.B. Du Bois as it appears in 4 different  <em>Chronicling America</em>  newspaper pages published during October and November of 1910 and digitized from microfilm <sup id="fnref1:28"><a href="#fn:28" class="footnote-ref" role="doc-noteref">28</a></sup>; <sup id="fnref1:29"><a href="#fn:29" class="footnote-ref" role="doc-noteref">29</a></sup>; <sup id="fnref1:30"><a href="#fn:30" class="footnote-ref" role="doc-noteref">30</a></sup>; <sup id="fnref1:31"><a href="#fn:31" class="footnote-ref" role="doc-noteref">31</a></sup>. The phenomenon described by Williams is immediately recognizable in these four images: Du Bois’s facial features are distorted by the grayscale saturation. In the case of the  <em>Iowa State Bystander</em> , Du Bois has been rendered into a silhouette.</p>
<p>Moreover, each digitized reproduction reveals unique visual qualities, varying in contrast, sharpness, and noise - a testament to the confluence of mediating conditions from printing through digitization that have rendered each newspaper photograph in digital form. Even in the case of the two images reproduced in the  <em>The Broad Ax</em> , which were digitized from the very same microfilm reel (reel #00280761059) by the University of Illinois at Urbana-Champaign Library, variations are still apparent. To understand how these subtle differences between images are amplified through digitization, we now turn to optical character recognition.</p>




























<figure ><img loading="lazy" alt="Images of W.E.B. Du Bois" src="/dhqwords/vol/15/4/000578/resources/images/image2.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/4/000578/resources/images/image2_hucec43a56b45cae513ff69f00ca438498_51528_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/15/4/000578/resources/images/image2_hucec43a56b45cae513ff69f00ca438498_51528_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/15/4/000578/resources/images/image2.jpg 960w" 
     class="landscape"
     ><figcaption>
        <p>The same image of W.E.B. Du Bois reproduced in 4 different digitized Black newspapers in <em>Chronicling America</em> from 1910. Note that the combined effects of printing, microfilming, and digitizing have led to different visual effects in each image, ranging from contrast to sharpness.
        </p>
    </figcaption>
</figure>
<h2 id="v-ocr">V. OCR</h2>
<p>Optical character recognition, commonly called OCR, refers to machine learning algorithms that are trained to read images of typewritten text and output machine-readable text, thereby providing the bridge between an image of typewritten text and the transcribed text itself. Because OCR algorithms are  “trained and evaluated using labeled data: examples with ground-truth classification labels that have been assigned by another means,”  the algorithms are considered a form of supervised learning in the machine learning literature <sup id="fnref:45"><a href="#fn:45" class="footnote-ref" role="doc-noteref">45</a></sup>. OCR engines are remarkably powerful in their ability to improve access to historic texts. Indeed, OCR is a crucial form of metadata for  <em>Chronicling America</em> , enabling keyword search in the search portal and making possible scholarship with the newspaper text at large scales.<sup id="fnref:46"><a href="#fn:46" class="footnote-ref" role="doc-noteref">46</a></sup>  However, OCR is not perfect. Although humans are able to discern an E from an R on a digitized page even if the type has been smudged, an OCR engine is not always able to do so: its performance is dependent on factors ranging from the sharpness of text in an image to printing imperfections to the specific typography on the page.</p>
<p>In Figure 3, I show the same four images shown in Figure 2, along with OCR transcriptions of the captions provided by  <em>Chronicling America</em> . All four transcriptions fail to reproduce the true caption with 100% accuracy, differing from one another by at least one character. Consequently, a keyword search of W. E. B. Du Bois over the raw text would not register the caption for any of the four photographs (the  <em>Chronicling America</em>  search portal utilizes a form of relevance search to alleviate this problem). These examples reveal how sensitive OCR engines are to slight perturbations, or  “noise,”  in the digitized images, from ink smudges to text sharpness to page contrast. Though the NDNP awardees who contributed these pages may have utilized different OCR engines or chosen different OCR settings, the OCR for the two image captions from  _The Broad Ax _ that have been digitized from the very same microfilm reel was in all likelihood generated using the same OCR engine and settings. Put succinctly, OCR engines amplify the noise from both the material page and the digitization pipeline.<sup id="fnref:47"><a href="#fn:47" class="footnote-ref" role="doc-noteref">47</a></sup></p>




























<figure ><img loading="lazy" alt="four images of W.E.B. Du Bois" src="/dhqwords/vol/15/4/000578/resources/images/image3.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/4/000578/resources/images/image3_hucec43a56b45cae513ff69f00ca438498_55732_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/15/4/000578/resources/images/image3_hucec43a56b45cae513ff69f00ca438498_55732_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/15/4/000578/resources/images/image3.jpg 960w" 
     class="landscape"
     ><figcaption>
        <p>The OCR transcriptions of the caption “W. E. B. DU BOIS, PH. D.” appearing in the image of W.E.B. Du Bois reproduced in 4 different digitized Black newspapers in <em>Chronicling America</em> . These OCR transcriptions are provided by <em>Chronicling America</em> .
        </p>
    </figcaption>
</figure>
<p>Though OCR engines have become standard components of digitization pipelines, it is important to remember that OCR engines are themselves machine learning models that have been trained on sets of transcribed typewritten pages. Like any machine learning model, OCR predictions are thus subject to biases encoded not only in the OCR engine’s architecture but also in the training data itself. Though it is often called algorithmic bias, this bias is undeniably human, in that the construction of training data machine learning models are imprinted with countless human decisions and judgment calls. For example, if an OCR engine is trained on transcriptions that consistently misspell a word, the OCR engine will amplify this misspelling across all transcriptions of processed pages.<sup id="fnref:48"><a href="#fn:48" class="footnote-ref" role="doc-noteref">48</a></sup>  A recurring theme of algorithmic bias is that it is a force for marginalization, especially in the context of how we navigate information digitally. In  <em>Algorithms of Oppression</em> , Safiya Noble describes how Google’s search engine consistently marginalizes women and people of color by displaying search results that reinforce racism <sup id="fnref:49"><a href="#fn:49" class="footnote-ref" role="doc-noteref">49</a></sup>. This bias is not restricted to Google: in  <em>Masked by Trust: Bias in Library Discovery</em> , Matthew Reidsma articulates how library search engines suffer from similar biases <sup id="fnref:50"><a href="#fn:50" class="footnote-ref" role="doc-noteref">50</a></sup>. Despite the fact that knowledge of algorithmic bias in relation to search engines and image recognition tools is becoming increasingly widespread among the cultural heritage community, the errors introduced by OCR engines are often accepted as inevitable without critical inquiry from this perspective. However, algorithmic bias is a useful framework for examining OCR engines <sup id="fnref:51"><a href="#fn:51" class="footnote-ref" role="doc-noteref">51</a></sup>.</p>
<p>Perhaps the most significant challenge to studying OCR engines is that the best-performing and most widely-used OCR engines are proprietary. Though ABBYY FineReader and Google Cloud Vision API offer high performance, the systems fundamentally are black boxes: we have no access to the underlying algorithms or the training data. The ability to audit a system is crucial to developing an understanding of how it works and the biases it encodes. The fact that many OCR engines are opaque prevents us from disentangling whether poor performance on a particular page is due to algorithmic limitations or due to a lack of relevant training data. The distinction is significant: the former may reflect an algorithmic upper bound, whereas the latter reflects decisions made by humans.</p>
<p>Indeed, algorithmic bias distorts and occludes the historical record, as it is made discoverable through OCR. Discrepancies in OCR performance for different languages and scripts is a consequence of human prioritization, from the collection of training data and lexicons to the development of the algorithms themselves. As articulated by Hannah Alpert-Abrams in  “Machine Reading the  <em>Primeros Libros</em> ,”    “the machine-recognition of printed characters is a historically charged event, in which the system and its data conspire to embed cultural biases in the output, or to affix them as supplementary information hidden behind the screen”   <sup id="fnref1:51"><a href="#fn:51" class="footnote-ref" role="doc-noteref">51</a></sup>. Alpert-Abrams’s work reveals how the OCR inaccuracies for indigenous languages recorded in colonial scripts perpetuate colonialism. For other languages such as Ladino, typically typeset in Rashi script, the lack of high-performing OCR has presented consistent challenges for digitization and scholarship.</p>
<p>In the case of  <em>Chronicling America</em> , the National Digital Newspaper Program is exemplary in its efforts to support OCR for non-English languages. In the Notice of Funding Opportunity for the National Digital Newspaper Program produced by the Division of Preservation of Access at the National Endowment for the Humanities, OCR performance in different languages is explicitly addressed:  “Applicants proposing to digitize titles in languages other than English must include staff with the relevant language expertise to review the quality of the converted content and related metadata”   <sup id="fnref2:37"><a href="#fn:37" class="footnote-ref" role="doc-noteref">37</a></sup>. I have included this discussion of OCR and algorithmic bias to offer a broader provocation regarding machine learning and digitization: how much text in digitized sources has been transmuted by this effect and thus effectively erased due to inaccessibility when using search and discovery platforms?</p>
<h2 id="vi-the-visual-content-recognition-model">VI. The Visual Content Recognition Model</h2>
<p>I will now turn to the  <em>Newspaper Navigator</em>  pipeline itself, in particular the visual content recognition model. Trained on annotations from the  _Beyond Words _ crowdsourcing initiative, as well as additional annotations of headlines and advertisements, the visual content recognition model detects photographs, illustrations, maps, comics, editorial cartoons, headlines, and advertisements on historic newspaper pages.</p>
<p>As described in the previous section, examining training data is an essential component of auditing any machine learning model, from understanding how the dataset was constructed to uncovering any biases in the composition of the dataset itself. For the visual content recognition model, this examination begins with  <em>Beyond Words</em> . Launched in 2017 by LC Labs,  _Beyond Words _ has collected to-date over 10,000 verified annotations of visual content in World War 1-era newspaper pages from  <em>Chronicling America</em> . The  _Beyond Words _ workflow consists of the three steps listed below:</p>
<p>A Mark step, in which volunteers are asked to draw bounding boxes around visual content on the page <sup id="fnref:52"><a href="#fn:52" class="footnote-ref" role="doc-noteref">52</a></sup>. The instructions read as follows:</p>
<blockquote>
<p>In the Mark step, your task is to identify and select pictures in newspaper pages. For our project, pictures means illustrations, photographs, comics, and cartoons. You&rsquo;ll use the marking tool to draw a box around the picture using your mouse. After you have marked all pictures on the newspaper page, click the ‘DONE’ button. Skip the page altogether by clicking the Skip this page button. If no illustrations, photographs, or cartoons appear on the page, click the DONE button. Not sure if a picture should be marked? Select the Done for now, more left to mark button so another volunteer can help finish that page. Please do not select pictures within advertisements.</p>
</blockquote>
<p>A Transcribe step, in which volunteers are asked to transcribe the caption of the highlighted visual content, as well as note the artist and visual content category (Photograph,  Illustration,  Map,  Comics/Cartoon,  Editorial Cartoon) <sup id="fnref:53"><a href="#fn:53" class="footnote-ref" role="doc-noteref">53</a></sup>. The transcription is pre-populated with the OCR falling within the bounding box in question. The instructions for this step state:</p>
<blockquote>
<p>Most pictures have captions or descriptions. Enter the text exactly as you see it. Include capitalization and punctuation, but remove hyphenation that breaks words at the end of the line. Use new lines to separate different parts of captions and descriptions. You can zoom in for better looks at the page. You can also select View the original page in the upper right corner of the screen to view the original high resolution image of the newspaper.</p>
</blockquote>
<p>An example of this step can be seen in Figure 4.</p>
<p>A Verify step, in which volunteers are asked to select the best caption for an identified region of visual content from at least two examples; alternatively, a volunteer can add another caption <sup id="fnref:54"><a href="#fn:54" class="footnote-ref" role="doc-noteref">54</a></sup>. The instructions state:</p>
<blockquote>
<p>Choose the transcription that most accurately captures the text as written. If multiple transcriptions appear valid, choose the first one. If the selected region isn&rsquo;t appropriate for the prompt, click Bad region.</p>
</blockquote>




























<figure ><img loading="lazy" alt="screenshot of a newspaper article and a text box where the article has been transcribed" src="/dhqwords/vol/15/4/000578/resources/images/image4.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/4/000578/resources/images/image4_huac20f953a7605e7c7f1c5edc8f444540_1116229_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/4/000578/resources/images/image4_huac20f953a7605e7c7f1c5edc8f444540_1116229_800x0_resize_box_3.png 800w,/dhqwords/vol/15/4/000578/resources/images/image4_huac20f953a7605e7c7f1c5edc8f444540_1116229_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/4/000578/resources/images/image4_huac20f953a7605e7c7f1c5edc8f444540_1116229_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/4/000578/resources/images/image4.png 1584w" 
     class="landscape"
     ><figcaption>
        <p>A screenshot showing an example of the Transcribe step of the _Beyond Words _ workflow. Note that the photograph caption is pre-populated using the OCR falling within the bounding box [^lclabs2017b].
        </p>
    </figcaption>
</figure>
<p>For the purposes of  <em>Newspaper Navigator</em> , only the bounding boxes from the Mark step and the category labels from the Transcribe step were utilized as training data; however, understanding the full workflow is essential because annotations are considered verified only if they have passed through the full workflow.</p>
<p>A number of factors contribute to which  _Chronicling America _ pages were processed by volunteers in  <em>Beyond Words</em> . First, the temporal restriction to World War 1-era pages affects the ability of the visual content recognition model to generalize: after all, if the model is trained on World War 1-era pages, how well should we expect it to perform on 19th century pages? I will return to this question later in the section. Moreover,  _Beyond Words _ volunteers could select either an entirely random page or a random page from a specific state, an important affordance from an engagement perspective, as volunteers could explore the local histories of states in which they are interested. But this affordance is also imprinted on the training data, as certain states - and thus, certain newspapers - appear at a higher frequency than if the World War-1 era  <em>Chronicling America</em>  pages had been drawn randomly from this temporal range in  <em>Chronicling America</em> .</p>
<p>Furthermore, it should be noted that the Mark and Transcribe steps - specifically, drawing bounding boxes and labeling the visual content category - are complex tasks. Because newspaper pages are remarkably heterogenous, ambiguities and edge-cases abound. Should a photo collage be marked as one unit or segmented into constituent parts? What precisely is the distinction between an editorial cartoon and an illustration? How much relevant textual content should be included in a bounding box? Naturally, volunteers did not always agree on these choices. In this regard, the notion of a ground-truth, a set of perfect annotations against which we can assess performance, is itself called into question. Moreover, with thousands of annotations, mistakes in the form of missed visual content, as well as misclassifications, are inevitable.<sup id="fnref:55"><a href="#fn:55" class="footnote-ref" role="doc-noteref">55</a></sup>  These ambiguities and errors are natural components of  <em>any</em>  training dataset and must be taken into account when analyzing a machine learning model’s predictions.</p>
<p>A breakdown of  <em>Beyond Words</em>  annotations included in the training data can be found in the second column of Table 1. I downloaded these 6,732 publicly-accessible annotations as a JSON file on December 1, 2019. Table 1 reveals an imbalance between the number of examples for each category; in the language of machine learning, this is called  <em>class imbalance</em> . While the discrepancy between maps and photographs is to be expected, the fact that so few maps were included was concerning from a machine learning standpoint: a machine learning algorithm’s ability to generalize to new data is dependent on having many diverse training examples. To address this concern, I searched  _Chronicling America _ and identified 134 pages published between January 1st, 1914, and December 31st, 1918, that contain maps. I then annotated these pages myself.</p>
<p>In addition, during the development of the  <em>Newspaper Navigator</em>  pipeline, I realized the value in training the visual content recognition model to identify headlines and advertisements. Consequently, I added annotations of headlines and advertisements for all 3,559 pages included in the training data. The statistics for this augmented set of annotations can be found in the third column of Table 1. Though I attempted to use a consistent approach to annotating the headlines and advertisements, my interpretation of what constitutes a headline is certainly not unimpeachable: I am not a trained scholar of periodicals or of print culture; even if I were, the task itself is inevitably subjective. Furthermore, I made decisions to annotate large grids of classified ads as a single ad to expedite the annotation process. Whether this was a correct judgment call can be debated. Lastly, annotating all 3,559 pages for headlines and advertisements required a significant amount of time, and there are inevitably mistakes and inconsistencies embedded within the annotations. My own decisions in terms of how to annotate, as well as my mistakes and inconsistencies, are embedded within the visual content recognition model through training. For those interested in examining the training data directly, the data can be found in the GitHub repository for this project <sup id="fnref:56"><a href="#fn:56" class="footnote-ref" role="doc-noteref">56</a></sup>.<br>
A breakdown of  _Beyond Words _ annotations included in the training data for the visual content recognition model, as well as all annotations constituting the training data.    Category  Beyond Words Annotations  Total Annotations      Photograph  4,193  4,254      Illustration  1,028  1,048      Map  79  215      Comic/Cartoon  1,139  1,150      Editorial Cartoon  293  293      Headline  -  27,868      Advertisement  -  13,581         <em>Total</em>     6,732  48,409   <br>
Beyond the construction of the training data, I made manifold decisions regarding the selection of the correct model architecture and the training of the model. Because this discussion surrounding these choices is quite technical, I refer the reader to <sup id="fnref1:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup> for an in-depth examination. However, I will state that the choice of model, the number of iterations for which the model was trained, and the choice of model parameters are all of significant import for the resulting trained model and consequently, the  _Newspaper Navigator _ dataset.</p>
<p>I will now turn to the visual content recognition model’s outputs in relation to the  _Newspaper Navigator _ pipeline. The model itself consumes a lower-resolution version of a  _Chronicling America _ page as input and then outputs a JSON file containing predictions, each of which consists of bounding box coordinates,<sup id="fnref:57"><a href="#fn:57" class="footnote-ref" role="doc-noteref">57</a></sup>  the predicted class (i.e., photograph, map, etc.), and a confidence score generated by the machine learning model.<sup id="fnref:58"><a href="#fn:58" class="footnote-ref" role="doc-noteref">58</a></sup>  Cropping out and saving the visual content required extra code to be written. Because the high-resolution images of the  <em>Chronicling America</em>  pages, in addition to the METS/ALTO OCR, amount to many tens of terabytes of data, questions of data storage became major considerations in the pipeline. I chose to save the extracted visual content as lower-resolution JPEG images in order to reduce the upload time and lessen the storage burden. Though the  <em>Newspaper Navigator</em>  dataset retains identifiers to all high-resolution pages in  _Chronicling America, _ the images in the  _Newspaper Navigator _ dataset are altered by the downsampling procedure. This downsampling procedure should be free of any significant biasing effects.</p>
<p>For visual content recognition,  “Newspaper Navigator”  utilized an object detection model, which is a type of widely-used computer vision technique for identifying objects in images. The performance for computer vision techniques is regularly measured using metrics such as average precision. For  “Newspaper Navigator” , the model’s performance on a specific page, as measured by average precision, is dependent on a confluence of factors. These factors include the page’s layout, artifacts and distortions introduced in the microfilming and digitization process, and - most importantly - the composition of the training data. Thus, each image is seen differently by the visual content recognition model. In Figure 5, I show the four images of W.E.B. Du Bois, as identified by the visual content recognition model and saved in the  _Newspaper Navigator _ dataset. Each image is cropped slightly differently. In the case of the image from the  <em>Iowa State Bystander</em> , extra text is included, while in the case of the images from  <em>The Broad Ax</em> , the captions are partially cut off. The loss in image quality is due to the aforementioned downsampling performed by the pipeline. This downsampling leads to artifacts such as the dots appearing on Du Bois’s face in the image from the  <em>Iowa State Bystander</em> , as well as the streaks in the image from  <em>Franklin’s Paper the Statesman</em> , that are not present in Figure 2.</p>
<p>Returning to the question of the visual content recognition model’s performance on pages published outside of the temporal range of the training data (1914-1918), it is possible to provide a quantitative answer by measuring average precision on test sets of annotated pages from different periods of time. In <sup id="fnref2:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>, I describe this analysis in detail and demonstrate that the performance declines for pages published between 1875 and 1900 and further declines for pages published between 1850 and 1875. This confirms that the composition of the training data directly manifests in the model’s performance. While it is certainly the case that the  <em>Newspaper Navigator</em>  dataset can still be used for scholarship related to 19th century newspapers in  <em>Chronicling America</em> , any scholarship with the 19th century visual content in the  _Newspaper Navigator _ dataset must consider how the dataset may skew what visual content is represented.</p>




























<figure ><img loading="lazy" alt="four images of W.E.B. Du Bois" src="/dhqwords/vol/15/4/000578/resources/images/image5.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/4/000578/resources/images/image5_hucec43a56b45cae513ff69f00ca438498_53115_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/15/4/000578/resources/images/image5_hucec43a56b45cae513ff69f00ca438498_53115_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/15/4/000578/resources/images/image5.jpg 960w" 
     class="landscape"
     ><figcaption>
        <p>The four images of W.E.B. Du Bois, as identified by the visual content recognition model and included in the _Newspaper Navigator _ dataset [^navigator1910a]; [^navigator1910c]; [^navigator1910e]; [^navigator1910g].
        </p>
    </figcaption>
</figure>
<p>Let me conclude this section with a discussion of the act of visual content extraction itself in relation to digitization. While this extraction enables a wide range of affordances for searching  <em>Chronicling America</em> , it is also an act of decontextualization: visual content no longer appears in relation to the  <em>mise-en-page</em> . In the Appendix, the full pages containing the photographs of W.E.B. Du Bois are reproduced, showing each photograph in context. Only by examining the full pages does it become clear that the article featuring W.E.B. Du Bois was printed with a second article in the  <em>Iowa State Bystander</em>  and  <em>The Broad Ax</em> , the headline of which reads:  “ANTI-LYNCHING SOCIETY ORGANIZED IN BOSTON — Afro-American Women Unite For Active Campaign Against Injustice.”  Furthermore, upon examination, the  _Iowa State Bystander _ front page features the article on  <em>The Crisis</em>  and W.E.B. Du Bois as the most prominent article of the issue. Though links between the extracted visual content and the original  <em>Chronicling America</em>  pages are always retained, this decontextualization inevitably transmutes  <em>how</em>  we perceive and interact with the visual content in  <em>Chronicling America</em> . Indeed, all uses of machine learning for metadata enhancement are a form of decontextualization, centering the user’s discovery and analysis of content around the metadata itself.</p>
<h2 id="vii-prediction-uncertainty">VII. Prediction Uncertainty</h2>
<p>Perhaps the most fundamental question to ask of the  _Newspaper Navigator _ dataset is: How many photographs does the dataset contain? Because the dataset has been constructed using a machine learning model, predictions are ultimately probabilistic in nature, quantified by the confidence score returned by the model. This begs the question of what counts as an identified unit of visual content: a user is much more inclined to tally a prediction of a map if it has an associated confidence score of 99% rather than 1%. However, choosing this cut is fundamentally a subjective decision, informed by the user’s end goals with the dataset. In the language of machine learning, picking a stringent confidence cut (i.e., only counting predictions with high confidence scores) emphasizes  <em>precision</em> : a prediction of a photograph likely corresponds to a true photograph, but the predictions will suffer from false negatives. Conversely, picking a loose confidence cut (i.e., counting predictions with low confidence scores) emphasizes  <em>recall</em> : most true photographs are identified as such, but the predictions will suffer from many false positives. In this regard, the total number of images in the  _Newspaper Navigator _ dataset is dependent on one’s desired tradeoff between precision and recall. In Table 2, I show the dynamic range of the dataset size, as induced by three different cuts on confidence score: 90%, 70%, and 50%. Figure 6 shows the effects of different cuts on confidence score for the page featuring W.E.B. Du Bois in the November 26,1910, issue of  <em>The Broad Ax</em> .<br>
The number of occurrences of each category of visual content in the  <em>Newspaper Navigator</em>  dataset with confidence scores above the listed thresholds (0.9, 0.7, 0.5).    Category  ≥ 90%  ≥ 70%  ≥ 50%      Photograph  1.59 x 106  2.63 x 106  3.29 x 106      Illustration  8.15 x 105  2.52 x 106  4.36 x 106      Map  2.07 x 105  4.59 x 105  7.54 x 105      Comic/Cartoon  5.35 x 105  1.23 x 106  2.06 x 106      Editorial Cartoon  2.09 x 105  6.67 x 105  1.27 x 106      Headline  3.44 x 107  5.37 x 107  6.95 x 107      Advertisement  6.42 x 107  9.48 x 107  1.17 x 108         <em>Total</em>     1.02 x 108  1.56 x 108  1.98 x 108   <br>




























<figure ><img loading="lazy" alt="screenshot of four newspaper pages" src="/dhqwords/vol/15/4/000578/resources/images/Figure_6.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/4/000578/resources/images/Figure_6_hub72a7f3cb3fab45d8318dc0ebca11125_612503_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/15/4/000578/resources/images/Figure_6_hub72a7f3cb3fab45d8318dc0ebca11125_612503_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/15/4/000578/resources/images/Figure_6.jpg 960w" 
     class="portrait"
     ><figcaption>
        <p>The same page of _The Broad Ax _ from November 26, 1910, along with predictions from the visual content recognition model, thresholded on confidence score at 5%, 50%, 70%, and 90% [^navigator1910g]; [^navigator1910h]. Note that red corresponds to a prediction of photograph, cyan corresponds to a prediction of headline, and blue corresponds to a prediction of advertisement.
        </p>
    </figcaption>
</figure></p>
<p>Rather than pre-selecting a confidence score threshold, the  _Newspaper Navigator _ dataset contains all predictions with confidence scores greater than 5%,<sup id="fnref:59"><a href="#fn:59" class="footnote-ref" role="doc-noteref">59</a></sup>  allowing the user to define their own confidence cut when querying the dataset. However, the website for the  <em>Newspaper Navigator</em>  dataset also includes hundreds of pre-packaged datasets in order to make it easier for users to work with the dataset. In particular, users can download zip files containing all of the visual content of a specific type with confidence scores greater than or equal to 90%, for any year from 1850 to 1963. I made this choice of 90% as the threshold cut for these pre-packaged datasets based on heuristic evidence from inspecting sample pre-packaged datasets by eye. However, as articulated above, based on different use cases, this cut of 90% may be too restrictive or permissive: relevant visual content may be absent from the pre-packaged dataset or lost in a sea of other examples. In Figure 7, I show the visual content recognition model’s confidence scores for the four images of W.E.B. Du Bois described throughout this data archaeology. The effect of a cut on confidence score can be seen here: selecting a cut of 95% would exclude the image from  “Franklin’s Paper the Statesman” . I raise this point to emphasize that even this seemingly innocuous choice of 90% for the pre-packaged datasets alters the discovery process and thus can have an impact on scholarship.</p>




























<figure ><img loading="lazy" alt="Four images of W.E.B. Du Bois" src="/dhqwords/vol/15/4/000578/resources/images/image8.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/4/000578/resources/images/image8_hucec43a56b45cae513ff69f00ca438498_55279_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/15/4/000578/resources/images/image8_hucec43a56b45cae513ff69f00ca438498_55279_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/15/4/000578/resources/images/image8.jpg 960w" 
     class="landscape"
     ><figcaption>
        <p>The visual content recognition model’s confidence score for each of the four images of W.E.B. Du Bois. Note how the model assigns a different confidence score to each identified image [^navigator1910b]; [^navigator1910d]; [^navigator1910f]; [^navigator1910h].
        </p>
    </figcaption>
</figure>
<p>Just as the bounding box predictions themselves are affected by the training data, as well as newspaper page layout, date of publication, and noise from the digitization pipeline, so too are the confidence scores. In particular, the visual content recognition model suffers from high-confidence misclassifications, for example, crossword puzzles that are identified as maps with confidence scores greater than 90%. High-confidence misclassifications pose challenges for machine learning writ large, and the field of explainable artificial intelligence is largely devoted to developing tools for understanding this type of misclassification <sup id="fnref:60"><a href="#fn:60" class="footnote-ref" role="doc-noteref">60</a></sup>. However, these high-confidence misclassifications can often be traced back to the composition of the training set. For example, the fact that the visual content recognition model sometimes identifies crossword puzzles as maps with high confidence is likely due to the fact that the training data did not contain enough labeled examples of maps and crossword puzzles for the visual content recognition model to differentiate them with high accuracy.</p>
<p>The questions surrounding confidence scores and probabilistic descriptions of items is by no means restricted to the  _Newspaper Navigator _ dataset. I echo Thomas Padilla’s assertion that  “attempts to use algorithmic methods to describe collections must embrace the reality that, like human descriptions of collections, machine descriptions come with varying measure of certainty”   <sup id="fnref1:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup>. Machine-generated metadata such as OCR are also fundamentally probabilistic in nature; this fact is not immediately apparent to end users of cultural heritage collections because cuts on confidence score are typically chosen before surfacing the metadata. Effectively communicating confidence scores, probabilistic descriptions, and the decisions surrounding them to end users remains a challenge for content stewards.</p>
<h2 id="viii-ocr-extraction">VIII. OCR Extraction</h2>
<p>In the  _Newspaper Navigator _ pipeline, a textual description of each prediction is obtained by extracting the OCR within each predicted bounding box. The resulting textual description is thus dependent on not only the OCR provided by  _Chronicling America _ but also the exact coordinates of the bounding box: if the coordinates of a word in the localized OCR extend beyond the bounds of the box, the word is excluded. I experimented with utilizing tolerance limits to allow words that extend just beyond the bounds of the boxes to be included, but doing so ultimately introduces false positives as well, as words from neighboring articles or visual content were inevitably included some fraction of the time. Once again, the tradeoff between false positives and false negatives is manifest.</p>
<p>In Figure 8, I show the textual descriptions of the four images of W.E.B. Du Bois, as identified by the  <em>Newspaper Navigator</em>  pipeline. Significantly, in the  _Newspaper Navigator _ dataset, the OCR is stored as a list of words, with line breaks removed; these lists are what appear in Figure 8. These four examples provide intuition as to how the captions are altered. While the examples from the  _Iowa State Bystander _ and  <em>Franklin’s Paper the Statesman</em>  both have very similar captions as shown in Figure 3, the captions for both of the examples from  <em>The Broad Ax</em>  are unrecognizable. Because the bounding boxes have clipped the caption, none of the characters from the proper OCR captions from Figure 3 are present. Furthermore, the captions contain OCR noise due to the OCR engine attempting to read text from the photographs. Consequently, the mentions of W.E.B. Du Bois are erased from the textual descriptions in the  _Newspaper Navigator _ dataset. The visual content in the  _Newspaper Navigator _ dataset is thus decontextualized not only in the sense that the visual content is extracted from the newspaper pages but also in the sense that the OCR extraction method further alters the textual descriptions. While the images from the  <em>Iowa State Bystander</em>  and  <em>Franklin’s Paper the Statesman</em>  are still recoverable with fuzzy keyword search, the two images from  _The Broad Ax _ are impossible to retrieve with  _any _ form of keyword search, revealing another instance in which employing automated techniques for collections processing affects discoverability.</p>




























<figure ><img loading="lazy" alt="four images of W.E.B. Du Bois" src="/dhqwords/vol/15/4/000578/resources/images/image9.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/4/000578/resources/images/image9_hu1756b1b6b0725208e81b779d62f11eee_60709_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/15/4/000578/resources/images/image9_hu1756b1b6b0725208e81b779d62f11eee_60709_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/15/4/000578/resources/images/image9.jpg 960w" 
     class="landscape"
     ><figcaption>
        <p>The textual descriptions of each image, as extracted from the OCR and saved in the _Newspaper Navigator _ dataset [^navigator1910b]; [^navigator1910d]; [^navigator1910f]; [^navigator1910h].
        </p>
    </figcaption>
</figure>
<p>Fortunately, visual content can still be recovered using similarity search over the images themselves; these methods are discussed in detail in the next section. However, in the case of headlines, the errors introduced by OCR engines and the subsequent OCR extraction have no recourse, as similarity search for images of headlines would only capture similar typography and text layout.<sup id="fnref:61"><a href="#fn:61" class="footnote-ref" role="doc-noteref">61</a></sup></p>
<p>To illustrate the effects of this OCR extraction on headlines, I reproduce in Table 3 the extracted OCR as it appears in the  _Newspaper Navigator _ dataset for Franklin F. Johnson’s headline:</p>
<p>NEW MOVEMENT</p>
<p>BEGINS WORK</p>
<p>Plan and Scope of the Asso-</p>
<p>ciation Briefly Told.</p>
<p>Will Publish the Crisis.</p>
<p>Review of Causes Which Led to the</p>
<p>Organization of the Association in</p>
<p>New York and What Its Policy Will</p>
<p>Be-Career and Work of Professor</p>
<p>W.E.B. Du Bois<br>
The extracted OCR associated with each of the four photographs of W.E.B. Du Bois <sup id="fnref:62"><a href="#fn:62" class="footnote-ref" role="doc-noteref">62</a></sup>; <sup id="fnref:63"><a href="#fn:63" class="footnote-ref" role="doc-noteref">63</a></sup>; <sup id="fnref:64"><a href="#fn:64" class="footnote-ref" role="doc-noteref">64</a></sup>; <sup id="fnref:65"><a href="#fn:65" class="footnote-ref" role="doc-noteref">65</a></sup>.       _Iowa State Bystander _  (14 Oct. 1910)      <em>Franklin’s Paper the Statesman</em>  (15 Oct. 1910)      _The Broad Ax _  (15 Oct. 1910)      _The Broad Ax _  (26 Nov. 1910)       98.72%  99.57%  99.76%  99.70%       [NEW , MOVEMENT , BEGINS , WORK , and , Plan , Scope , of , the , Asso\u00ad , ciation , Briefly , Told. , WILL , PUBLISH , THE , CRISIS. , Review , of , Causae , Which , Lad , to , the , Organisation , of , the , Auooiation , In , Naw , York , and , JWhat , It* , Polioy , Will , Ba\u2014Career , and , Wark , of , Profeasor]    [NEW , MOVEMENT , BEGINS , WORK , Plan , and , Scope , of , the , Asso , ciation , Briefly , Told. , WILL , PUBLISH , THE , CRISIS.]    [NEW , MOVEMENT , BEGINS , WORK , Plan , and , Sep , if , the , Asso , ciation , Briefly , Told. , WILL , PUBLISH , THE , CRISIS, , Be , Career , nnd , Work , of , Professor , W. , E. , B. , Du , Bois. , Review , of , Causes , Which , Led , to , the , Oraanteallon , of , th. , A.Me!.!?n , i , i , New , York , and , What , IU , Policy , Will]    [NEW , MOVEMENT , BEGINS , WORK , Plan , and , Scope , of , the , Asso , ciation , Briefly , Told. , WILL , PUBLISH , THE , CRISIS. , Review , of , Causes , Which , Lad , to , tha , Organization , of , the&quot; , Association , In , New , York , and , What , Its , Policy , Will]    <br>
The full pages are reproduced in the appendix for reference. Notably, all four extracted headlines contain OCR errors, as well as missing words due to the OCR extraction. The visual content recognition model consistently fails to include the last line of the headline,  “W.E.B. Du Bois,”  revealing another case in which Du Bois’s name is rendered inaccessible by keyword search in the  _Newspaper Navigator _ dataset.</p>
<h2 id="ix-image-embeddings">IX. Image Embeddings</h2>
<p>An image embedding canonically refers to a low-dimensional representation of an image, often a list of a few hundred or a few thousand numbers, that captures much of the image’s semantic content. Image embeddings are typically generated by feeding an image into a pre-trained neural image classification model (i.e., a model that takes in an image and outputs a label of dog or cat) and extracting a representation of the image from one of the model’s hidden layers, often the penultimate layer.<sup id="fnref:66"><a href="#fn:66" class="footnote-ref" role="doc-noteref">66</a></sup>  Image embeddings are valuable for three reasons:</p>
<p>Image embeddings are remarkably adept at capturing semantic similarity between images. For example, images of dogs tend to be clustered together in embedding space, with images of bicycles in another cluster and images of buildings in yet another. These clusters can be fine-grained: sometimes, the red bicycles are grouped closer together than the blue bicycles.  Image embeddings can be constructed by feeding images into an image classification model already trained on another dataset (such as ImageNet), meaning that generating image embeddings is a useful method for comparing images without having to construct training data by labeling images.   Image embeddings are low-dimensional and thus much smaller in size than the images themselves (i.e., on the order of kilobytes instead of megabytes). As a result, image embeddings are much less computationally expensive to compare to one another when conducting similarity search, clustering, or related tasks. In short, image embeddings speed up image comparison.</p>
<p>Utilizing image embeddings to visualize and explore large collections of images has become an increasingly common approach among cultural heritage practitioners. Projects and institutions that have utilized image embeddings for visualizing cultural heritage collections include the Yale Digital Humanities Lab’s PixPlot interface <sup id="fnref:67"><a href="#fn:67" class="footnote-ref" role="doc-noteref">67</a></sup>, the National Neighbors project <sup id="fnref:68"><a href="#fn:68" class="footnote-ref" role="doc-noteref">68</a></sup>, Google Arts and Culture <sup id="fnref:69"><a href="#fn:69" class="footnote-ref" role="doc-noteref">69</a></sup>, The Norwegian National Museum’s Principal Components project <sup id="fnref:70"><a href="#fn:70" class="footnote-ref" role="doc-noteref">70</a></sup>, the State Library of New South Wales’s Aero Project <sup id="fnref:71"><a href="#fn:71" class="footnote-ref" role="doc-noteref">71</a></sup>, the Royal Photographic Society <sup id="fnref:72"><a href="#fn:72" class="footnote-ref" role="doc-noteref">72</a></sup>, The American Museum of Natural History <sup id="fnref:73"><a href="#fn:73" class="footnote-ref" role="doc-noteref">73</a></sup>, and The National Library of the Netherlands <sup id="fnref:74"><a href="#fn:74" class="footnote-ref" role="doc-noteref">74</a></sup>; <sup id="fnref:75"><a href="#fn:75" class="footnote-ref" role="doc-noteref">75</a></sup>. These visualizations provide insights into broader themes in the collections, thereby allowing curators, researchers, and the public to explore collections at a scale previously only possible by organizing images by color or other low-level features.<sup id="fnref:76"><a href="#fn:76" class="footnote-ref" role="doc-noteref">76</a></sup>  In this regard, image embeddings provide new affordances for searching over images that complement canonical faceted and keyword search.</p>
<p>Because these image embeddings enable these visualization approaches and open the door to similarity search and recommendation, I opted to include image embeddings as part of the  <em>Newspaper Navigator</em>  pipeline. Indeed, these image embeddings power the similarity search functionality in the  <em>Newspaper Navigator</em>  user interface and, in this regard, are crucial to the broader vision of the project <sup id="fnref:77"><a href="#fn:77" class="footnote-ref" role="doc-noteref">77</a></sup>.<sup id="fnref:78"><a href="#fn:78" class="footnote-ref" role="doc-noteref">78</a></sup>  To generate the embeddings, I utilized ResNet-18 and ResNet-50, two variants of a prominent deep learning architecture for image classification, both of which had already been pre-trained on ImageNet <sup id="fnref:79"><a href="#fn:79" class="footnote-ref" role="doc-noteref">79</a></sup>.</p>
<p>ImageNet is perhaps the most well-known image dataset in the history of machine learning. Constructed by scraping publicly available images from the internet and recruiting Amazon Mechanical Turk workers to annotate the images, ImageNet contains approximately 14 million images across 20,000 categories <sup id="fnref:80"><a href="#fn:80" class="footnote-ref" role="doc-noteref">80</a></sup>; <sup id="fnref:81"><a href="#fn:81" class="footnote-ref" role="doc-noteref">81</a></sup>. Kate Crawford and Trevor Paglen’s essay  “Excavating AI: The Politics of Images in Machine Learning Training Sets”  offers a history and incisive critique of the classification schema of ImageNet; here, I will summarize the most salient critiques. First, many of the categories in the taxonomy utilized are themselves marginalizing <sup id="fnref1:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup>. Though many of the classes relating to people were removed in 2019, ImageNet had previously bifurcated the Natural Object  <code>&gt;</code>  Body  <code>&gt;</code>  Adult Body category into Male Body and Female Body subcategories. Second, ethnic classes were included, implying that 1) classification into rigid categories of ethnicity is possible and appropriate and 2) a machine learning system could learn how to classify ethnicity from these images. Diving deeper, the classifications become horrifying in their supposed granularity: until 2019, an image of a woman in a bikini was accompanied with the tags  “slattern, slut, slovenly woman, trollop”   <sup id="fnref2:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup>. Though many embedding models are pre-trained on subsets of ImageNet categories included in the ImageNet Large Scale Visual Recognition Challenge that elide these particularly troubling classifications, these classifications nonetheless necessitate a reckoning with our use of ImageNet writ large, especially in regard to how the semantics of ImageNet are projected onto any image embedding generated with such a model <sup id="fnref:82"><a href="#fn:82" class="footnote-ref" role="doc-noteref">82</a></sup>. <sup id="fnref:83"><a href="#fn:83" class="footnote-ref" role="doc-noteref">83</a></sup></p>
<p>However, questions probing the data in ImageNet fail to critique the ethically questionable practices on which ImageNet is built. Though the researchers responsible for the dataset scraped all 14 million images from public URLs, ImageNet does not provide any guarantees on image copyright, as only the URLs are provided in the database:  “The images in their original resolutions may be subject to copyright, so we do not make them publicly available on our server”   <sup id="fnref:84"><a href="#fn:84" class="footnote-ref" role="doc-noteref">84</a></sup>. It is highly unlikely that a photographer with an image in the dataset could have known that a photograph could be used this way, much less actively consent to the image’s inclusion, as is the case with subjects in the photographs. Furthermore, the labels themselves were collected using Amazon’s Mechanical Turk platform, which has been repeatedly criticized for its exploitative labor practices: as of 2017, workers earned a median wage of approximately $2 an hour on the platform <sup id="fnref:85"><a href="#fn:85" class="footnote-ref" role="doc-noteref">85</a></sup>. Scholars including Natalia Cecire, Bonnie Mak, and Paul Fyfe have highlighted how outsourced marginalized labor underpins digitization efforts, and the reliance on Mechanical Turk for the production of ImageNet further entrenches the digitization and discovery process within a system of labor exploitation <sup id="fnref:86"><a href="#fn:86" class="footnote-ref" role="doc-noteref">86</a></sup>; <sup id="fnref1:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup>; <sup id="fnref1:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup>. As cultural heritage practitioners and humanities researchers, we must acknowledge these exploitative practices, and we must reckon with how we perpetuate them through the use of ImageNet as a training source for image search and discovery.</p>
<p>In offering these critiques, my intention is not to dismiss ImageNet in a wholesale manner. Certainly, the benefits of utilizing ImageNet are manifold, as evidenced by widespread community adoption, as well as new affordances for searching cultural heritage collections enabled by the dataset that are shaping the contours of digital scholarship. In the case of my own scholarship with Newspaper Navigator, I have elected to utilize machine learning models pre-trained on ImageNet precisely for these reasons. I offer these provocations instead to question how we can do better as a community, not only in imagining alternatives but in bringing them to fruition. Classification is an act of interpretive reduction, whether by human or machine, and thus manifests all too often as an act of oppression.<sup id="fnref:87"><a href="#fn:87" class="footnote-ref" role="doc-noteref">87</a></sup>  And yet, the structure imposed by classification constitutes the very basis for search and discovery systems. The salient question is thus not how we dispense of these systems but rather how we progressively realize a more inclusive vision of these systems, from the labor practices behind their construction to the very classification taxonomies themselves.</p>
<p>How, then, do image embeddings derived from ImageNet mediate our interactions with the photographs in  <em>Newspaper Navigator</em> ? Figure 9 shows a visualization of 1,000 photographs from the  _Newspaper Navigator _ dataset published during the year 1910. This visualization was created using the ResNet-50 image embeddings, as well as a dimensionality reduction algorithm known as T-SNE <sup id="fnref:88"><a href="#fn:88" class="footnote-ref" role="doc-noteref">88</a></sup>. With T-SNE, a cluster of photographs indicates that the photographs are likely semantically similar, but the size of the cluster and distances from other clusters bear no meaning <sup id="fnref:89"><a href="#fn:89" class="footnote-ref" role="doc-noteref">89</a></sup>. With this in mind, we can examine the clusters. Despite the fact that the high-contrast, grayscale photographs in  <em>Newspaper Navigator</em>  are markedly different, or  “out-of-sample,”  in comparison to the clear, color images in ImageNet, the clusters nonetheless capture semantic similarity. In Figure 9, we observe the clustering of photographs depicting crowds of people, as well as photographs depicting ships and the sea. This visualization technique with the image embeddings is thus powerful in helping to navigate large collections of photographs by their semantic content.</p>




























<figure ><img loading="lazy" alt="image of a network graph" src="/dhqwords/vol/15/4/000578/resources/images/image10.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/4/000578/resources/images/image10_hudceb583da96818911575874c6008651a_61476_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/15/4/000578/resources/images/image10_hudceb583da96818911575874c6008651a_61476_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/15/4/000578/resources/images/image10.jpg 960w" 
     class="landscape"
     ><figcaption>
        <p>A visualization of 1,000 photographs from the year 1910 in the _Newspaper Navigator _ dataset, generated using the <em>Newspaper Navigator</em> ResNet-50 image embeddings.
        </p>
    </figcaption>
</figure>
<p>What about the photographs of W.E.B. Du Bois? In Figure 10, I show the clusters containing these four photographs. This visualization affords us a lens into the limitations of image embeddings. First, it is evident that image embeddings are directly impacted by the distortions of the digitization process: while the three photographs from  _Franklin’s Paper the Statesman _ and  <em>The Broad Ax</em>  are clustered together with other portraits, the photograph from the  <em>Iowa State Bystander</em>  is located in an entirely different cluster - a consequence of the fact that the  <em>Iowa State Bystander</em>  photograph is saturated and that W.E.B. Du Bois’s facial features are obscured (notably, neighboring photographs suffer from similar distortions). A search engine powered with these image embeddings would in all likelihood return the three photographs from  <em>Franklin’s Paper the Statesman</em>  and  <em>The Broad Ax</em>  together, but the fourth photograph would effectively be lost. This algorithmic mediation is particularly troubling because, as described in Section IV, the microfilming digitization process causes newspaper photographs of darker-skinned people to lose contrast. While this loss in image quality is marginalizing in its own right, image embeddings perpetuate this marginalization: digitized newspaper portraits of darker-skinned individuals are more likely to suffer from saturated facial features, in turn resulting in these photographs being lost during the discovery and retrieval process, as is the case with the saturated  _Iowa State Bystander _ photograph of W.E.B. Du Bois in Figure 10. Understanding these limitations of image embeddings are particularly relevant in the case of  <em>Newspaper Navigator</em> , as these image embeddings power the visual similarity search affordance within the publicly-deployed  _Newspaper Navigator _ search application <sup id="fnref1:77"><a href="#fn:77" class="footnote-ref" role="doc-noteref">77</a></sup>. Though machine learning methods are often offered as panaceas for automation, this algorithmic erasure reminds us that traditional methods of scholarship and historiography, such as detailed analyses and close readings of Black newspapers in  <em>Chronicling America</em> , are more important than ever to counter algorithmic bias.</p>




























<figure ><img loading="lazy" alt="image of a network graph focused on W.E.B. Du Bois" src="/dhqwords/vol/15/4/000578/resources/images/image11.jpg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/4/000578/resources/images/image11_hu1545f165794f728ff2a9b20755b773b4_69931_500x0_resize_q75_box.jpg 500w,
    /dhqwords/vol/15/4/000578/resources/images/image11_hu1545f165794f728ff2a9b20755b773b4_69931_800x0_resize_q75_box.jpg 800w,/dhqwords/vol/15/4/000578/resources/images/image11.jpg 960w" 
     class="landscape"
     ><figcaption>
        <p>The same visualization as in Figure 9, this time showing the locations of the four photographs of W.E.B. Du Bois.
        </p>
    </figcaption>
</figure>
<h2 id="x-environmental-impact">X. Environmental Impact</h2>
<p>Any examination of a dataset whose construction required large-scale computing would be remiss in not investigating the environmental impact of the computation itself. The carbon emissions generated from training a state-of-the-art machine learning model such as BERT is comparable to a single flight across the United States; however, factoring in experimentation and tuning, the carbon emissions can quickly amount to the carbon emissions of a car over its entire lifetime, including fuel <sup id="fnref:90"><a href="#fn:90" class="footnote-ref" role="doc-noteref">90</a></sup>. OpenAI’s GPT-3 model required several thousand petaflop/s-days to train; without specific numbers, the carbon emissions are not possible to calculate exactly, but they are nonetheless substantial <sup id="fnref:91"><a href="#fn:91" class="footnote-ref" role="doc-noteref">91</a></sup>. In response, machine learning researchers have recommended ideas such as Green AI, with the goal of encouraging the community to value computational efficiency and not just accuracy <sup id="fnref:92"><a href="#fn:92" class="footnote-ref" role="doc-noteref">92</a></sup>.</p>
<p>In the case of  <em>Newspaper Navigator</em> , most of the compute time was devoted to processing all 16.3 million  <em>Chronicling America</em>  pages with the visual content recognition model, as opposed to training the model itself. In Tables 4 and 5, I report details on training the model and running the pipeline, as well as the carbon emissions generated by each step, computed using the Machine Learning Impact Calculator <sup id="fnref:93"><a href="#fn:93" class="footnote-ref" role="doc-noteref">93</a></sup>. In total, approximately 380 kg CO2 were emitted during the construction of the  _Newspaper Navigator _ dataset, including development, experimentation, training, pipeline processing, and post-processing. It should be noted that this number is an estimate, as the statistics for experimentation and post-processing are difficult to quantify exactly. Nonetheless, this is approximately equivalent to the carbon emissions incurred by a single person flying from Washington, D.C. to Boston <sup id="fnref:94"><a href="#fn:94" class="footnote-ref" role="doc-noteref">94</a></sup>. I include these numbers in the hope that cultural heritage practitioners will consider the environmental impact of utilizing machine learning and artificial intelligence for digital content stewardship. Doing so is essential to the data archaeology: given that climate change will disproportionately affect cultural heritage institutions in regions unable to develop proper infrastructure to withstand rapid temperature fluctuations and unprecedented flooding, even the environmental impacts of utilizing machine learning within digital content stewardship has the capacity to contribute to erasure and marginalization.<br>
Carbon emissions from the GPU usage for  <em>Newspaper Navigator</em> , broken down by project component. Note that all computation was done on Amazon AWS g4dn instances in the zone us-east-2. The carbon emissions were calculated using the Machine Learning Impact Calculator <sup id="fnref1:93"><a href="#fn:93" class="footnote-ref" role="doc-noteref">93</a></sup>.    Activity  # of NVIDIA T4 GPUs  GPU Hours (each)  Carbon Emissions      Training  1  19  0.96 kg CO2      Pipeline Processing  8  456  144.56 kg CO2      Experimentation for Training and Pipeline Processing (estimate)  8  24  7.66 kg CO2         <em>Total</em>     -  -  153.18 kg CO2        Carbon emissions from the CPU usage for  <em>Newspaper Navigator</em> , broken down by project component. Note that all computation was done on Amazon AWS g4dn instances in the zone us-east-2. The CPU processors are all 2nd generation Intel Xeon Scalable Processors (Cascade Lake) <sup id="fnref:95"><a href="#fn:95" class="footnote-ref" role="doc-noteref">95</a></sup>. The 48-core processor outputs approximately 350 W; the 4-core processor outputs approximately 104 W <sup id="fnref:96"><a href="#fn:96" class="footnote-ref" role="doc-noteref">96</a></sup>; <sup id="fnref:97"><a href="#fn:97" class="footnote-ref" role="doc-noteref">97</a></sup>. The carbon emissions were calculated using the Machine Learning Impact Calculator <sup id="fnref2:93"><a href="#fn:93" class="footnote-ref" role="doc-noteref">93</a></sup>. Note that the energy consumption by RAM is not factored in, but it is insignificant in comparison to the CPU and GPU energy consumption.    Activity  CPU Processor (#)  # Processor CPU Cores  CPU Hours (each)  Carbon Emissions      Training  1  4 CPUs  19  1.13 kg CO2      Pipeline Processing  2  48 CPUs  456  181.9 kg CO2      Experimentation for Training and Pipeline Processing ( <em>estimate</em> )  2  48 CPUs  24  9.57 kg CO2      Extra Computation (dataset post-processing, etc.,  <em>estimate</em> )  1  48 CPUs  168  33.52 kg CO2         <em>Total</em>     -  -  -  226.12 kg CO2</p>
<h2 id="xi-conclusion">XI. Conclusion</h2>
<p>In this data archaeology, I have traced four  <em>Chronicling America</em>  pages reproducing the same photograph of W.E.B. Du Bois as they have traveled through the  <em>Chronicling America</em>  and  <em>Newspaper Navigator</em>  pipelines. The excavated genealogy of digital artifacts has revealed the imprintings of the complex interactions between humans and machines. Indeed, the journey of each newspaper page through the  _Chronicling America _ and  <em>Newspaper Navigator</em>  pipelines is one of refraction, mediation, and decontextualization that is compounded upon with each step. Decisions made decades ago when microfilming a newspaper page inevitably affect how the machine learning models employed for OCR, visual content extraction, and image embedding generation ultimately process the pages, render them as digital artifacts in the  _Newspaper Navigator _ dataset, and mediate their discoverability.</p>
<p>As articulated by Trevor Owens in  <em>The Theory and Craft of Digital Preservation</em> , machine learning and artificial intelligence are the  “underlying sciences for digital preservation”   <sup id="fnref1:36"><a href="#fn:36" class="footnote-ref" role="doc-noteref">36</a></sup>. Though machine learning techniques provide us with new affordances for searching and studying cultural heritage materials, they have the power to perpetuate and amplify the marginalization and erasure of entire communities within the archive. This erasure, coupled with the labor practices involved in creating training data as well as the environmental impact of training and deploying machine learning models in large-scale digitization pipelines, necessitates that we continue to examine the broader socio-technical ecosystems in which we participate. In doing so, we can work toward a more inclusive vision of the digital collection and the ways in which we render its contents discoverable.</p>
<p>How, then, is  <em>Newspaper Navigator</em>  situated within this vision? In reimagining how we search over the visual content in  <em>Chronicling America</em> , one explicit goal of the project is to engage the public with the rich history preserved within historic American periodicals and thus build on  <em>Chronicling America</em>  as a free-to-use, public domain resource for scholars, educators, students, journalists, genealogists, and beyond <sup id="fnref:98"><a href="#fn:98" class="footnote-ref" role="doc-noteref">98</a></sup>. <sup id="fnref:99"><a href="#fn:99" class="footnote-ref" role="doc-noteref">99</a></sup>. With  <em>Newspaper Navigator</em> , it is my belief that the new modes of interacting with  <em>Chronicling America</em>  have the capacity to not only enable a breadth of new scholarship but also foster engagement in and reckoning with America’s multilayered history of oppression. In documenting the different components of the project with this data archaeology and corresponding technical paper <sup id="fnref3:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>, as well as releasing the full dataset and all code into the public domain, I have intended to be as transparent as possible with the tools and methodologies employed.  _Newspaper Navigator _ is not without its shortcomings, but my hope is that the project contributes to this vision of the digital collection through transparency and inclusivity, as well as the scholarship and pedagogy that it has enabled.</p>
<p>I offer this case study not only to contextualize the  _Newspaper Navigator _ dataset but also to advocate for the autoethnographic data archaeology as a valuable apparatus for reflecting on a cultural heritage dataset from a humanistic perspective. Though the digital humanities community has yet to adopt the data archaeology as standard practice when creating and releasing cultural heritage datasets, doing so has the capacity to improve accountability and context surrounding applications of machine learning for both practitioners and end users. Given the manifold ways in which machine learning mediates access to the archive and perpetuates erasure, reflecting critically on these systems is not only urgent but essential for transparency and inclusivity.</p>
<h2 id="sources-of-funding">Sources of Funding</h2>
<p>This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant DGE-1762114, as well as the Library of Congress Innovator in Residence Position.</p>
<h2 id="acknowledgments">Acknowledgments</h2>
<p>I would like to thank Eileen Jakeway, Jaime Mears, Laurie Allen, Meghan Ferriter, Robin Butterhof, and Nathan Yarasavage at the Library of Congress, as well as Molly Hardy and Joshua Ortiz Baco at the National Endowment for the Humanities, for their thoughtful and enlightening feedback on drafts of this article. I am grateful to my Ph.D. advisor, Daniel Weld, at the University of Washington, for his support, guidance, and invaluable advice with  <em>Newspaper Navigator</em> . In addition, I would like to thank Kurtis Heimerl and Esther Jang at the University of Washington for the opportunity to formulate and write early sections of this data archaeology as part of this Spring’s CSE 599:  “Computing for Social Good”  course.</p>
<p>Lastly, I would like to thank the following people who have shaped  <em>Newspaper Navigator</em> : Kate Zwaard, Leah Weinryb Grohsgal, Abbey Potter, Chris Adams, Tong Wang, John Foley, Brian Foo, Trevor Owens, Mark Sweeney, and the entire National Digital Newspaper Program staff at the Library of Congress; Devin Naar, Stephen Portillo, Daniel Gordon, and Tim Dettmers at the University of Washington; Michael Haley Goldman, Robert Ehrenreich, Eric Schmalz, and Elliott Wrenn at the United States Holocaust Memorial Museum; Jim Casey at The Pennsylvania State University; Sarah Salter at Texas A&amp;M University-Corpus Christi; and Gabriel Pizzorno at Harvard University.</p>
<h2 id="appendix">Appendix:</h2>




























<figure ><img loading="lazy" alt="screenshot of a newspaper page" src="/dhqwords/vol/15/4/000578/resources/images/image12.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/4/000578/resources/images/image12_hu25bbdc990c9bbce604c766ed0c52ee30_765549_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/4/000578/resources/images/image12_hu25bbdc990c9bbce604c766ed0c52ee30_765549_800x0_resize_box_3.png 800w,/dhqwords/vol/15/4/000578/resources/images/image12.png 945w" 
     class="landscape"
     ><figcaption>
        <p>_ Iowa state bystander_ . [volume] (Des Moines, Iowa), 14 Oct. 1910. Chronicling America: Historic American Newspapers. Lib. of Congress. <a href="https://chroniclingamerica.loc.gov/lccn/sn83025186/1910-10-14/ed-1/seq-1/">https://chroniclingamerica.loc.gov/lccn/sn83025186/1910-10-14/ed-1/seq-1/</a>
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="screenshot of a newspaper page" src="/dhqwords/vol/15/4/000578/resources/images/image13.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/4/000578/resources/images/image13_huff8071bbd709c78e084077f74a266d27_650561_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/4/000578/resources/images/image13_huff8071bbd709c78e084077f74a266d27_650561_800x0_resize_box_3.png 800w,/dhqwords/vol/15/4/000578/resources/images/image13.png 945w" 
     class="landscape"
     ><figcaption>
        <p><em>Franklin&rsquo;s paper the statesman</em> . (Denver, Colo.), 15 Oct. 1910. Chronicling America: Historic American Newspapers. Lib. of Congress. <a href="https://chroniclingamerica.loc.gov/lccn/sn91052311/1910-10-15/ed-1/seq-16/">https://chroniclingamerica.loc.gov/lccn/sn91052311/1910-10-15/ed-1/seq-16/</a>
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="screenshot of a newspaper page" src="/dhqwords/vol/15/4/000578/resources/images/image14.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/4/000578/resources/images/image14_hufe57ca21519b4e1ae0671c5f93816be0_697409_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/4/000578/resources/images/image14_hufe57ca21519b4e1ae0671c5f93816be0_697409_800x0_resize_box_3.png 800w,/dhqwords/vol/15/4/000578/resources/images/image14.png 945w" 
     class="landscape"
     ><figcaption>
        <p><em>The broad ax.</em> [volume] (Salt Lake City, Utah), 15 Oct. 1910. Chronicling America: Historic American Newspapers. Lib. of Congress. <a href="https://chroniclingamerica.loc.gov/lccn/sn84024055/1910-10-15/ed-1/seq-2/">https://chroniclingamerica.loc.gov/lccn/sn84024055/1910-10-15/ed-1/seq-2/</a>
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="screenshot of a newspaper page" src="/dhqwords/vol/15/4/000578/resources/images/image15.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/4/000578/resources/images/image15_hue2722590f04e9a16cf78d0443474a88c_671619_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/4/000578/resources/images/image15_hue2722590f04e9a16cf78d0443474a88c_671619_800x0_resize_box_3.png 800w,/dhqwords/vol/15/4/000578/resources/images/image15.png 945w" 
     class="landscape"
     ><figcaption>
        <p><em>The broad ax.</em> [volume] (Salt Lake City, Utah), 26 Nov. 1910. Chronicling America: Historic American Newspapers. Lib. of Congress. <a href="https://chroniclingamerica.loc.gov/lccn/sn84024055/1910-11-26/ed-1/seq-3/">https://chroniclingamerica.loc.gov/lccn/sn84024055/1910-11-26/ed-1/seq-3/</a>
        </p>
    </figcaption>
</figure>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>More on the organizational considerations surrounding  _Newspaper Navigator _ can be found in <sup id="fnref1:99"><a href="#fn:99" class="footnote-ref" role="doc-noteref">99</a></sup>.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Mears, J.  <em>National Digital Newspaper Program Impact Study 2004-2014</em> , National Endowment for the Humanities (2014). Available at: <a href="https://www.neh.gov/divisions/preservation/featured-project/neh-releases-national-digital-newspaper-program-impact-study">https://www.neh.gov/divisions/preservation/featured-project/neh-releases-national-digital-newspaper-program-impact-study</a>. (Accessed 29 May 2020).&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>The National Digital Newspaper Program (NDNP) Technical Guidelines for Applicants 2020-22 Awards (2020). Available at: <a href="https://www.loc.gov/ndnp/guidelines/">https://www.loc.gov/ndnp/guidelines/</a> (Accessed 28 June 2020).&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>The public search interface is available at: <a href="https://chroniclingamerica.loc.gov/">https://chroniclingamerica.loc.gov/</a>&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>Ferriter, M.  “Introducing Beyond Words | The Signal,”  (2017). Available at: <a href="https://doi.org/blogs.loc.gov/thesignal/2017/09/introducing-beyond-words/">//blogs.loc.gov/thesignal/2017/09/introducing-beyond-words/</a>. (Accessed: 13 July 2020).&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>For more information on the  <em>Beyond Words</em>  workflow, see <sup id="fnref:100"><a href="#fn:100" class="footnote-ref" role="doc-noteref">100</a></sup>, as well as <sup id="fnref4:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>.&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>In particular, the annotations were used to finetune an object detection model that had been pre-trained on Common Objects in Context, a common dataset for benchmarking object detection algorithms.&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>A screenshot of the workflow can be found later in this article in Figure 4.&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>For those who are not familiar with image embeddings, a detailed description is provided in Section IX.&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>For the dataset, see: <a href="https://news-navigator.labs.loc.gov">https://news-navigator.labs.loc.gov</a>; for the code, see <a href="https://github.com/LibraryOfCongress/newspaper-navigator">https://github.com/LibraryOfCongress/newspaper-navigator</a>.&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>Lee, B., Mears, J., Jakeway, E., Ferriter, M., Adams, C., Yarasavage, N., Thomas, D., Zwaard, K., and Weld, D.  “The Newspaper Navigator Dataset: Extracting And Analyzing Visual Content from 16 Million Historic Newspaper Pages in Chronicling America,”   <a href="https://dl.acm.org/doi/proceedings/10.1145/3340531">CIKM &lsquo;20: Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management</a>   _, _ pp. 3055–3062 (2020). Available at: <a href="https://doi.org/10.1145/3340531.3412767">https://doi.org/10.1145/3340531.3412767</a>.&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref3:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref4:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p>LC Labs and Digital Strategy Directorate,  “Machine Learning + Libraries Summit Event Summary” (2020). Available at: <a href="https://labs.loc.gov/static/labs/meta/ML-Event-Summary-Final-2020-02-13.pdf">https://labs.loc.gov/static/labs/meta/ML-Event-Summary-Final-2020-02-13.pdf</a>.&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13">
<pre><code>“Digital Strategy | Library of Congress,”  Library of Congress (2019). Available at: [https://www.loc.gov/digital-strategy/](https://www.loc.gov/digital-strategy/) (Accessed: 30 May 2020).  
</code></pre>
&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></li>
<li id="fn:14">
<p>Cordell, R.  “Machine Learning + Libraries: A Report on the State of the Field”  (2020). Available at: <a href="https://labs.loc.gov/static/labs/work/reports/Cordell-LOC-ML-report.pdf?loclr=blogsig">https://labs.loc.gov/static/labs/work/reports/Cordell-LOC-ML-report.pdf?loclr=blogsig</a>.&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:15">
<p>Padilla, T.  <em>Responsible Operations: Data Science, Machine Learning, and AI in Libraries</em>  (2019). Available at: <a href="https://doi.org/10.25333/xk7z-9g97">https://doi.org/10.25333/xk7z-9g97</a>.&#160;<a href="#fnref:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:16">
<p>Lorang, E., Soh, L., Liu, Y., and Pack, C.  “Digital Libraries, Intelligent Data Analytics, and Augmented Description: A Demonstration Project”  (2020). Available at: <a href="https://digitalcommons.unl.edu/libraryscience/396/">https://digitalcommons.unl.edu/libraryscience/396/</a>.&#160;<a href="#fnref:16" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:17">
<p>Fyfe, P.  “An Archaeology of Victorian Newspapers,”    <em>Victorian Periodicals Review</em>  49:4, pp. 546–77 (2016). Available at: <a href="https://doi.org/10.1353/vpr.2016.0039">https://doi.org/10.1353/vpr.2016.0039</a>.&#160;<a href="#fnref:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:18">
<p>Mak, B.  “Archaeology of a Digitization,”    <em>Journal of the Association for Information Science and Technology</em>  65:8, pp. 1515–26 (2014). Available at: <a href="https://doi.org/10.1002/asi.23061">https://doi.org/10.1002/asi.23061</a>.&#160;<a href="#fnref:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:19">
<p>Crawford, K., and Paglen, T.  “Excavating AI: The Politics of Training Sets for Machine Learning”  (2019). Available at: <a href="https://excavating.ai">https://excavating.ai</a> (Accessed: 19 September 2019).&#160;<a href="#fnref:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:20">
<p>Cordell, R.  “‘Q i-Jtb the Raven’: Taking Dirty OCR Seriously,”    <em>Book History</em>  20:1, pp. 188–225 (2017). Available at: <a href="https://doi.org/10.1353/bh.2017.0006">https://doi.org/10.1353/bh.2017.0006</a>.&#160;<a href="#fnref:20" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:20" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:20" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:21">
<p>Owens, T., and Padilla, T.  “Digital Sources and Digital Archives: Historical Evidence in the Digital Age,”    _International Journal of Digital Humanities _ (2020). Available at:<a href="https://doi.org/10.1007/s42803-020-00028-7">https://doi.org/10.1007/s42803-020-00028-7</a>  <a href="https://doi.org/10.1007/s42803-020-00028-7">https://doi.org/10.1007/s42803-020-00028-7</a>.&#160;<a href="#fnref:21" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:22">
<p>Gebru, T., Morgenstern, J., Vecchione, B., Vaughan, J., Wallach, H., Daumé III, H., and Crawford, K.  “Datasheets for Datasets.”  <em>ArXiv:1803.09010 [Cs]</em> , March 19, 2020. <a href="http://arxiv.org/abs/1803.09010">http://arxiv.org/abs/1803.09010</a> (Accessed: July 29 2021).&#160;<a href="#fnref:22" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:23">
<p>Holland, S., Hosny, A., Newman, S., Joseph, J., and Chmielinski, K. “The Dataset Nutrition Label: A Framework To Drive Higher Data Quality Standards.”    <em>ArXiv:1805.03677 [Cs]</em> , May 9, 2018. <a href="http://arxiv.org/abs/1805.03677">http://arxiv.org/abs/1805.03677</a> (Accessed 29 July 2021).&#160;<a href="#fnref:23" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:24">
<p>Bender, E., and Friedman, B.  “Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science.”    <em>Transactions of the Association for Computational Linguistics</em>  6 (2018): 587–604. <a href="https://doi.org/10.1162/tacl_a_00041">https://doi.org/10.1162/tacl_a_00041</a> (Accessed 29 July 2021).&#160;<a href="#fnref:24" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:25">
<p>Mitchell, M., Wu, S., Zaldivar, A., Barnes, P., Vasserman, L., Hutchinson, B., Spitzer, E., Raji, I., and Gebru, T.  “Model Cards for Model Reporting.”    <em>Proceedings of the Conference on Fairness, Accountability, and Transparency</em> , January 29, 2019, 220–29. <a href="https://doi.org/10.1145/3287560.3287596">https://doi.org/10.1145/3287560.3287596</a>.&#160;<a href="#fnref:25" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:26">
<p>Reisman, D., Schultz, J., Crawford, K., Whittaker, M.  <em>Algorithmic Impact Assessments: A Practical Framework for Public Agency Accountability</em>  (2018). Available at: <a href="https://ainowinstitute.org/aiareport2018.pdf">https://ainowinstitute.org/aiareport2018.pdf</a>.&#160;<a href="#fnref:26" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:27">
<p>Farrar, H.  <em>The Baltimore Afro-American, 1892-1950</em> . Greenwood Publishing Group (1998).&#160;<a href="#fnref:27" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:28">
<p>Iowa state bystander. [volume] (Des Moines, Iowa), 14 Oct. 1910.  <em>Chronicling America: Historic American Newspapers</em> . Library of Congress. Available at: <a href="https://chroniclingamerica.loc.gov/lccn/sn83025186/1910-10-14/ed-1/seq-1/">https://chroniclingamerica.loc.gov/lccn/sn83025186/1910-10-14/ed-1/seq-1/</a>&#160;<a href="#fnref:28" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:28" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:29">
<p>Franklin&rsquo;s paper the statesman. (Denver, Colo.), 15 Oct. 1910.  <em>Chronicling America: Historic American Newspapers</em> . Library of Congress. Available at: <a href="https://chroniclingamerica.loc.gov/lccn/sn91052311/1910-10-15/ed-1/seq-16/">https://chroniclingamerica.loc.gov/lccn/sn91052311/1910-10-15/ed-1/seq-16/</a>&#160;<a href="#fnref:29" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:29" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:30">
<p>The broad ax. [volume] (Salt Lake City, Utah), 15 Oct. 1910.  <em>Chronicling America: Historic American Newspapers</em> . Library of Congress. Available at: <a href="https://chroniclingamerica.loc.gov/lccn/sn84024055/1910-10-15/ed-1/seq-2/">https://chroniclingamerica.loc.gov/lccn/sn84024055/1910-10-15/ed-1/seq-2/</a>&#160;<a href="#fnref:30" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:30" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:31">
<p>The broad ax. [volume] (Salt Lake City, Utah), 26 Nov. 1910.  <em>Chronicling America: Historic American Newspapers</em> . Library of Congress. Available at: <a href="https://chroniclingamerica.loc.gov/lccn/sn84024055/1910-11-26/ed-1/seq-3/">https://chroniclingamerica.loc.gov/lccn/sn84024055/1910-11-26/ed-1/seq-3/</a>&#160;<a href="#fnref:31" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:31" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:32">
<p>Hardy, M., and DiCuirci, L.  “Critical Cataloging and the Serials Archive: The Digital Making of ‘Mill Girls in Nineteenth-Century Print,’”  Archive Journal, Available at: <a href="http://www.archivejournal.net/?p=8073">http://www.archivejournal.net/?p=8073</a>.&#160;<a href="#fnref:32" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:32" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:33">
<p>Indeed, compiling bibliographies of serials published after 1820 remains an immensely difficult task <sup id="fnref1:32"><a href="#fn:32" class="footnote-ref" role="doc-noteref">32</a></sup>.&#160;<a href="#fnref:33" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:34">
<p>The extent to which newspaper microfilming was driven by credible fear of deterioration versus other factors, such as microfilm marketing, is an important question that is rightly debated. For more on this topic, see <sup id="fnref1:44"><a href="#fn:44" class="footnote-ref" role="doc-noteref">44</a></sup>.&#160;<a href="#fnref:34" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:35">
<p>For example, a 2017 article describing the West Virginia University Libraries’ West Virginia &amp; Regional History Center and its participation in the National Digital Newspaper Program states: “ By August 2017, all known issues of West Virginia’s African-American newspapers from the 19th and early 20th centuries will have been digitized ”   <sup id="fnref:101"><a href="#fn:101" class="footnote-ref" role="doc-noteref">101</a></sup>. The article describes Curator Stewart Plein’s efforts to locate surviving copies of three Black West Virginia newspapers in order to digitize and include them in  <em>Chronicling America</em> .&#160;<a href="#fnref:35" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:36">
<p>Owens, T.  <em>The Theory and Craft of Digital Preservation</em> . Johns Hopkins University Press, Baltimore (2018).&#160;<a href="#fnref:36" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:36" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:37">
<p>Division of Preservation and Access (NEH),  “Notice of Funding Opportunity, National Digital Newspaper Program”  (2020). Available at: <a href="https://www.neh.gov/sites/default/files/inline-files/National-Digital-Newspaper-Program-NOFO-January-2020.pdf">https://www.neh.gov/sites/default/files/inline-files/National-Digital-Newspaper-Program-NOFO-January-2020.pdf</a>. (Accessed 28 June 2020).&#160;<a href="#fnref:37" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:37" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:37" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:38">
<p>For a thorough case study of this process, I direct the reader to  “Qi-jtb the Raven,”  in which Ryan Cordell walks through an example with the Pennsylvania Digital Newspaper Program <sup id="fnref1:20"><a href="#fn:20" class="footnote-ref" role="doc-noteref">20</a></sup>.&#160;<a href="#fnref:38" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:39">
<pre><code>“Content Selection - National Digital Newspaper Program (Library of Congress)” (2020). Available at: [https://www.loc.gov/ndnp/guidelines/selection.html](https://www.loc.gov/ndnp/guidelines/selection.html) (Accessed 3 July 2020).   
</code></pre>
&#160;<a href="#fnref:39" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:39" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></li>
<li id="fn:40">
<p>Barrall, K. and Guenther, C.  “Microfilm Selection for Digitization,”  (2005). Available at: <a href="https://www.loc.gov/ndnp/guidelines/NEH_MicrofilmSelectionNDNP.pdf">https://www.loc.gov/ndnp/guidelines/NEH_MicrofilmSelectionNDNP.pdf</a>.&#160;<a href="#fnref:40" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:41">
<pre><code>“Meta | Morphosis: Tutorials,”  National Digital Newspaper Program and the University of Kentucky Libraries. Available at: [https://www.uky.edu/Libraries/NDNP/metamorphosis/tutorials.html](https://www.uky.edu/Libraries/NDNP/metamorphosis/tutorials.html) (Accessed 3 July 2020).  
</code></pre>
&#160;<a href="#fnref:41" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></li>
<li id="fn:42">
<pre><code>“About the Program - National Digital Newspaper Program (Library of Congress),”  (2019). Available at: [https://www.loc.gov/ndnp/about.html](https://www.loc.gov/ndnp/about.html) (Accessed 3 July 2020).   
</code></pre>
&#160;<a href="#fnref:42" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></li>
<li id="fn:43">
<p>Williams, L.  “What Computational Archival Science Can Learn from Art History and Material Culture Studies,”  in  <em>2019 IEEE International Conference on Big Data (Big Data)</em> , 2019, pp. 3153–55. Available at: <a href="https://doi.org/10.1109/BigData47090.2019.9006527">https://doi.org/10.1109/BigData47090.2019.9006527</a>.&#160;<a href="#fnref:43" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:43" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:43" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref3:43" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref4:43" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:44">
<p>Baker, N.  <em>Double Fold: Libraries and the Assault on Paper</em> . Random House (2001).&#160;<a href="#fnref:44" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:44" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:45">
<p>Lee, B.  “Machine Learning, Template Matching, and the International Tracing Service Digital Archive: Automating the Retrieval of Death Certificate Reference Cards from 40 Million Document Scans,”    <em>Digital Scholarship in the Humanities</em>  34:3, pp. 513-535 (2019). <a href="https://doi.org/10.1093/llc/fqy063">Available at: </a>  <a href="https://doi.org/10.1093/llc/fqy063">https://doi.org/10.1093/llc/fqy063</a>.&#160;<a href="#fnref:45" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:45" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:46">
<p>For exemplary research collaborations that utilize the  _Chronicling America _ bulk OCR, see the Viral Text Project and the Oceanic Exchanges Project <sup id="fnref2:20"><a href="#fn:20" class="footnote-ref" role="doc-noteref">20</a></sup>; <sup id="fnref:102"><a href="#fn:102" class="footnote-ref" role="doc-noteref">102</a></sup>.&#160;<a href="#fnref:46" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:47">
<p>For other examinations of how OCR mediates our interactions with digital archives, see <sup id="fnref:103"><a href="#fn:103" class="footnote-ref" role="doc-noteref">103</a></sup>; <sup id="fnref:104"><a href="#fn:104" class="footnote-ref" role="doc-noteref">104</a></sup>; <sup id="fnref:105"><a href="#fn:105" class="footnote-ref" role="doc-noteref">105</a></sup>; <sup id="fnref:106"><a href="#fn:106" class="footnote-ref" role="doc-noteref">106</a></sup>; <sup id="fnref:107"><a href="#fn:107" class="footnote-ref" role="doc-noteref">107</a></sup>.&#160;<a href="#fnref:47" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:48">
<p>For a concrete example of a similar phenomenon in the image domain, see <sup id="fnref1:45"><a href="#fn:45" class="footnote-ref" role="doc-noteref">45</a></sup>, in which a machine learning algorithm was trained to classify digitized images but consistently misclassified images that had been misoriented 180 degrees in the scanning bed - a consequence of the classifier not having seen enough instances of these misoriented scans during training.&#160;<a href="#fnref:48" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:49">
<p>Noble, S.  <em>Algorithms of Oppression: How Search Engines Reinforce Racism</em> . NYU Press, New York (2018).&#160;<a href="#fnref:49" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:50">
<p>Reidsma, M.  <em>Masked by Trust: Bias in Library Discovery.</em>  Litwin Books, Sacramento (2019).&#160;<a href="#fnref:50" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:51">
<p>Alpert-Abrams, H. “Machine Reading the Primeros Libros,”  <em>Digital Humanities Quarterly</em>  10:4 (2016).&#160;<a href="#fnref:51" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:51" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:52">
<p>LC Labs,  “Beyond Words: Mark” Available at: <a href="http://beyondwords.labs.loc.gov/#/mark">http://beyondwords.labs.loc.gov/#/mark</a> (Accessed 5 June, 2020).&#160;<a href="#fnref:52" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:53">
<p>LC Labs,  “Beyond Words: Transcribe,”  Available at: <a href="http://beyondwords.labs.loc.gov/#/transcribe">http://beyondwords.labs.loc.gov/#/transcribe</a> (Accessed 5 June, 2020).&#160;<a href="#fnref:53" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:54">
<p>LC Labs,  “Beyond Words: Veriffy,”  Available at: <a href="http://beyondwords.labs.loc.gov/#/verify">http://beyondwords.labs.loc.gov/#/verify</a> (Accessed 5 June, 2020).&#160;<a href="#fnref:54" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:55">
<p>It should be noted that  _Beyond Words _ was introduced by LC Labs as an experiment, with no interventions in workflow or community management.&#160;<a href="#fnref:55" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:56">
<p>Lee, B.  <em>LibraryOfCongress/Newspaper-Navigator</em> , GitHub Repository ( Library of Congress, 2020).<a href="https://github.com/LibraryOfCongress/newspaper-navigator"> Available at: </a>  <a href="https://github.com/LibraryOfCongress/newspaper-navigator">https://github.com/LibraryOfCongress/newspaper-navigator</a>.&#160;<a href="#fnref:56" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:57">
<p>Bounding box coordinates refer to the positions of the corners of the predicted bounding box, relative to the image coordinates.&#160;<a href="#fnref:57" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:58">
<p>The confidence score is examined in more detail in the next section.&#160;<a href="#fnref:58" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:59">
<p>This modest cut is provided to remove the large number of predictions with confidence scores between 0% and 5%, which have high false-positive rates, and thus reduce the size of the  _Newspaper Navigator _ dataset.&#160;<a href="#fnref:59" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:60">
<p>Weld, D., and Bansal, G. 2019. The challenge of crafting intelligible intelligence. Commun. ACM 62: 6, pp. 70–79 (2019). Available at: <a href="https://doi.org/10.1145/3282486">https://doi.org/10.1145/3282486</a>.&#160;<a href="#fnref:60" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:61">
<p>The _ Newspaper Navigator _ dataset does not retain the cropped images of headlines, as the textual content is more salient than visual snippets in the case of headlines.&#160;<a href="#fnref:61" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:62">
<p>[Newspaper Navigator 1910b]  _Newspaper Navigator _  metadata for the  <em>Iowa State Bystander</em>  (14 October 1910). From the Library of Congress, Newspaper Navigator dataset: Extracted Visual Content from Chronicling America. Available at: <a href="https://news-navigator.labs.loc.gov/data/iahi_ames_ver01/data/sn83025186/00202198417/1910101401/1015.json">https://news-navigator.labs.loc.gov/data/iahi_ames_ver01/data/sn83025186/00202198417/1910101401/1015.json</a>.&#160;<a href="#fnref:62" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:63">
<pre><code>_Newspaper Navigator _  metadata for  _Franklin’s Paper the Statesman_  (15 October 1910). From the Library of Congress, Newspaper Navigator dataset: Extracted Visual Content from Chronicling America. Available at: [https://news-navigator.labs.loc.gov/data/cohi_abbeyville_ver01/data/sn91052311/00279550730/1910101501/2272](https://news-navigator.labs.loc.gov/data/cohi_abbeyville_ver01/data/sn91052311/00279550730/1910101501/2272.json)  [.json](https://news-navigator.labs.loc.gov/data/cohi_abbeyville_ver01/data/sn91052311/00279550730/1910101501/2272.json)    
</code></pre>
&#160;<a href="#fnref:63" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></li>
<li id="fn:64">
<pre><code>_Newspaper Navigator _ metadata for  _The Broad Ax _ (15 October 1910). From the Library of Congress, Newspaper Navigator dataset: Extracted Visual Content from Chronicling America. Available at: [https://news-navigator.labs.loc.gov/data/iune_charlie_ver01/data/sn84024055/00280761059/1910101501/0538.json](https://news-navigator.labs.loc.gov/data/iune_charlie_ver01/data/sn84024055/00280761059/1910101501/0538.json)    
</code></pre>
&#160;<a href="#fnref:64" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></li>
<li id="fn:65">
<pre><code>_Newspaper Navigator _ metadata for The Broad Ax (26 November 1910). From the Library of Congress, Newspaper Navigator dataset: Extracted Visual Content from Chronicling America. Available at: [https://news-navigator.labs.loc.gov/data/iune_charlie_ver01/data/sn84024055/00280761059/1910112601/0564.json](https://news-navigator.labs.loc.gov/data/iune_charlie_ver01/data/sn84024055/00280761059/1910112601/0564.json)    
</code></pre>
&#160;<a href="#fnref:65" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></li>
<li id="fn:66">
<p>If these words are unfamiliar, the three takeaways listed are more important.&#160;<a href="#fnref:66" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:67">
<pre><code>“Yale Digital Humanities Lab - PixPlot”  (2020). Available at: [https://dhlab.yale.edu/projects/pixplot/](https://dhlab.yale.edu/projects/pixplot/) (Accessed 11 June 2020).  </code></pre>
&#160;<a href="#fnref:67" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></li>
<li id="fn:68">
<p>Lincoln, M., Levin, G., Conell, S., and Huang, L. (2019)  “National Neighbors: Distant Viewing the National Gallery of Art&rsquo;s Collection of Collections”  (2019) Available at: <a href="https://nga-neighbors.library.cmu.edu/">https://nga-neighbors.library.cmu.edu</a>. (Accessed: 30 May 2020).&#160;<a href="#fnref:68" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:69">
<pre><code>“Google Arts &amp; Culture Experiments - t-SNE Map Experiment”  (2018). Available at: [https://artsexperiments.withgoogle.com/tsnemap/](https://artsexperiments.withgoogle.com/tsnemap/) (Accessed: 11 June 2020).  
</code></pre>
&#160;<a href="#fnref:69" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></li>
<li id="fn:70">
<pre><code>“Project: «Principal Components»,”  Nasjonalmuseet (2018). Available at: [https://www.nasjonalmuseet.no/en/about-the-national-museum/collection-management — -behind-the-scenes/digital-collection-management/project-principal-components/](https://www.nasjonalmuseet.no/en/about-the-national-museum/collection-management---behind-the-scenes/digital-collection-management/project-principal-components/) (Accessed 11 June 2020).  
</code></pre>
&#160;<a href="#fnref:70" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></li>
<li id="fn:71">
<p>Giraldo, M.  “Building Aereo,”  DX Lab | State Library of NSW (2020). Available at: <a href="https://dxlab.sl.nsw.gov.au/blog/building-aereo">https://dxlab.sl.nsw.gov.au/blog/building-aereo</a> (Accessed: 2 July 2020).&#160;<a href="#fnref:71" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:72">
<p>Vane, O.  “Visualising the Royal Photographic Society Collection: Part 2 • V&amp;A Blog,”    <em>V&amp;A Blog</em>  (2018). Available at: <a href="https://www.vam.ac.uk/blog/digital/visualising-the-royal-photographic-society-collection-part-2">https://www.vam.ac.uk/blog/digital/visualising-the-royal-photographic-society-collection-part-2</a>.&#160;<a href="#fnref:72" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:73">
<p>Foo, B.  “AMNH Photographic Collection,”  (2020). Available at: <a href="https://amnh-sciviz.github.io/image-collection/about.html">https://amnh-sciviz.github.io/image-collection/about.html</a> (Accessed: 11 June 2020).&#160;<a href="#fnref:73" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:74">
<p>Lonij, J., and Wevers, M. (2017) SIAMESE. KB Lab: The Hague (2017). Available at: <a href="http://lab.kb.nl/tool/siamese">http://lab.kb.nl/tool/siamese</a>.&#160;<a href="#fnref:74" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:75">
<p>Wevers, M., and Smits, T.  “The Visual Digital Turn: Using Neural Networks to Study Historical Images,”    <em>Digital Scholarship in the Humanities</em>  35:1, pp. 194-207 (2020). Available at: <a href="https://doi.org/10.1093/llc/fqy085">https://doi.org/10.1093/llc/fqy085</a>.&#160;<a href="#fnref:75" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:76">
<p>For an introduction to some of these methods with lower-level features, see <sup id="fnref:108"><a href="#fn:108" class="footnote-ref" role="doc-noteref">108</a></sup>.&#160;<a href="#fnref:76" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:77">
<p>Lee, B., and Weld, D.  “Newspaper Navigator: Open Faceted Search for 1.5 Million Images,”   <a href="https://dl.acm.org/doi/proceedings/10.1145/3379350">UIST &lsquo;20 Adjunct: Adjunct Publication of the 33rd Annual ACM Symposium on User Interface Software and Technology</a>, pp. 120-122 (2020). Available at: <a href="https://doi.org/10.1145/3379350.3416143">https://doi.org/10.1145/3379350.3416143</a>.&#160;<a href="#fnref:77" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:77" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:78">
<p>The search application can be found at: <a href="https://news-navigator.labs.loc.gov/search">https://news-navigator.labs.loc.gov/search</a>.&#160;<a href="#fnref:78" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:79">
<p>He, K., Zhang, X., Ren, S., and Sun, J. “Deep Residual Learning for Image Recognition,”  in  <em>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em> , 2016, pp. 770–78,<a href="https://doi.org/10.1109/CVPR.2016.90"> Available at: </a>  <a href="https://doi.org/10.1109/CVPR.2016.90">https://doi.org/10.1109/CVPR.2016.90</a>.&#160;<a href="#fnref:79" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:80">
<p>Deng, J., Dong, W., Socher, R., Li, L., Li, K., and Fei-Fei, L.  “ImageNet: A Large-Scale Hierarchical Image Database,” in  <em>2009 IEEE Conference on Computer Vision and Pattern Recognition</em>  (2009), pp. 248–55,<a href="https://doi.org/10.1109/CVPR.2009.5206848"> Available at: </a>  <a href="https://doi.org/10.1109/CVPR.2009.5206848">https://doi.org/10.1109/CVPR.2009.5206848</a>.&#160;<a href="#fnref:80" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:81">
<pre><code>“ImageNet, ” Available at: [http://image-net.org/index](http://image-net.org/index) (Accessed: 8 June 2020).  
</code></pre>
&#160;<a href="#fnref:81" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></li>
<li id="fn:82">
<p>Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A., and Fei-Fei, L.  “ImageNet Large Scale Visual Recognition Challenge,”  <em>International Journal of Computer Vision</em>  115:3, pp. 211-252 (2015). Available at:<a href="https://doi.org/10.1007/s11263-015-0816-y">https://doi.org/10.1007/s11263-015-0816-y</a>  <a href="https://doi.org/10.1007/s11263-015-0816-y">https://doi.org/10.1007/s11263-015-0816-y</a>.&#160;<a href="#fnref:82" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:83">
<p>The specific categories used in the challenge can be found at: <a href="http://image-net.org/challenges/LSVRC/2010/browse-synsets">http://image-net.org/challenges/LSVRC/2010/browse-synsets</a>.&#160;<a href="#fnref:83" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:84">
<pre><code>“What about the images?” Available at: [http://image-net.org/download-faq](http://image-net.org/download-faq) (Accessed: 8 June 2020).  
</code></pre>
&#160;<a href="#fnref:84" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></li>
<li id="fn:85">
<p>Hara, K. et al. “A Data-Driven Analysis of Workers’ Earnings on Amazon Mechanical Turk,” in  <em>Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems</em> , CHI ’18 (Montreal QC, Canada: Association for Computing Machinery, 2018), pp. 1–14. Available at: <a href="https://doi.org/10.1145/3173574.3174023">https://doi.org/10.1145/3173574.3174023</a>.&#160;<a href="#fnref:85" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:86">
<p>Cecire, N. “Works Cited: The Visible Hand,”  _ Works Cited_  (blog) (2011). Available at: <a href="http://nataliacecire.blogspot.com/2011/05/visible-hand.html">http://nataliacecire.blogspot.com/2011/05/visible-hand.html</a>.&#160;<a href="#fnref:86" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:87">
<p>For more reading on this topic, see <sup id="fnref:109"><a href="#fn:109" class="footnote-ref" role="doc-noteref">109</a></sup>.&#160;<a href="#fnref:87" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:88">
<p>van der Maaten, L., and Hinton, G.  “Visualizing Data Using T-SNE,”    <em>Journal of Machine Learning Research</em>  9, pp. 2579-2605 (2008). Available at: <a href="http://www.jmlr.org/papers/v9/vandermaaten08a.html">http://www.jmlr.org/papers/v9/vandermaaten08a.html</a>.&#160;<a href="#fnref:88" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:89">
<p>Wattenberg, M., Viégas, F., and Johnson, I.  “How to Use T-SNE Effectively,”    <em>Distill</em>  1:10 (2016). Available at: <a href="https://doi.org/10.23915/distill.00002">https://doi.org/10.23915/distill.00002</a>.&#160;<a href="#fnref:89" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:90">
<p>Strubell, E., Ganesh, A., and McCallum, A.  “Energy and Policy Considerations for Deep Learning in NLP,”  ArXiv:1906.02243 [Cs] (2019). Available at: <a href="http://arxiv.org/abs/1906.02243">http://arxiv.org/abs/1906.02243</a>.&#160;<a href="#fnref:90" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:91">
<p>Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei D.  “Language Models Are Few-Shot Learners,”  <em>ArXiv:2005.14165 [Cs]</em>  (2020),<a href="http://arxiv.org/abs/2005.14165"> Available at: </a>  <a href="http://arxiv.org/abs/2005.14165">http://arxiv.org/abs/2005.14165</a> (Accessed: 6 June 2020).&#160;<a href="#fnref:91" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:92">
<p>Schwartz, R., Dodge, J., Smith, N., and Etzioni, O.  “Green AI,”  ArXiv:1907.10597 [Cs, Stat], (2019). Available at:<a href="http://arxiv.org/abs/1907.10597">http://arxiv.org/abs/1907.10597</a>  <a href="http://arxiv.org/abs/1907.10597">http://arxiv.org/abs/1907.10597</a>.&#160;<a href="#fnref:92" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:93">
<p>Lacoste, A., Luccioni, A., Schmidt, V., and Dandres, T.  “Quantifying the Carbon Emissions of Machine Learning,”    <em>ArXiv:1910.09700 [Cs]</em>  (2019). Available at: <a href="http://arxiv.org/abs/1910.09700">http://arxiv.org/abs/1910.09700</a>.&#160;<a href="#fnref:93" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:93" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:93" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:94">
<pre><code>“Carbon Footprint Calculator,”   [Available at: ](https://calculator.carbonfootprint.com/calculator.aspx?lang=en-GB&amp;tab=3)  [https://calculator.carbonfootprint.com/calculator.aspx?lang=en-GB&amp;tab=3](https://calculator.carbonfootprint.com/calculator.aspx?lang=en-GB&amp;tab=3). (Accessed: 6 June 2020).  
</code></pre>
&#160;<a href="#fnref:94" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></li>
<li id="fn:95">
<pre><code>“Amazon EC2 Instance Types - Amazon Web Services,”  (2020) Amazon Web Services, Inc.[ Available at: ](https://aws.amazon.com/ec2/instance-types/)  [https://aws.amazon.com/ec2/instance-types/](https://aws.amazon.com/ec2/instance-types/). (Accessed: 5 June 2020).  
</code></pre>
&#160;<a href="#fnref:95" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></li>
<li id="fn:96">
<pre><code>“Intel® Xeon® Platinum 9242 Processor (71.5M Cache, 2.30 GHz) Product Specifications,”  Available at: [https://ark.intel.com/content/www/us/en/ark/products/194145/intel-xeon-platinum-9242-processor-71-5m-cache-2-30-ghz.html](https://ark.intel.com/content/www/us/en/ark/products/194145/intel-xeon-platinum-9242-processor-71-5m-cache-2-30-ghz.html) (Accessed: 5 June 2020).  
</code></pre>
&#160;<a href="#fnref:96" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></li>
<li id="fn:97">
<pre><code>“Intel® Xeon® Platinum 8256 Processor (16.5M Cache, 3.80 GHz) Product Specifications,”  Available at: [https://ark.intel.com/content/www/us/en/ark/products/192467/intel-xeon-platinum-8256-processor-16-5m-cache-3-80-ghz.html](https://ark.intel.com/content/www/us/en/ark/products/192467/intel-xeon-platinum-8256-processor-16-5m-cache-3-80-ghz.html) (Accessed: June 5, 2020).  
</code></pre>
&#160;<a href="#fnref:97" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></li>
<li id="fn:98">
<p>Lee, B., Berson, I., and Berson, M.  “Machine Learning and the Social Studies,”    <em>Social Education</em>  85:2, pp. 88-92 (2021). Available at: <a href="https://www.socialstudies.org/social-education/85/2/machine-learning-and-social-studies">https://www.socialstudies.org/social-education/85/2/machine-learning-and-social-studies</a>.&#160;<a href="#fnref:98" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:99">
<p>Lee, B., Mears, J., Jakeway, E., Ferriter, M., and Potter, A.  “Newspaper Navigator: Putting Machine Learning in the Hands of Library Users,”  <em>EuropeanaTech Insight</em>  16 (2021). Available at: <a href="https://pro.europeana.eu/page/issue-16-newspapers">https://pro.europeana.eu/page/issue-16-newspapers</a>.&#160;<a href="#fnref:99" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:99" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:100">
<p>LC Labs, Beyond Words | Experiments. Available at: <a href="https://labs.loc.gov/work/experiments/beyond-words/">https://labs.loc.gov/work/experiments/beyond-words/</a> (Accessed 5 June, 2020).&#160;<a href="#fnref:100" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:101">
<p>Maxwell, M.  “WVU Today | WVRHC Seeking Copies of Rare African-American Newspapers”  (2017). Available at: <a href="https://wvutoday.wvu.edu/stories/2017/01/19/wvrhc-seeking-copies-of-rare-african-american-newspapers">https://wvutoday.wvu.edu/stories/2017/01/19/wvrhc-seeking-copies-of-rare-african-american-newspapers</a>. (Accessed 11 July 2020).&#160;<a href="#fnref:101" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:102">
<p>Oceanic Exchanges Project Team. Oceanic Exchanges: Tracing Global Information Networks In Historical Newspaper Repositories, 1840-1914 (2017). Available at: 10.17605/OSF.IO/WA94S.&#160;<a href="#fnref:102" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:103">
<p>Hitchcock, T.  “Confronting the Digital,”    <em>Cultural and Social History</em>  10:1. pp. 9–23 (2013). Available at: <a href="https://doi.org/10.2752/147800413X13515292098070">https://doi.org/10.2752/147800413X13515292098070</a>.&#160;<a href="#fnref:103" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:104">
<p>Milligan, I.  “Illusionary Order: Online Databases, Optical Character Recognition, and Canadian History, 1997–2010,”    <em>Canadian Historical Review</em>  94:4, pp. 540–69 (2013). Available at: <a href="https://doi.org/10.3138/chr.694">https://doi.org/10.3138/chr.694</a>.&#160;<a href="#fnref:104" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:105">
<p>Strange, C., McNamara, D., Wodak, J., and Wood, I.  “Mining for the Meanings of a Murder: The Impact of OCR Quality on the Use of Digitized Historical Newspapers,”    <em>Digital Humanities Quarterly</em>  8:1 (2014). Available at: <a href="/dhqwords/vol/8/1/000168/">http://www.digitalhumanities.org/dhq/vol/8/1/000168/000168.html</a>.&#160;<a href="#fnref:105" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:106">
<p>Traub, M., van Ossenbruggen, J., and Hardman, L.  “Impact Analysis of OCR Quality on Research Tasks in Digital Archives,”  in  <em>Research and Advanced Technology for Digital Libraries</em> , ed. Sarantos Kapidakis, Cezary Mazurek, and Marcin Werla (Cham: Springer International Publishing, 2015), 252–263.&#160;<a href="#fnref:106" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:107">
<p>Wright, R.  “Typewriting Mass Observation Online: Media Imprints on the Digital Archive,”    <em>History Workshop Journal</em>  87, pp. 118–38 (2019). Available at: <a href="https://doi.org/10.1093/hwj/dbz005">https://doi.org/10.1093/hwj/dbz005</a>.&#160;<a href="#fnref:107" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:108">
<p>Manovich, L.  “How to Compare One Million Images?,”  in  <em>Understanding Digital Humanities</em> , ed. David M. Berry (London: Palgrave Macmillan UK, 2012), pp. 249–78. Available at: <a href="https://doi.org/10.1057/9780230371934_14">https://doi.org/10.1057/9780230371934_14</a>.&#160;<a href="#fnref:108" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:109">
<p>Bowker, G., and Star, S.  <em>Sorting Things Out: Classification and Its Consequences</em> . MIT Press, Cambridge (2000).&#160;<a href="#fnref:109" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry><entry><title type="html">A Named Entity Recognition Model for Medieval Latin Charters</title><link href="https://rlskoeser.github.io/dhqwords/vol/15/4/000574/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://rlskoeser.github.io/dhqwords/vol/15/4/000574/</id><author><name>Pierre Chastang</name></author><author><name>Sergio Torres Aguilar</name></author><author><name>Xavier Tannier</name></author><published>2021-11-12T00:00:00+00:00</published><updated>2021-11-12T00:00:00+00:00</updated><content type="html"><![CDATA[<h2 id="1-introduction">1. Introduction</h2>
<p>In this paper, we introduce a new model for the automatic recognition of named entities in medieval Latin charters. The named entity concept involves all physical and real objects which can be designated with a specific name. A named entity recognition (NER) model detects entities in the text and classifies them (e.g., a person&rsquo;s name, a location name, an organization&rsquo;s name, a date). It becomes possible, then, to index the resulting information units and to extract relationships between them. In that sense, automatic NER systems can help to improve current information systems by building a knowledge base of all the spatial, social and personal entities in a set of documents; this enables a searchable and quotable corpus matrix using actors and places as dynamic keywords which help to provide meaningful answers to questions asked on a large textual database.</p>
<p>In recent years, some projects have entailed the production of structured data through manual tagging of named entities in thousand-item historical corpora. This manual operation takes years for an entire team and can obviously not be scaled up to the almost 500,000 (and increasing) medieval digitized documents. This lengthy process can be completed only with massive acceleration through a computational operation of converting automatically thousands of documents in raw data format into structured data in a short time.</p>
<p>In the present project we use one of these hand-annotated corpora as our dataset: the CBMA ( <em>Corpus Burgundiae Medii Aevi</em> ), which contains a collection of medieval charters and cartularies produced in Burgundy. From the 11th century on, many ecclesiastical institutions started to compose cartularies in which they transcribed their own documents, especially titles relating to their property and their land rights, in order to better preserve written deeds and keep legal proof of ecclesiastical possessions and social relationships. Today, these registers stand as one of the major sources for medieval studies. Among the texts collected in the CBMA we isolated a set of 5300-items (1,2 million words), which correspond mostly to cartularies from Cluny. This large sub-set contains mainly donations, exchanges and sales, that is, written records dealing with the transfer of lands, goods and rights between ecclesiastical institutions and between private and public actors. While these documents are relatively formulaic, with only limited vocabulary and a relatively small number of turns of phrase, their geographical provenance is as diverse (more than one hundred different places) as the chronological scope of their production is wide (from the early 10th century to the middle of the 13th century). Moreover, they include many different kinds of documents: charters, notices, bulls, diplomas, letters, lawsuits, etc.</p>
<p>As we will show, developing a model able to achieve entity recognition for this corpus presents several challenges. The need to choose a single corpus for stability and coherence can affect the ability of the model to be generalized to a wide range of documents, because it reduces the variety of samples and can hide more specific phenomena. Constructing a robust model requires finding a technical and intellectual compromise between collecting sufficiently varied data and avoiding extreme training on some similar structures <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. This issue becomes even more complicated if we consider the particular characteristics of our main corpora: on the one hand, diplomatic charters combine formulaic structure and variable lists of named entities <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>, which can be an advantage regarding iteration in training. But it also means fitting problems of recognition over other corpora, because there are many formulaic traditions, different models in according to juridical actions and regional redactional dependencies. On the other hand, because Latin is a low-resource language in natural language processing, the work must be conducted using tools and vocabularies not initially developed for Latin or developed for classical Latin literature <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>; not to mention the technical adaptations required for the specificities of medieval Latin due to several linguistics alterations. Above all, less technical but more extended, pre-processing must be completed, such as the normalization of textual spelling, the manual validation of much inflected language arbitrariness, and the listing of difficulties due to overlapping and to textual ambiguities.</p>
<p>The construction of a proper validation environment, proving the robustness of our model, has led us to build several corpora and sub-corpora variations and to annotate manually other smaller corpora. We finally developed a validation protocol with an extensive set of evaluations in order to show that our model can be applied to a wide range of unannotated charters with different typologies and belonging to different scriptural traditions and regional origins.</p>
<p>This paper describes, first, the historical nature of our corpus and the features of our gold-standard corpus. It explains the method adopted in generating the model with a machine-learning approach based on conditional random fields (CRF). We detail the construction of the model and sub-models, as well as the results obtained by the application of our general model and by the different experiments using cross-validation. In the discussion, we try to develop a broader argument for results by referring to current onomastic studies, historical and scriptural usages, and language variations, thus offering a contextualized explanation that can help to clarify, from a non-technical and humanistic perspective, matches and errors obtained in the application of the model. The constitutive models of this project have been deployed in a web-based application adapted to expert and non-expert users located at <a href="https://entities-recognition.irht.cnrs.fr/">https://entities-recognition.irht.cnrs.fr</a>. An automatic and accessible web-based annotation workflow, based on models developed in this project, has been deployed at <a href="https://entities-recognition.irht.cnrs.fr/">https://entities-recognition.irht.cnrs.fr</a>. Finally, we propose some suggestions for the fruitful application of these models in the wider context of digital humanities.</p>
<h2 id="2-academic-background">2. Academic Background</h2>
<p>Named entity recognition (NER) is one of the most promising technical tools in the digital humanities field. Some classical digital humanities projects have been conducted with NER, thereby opening new ways to explore and query large databases. Actively used in the medical, biological and journalistic domains <sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>  <sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>, named entities prove to be meaningful pieces of information intimately connected to the main issues addressed by humanistic studies. NER techniques are an excellent way to provide an overview of a corpus. On the one hand, they help to classify data into pre-defined categories, which is primordial to indexing and describing data and eventually to building structured databases. On the other hand, they can activate data exploration at different scope levels. For example, NER techniques can be used to identify core concepts and cluster them into vocabularies, ontologies, and actor&rsquo;s attributes on documentary series They are also indispensable in the pre-treatment of texts since they provide pieces of information that do not appear in the language dictionaries. Alternatively, at a corpora level, NER techniques can serve to build thick social, spatial and semantic relationship networks using named entities as nodes.</p>
<p>NER work in the digital humanities community consists of two subtasks: (i) detection and classification of named entities in classical and modern language texts and (ii) entity linking towards already existing knowledge databases. Concerning the first task, experiments have proven that supervised approaches are more suitable for large-scale databases than rule-based or unsupervised ones <sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>. In these supervised methods, especially statistics-sequential methods, algorithms use labelled data and meta-information obtained by different types of syntactical analysis, such as POS tagging, parsing, chunking, from word-based to sentence-based levels. The system then builds a statistical model based on these labelled data and suggests the best labels for a new, unlabelled sequence of words. Some digital humanities projects in recent years <sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>  <sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>  <sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup> produce supervised models exceeding 80% on the F-measure (harmonic mean of precision and recall).</p>
<p>Nevertheless, because of the lack of annotated data, rule-based or dictionary-based models are still very popular, despite the fact that they generally achieve a low recall ratio (for the former) or a low precision ratio (for the latter). Training a dictionary-based recognition model against a list of names can lead to a high ratio of recognition for a particular corpus, but the model is often not robust when applied to unseen texts or different types of data. On the other hand, rule-based models trained using rule definitions and descriptions to categorize entities show a valid global recall, but a slight tendency to poor precision-ratio on unseen texts. In spite of that, these methods have proven very useful for rapid textual annotation of small corpora or more standardized corpora like those from literature and journals <sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup>  <sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>  <sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>.</p>
<p>Recent years have seen an increasing popularity of hybrid, semi-supervised approaches. Bootstrapping techniques use a small set of annotated data in order to obtain automatically tagged data by taking advantage of specific repetitive and contextual patterns. Some work on this topic has introduced interesting propositions with excellent results <sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup>  <sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup>  <sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup>.</p>
<p>Both unsupervised and supervised methods need annotated corpora in order to build better linguistic analyzers and evaluation tools. This problem has motivated, in recent years, different actions in the digital humanities community. The main one is the production of handcrafted corpora with one or more fully annotated attributes, such as morphological features, semantical descriptions, syntactic relationships, named entities, etc. The medieval corpora made available by <a href="http://www.cbma-project.eu/">CBMA,</a>  <a href="http://cdlm.unipv.it/">CDLM,</a> and <a href="http://srcmf.org/">SRCMF</a> projects are among these, as well as examples of well-constructed lists of authorities, gazetteers and treebanks such as proposed by <a href="http://commons.pelagios.org/">Pelagios</a>, <a href="http://web.philo.ulg.ac.be/lasla/">LASLA,</a> and <a href="http://www.perseus.tufts.edu">Perseus,</a> which are used in many projects to improve semi-supervised methods specially addressed to classical and medieval literature. In addition, the growing number of platforms created to facilitate morphological analysis of new corpora, like <a href="http://outils.biblissima.fr/fr/collatinus-web/">Collatinus</a>  <a href="http://www.lemlat3.eu/">Lemlat,</a> or <a href="https://gutentag.sdsu.edu/">GutenTag</a>, is remarkable; these platforms offer a gateway to rapid extraction of a full range of linguistic attributes.</p>
<p>All these efforts are complemented by an increasing availability of programming packages including easy access to classical techniques like POS tagging, parsing, chunking and named entity resolution in combination with annotating tools based on treebanks-training such as TreeTagger and Lapos. They are contributing to the incorporation of NER in mid-range projects. <a href="https://stanfordnlp.github.io/CoreNLP/">Stanford CoreNLP</a>, <a href="http://nlp.lsi.upc.edu/freeling/node/1">Freeling,</a>, <a href="https://www.nltk.org/">Natural Language Toolkit</a> and <a href="http://scikit-learn.org/stable/">Scikit-learn</a> are among the most used set of tools and libraries.</p>
<p>Concerning the task of disambiguation (entity linking), projects are close to semantic Web and linked open data principles, based on interoperability and machine-readable data. Natural languages are ambiguous by definition, and retrieved entity mentions must be clearly identified. There are several propositions, especially from library domains, calling to use lists of authors and authoritative databases to link recovered entities with unique referents stocked in big databases like DBpedia <sup id="fnref:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup>, Freebase <sup id="fnref:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup>, or Yago <sup id="fnref:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup> using URI’s (Uniform Resource Identifier) <sup id="fnref:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup>  <sup id="fnref:20"><a href="#fn:20" class="footnote-ref" role="doc-noteref">20</a></sup>. Some very interesting works based on small-size corpora have been taking this direction, especially studies by literary scholars <sup id="fnref:21"><a href="#fn:21" class="footnote-ref" role="doc-noteref">21</a></sup>  <sup id="fnref:22"><a href="#fn:22" class="footnote-ref" role="doc-noteref">22</a></sup>. However, they are limited to databases containing knowledge about relevant people or mapping modern texts. For instance, these works are tightly focused on geographical disambiguation, because places do not show a high number of possibilities.</p>
<p>To address both problems, automatic entity detection and entity disambiguation, it is necessary to have large, hand-made annotated corpora whose production is highly time-consuming work. For instance, computer acceleration is the most promising solution for dealing with the large and growing number of available digital historical documents. Algorithm-based techniques can do intelligent tasks and provide a very valuable result. However, experience shows that high context dependency at different levels compels the combination of the automatic result with an academic methodology to define correct heuristics tasks and to incorporate the most adequate epistemological rules.</p>
<h2 id="3-corpora-and-model">3. Corpora and Model</h2>
<h2 id="31-cbma-corpus-description">3.1. CBMA corpus description</h2>
<p>The main corpus we use comes from a database of medieval Burgundian charters provided by the CBMA group.<sup id="fnref:23"><a href="#fn:23" class="footnote-ref" role="doc-noteref">23</a></sup>  The size of the complete database is about 29,000 documents stocked in cartularies and collections of originals, among which a sub-corpus of 5,300 items has been annotated with both person and location entities by the CBMA scientific team. The document set is mainly comprised of private charters produced between the 10th and 14th centuries in Cluniac abbeys, and a small part in Cistercian abbeys. The items come from almost 100 small places in Burgundy, and they are stored in ten different cartularies (mainly in four: Cartulary A and B from the abbey of Cluny, the cartulary of St Vincent of Maçon, the cartulary of the priory of Jully-les-Nonnains, and the cartulary from the Cistercian abbey of Vauluisant).<sup id="fnref:24"><a href="#fn:24" class="footnote-ref" role="doc-noteref">24</a></sup>  These cartularies were edited, during the 19th and 20th centuries, with different diplomatic and philological editorial standards. (Figure 1 shows how these acts spread over time.)</p>
<p>Typewritten texts digitized in modern editions are the primary source of available textual corpora, where elements such as capitalization, punctuation, and development of abbreviations have been added, therefore modernizing original sources for easier reading. Plain text has been stored in a database, and a team of historical and philological experts has manually annotated named entities. Due to lack of time and resources, personal and geographical entities have been tagged, but not juridical and institutional entities.</p>
<p>Our gold-standard corpus composed by the CBMA team is mainly distributed through five main types of acts: diplomas, charters, bulls, notices, and census lists forming a corpus of 5,300 items with 1.2 million words and almost 85,000 named entities.</p>




























<figure ><img loading="lazy" alt="Line graph displaying distribution and legal act classification in the CBMA corpus" src="/dhqwords/vol/15/4/000574/resources/images/image1.jpeg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/4/000574/resources/images/image1_hu4edde338e6af52e29e7eebe88f708cd6_240240_500x0_resize_q75_box.jpeg 500w,
    /dhqwords/vol/15/4/000574/resources/images/image1_hu4edde338e6af52e29e7eebe88f708cd6_240240_800x0_resize_q75_box.jpeg 800w,/dhqwords/vol/15/4/000574/resources/images/image1_hu4edde338e6af52e29e7eebe88f708cd6_240240_1200x0_resize_q75_box.jpeg 1200w,/dhqwords/vol/15/4/000574/resources/images/image1_hu4edde338e6af52e29e7eebe88f708cd6_240240_1500x0_resize_q75_box.jpeg 1500w,/dhqwords/vol/15/4/000574/resources/images/image1_hu4edde338e6af52e29e7eebe88f708cd6_240240_1800x0_resize_q75_box.jpeg 1800w,/dhqwords/vol/15/4/000574/resources/images/image1.jpeg 2500w" 
     class="landscape"
     ><figcaption>
        <p>Chronological distribution and legal act classification in the CBMA annotated corpus
        </p>
    </figcaption>
</figure>
<p>The importance of cartularies in medieval history is well known, for they gather transcriptions of original records and copies from various kinds of acts: royal deeds, privileges, judgments, private business, donations, etc. Such sources prove to be formal documents written by following formulaic patterns, which provide them with a more or less stereotyped structure. In these discourses, the names of people, places, ecclesiastical or other organizations, dates, titles, etc., are specific elements connected to the particular context of writing. All the documents are written in medieval variants of Latin, which uses special rules and vocabularies, and presents much more linguistic variation than classical Latin.</p>
<p>An example here of these documents may be enlightening. Dated between 994 and 1007 A. D,<sup id="fnref:25"><a href="#fn:25" class="footnote-ref" role="doc-noteref">25</a></sup>  this donation act to the Cluny abbey begins with a short protocol followed immediately by the dispositive indicating the object and purpose of the donation and the participants in the act:</p>
<blockquote>
<p><em>In nomine Verbi incarnati. Cuncti noverint populi, quod ego Adalguis dono Domino Deo et sanctis apostolis ejus Petro et Paulo, ad loco Cluniaco, ubi domnus et reverentissimus Odilo abbas magis videtur preesse quam prodesse, pro redemptione omnium peccatorum meorum, ut Dominus dignetur auxiliare in extremi diem judicii.</em><br>
<sup id="fnref:26"><a href="#fn:26" class="footnote-ref" role="doc-noteref">26</a></sup></p>
</blockquote>
<p>Coming up next is a detailed description and location of lands and properties that are the object of exchange in the donation act. The descriptions always involve nominal reference (metonymy) and they follow a hierarchy in land organization, in this case, comitum, villa, terra, inside which we can find the land properties classified in specialized parcels, here campum, vinea, curtilus, pratum, etc.:</p>
<blockquote>
<p>_Sunt ergo ipsas res in comitatu Matisconensi, in villa Tasiaco: hoc est in primis unum curtile cum domo et vinea, et campum; et habet fines de tres partes, de una terra Rodulfo, de alia Seguini, de tertia terra francorum, de quarta via publica. In alio loco alium campum; habet fines de quattuor partes, de una parte terra Rodulfo, de alia terra Fulchardo, de tertia via publica. Et in aliis duobus locis duabus peciolas de prato; habent fines de tres partes, de una parte terra Sancti Quintini, de alia Sancti Martini, de alia via publica. _</p>
</blockquote>
<p>Closing the dispositive is a sanctio containing penal clauses designed to reinforce and guarantee legal action:</p>
<blockquote>
<p>_ De hanc donationem faciunt rectores de Cluniaco quicquid facere voluerint ab hodierno die. Si quis vero ullus homo donationem hanc contrariare voluerit aliquam litem, ira Dei incurrat super illum, et sit demersus in infernum vivus, si ad emendationem non venerit. _</p>
</blockquote>
<p>Finally, there is a two-fold eschatocol with the subscriptions of the donor and of witnesses, containing their names and an “S.” for signum(signature):</p>
<p>_</p>
<blockquote>
<p>S. Adalguis femina, qui hanc cartam fieri jussit et firmare rogavit. S. Ingelelmi. S. Arlei. S. item Arlei. S. Bernardi. S. Ebrardi.<br>
_</p>
</blockquote>
<p>The writing style, the size and purpose of acts continuously vary, but they are follow a similar formulaic pattern. Except in long preambles – part of the protocol – where the writers of acts can express ideological motivations of actions by using quotation of authority texts, the acts do not include personal or narrative details. That gives the impression that we are facing a repetitive and modularly constructed source. This  <em>dryness</em>  is a necessity in documents that aim to become proofs of rights. The formulary pattern shapes the notarial written expression, and it is the named entity that specifies and individualizes the document. As we can notice in the above example, conventional structures also include hierarchical and referential elements and provide support for a dynamic use of named entities in each part of the charter. Indeed, the recovery of one named entity can involve the recovery of one piece of a complex regional landscape, the name of one tenant of a shared property, one node of a familiar or communal network, or one named participant in a long series of ecclesiastical transactions. Put together, these pieces can help to rebuild topographical indices and villas layouts, to propose social, juridical and economic frameworks established around land possession, or to describe a more detailed image of economic life in abbeys. From this, we can easily imagine the power of detection and recovery of named entities to quickly and extensively examine large databases of medieval documentation.</p>
<p>Conducting research using NER with historical corpora is not more complicated than working with Newswire or literature. Indeed, the different degrees of difficulty to complete a recognition model in any of these three areas comes from the varying degrees of observation necessary to detect main phenomena affecting the extraction of linguistic features. Historical literature about Cluniac cartularies is not lacking, and a bibliographical inspection can prevent some major problems affecting the collection of named entity occurrences. Four main highlights must be reported.</p>
<p>In the first place, because corpus goes from the late 9th century to the mid-13th century, we span the process of anthroponymic revolution in western Europe that begins in the early 10th century and extends until the 13th century. This is a process very well described by recent studies <sup id="fnref:27"><a href="#fn:27" class="footnote-ref" role="doc-noteref">27</a></sup>. Roughly speaking, the single name denomination, a reduction of the Roman format used since early medieval times, was replaced around the late 10th century by a new tradition using the bi-name with two or more components (usually fathers’ names and locatives), and this new tradition was consolidated throughout the 11th and 12th centuries. From the mid-12th century on, this formulaic denomination adopted more complex modes using periphrasis, familiar references, professions, regional origin, even surnames, to form personal names with three to six components. The increasing irregularity of names represents one significant challenge to our model.</p>
<p>In the second place, we are often faced with overlapping and ambiguous series of entities which arise from three main documentary issues:</p>
<p>(1) the rise of the donatio pro anima formulary model around the beginning of the 10th century. This charter model testifying to a property donation to the church takes place within the alms-giving doctrine <sup id="fnref:28"><a href="#fn:28" class="footnote-ref" role="doc-noteref">28</a></sup>. Because cartularies gather together ecclesiastical property documents, a substantial part of our corpora follows this model. With the donation, the donor gives a property to a saint, who acts as an intermediary, and not directly to the church, and this property becomes part of lands under a saint’s dominium, which is managed by a monastery or an abbey acting as a juridical person. In this panorama, charters contain extensive lists of named entities mixing personal, geographical and institutional functions under one name (ex.  _ rebus Sancti Vincentii Matiscensis, terra de Sancta Maria de Optimo Monte _ ,  _ in honore Sancti Petri constructam _ , etc.). These nested entities, classified as place names rather than as names of persons, introduce a substantial bias factor to the model (See figure 2).</p>
<p>(2) a significant number of toponyms, starting from the 11th century, occurring as part of the bi-name personal denomination (acting as a locative complement). The overlapped entities usually have two or three components (ex.  _ Lambertus de Malliaco, Stephano de Cave Rupe _ ), but in the 13th century it is not rare to find entities with four or five components (ex.  _ Guillelmus de Sancti Stephano de Ponte _ ). This is a difficult technical problem, because disambiguation needs an advanced model able to classify one occurrence into two or more classes, and that is not always possible. If we take into account only the largest entity (in these cases, a personal name), we could lose thousands of associated micro-toponyms.</p>
<p>(3) the rapid extension of a common vocabulary in cartularies to describe lands, properties, and goods, which tended to fix spatial descriptions inside a stereotypical textual model. The textual environment uses and reuses concepts, formulae, and word associations arranged in a discourse containing parts usually well distinguished (formulary). This can be an essential factor of recognition, but it can also become a source of overfitting and overgeneralized training on co-occurrences.</p>
<p>In the third place, the production of the charters covers at least three periods of change linked to profound transformations in territorial reality, in the organization of writing, and in the legality of acts. Charters from the 10th century, which almost entirely concern ecclesiastical patrimony, are very close to the formulary; personal changes and adaptations are almost absent. They present long preambles and very precise and descriptive dispositio. In the early 11th century, even if the topics of charters do not change, they are partly replaced by notices, which are summaries of the charters drawn up more freely. Over the course of this century, numerous transformations in written vocabulary relate to profound changes in the spatial and legal structures affecting the redaction of charters [Bange 1984]. The broad nature of these changes in Western Europe, linked to feudalism, fuelled in 90’s a historiographical debate concerning the mutatio of the year 1000 <sup id="fnref:29"><a href="#fn:29" class="footnote-ref" role="doc-noteref">29</a></sup>.</p>
<p>In the fourth and last place, documents present a particular state of the language. Our four single-century corpora show linguistic features with a high degree of heterogeneity, because the documents were written during the course of a vernacularization process with progressive emancipation from the rules of classical Latin <sup id="fnref:30"><a href="#fn:30" class="footnote-ref" role="doc-noteref">30</a></sup>. Much of the inconsistency and arbitrariness encountered during construction of an automatic model comes from this issue. The task is even more complicated if we consider that scarce NLP resources for Latin are adapted to literary texts based on classical Latin and devoted to the extraction of literary or philological features <sup id="fnref:31"><a href="#fn:31" class="footnote-ref" role="doc-noteref">31</a></sup>. In fact, the model proposed in this article tries to fill a major gap in this NLP research.</p>
<h2 id="32-corpus-modification-and-extension">3.2. Corpus Modification and Extension</h2>
<p>Modifications and adaptations of the CBMA corpus were motivated by questions related to the representativeness and robustness of our models. We aim to develop a model based on a regional corpus and to investigate whether it can be applied to another corpus created in different circumstances or belonging to different areas and traditions of writing.</p>
<p>There are three reasons why we created multiple sub-corpora in our study:</p>
<p>(i) First, scriptoria are localized in the Burgundy region, where Cluniac and Cistercian abbeys were founded, and where they developed their influence during the 10th, 11th and 12th centuries.</p>
<p>(ii) Secondly, the chronological distribution of the corpus is concentrated on the 10th and 11th centuries, which are the period of the most substantial Cluniac production <sup id="fnref:32"><a href="#fn:32" class="footnote-ref" role="doc-noteref">32</a></sup>. In our corpus, the documentation before the 10th century is very scarce, and Latin is no longer the common language after ca. 1260.</p>
<p>(iii) Third, the charters have been obtained from ten different editions and twelve different cartularies concerning legal transactions from almost 100 small areas.</p>
<p>In other words, we are confronting a problem that is double in scope: on the one hand, we have a central institution (Cluny) that is the recipient of the documents and that applies certain corrections and alterations to them when making copies. Because the documents come from different areas with different scriptural traditions and preferences, the differences are largely authorized, and not the result of error. On the other hand, the over-representation of Cluniac and Cistercian charters can hide minor stylistic variations coming from smaller institutions.</p>
<p>In order to study the impact of all of these issues on the quality of our model, we performed different cross-validation and evaluation experiments; each one focused on a single, isolated aspect: corpus size, temporal variations, and regional variations.</p>
<h2 id="322-generating-sub-corpora">3.2.2. Generating sub-corpora</h2>
<p>Several sub-corpora were produced during the course of our experimentations by modifying the number of documents and the temporal distribution:</p>
<ul>
<li>The reduction of the size of the training dataset generally affects the accuracy of the model when used upon new data. We tested different sizes of training corpus to estimate the minimal amount of training data that is necessary to obtain good results. With our 5300-document corpus, we experimented with different sizes of training corpus: 4000, 2500, 1500, 1000 and 500 documents (we reserved a fifth of the documents in each set to use as a test corpus), and we applied the same training protocols in all cases. These experiments are important not only to the constitution of an extensible model, but also to the formation of a lighter model requiring lower computational resources.</li>
<li>On the other hand, the purpose of temporal variations is to build a cross-validation environment for testing the robustness of the model on different chronological units, and then validating, or not, the application of the model to a wider temporal range. For this reason, we built century models, that is, learning models using only documents from the same century. The accuracy of this experiment may be questioned because, in many cases, the date of a document’s creation is not certain, and the document is dated, then, using a time interval. This problem was corrected by building sub-corpora, considering a charter inside two corpora if its estimated date was near a transition period. For example, a charter dated around 980-1020 was considered to be in both the 10th- and 11th-century models. Thus, we obtained a biased result in contiguous model comparison due to the portion of shared charters, but, in compensation, we obtained sub-corpora that were more representative of each century, since changes of style or scriptural practices do not follow rigid chronological divisions. The distribution of all these corpora and sub-corpora can be observed in our Table 1.</li>
</ul>
<h2 id="323-manual-annotation-of-extra-corpora">3.2.3. Manual annotation of extra corpora</h2>
<p>Nevertheless, both types of the previously mentioned experiments decrease the variety in the training corpus, and the features provided to the model can hide specific scriptural phenomena. To investigate this issue, an extra corpus of 400 items extracted from unlabelled data was tagged, with named entities covering the grey chronological areas of the 9th, 12th and 13th centuries. By adding this additional corpus to our sub-corpora, we expected to assure a chronological variety of smaller corpora and to avoid the loss of scriptural varieties. For that reason, documents were chosen in the decades that were the least represented in the original corpus. This supplementary corpus was also divided by centuries and each part was added as an auxiliary group to the century corpora.</p>
<p>Taking it even further, corpora and sub-corpora models were trained using charters from the same corpus. The robustness of these models had to be compared using non-Burgundian charters. To accomplish this task, four new small-sized corpora were annotated by hand. These documents were extracted from Parisian, English, Lombard and Castilian charters from four regions with an intense charter production displaying different scriptural models. They are part of four of the most complete medieval charter corpora available online: the Ile-de-France cartulary (12th -13th centuries) published by the <a href="http://elec.enc.sorbonne.fr/cartulaires/">École des Chartes</a>; the <a href="https://deeds.library.utoronto.ca/cartularies">DEEDS corpus</a> (10th-13th centuries); the <a href="http://www.lombardiabeniculturali.it/cdlm/">CDML</a> ( _ Codice diplomatico della Lombardie Medievale _ , 11th-12th centuries) containing ecclesiastical and chancery charters; and the <a href="http://corpuscodea.es/corpus/consultas.php">CODEA (CHARTA)</a> corpus (10th-18th centuries) containing documents to map the rise of medieval Spanish.</p>
<p>The limited size of these sub-corpora (from 70 to 100 charters) made sub-corpora formation by century impossible in these cases, but we tried to balance them chronologically and to include not only cartulary documents but also some diplomas, bulls and administrative acts to increase heterogeneity.<br>
Table 1: Nº of charters for each century and global features in gold-standard corpus and external corpora     Century/Corpora   ENG-LAND  CASTILE  ILEFRANCE  LOMBARDY  Original Corpus  Modified Corpus  400_Set      10th  10  7  9  8  2292  3230  12      11th  24  13  23  17  1510  2050  27      12th  24  16  54  23  816  860  182      13th  12  14  63  2  638  730  149      Nº Tokens  11110  15616  41608  12441  1096095  1096095  104330      Nº Entities  1326  1841  3594  1222  84752  84752  8263</p>
<h2 id="33-crf-modeling">3.3. CRF modeling</h2>
<h2 id="331-normalization-and-segmentation">3.3.1. Normalization and segmentation</h2>
<p>This section describes the pre-processing steps applied to all our corpora. First, we did a manual validation of doubtful items in the training data, mostly for detecting nested, overlapping entities and ill-formed annotations. For example, the description of territorial boundaries with abuse of possessive genitive (or nominative) forms leads to overlapping entities, but also to entities with unclear boundaries, and is due to a much freer order in the phrase and use of declensions than in classical Latin (see examples in Table 5). Overlapping entities are usually connected to names serving different functions, frequently saints’ names. They can refer to a personal name or the name of an abbey, a piece of land, a feudal territory, or even a festivity date. On the other hand, the texts from philological and diplomatic editions can contain paratextual data such as special characters, glosses, titles, etc., that can introduce noise into the model. A process of normalization in textual spelling is completed through scripts. This normalization decreases error rate and makes the transformation of the original format corpus easier.</p>
<p>We then split the main corpus into three sets necessary for the creation of the model, i.e., training set, development set, and test set. We randomly selected a training corpus of 4,000 documents ( <code>&gt;</code> one million words), a test corpus of 1,000 documents, a development corpus of 300 documents forming the development set, used to avoid overfitting, i.e. a model that is too close to the test set and thus not robust to new data. To ensure the chronological presence of documents in each sub-corpus, we also tested semi-random distribution by creating clusters of documents every 25 years, but this approach did not show significant differences in the results. That is why we preferred the random method in order to maintain the original distribution of data. The temporal distribution of the three datasets turns out to be very similar to the distribution of the entire corpus.</p>
<h2 id="332-model">3.3.2. Model</h2>
<p>We see our NER problem as a problem of traditional sequence classification, and we use the well-known BIO format to represent the data. In this format, each token (in our case, each word) is assigned to a BIO class: B-entity, I-entity, and O-entity, respectively, representing the beginning (B), continuation (I as inside), or absence (O as outside) of named entities (See Table 2). We then apply Conditional Random Fields <sup id="fnref:33"><a href="#fn:33" class="footnote-ref" role="doc-noteref">33</a></sup>, one of the most popular methods for this category of problems. To apply this supervised machine-learning approach, each word in a sentence must be regarded as a token. The entire corpus was converted into a tabular format, providing lexical, syntactical, and morphological information at a token-level. Each word was represented by the following:</p>
<ul>
<li>TOKEN (original word)</li>
<li>POS (part-of-speech category)</li>
<li>LEMMA</li>
<li>CASE (whether the first letter is in upper or lower case)</li>
<li>SUFFIX (last three letters of the surface form)</li>
</ul>
<p>The first three features (TOKEN, POS and LEMMA) were obtained from a version of the tool <a href="http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/">TreeTagger</a> adapted to Medio Latin, created by the <a href="http://www.glossaria.eu/treetagger/">OMNIA</a> group in 2013. These three features help the model to consider the grammatical and morphological information from the text. The next two columns (CASE and SUFFIX) help the classifier to exploit capitalization and the inflected nature of Latin, in which the end of the word determines its grammatical function.</p>
<p>We considered the problem as a two-step classification: the first step extracts the personal names, while the second step extracts location names. A single classifier extracting personal and location names jointly could not be implemented, as the corpus contains a lot of overlapping entities. That is why the last columns of Figure 2 list the classes in BIO format.</p>




























<figure ><img loading="lazy" alt="Screenshot of a set of rows and columns" src="/dhqwords/vol/15/4/000574/resources/images/image2.jpeg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/4/000574/resources/images/image2_hu30faf8c06474e07d76a736f338b4acfd_58705_500x0_resize_q75_box.jpeg 500w,
    /dhqwords/vol/15/4/000574/resources/images/image2_hu30faf8c06474e07d76a736f338b4acfd_58705_800x0_resize_q75_box.jpeg 800w,/dhqwords/vol/15/4/000574/resources/images/image2.jpeg 923w" 
     class="landscape"
     ><figcaption>
        <p>Example of two and three nested levels of overlapped entities
        </p>
    </figcaption>
</figure>
<p>The CRF method operates by forming a discriminative model and finding the best state assignment from a set of training corpora containing tagged features. That is, given a series of labelled observations it constructs an interpretation and from there it determines probabilistically the most approximate label for a new unseen sequence <sup id="fnref:34"><a href="#fn:34" class="footnote-ref" role="doc-noteref">34</a></sup>. As explained above, the corpus has been divided into three different sections: training set, validation (development) set, and test set. After each iteration on the training set, the learned model is applied to the validation set, and the process is stopped when the validation result does not improve anymore ( “early stopping” ). The test set was reserved to evaluate the performance of the model with data that did not participate in the learning phase, thus avoiding any bias.</p>
<p>The features used by the CRF model are specified by a list of patterns determining the observation rules for each element (word) of the sequence (document). A token feature can be one of the seven columns on the token row, the rows before or after, or a combination of these. This allows using the immediate context of each word. The pattern that we have written combines unigrams and bigrams in an extended sequence of grams (i.e., regarding words one by one or two at a time), combining two forward and two backward token-line positions. An efficient feature selection and induction by the L-BFGS algorithm was provided by Wapiti <sup id="fnref:35"><a href="#fn:35" class="footnote-ref" role="doc-noteref">35</a></sup> in a sequence-labelling toolkit developed at LIMSI-CNRS.<br>
Table 2: Training sample for the sequence “Quod ego Hugo de Berziaco perpendens.” The gray zone indicates one single observation (i.e., the word de) combining features from all columns in a window of five tokens (from two tokens before to two tokens after the observed token).    TOKEN  POS  LEMMA  CASE  SUFFIX  ENTITY  ENTITY      Quod  CON  Quod  UPPER  Uod  O  O      ego  <em>[2,0]</em>     PRO  Ego  LOWER  Ego  O  O      Hugo  <em>[1,0]</em>     NAM  -  UPPER  ugo  B-PERS  O      de  <em>[0,0]</em>     PRE  <em>[0,1]</em>     de  <em>[0,2]</em>     LOWER  <em>[0,3]</em>     de  <em>[0,4]</em>     I-PERS  <em>[0,5]</em>     O  <em>[0,6]</em>         Berziaco [-1,0]   NAM  -  UPPER  aco  I-PERS  B-LOC      perpendens [-2,0]   VBE  Perpendeo  LOWER  ens  O  O      ,  PON  ,  LOWER</p>
<h2 id="4-evaluation">4. Evaluation</h2>
<h2 id="41-model-experimentations">4.1. Model experimentations</h2>
<p>All models were evaluated with traditional precision, by recall and F1-measure with two different configurations, as implemented in the tool <a href="https://bitbucket.org/nicta_biomed/brateval">BratEval</a>: exact match counts a true positive if an extracted named entity has the correct type (person or location) and if the boundaries of the extracted entity match perfectly with the gold standard. On the other hand, partial match counts a true positive even if the extracted entity shares only a partial overlap with the gold standard.</p>
<h2 id="411-general-model">4.1.1. General model</h2>
<p>We first ran our model trained with the entire training and development CBMA set, regardless of its temporal and geographical characteristics. This model achieves an exact match F1-measure of 0.95 and a partial match of 0.96 on test data for people’s names; it achieves an exact match of 0.91 and a partial match of 0.92 for location names. All experiments show a similar result in precision and recall parameters. The difference between exact and partial results is not more than two points, confirming that boundary detection is not very problematic, although it is often a hard task in NER.<br>
Table 3: Best current ratio recognition expressed in BIO-tag ratio of recognition and number of occurrences according to Brateval tool which provides a pairwise comparison of annotation sets in Brat format. The tool displays two results: Exact match (EM)when entities agree in type and extension, and Partial match (PM) when entities agree in type but not in extension. True positive (TP), False Positive (FP), False negative (FN), Recall (Rc), Precision (Pr) and F1-measure    Personal name  PR  RC  F1  TP  FP  FN      B-PERS  0.95  0.96  0.96            I-PERS  0.88  0.92  0.90            Partial match  0.95  0.97  0.96  12965  615  291      Exact match  0.93  0.96  0.95  12729  851  529      Location name                  B-LOC  0.91  0.93  0.92            I-LOC  0.81  0.80  0.80            Partial match  0.92  0.92  0.92  7171  590  550      Exact match  0.90  0.91  0.91  7035  726  681</p>
<h2 id="412-century-models">4.1.2. Century models</h2>
<p>Tables 4 (personal names) and 5 (location names) show the result of our cross-validation experiments on century models. For each one of the four considered centuries (from the 10th to the 13th centuries), we applied a model trained on one century to another century set. Cross-results did not show a considerable heterogeneity, and the results are very close to those obtained with big-sized models. The main differences are detected while evaluating datasets and models formed on the basis of charters from the 10th and 13th centuries. Here there are three highlights: first, the 10th-century model constructed from the largest dataset gives us the worst results when applied to the other charters, especially to the 12th- and 13th-century datasets; second, the application of the 10th-, 11th-, and 12th-century models to the 13th-century dataset offers the lowest compared performance; third, the worst individual performance is obtained by crossing the 10th-century model with the dataset of the 13th-century charters.</p>
<p>All this suggests two major observations: (i) there are no significant gaps in the variety of features of corpora modelled on four centuries of charters, thus maintaining a regular detection of naming phenomena that produces results similar to those of single-century models; (ii) in consequence, detection problems are concentrated at a feature-level in irregular denominations and in severe changes in the name&rsquo;s composition (or in its textual disposition) that have taken place in the 13th but not in the 10th century or vice versa.</p>
<h2 id="413-european-charters">4.1.3. European charters</h2>
<p>Tables 6 and 7 (corresponding to names of persons and locations) show the results obtained by the application of our different models to the  “foreign”  corpora described in section 3.2. We apply to these regional corpora the models trained with the general CBMA datasets (two models with different sizes) and the four, single-century sub-corpora trained from the same dataset. The six results obtained show great proximity among them. General models provide better coverage, normally around 3%-5% superior to the century models, and again, we can see that the model trained on 1,500 charters shows a small decrease in performance (1-2 points) compared to the full model. The use of a smaller set of annotated documents could be acceptable.</p>
<p>The performance of general model is quite appropriate: PERS partial-match recognition is between 94% and 93%, and exact-match recognition shows results between 87% and 81% for the four regional corpora. In the case of LOC entities, the partial-match recognition reaches results between 85% and 82%, and exact-match recognition is between 81% and 73%. Analysing only the numerical results from single-century models, we noticed many similarities to the results obtained in previous century model evaluations: (i) the best performance models are the central models from the 11th and 12th centuries; (ii) boundary models (from the 10th and 13th centuries) are less adequate for recognition, especially of compound entities; and (iii) the model from the 10th century, trained with the largest number of documents, offers the lowest efficiency on foreign corpora recognition.</p>
<p>We also noticed a correlation between single-century models and regional corpora. For example, the Parisian corpus, composed mostly of later medieval charters, offered slightly higher numbers when evaluated with the 12th- and 13th-century models; the Lombard corpus, concentrated in the 11th and 12th centuries without any 13th-century charters, showed lower performance when evaluated by 13th century model, while the English and Hispanic corpora, with the most balanced chronology (the Hispanic corpus is not really an organic corpus), presented greater distance between partial and exact matches when evaluated with all four of the century models.</p>
<p>To summarize, the similar results obtained from the six models help to reinforce the hypothesis about the results of century model evaluations: (i) there are long series of features in documents that resist changes in the chronology, size and origin of the charters, features that are better treated by an iterative model based on the formulary and on established scriptural traditions, assuring thereby an initially acceptable ratio of recognition; (ii) there are more discrete sets of features, added to previously mentioned ones, determined by local specificities, document changes and highly specific phenomena, which become the primary source of errors. In addition to these, the set of particularities from the four external corpora were treated without a considerable decrease in the general performance, suggesting that there are global similarities in the composition of the name that are much stronger than differences suggested by each regional tradition.</p>




























<figure ><img loading="lazy" alt="Screenshot of a table" src="/dhqwords/vol/15/4/000574/resources/images/image3.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/4/000574/resources/images/image3_huca38bcec191aed64454abb3c24b6557a_648042_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/4/000574/resources/images/image3_huca38bcec191aed64454abb3c24b6557a_648042_800x0_resize_box_3.png 800w,/dhqwords/vol/15/4/000574/resources/images/image3_huca38bcec191aed64454abb3c24b6557a_648042_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/4/000574/resources/images/image3_huca38bcec191aed64454abb3c24b6557a_648042_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/4/000574/resources/images/image3.png 1752w" 
     class="landscape"
     ><figcaption>
        <p>Table 4: Accuracy scores obtained on cross evaluation for names of persons and places using century and general models on European and century set test.
        </p>
    </figcaption>
</figure>
<h2 id="42-match-and-error-analysis">4.2. Match and error analysis</h2>
<p>High performance on a partial match means that the model detects the presence of named entities no matter what size they are. In the case of single-named entities, there is only one single target element, and we can assume that the results in the detection are almost the same as the classification of the entity. In the case of complex entities (two or more components), complete classification is less efficient than detection by around 5-10 points (the difference between partial and exact matching), but there is still high performance at the task of recognition.</p>
<p>These results can become more meaningful if we remark three significant situations detected concerning named entities in our corpus and in evaluated corpora:</p>
<p>(1) As we mentioned above (section 3.1), the most widely diffused type of complex name is formed by the association of two or three single entities. In fact, around 86% of the compound personal names in the CBMA corpus have between two and three parts. This denomination format is generally composed of  <em>name + de + toponym</em>  (ex.  _ Bertrannus de Verziaco _ ). But in other cases, this double name format can be produced though declension, syntagmatic or periphrastic denominations, or even adding a second component using a genitive or nominative. There is a juxtaposition of two single names in the  <em>name + name</em>  (or  <em>noun</em> ) or  <em>name + nexus + name</em>  (or  <em>noun</em> ) formats (ex.  _ Rudericus Didaci, Bellonus Mangacii, Gariardus de loco Antimiano _ ).</p>
<p>(2) From the 11th century on, more than half of the geographical entities detected form part of a personal name. The  <em>name + de + toponym</em>  format enjoyed great success until the 13th century. In this format, we generally consider the last name as a location indicator (usually a micro-toponym or hagio-toponym), which means that well-performed boundary detection for a double name involves full geographical entity detection (that is a B-PERS +  <em>de</em>  + B-LOC pattern).</p>
<p>(3) Complex and single personal and geographic entities are closely related to an extensive and precise vocabulary of co-occurring words. This is a necessity in legal texts about properties where donors must be clearly identified, properties must be well-described, and signatures and signs of validations must be included. Lands and people are identified by using terms and titles of presentation expressed in a common vocabulary that responds to a spatial and social reality, but also to the uses signified by the formulary.</p>
<p>According to the evaluation results, the model can detect the apparition of entities with up to 95% efficiency and classify them with an accuracy of up to 90% in the case of single entities as well as in the case of the most extended forms of composed entities: the double locative name and the double name using the  <em>de</em>  particle, the genitive, or any other single nexus.</p>
<p>In this respect, we can confirm a model able, on the one hand, to classify with high performance both the single names used almost exclusively until the 10th century and the most widely diffused name format in medieval times since the 11th century: the double name in all its variations. We can also confirm a model able, on the other hand, to recognize the non-complex entities associated with a large, social, and professional vocabulary. (See Table 4.)</p>
<p>Furthermore, as long as the double name maintains a two- or three-part compound format, the model has a high level of efficiency (90% or more) due to limited variations and not very complex n-gram observations. Providing information related to lemma and termination (or declension), capital letters and grammar features is usually enough for the model to accomplish named entity recognition on compound names.</p>
<p>Taking that into account, the most frequent errors in recognition must necessarily be linked to long, complex denominations and specific phenomena such as grammatical forms in the Latin phrases or to changes in underlying textual structures. In Table 5, we can see displayed some examples of frequent errors made by the system. For example, in Lombard texts the prevalence of periphrastic denominations produces a lower ratio for the recovery of inside parts of named entities because of the high frequency of non-entity words between names (see examples 1 and 2). In 11th- and 12th-century charters, the increasing use of the genitive in naming entities produces a large group of entities without particles. In addition, the coexistence in medieval Latin of the inflectional rules of classical Latin, the expansion in the use of prepositions, and the gradual extinction of Latin cases can all lead the model to some confusions (see example 4).</p>
<p>It is especially hard to handle the apparition and rapid expansion of institutional names under saints’ avocations in diplomatic donation models in the 10th century due to overlapping between personal, institutional, and place names (examples 6 and 8). The model does not work well with overlapped entities because most machine learning classifiers are not designed to attribute more than one class to each instance. In that sense, the confusion between place names and personal names must be solved by designating one class for each entity.</p>
<p>The overgeneralization of very common particles (such as  <em>de</em>  in compound entities), as well as of location trigger words (such as  _ terra, serum, pars, domus, manus, apud _ ) and also of personal co-occurrent words (such as  _ episcopus, beatus, dominus, sacerdos, miles _ , etc.) can lead to false positives when the model finds an entity different than that expected, as illustrated by examples 5 and 7. On the other hand, because of the more flexible nature of the phrase order in Latin, the system can classify as inside entity (I-PERS, I-LOC) those nouns that are single-named entities or non-name entities.</p>
<p>Finally, in the course of all these experiments, we detected an important number of errors related to the original format of the documents. These texts are not raw data, and they contain many added features characteristic of a philological, palaeographical, or diplomatic edition, such as special characters, textual gaps, abbreviations, headlines, and orthographic redundancy (example 9). Not all this information is relevant to the model, and this can produce an important amount of noisy training data. Automatic and manual cleaning can resolve a significant part of this problem.</p>




























<figure ><img loading="lazy" alt="Screenshot of a table" src="/dhqwords/vol/15/4/000574/resources/images/image4.jpeg"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/4/000574/resources/images/image4_hu8d567ebc0eacd6d8fb441ab9600b4c28_717228_500x0_resize_q75_box.jpeg 500w,
    /dhqwords/vol/15/4/000574/resources/images/image4_hu8d567ebc0eacd6d8fb441ab9600b4c28_717228_800x0_resize_q75_box.jpeg 800w,/dhqwords/vol/15/4/000574/resources/images/image4_hu8d567ebc0eacd6d8fb441ab9600b4c28_717228_1200x0_resize_q75_box.jpeg 1200w,/dhqwords/vol/15/4/000574/resources/images/image4_hu8d567ebc0eacd6d8fb441ab9600b4c28_717228_1500x0_resize_q75_box.jpeg 1500w,/dhqwords/vol/15/4/000574/resources/images/image4_hu8d567ebc0eacd6d8fb441ab9600b4c28_717228_1800x0_resize_q75_box.jpeg 1800w,/dhqwords/vol/15/4/000574/resources/images/image4.jpeg 1839w" 
     class="landscape"
     ><figcaption>
        <p>Table 5: Examples of frequent errors made by the system on European charters. The first column of each example shows the text excerpt; the second and third column shows the gold standard for location and person names, and the last column shows the system automatic classification.
        </p>
    </figcaption>
</figure>
<h2 id="5-discussion">5. Discussion</h2>
<p>In this work, we use different types of information to build a model for automatic named entity recognition: grammatical categories, lemma, case, entity categorization and n-gram observations. A model based on a probabilistic CRF-approach that extracts correlations dependencies between this information and observations leads us to high performance in the process of categorizing entities. High percentages in the results, after having tested the model on corpora of different geographical origins and different periods, demonstrate that the linguistic and statistical algorithmic approach has proven to be robust. In fact, the results are quite conclusive, but the evaluation process involves an important level of heterogeneity needing an explanation that exceeds the most technical level and requires the use of humanistic knowledge. We need to answer why a model created from a regional corpus can obtain a high recognition rate on medieval documents from other regions, chronologies and even traditions.</p>
<p>Thus, references to the historical background, social usages and scriptural variations can provide a more meaningful explanatory context, enriching the evidence obtained by machine-learning methods. Reviewing the literature about historical names is a good first step. Anthroponymic studies suggest a double movement in Western Europe after the 10th century: first, a reduction of name stock and consequently a reduction of the variety of common names; and second, an extension of double names in variable forms:  _ nomen paternum _  (name + name of father), locatives (name + place-name) and nicknames (name + profession, qualities or titles). In fact, the nature of the second element in bi-names distinguishes several naming traditions in Europe. In France, the meridional tradition incorporated the  _ nomen paternum _  quickly, that is, the hereditary second name coming from the father, but central and northern regions (like Burgundy and Île-de-France) developed a preference for locatives and periphrastic forms <sup id="fnref1:27"><a href="#fn:27" class="footnote-ref" role="doc-noteref">27</a></sup>.</p>
<p>Single-name forms disappeared in the 12th century, and traditional and local names were replaced by the universal Christian or princely names. At the same time, in the northern Hispanic regions and Castile, the tradition promoted an incorporation of  _ nomen paternum _ , formed with the declension in the original name of the father (v.g.  _ Gundisaluo Roderici, Rodericus Ferrandi _ ) and, later, a composition of three names in association with locatives and nicknames (v.g  _ Ferrando Sanchiz de Fenosa _ ) <sup id="fnref:36"><a href="#fn:36" class="footnote-ref" role="doc-noteref">36</a></sup>. Norman traditions heavily influenced the central and southern regions of England, which adopted double locative or triple names and  _ nomen paternum _  formed by the second name of the father (v.g  _ Guidonis Nerioli de Buxiaco, Aimo de Monte Pauonis _ ) <sup id="fnref:37"><a href="#fn:37" class="footnote-ref" role="doc-noteref">37</a></sup>. The tradition operating in the Lombardy region presents the same phenomena of duplication in anthroponymic elements during the 11th century, but prefers a system that promotes a more broadly referential second name (v.g.  _ Otonnis qui dicitur de Suso, Grioulum filium Lafranci Caipeni _ ) which takes the form of a periphrastic composition promoting surnames, geographical origin, and ancestor’s filiations <sup id="fnref:38"><a href="#fn:38" class="footnote-ref" role="doc-noteref">38</a></sup>.</p>
<p>Concerning geographical names, we must make a distinction between landscape entities and nominal entities. The first are used to describe or to locate a property, whereas the second appear as a locative complement of a compound name. Indeed, a large proportion of geographical entities is linked to the rise of double names between the 11th and 13th centuries, when the locative complement became a form of familial or personal social distinction. The expansion and precise definition in Europe of the  <em>name + locative</em>  combination through use of the  <em>de</em>  particle (genitive or nominative without preposition) produced a very strong increase in binomial name entities combining personal and geographical components. This is a process that quickly produces great variety by proposing the origin, residence, feudal, or ethnic place as locative complement (v.g.  _ Iohannes Allemanus _ ) completing the information about an individual person. In fact, all these locatives are generally real places, but they are usually defined as hagio-toponyms, old toponyms, and micro-toponyms hardly connected with real spaces on a map.</p>
<p>Nevertheless, most of the geographical entities are not associated with a personal name but correspond to the first above-mentioned type: landscape and territorial names. They are the names of a  _ terra _ , a  _ villa _ , a  _ pagus _ , an  _ ager _ , a river, a building, a church, a monastery, a so-called place, etc. Names and master words or co-occurrences are closely related in charters (see Table 5) because cartularies produce, by classifying records geographically and transcribing them in a codex, a spatial knowledge of ecclesiastical estates which uses many of the same global geographical entities <sup id="fnref:39"><a href="#fn:39" class="footnote-ref" role="doc-noteref">39</a></sup>. The first effect of this is the constant reference in formulary charters to a well-delineated territorial space, a cadastral order that today is almost impossible to reconstruct due to numerous information gaps <sup id="fnref:40"><a href="#fn:40" class="footnote-ref" role="doc-noteref">40</a></sup>.<br>
Table 5: Co-occurrences or presenting words (appositional, periphrastic and attributive nouns) of personal and geographical named entities. This table corresponds in the broadest sense to a jurisdictional and social vocabulary of personal and geographic categorization.     Personal entities  Geographical entities      Professions and social activities:  _ camerarius, cantor, magister, miles, monachus, notarius, sacerdos _    <br>
Landform spaces:  _ boscus, fluvius, locus, mons, nemus, pratus, rivus, silva _<br>
Secular and religious titles:  _ abbas, beatus, comes, domnus, dominus, dux, episcopus, papa, presbyter, princeps, rex _     Seigniorial and ecclesiastical division of sedentary lands:  _ ager, conventus, curtilus, domus, feudus, grangia, mansus, pagus, vicus _         Dignities and Nicknames:  _ benedictus, brunus, grossus, humilis, largus, normandus, paganus, servus, venerabilis _     Legal and jurisdictional division:  _ area, castrum, civitas, diocesis, ecclesia, provincia, sedes, terra, villa _         Periphrastic nexus:  _ appelatus, cognomen, dictus, nomen, vocatus _     Landmarks and micro-spaces:  _ altar, atrium, capella, capitulum, castellum, cenobium, domus, ecclesia, hospital, monasterium _         Words with nominal value:  _ alius, ego, filius, frater, idem, nepos, signum (S.), uxor _     Prepositions, adverbs or global locative value terms:  _ ad, apud, fines, inter, manus, meridies, pars, pro, supra, vocabulum _      <br>
According to the evaluation of our general model, the results obtained in B-LOC recognition (geographical Begin-entity) are just five points below (90% compared to 95%) the best performance obtained in B-PERS (personal Begin-entity). That means that performance on detection of geographical entities and the classification of single entities is almost as excellent as obtained on personal names. Geographical entities usually appear in less complex forms than the entities of the name, which raises the percentage of this first result. But performance decreases almost ten points if inside entities (I-PERS and I-LOC) are compared (90% and 80%), which indicates that elements involved in composed geographical entities are harder to model. This proportionality is more or less maintained in evaluating foreign corpora (see Tables 6 and 7). All results are, on average, acceptable, but, once more, the problems are concentrated on inside-entities, as is shown when comparing partial and exhaustive Brateval results.</p>
<p>The most common format for complex entities is the double name (bi-name), but as we have seen, our model can handle well the recognition of this prolific format and its variants, which suggests that we need to examine more complex forms of composed entities. Thus, regarding the apparition of complex geographical entities, we notice different situations depending on regional traditions. Concerning personal names, Castile and Southern England are regions where the triple name expansion takes place one century before it does in Central France. Both regions form bi-names with two nominal components, and the incorporation of locatives entails triple or quadruple names (v.g.  _ Guilielmi de Sancto Satiro _ ,  _ Martinus Garciez de Stallaia _ ). In Lombardy, the formation of complex names never takes root, but the addition of periphrastic composition (through  _ dicitur, nominatur, filius, de loco _ , etc.) creates some long entities impossible to dissolve. Almost the same thing happens in central French regions where representative quantities in triple names are not present before the 13th century. But the use of locatives and periphrastic forms since the 11th century contributes to the apparition of complex entities of up to five elements (v.g.  _ Adalbertus de vico Camonaco filii Iohanni _ ).</p>
<p>Nonetheless, the most complex entity format is related to overlapped and nested entities (see Section 4.2). Personal entities, acting as possessive genitive or nominative, can be found fulfilling the function of a geographical entity in the land and boundary land descriptions in donations, purchases, inheritances, or legal disputes (v.g  _ a mane terra Bertrannus; sub domi ipsius Ansaldi _ ). Furthermore, since the 10th century, saints and ecclesiastical institutions were currently present in the formulae of the  _ donationes pro anima _ , where they performed the function of intermediate receptors and defenders of properties. Their presence in charters contributes to creating complex entities with four or more elements taking, associating, or superimposing a saint’s name, an institutional name, and a geographical name.</p>
<p>In part, this complexity is, even more, a result of medieval variants of the Latin language. The gradual expansion of use of the genitive since the 11th century created large groups of entities without intermediate particles; at the same time, there was an increase in prepositions and the extinction of many Latin case endings, all of which lead the model, in many instances, to recognize the words of non-entities as named entities. Latin phrase order is irregular, and exceptions in medieval variants are almost infinite; consequently, training taking into account grammatical rules, co-occurrences, and context can generate many false positives even with an approach that is not rule-based.</p>
<p>The difficulty in recognizing entities involved in these complex phenomena lies not so much in their quantity as in their extensive consequences. The percentage of complex entities (more than three elements) does not exceed 11% of the total in our corpora, but the statistical impact on results due to bad recognition of such entities is more elevated when they are composed of more than four elements: B-PERS + (3) I-PERS (that means three inside-entity errors for each non-recovered entity). Moreover, the impact of these phenomena is increased if we consider that the original mark-up of our database, in which non-physical personal names and juridical actors are classified as place-names, can elevate the number of false positives in automatic recognition of complex place-names.</p>
<p>The influence of these circumstances is more evident in the different tests of temporal variability that we have performed. On one hand, the results of century models applied to European charters are very stable, which confirms, first, the presence of important homogeneous structures in name compositions and formulaic patterns and, second, the presence of a larger stock of name compositions than we would have expected in the Burgundy region in each century, which provides more variety in the training process. On the other hand, in all cases, the model developed with charters from the 10th century, when the double name was not yet established, is less adequate for recognition in other centuries; at the same time, the models from the 10th, 11th, and 12th centuries are less efficient when used on 13th century charters, because larger and more complex compositions increased in this century.</p>
<p>Finally, the excellent statistical results in spite of temporal variability also show that there is a high degree of iteration in late medieval formulaic discourse. This characteristic helped us to form a valid model starting from an incomplete and not very structured corpus. We obtained an acceptable ratio of recognition when we tested the model on charters representing different legal acts: donations, transactions, royal orders, deeds and notarial actions from four European regions. That leads us to assume a vast radius of application for our model on Western European legal acts of various types. In addition, single-century models do not show substantial improvements compared to the results of general models when applied to documents from each century, which confirms that our general model can provide the best ratios of recognition along a temporal line of at least four centuries, from the 10th to the 13th century.</p>
<h2 id="6-conclusion">6. Conclusion</h2>
<p>We present a named entities recognition model applied to medieval Latin charters, and we implement an adequate evaluation environment. The model produced from long data sets of Burgundian medieval charters is able to accomplish a high ratio of recognition of the personal and geographical names of medieval Latin documentary sources, especially from the 10th to the 13th century. The evaluation is focused on demonstration of the robustness of the model in diverse situations. We performed different cross-validation experiments modifying size, chronology, typology, and regional origin in several sub-corpora, obtaining a very acceptable ratio of recognition in each case, which confirms that our model has a wide range of applications with respect to medieval documentary sources. By crossing results, we can confirm that the model supports an accurate semi-automatization of named entities recognition and, in consequence, that it can provide a primordial level of structuration of data, saving many human efforts.</p>
<p>Validation and cross-experiments also show us that questions about representativeness in the learning model do not generate insurmountable issues. Our discussion tries to confirm that the most significant problems in recognition are an expression of the intense dependency of data on complex historical, social and scriptural conditions, and not only on linguistic or statistical concerns. Several historical and philological conclusions helped us to refine and normalize algorithmic approaches and to understand numerical results provided by the machine. This also led to corrections of the original dataset and to complement it by the integration of the new annotated datasets that we have produced during this project.</p>
<p>The use of this NER model can be fruitful in different fields of research in the Humanities, including, in particular, recovery information systems, automatic indexing and distant reading. Since named entities are not subject to a lot of variation, they can be used as keywords and meaningful terms improving internal workflows of search engines. A NER model which can automatically spot these keywords within a large dataset, becomes a useful tool to speed up, classify and refine requests on ancient documents that are available in plain text editions. The recovery of entities, acting as nodes of action, can serve as a first step in the production of historical GIS-maps, and the reconstruction of event timelines or social networks. These widely used forms of data visualization can benefit from automatic tagging while automatic tagging can also help to solve questions concerning the production or the dating of ancient texts by providing data that scholars have traditionally used in their textual or manuscripts studies – mention of persons, places, words or formulas. Since it enriches datasets with specific linguistic and semantic features, our NER model can even be integrated into a pipeline of tasks, such as topic classification of ancient texts, automatic handwriting recognition or word sense disambiguation.</p>
<ul>
<li>This project is supported by the &ldquo;IDI 2016&rdquo; project funded by the IDEX Paris-Saclay, ANR-11-IDEX-0003-02</li>
</ul>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Plank, B. (2016).  <em>What to do about non-standard (or non-canonical) language in NLP.</em>  arXiv preprint arXiv:1608.07836.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<pre><code>**Zimmermann 2003**  Zimmermann, M. (2003),  _Écrire et lire en Catalogne : IXe-XIIe siècle (Vol. 1)._  Casa de Velázquez, Madrid, pp.251-284.  </code></pre>
&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></li>
<li id="fn:3">
<p>Eger, S., Vor der Brück, T., Mehler, A. (2015).  “Lexicon-assisted tagging and lemmatization in Latin: A comparison of six taggers and two lemmatization methods.”  In  <em>Proceedings of the 9th SIGHUM Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities</em>  (LaTeCH 2015), pp. 105-113.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Abacha, A. B, Zweigenbaum, Pierre. (2011).  “Medical entity recognition: A comparison of semantic and statistical methods.”  In  <em>Proceedings of BioNLP</em>  2011 Workshop. Association for Computational Linguistics, p. 56-64.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>Eltyeb, S. and Salim, N. (2014).  “Chemical named entities recognition: a review on approaches and applications.”    <em>Journal of cheminformatics</em> , vol. 6, no 1, p. 17.&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>Nadeau, D., Sekine, S. (2007).  “A survey of named entity recognition and classification.”    <em>Lingvisticae Investigationes</em> , vol. 30, no 1, pp. 3-26.&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>Curran, J. R., Clark, S. (2003).  “Language independent NER using a maximum entropy tagger.”  In  <em>Proceedings of the seventh conference on Natural language learning</em>  at HLT-NAACL 2003-Volume 4. Association for Computational Linguistics, p. 164-167.&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>Won, M., Murrieta-Flores, P. and Martins, B. (2018)  “Ensemble Named Entity Recognition (NER): Evaluating NER Tools in the Identification of Place Names in Historical Corpora.”  Front.  <em>Digit. Humanit.</em>  5 :2. doi: 10.3389/fdigh.2018.00002&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>Rayson, P., et al. (2017).  “A deeply annotated testbed for geographical text analysis: The Corpus of Lake District Writing.”  In  <em>Proceedings of the 1st ACM SIGSPATIAL Workshop on Geospatial Humanities</em> . ACM, pp. 9-15.&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>Grover, C., Givon, S., Tobin, R., and Ball, J. (2008).  “Named Entity Recognition for Digitised Historical Texts.”  In  <em>LREC</em> .&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>Mosallam, Y., Abi-Haidar, A., Ganascia, J. (2014).  “Unsupervised named entity recognition and disambiguation: an application to old French journals.”  In  <em>Industrial Conference on Data Mining.</em>  Springer, Cham, pp. 12-23.&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p>Ehrmann, Maud, et al. (2016). Diachronic  “Evaluation of NER Systems on Old Newspapers.”  In  <em>Proceedings of the 13th Conference on Natural Language Processing</em>  (KONVENS 2016)).  “Bochumer Linguistische Arbeitsberichte” , p. 97-107.&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13">
<p>Brooke, J., Hammond, A., Baldwin, T. (2016).  “Bootstrapped text-level named entity recognition for literature.”  In  <em>Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</em>  (Volume 2: Short Papers), pp. 344-350.&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14">
<p>Neelakantan, A., Collins, M. (2015).  <em>Learning dictionaries for named entity recognition using minimal supervision.</em>  arXiv preprint arXiv:1504.06650.&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:15">
<p>Klein, E., Alex, B., Clifford, J. (2014).  “Bootstrapping a historical commodities lexicon with SKOS and DBpedia.”  In  <em>Proceedings of the 8th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities</em>  (LaTeCH), pp. 13-21.&#160;<a href="#fnref:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:16">
<p>Lehmann, J.; Isele, R.; Jakob, M.; Jentzsch, A.; Kontokostas, D.; Mendes, P. N.; Hellmann, S.; Morsey, M.; van Kleef, P.; Auer, Sö. &amp; Bizer, C.  “DBpedia - A Large-scale, Multilingual Knowledge Base Extracted”  from  <em>Wikipedia Semantic Web Journal</em> , 2013&#160;<a href="#fnref:16" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:17">
<p>Bollacker, K., Evans, C., Paritosh, P., Sturge, T. and Taylor, J. (2008).  “Freebase: A Collaboratively Created Graph Database for Structuring Human Knowledge.”    <em>Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data</em> , ACM, pp. 1247-1250.&#160;<a href="#fnref:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:18">
<p>Hoffart, J., Suchanek, F. M., Berberich, K. and Weikum, G. (2013).  “YAGO2: A Spatially and Temporally Enhanced Knowledge Base from Wikipedia Artificial Intelligence,”  special issue on Wikipedia and Semi-Structured Resources.&#160;<a href="#fnref:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:19">
<p>Nouvel, D., Ehrmann, M., Rosset, S. (2016).  <em>Named Entities for Computational Linguistics.</em>  ISTE, cognitive science series.&#160;<a href="#fnref:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:20">
<p>Rizzo, G., Troncy, R. (2011).  “Nerd: evaluating named entity recognition tools in the web of data.”  In  <em>Workshop on Web Scale Knowledge Extraction</em>  (WEKEX’11).&#160;<a href="#fnref:20" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:21">
<p>Frontini, F., Brando, C., Riguet, M., Jacquot, C., and Jolivet, V. (2016).  “Annotation of Toponyms in TEI Digital Literary Editions and Linking to the Web of Data,”    <em>MATLIT: Materialidades da Literatura</em> , 4(2), pp. 49-75.&#160;<a href="#fnref:21" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:22">
<p>Elson, D. K., Dames, N., McKeown, K. R. (2010).  “Extracting social networks from literary fiction.”  In  <em>Proceedings of the 48th annual meeting of the association for computational linguistics</em> . Association for Computational Linguistics, p. 138-147.&#160;<a href="#fnref:22" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:23">
<p>The entire corpus is freely available in multiple formats on the website of the CBMA project: <a href="http://www.cbma-project.eu/bdds2/2014-07-10-14-27-56.html">http://www.cbma-project.eu/bdds2/2014-07-10-14-27-56.html</a>&#160;<a href="#fnref:23" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:24">
<p>Bernard, Auguste &amp; A.Bruel. Recueil des chartes de l&rsquo;abbaye de Cluny. Imprimerie nationale, 1876; Petit, Ernest. Cartulaire du prieuré de Jully-les-Nonnains. Imprimerie de Georges Rouillé, 1881; Ragut, Camille; Chavot, Th. Cartulaire de Saint-Vincent de Mâcon: connu sous le nom de Livre enchaîné. Impr. d&rsquo;É. Protat, 1864.&#160;<a href="#fnref:24" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:25">
<p>BERNARD A., BRUEL A.,  <em>op.cit</em> , tome 3 : 987-1027, p. 245, n° 2039 (Collection de documents inédits sur l&rsquo;histoire de France. Première série, Histoire politique).&#160;<a href="#fnref:25" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:26">
<p>« In the name of the Incarnate Word. Let all the people know that I, Adalgis, donate to the Lord God and to his holy apostles, Peter and Paul, in the place of Cluny, where the master and most venerable abbot Odilon seems to rule more than to benefit, for the redemption of all of my sins, and for the Lord to assist me in the last day of judgment.There are, then, these things in the county of Macon, in the villa of Tasiaco: that is, in the first place, one curtilus (small estate) with a house and vine and field; and it has boundaries from three sides, from one Rodulfus&rsquo; land, from another Seguinus&rsquo; land, from the third Franks&rsquo; land and from the forth the public road.Another field in other place; it has boundaries from four sides, from a side Rodulfus&rsquo; land, from another Fulchardus&rsquo; land and last one the public road. And two small pieces of meadow land; they have boundaries from three sides, from one side Sanctus Quintinus&rsquo; land, from another Sanctus Martinus&rsquo; land and from the other the public road. The rectors of Cluny can do whatever they want, from today on, with this donation. However, if any person would want any legal dispute to stand against this donation, let God&rsquo;s wrath fall on him, and let him be submerged alive in hell if he doesn’t return to amendment.Sign(Signum) Adalgis’, woman, who ordered this act to be made and asked to validate it. S(ignum) Ingelelmi, S. Arlei, S. another Arlei. S. Bernardi. S. Ebrardi. »&#160;<a href="#fnref:26" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:27">
<p>Bourin, Monique. (1996).  “France du Midi et France du Nord : deux systèmes anthroponymiques? L’anthroponymie document de l’histoire sociale des mondes méditerranéens médiévaux.”    <em>Publications de l&rsquo;École française de Rome</em> , 226(1), pp.179-202.&#160;<a href="#fnref:27" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:27" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:28">
<p>Magnani, Eliana. (2002).  “Le don au moyen âge.”    <em>Revue du MAUSS</em> , no 1, pp. 309-322.&#160;<a href="#fnref:28" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:29">
<p>Barthélemy, D. (1997).  “ La mutation de l&rsquo;an mil, a-t-elle eu lieu ?”    <em>En Annales. Histoire, Sciences Sociales.</em>  Cambridge University Press, p. 767-777.&#160;<a href="#fnref:29" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:30">
<p>Brunner, T. (2009).  “Le passage aux langues vernaculaires dans les actes de la pratique en Occident.”     <em>Le Moyen Age</em> , vol. 115, no 1, pp. 29-72.&#160;<a href="#fnref:30" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:31">
<p>Passarotti, Marco. (2014).  “From Syntax to Semantics. First Steps Towards Tectogrammatical Annotation of Latin.”  In   <em>Proceedings of the 8th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities</em>  (LaTeCH). pp. 100-109.&#160;<a href="#fnref:31" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:32">
<p>Iogna-Prat, D., et al. (2013),  <em>Cluny : les moines et la société au premier âge féodal.</em>  Presses universitaires de Rennes. Collection « Art et Société ».&#160;<a href="#fnref:32" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:33">
<p>Lafferty, J., McCallum, A., and Pereira, F. (2001).  “Conditional random fields: Probabilistic models for segmenting and labeling sequence data.”  In  <em>Proceedings of the eighteenth international conference on machine learning</em> , ICML, Vol. 1, pp. 282-289.&#160;<a href="#fnref:33" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:34">
<p>Wallach, H. M. (2004),  “Conditional random fields: An introduction.”     <em>Technical Reports</em>  (CIS), pp. 22.&#160;<a href="#fnref:34" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:35">
<p>Lavergne, T., Cappé, O., and Yvon, F. (2010),  “Practical very large scale CRFs.”  In  <em>Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics</em> , pp. 504-513.&#160;<a href="#fnref:35" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:36">
<p>Sopena, P. M. (1996).  “L&rsquo;anthroponymie de l&rsquo;Espagne chrétienne entre le IXe et le XIIe siècle.”    <em>L’anthroponymie document de l’histoire sociale des mondes méditerranéens médiévaux, Publications de l&rsquo;École française de Rome</em> , 226(1), pp. 63-85.&#160;<a href="#fnref:36" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:37">
<p>Billy, P. (1995).  “Nommer en Basse-Normandie aux Xle-XVe siècles.”    <em>Cahier des Annales de Normandie</em> , 26(1), pp.223-232.&#160;<a href="#fnref:37" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:38">
<p>Corrarati, P. (1994).  “Nomi, individui, famiglie a Milano nel secolo XI. Mélanges de l&rsquo;Ecole française de Rome.”    <em>Moyen-Age</em> , 106(2), pp. 459-474.&#160;<a href="#fnref:38" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:39">
<p>Chastang, P. (2007).  “Du locus au territorium. Quelques remarques sur l’évolution des catégories en usage dans le classement des cartulaires méridionaux au XIIe siècle.”  In  <em>Annales du Midi : revue archéologique, historique et philologique de la France méridionale</em> , Tome 119, N°260, pp. 457-474.&#160;<a href="#fnref:39" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:40">
<p>Bange, F. (1984).  “L&rsquo;ager et la villa : structures du paysage et du peuplement dans la région mâconnaise à la fin du Haut Moyen Age (IXe-XIe siècles).”  Annales. Economies, sociétés, civilisations, 39(03), pp.529-569.&#160;<a href="#fnref:40" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry><entry><title type="html">Automatic Identification of Types of Alterations in Historical Manuscripts</title><link href="https://rlskoeser.github.io/dhqwords/vol/15/4/000576/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://rlskoeser.github.io/dhqwords/vol/15/4/000576/</id><author><name>David Lassner</name></author><author><name>Anne Bailot</name></author><author><name>Sergej Dogadov</name></author><author><name>Klaus-Robert Müller</name></author><author><name>Shinichi Nakajima</name></author><published>2021-11-12T00:00:00+00:00</published><updated>2021-11-12T00:00:00+00:00</updated><content type="html"><![CDATA[<h2 id="editorial-prolegomenon">Editorial prolegomenon</h2>
<p>Classical scholarly editing has a long-standing tradition in distinguishing between different types of editions<a class="footnote-ref" href="#witkowski1924"> [witkowski1924] </a>. The characteristics of specific edition forms usually align with the intended readership, but they also take into account a bibliographic history that tends to differentiate more and more along time according to linguistic areas. In the German-speaking area, historical-critical editions that comprise an extensive historical-critical apparatus are often distinguished - with a clear hierarchical difference - from so-called study editions<a class="footnote-ref" href="#plachta2006"> [plachta2006] </a>. The common denominator between these two types of editions is that they aim to offer areliable textas a central component<a class="footnote-ref" href="#plachta2006"> [plachta2006] </a>. In contrast to these types of editions, it is also possible to publish a reproduction of the manuscript image (facsimile edition). Plachta points out, however, that a facsimile edition is no substitute for the above two types of editions<a class="footnote-ref" href="#plachta2006"> [plachta2006] </a>.</p>
<p>Another way of differentiating between types of editions is to compare the intention in the text construction, which corresponds to the philosophy according to which the anglo-saxon area has mainly structured their approach. According to Andrews, “the old methods that have their root in classical philology” strive to assemble theidealtext, while thenew philologyseeks to find therealtext<a class="footnote-ref" href="#andrews2013"> [andrews2013] </a>. In this conception, the ideal text tries to approach the author&rsquo;s intention, while the real text seeks to emulate the existing sources.</p>
<p>The type of edition an editor goes for is often defined by economic factors in printed editions, while in digital editions, this limitation can be obsolete in terms of the amount of pages available, or located on a different level (for instance due to the price of specific, cost-expensive technologies). More generally, in digital scholarly editions, differentiation characteristics can be renegotiated. As Andrews states, there are hardly any technical limitations in digital editions with regard to the size of the apparatus, and the number and resolution of facsimiles provided<a class="footnote-ref" href="#andrews2013"> [andrews2013] </a>.</p>
<p>This is not the only specificity that distinguishes digital from print editions. They also are machine-readable. With digital editions being available in digital formats, computers can not only handle repetitive tasks in the creation of the edition<a class="footnote-ref" href="#andrews2013"> [andrews2013] </a>, they can also be used to perform tasks that use the edition as source material. The most obvious example for this type of use is the full-text search, but the machine-readable form also allows the creation of a multitude of statistics and customed visualizations with very little effort<a class="footnote-ref" href="#ralle2016"> [ralle2016] </a>. Furthermore, Ralle emphasizes that the digitization of editions and scholarly editing in general allow to pay special attention to the processual aspects of the edition<a class="footnote-ref" href="#ralle2016"> [ralle2016] </a>. An edition can be extended or enriched after it has been initially published and does not need to befinishedat a specific moment in time. A digital edition can be modified dynamically, for instance like the Carl-Maria-von-Weber-Gesamtausgabe with a front page field called “What happened today?” that connects to all instances of the current date in the corpus and highlights them - a content that changes from day to day and offers a different approach to the corpus than the traditional keyword search. Also, user interaction can be funneled back into the edition, for example when subsequent publications that are based on the edition are listed there. Interaction in and of itself can also be included: the search behaviour of users can be analyzed for better future suggestions or the edition can be enriched by third-party data. Every user of a digital edition, whether computer or human, is thus potentially able to engage in one form of editorial participation or the other<a class="footnote-ref" href="#schlitz2014"> [schlitz2014] </a><a class="footnote-ref" href="#siemens2012"> [siemens2012] </a><a class="footnote-ref" href="#shillingsburg2013"> [shillingsburg2013] </a>.</p>
<p>These special features of digital editions allow for paleography<a class="footnote-ref" href="#baillot2015b"> [baillot2015b] </a>to reach out to research questions hence unexplored in the Humanities due to the lack of tools and corpora allowing an automatic evaluation of alteration phenomena. It enables for instance to thoroughly reconstruct the history of a document by considering physical traces of alterations, meaning any smaller or larger text modifications on the manuscript, performed either by the author himself or herself or by others (see Methods). This approach provides insights into the way in which authors, editors and other contributors work together, hence impacting our understanding of text genesis as a collaborative process.</p>
<p>In order to achieve substantial results in this field of research, fast and well-structured access to the document variants is required. Digital editions presenting the manuscript alterations allow to focus on diplomatic transcription or facsimile, as opposed to print editions where the focus is on a single copy text, itself usually optimized for readability. Examples of digital editions representing the document history include faustedition.net<a class="footnote-ref" href="#goethe2017"> [goethe2017] </a>, bovary.fr<a class="footnote-ref" href="#leclerc2009"> [leclerc2009] </a>, beckettarchive.org<a class="footnote-ref" href="#beckett2011"> [beckett2011] </a>and the edition that provided the background for the methodology we propose here: the digital scholarly edition “Letters and texts. Intellectual Berlin around 1800” , berlinerintellektuelle.eu<a class="footnote-ref" href="#baillot"> [baillot] </a>, <em>BI</em> in the following.</p>
<h2 id="introduction">Introduction</h2>
<p><em>Letters and Texts. Intellectual Berlin around 1800</em> is a digital scholarly edition of manuscripts by men and women writers of the late 18th and early 19th century. The connection these writers have to the intellectual networks in the Prussian capital city are either direct (authors living and writing in Berlin) or indirect (editorial or epistolar relationship with Berlin-based intellectuals). The originality of this digital scholarly edition is that it is neither author-centered nor genre-based, but presents different types of selected manuscripts that shed light on the intellectual activity of Berlin at the turn of the 18th to the 19th century. This editorial choice is presented at length in<a class="footnote-ref" href="#baillot2014"> [baillot2014] </a>, where light is also shed on the uniqueness of the Prussian Capital City in the context of the period. While correspondences play a key role in the edition, they are considered as a part of the circulation of ideas that is at the core of the project, so that letters, and more generally egodocuments, are complemented by drafts of either literary works (among which two major romantic texts), scholarly writings (one dissertation) or administrative documents (related to the development of the Berlin University). A first editorial phase (2010-2016) allowed to publish manuscripts that cover different thematic areas and historical phases of the development of intellectual Berlin. They were selected based mainly on their scholarly relevance and on their accessibility (the publication policy of archives holding manuscripts having a major impact on their integration to a digital edition that displays a facsimile like this one does). Four main topics have emerged in this first phase: French, e.g. Huguenot Culture, Berlin University, Literary Romanticism and Women Writers. Depending on the topics, the letters published were complemented by other types of texts that document the circulation of ideas and of literary and scholarly works in the late 18th and early 19th century.</p>
<p>The edition can be browsed by theme, by author, by period, by holding institution, or by date. The single document can be displayed on one or two columns presenting at the user’s choice a facsimile of the current manuscript page, a diplomatic transcription, a reading version, the metadata, the entities occurring on the page and the XML file corresponding to the document. In this first development phase, 248 documents and 17 authors were encoded and presented in BI. In Figure 1, a quantitative summary of the BI corpus is given, which consists of introductory figures for the whole corpus in terms of size, temporal span and number of alterations and detailed information about individual authors.</p>




























<figure ><img loading="lazy" alt="Picture of various bar charts" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Quantitative summary of the BI corpus. Subfigure (a) shows the number of documents grouped by each author. Adelbert Chamisso holds the largest share of documents. In (b), the actual number of characters by author shows a slightly different picture, instead of Chamisso, J. Euler has the largest portion. In Subfigure (c), the alterations in terms ofnumber of changed charactersbetween different versions of the documents are depicted for each author. A Large number of total characters of an author does not necessarily mean a lot of alterations. Although Euler encompasses the largest number of characters there are close to no alterations in the documents he authored. In (d), total counts for the whole BI corpus are stated to show the extent of the annotational effort. In (e), the temporal distribution of the creation of the documents is shown for the whole corpus (top) and for each individual author (bottom). In this subfigure ((e), bottom), the number of documents created in each year is encoded in the intensity of the color.
        </p>
    </figcaption>
</figure>
<p>A major novelty about the BI edition is that it combines genetic edition and entity annotation in order to gain insight in intellectual networks, on the actual editing process of manuscripts (of literary and scholarly works) and on the discourse about this editing process (letters – most letters are interestingly also partly transformed into literary works in their own right and subject to editing). The genetic encoding gives precise information regarding deletions and additions in the manuscript text. The BI encoding guidelines make extensive use of the following specific sections of the the TEI (P5)<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> guidelines additionally to the standard structure (Chapter 1-4): Manuscript Description (Chapter 10,<a href="https://www.tei-c.org/release/doc/tei-p5-doc/en/html/MS.html">https://www.tei-c.org/release/doc/tei-p5-doc/en/html/MS.html</a>), Representation of Primary Sources (Chapter 11,<a href="https://www.tei-c.org/release/doc/tei-p5-doc/en/html/PH.html">https://www.tei-c.org/release/doc/tei-p5-doc/en/html/PH.html</a>) and Names, Dates, People, and Places (Chapter 13,<a href="https://www.tei-c.org/release/doc/tei-p5-doc/en/html/ND.html">https://www.tei-c.org/release/doc/tei-p5-doc/en/html/ND.html</a>), which offers the possibility to markup text alterations with tags such asaddanddel.</p>
<p>As already mentioned, the BI Edition features annotations on the genesis of the documents (genetic edition), which, being a digital edition, are machine-readable. The core question we will address in the following is therefore whether machine learning models<a class="footnote-ref" href="#wainwright2008"> [wainwright2008] </a><a class="footnote-ref" href="#rasmussen2006"> [rasmussen2006] </a><a class="footnote-ref" href="#nakajima2019"> [nakajima2019] </a>that analyze the alterations within the documents can be used to gain new insights into author, editor, and archivist practices, as well as practices of the intellectual societies in the document’s creation time. The investigation of this question is only made possible by the meticulous (digital) annotation of the historical documents that provides previously unavailable enrichments and perspectives on the sources.</p>
<p>From the perspective of edition theory Ehrmann stresses that the importance of analyzing the alterations in manuscripts for literary studies and scholarly editing lies not only in the fact that they allow an insight into the author&rsquo;s writing process in the case of author-made changes, but also in the fact that they help identify the respective contribution in the case of co-authorships<a class="footnote-ref" href="#ehrmann2016"> [ehrmann2016] </a>. The first question that arises when examining every alteration is the question of the underlying reason, be it for a minor correction of mistakes or a wide-ranging content-related alteration. This leads to the question of the originator of the alteration and, as Ehrmann stresses, whether the alteration is wanted by the author<a class="footnote-ref" href="#ehrmann2016"> [ehrmann2016] </a>. In the specific case of an edition of correspondence, the intended readership is bound to change dramatically in the aftermath of publication. A letter that was originally written to a friend is made public to a large readership and in the process of preparation, the editor applies alterations to the original letter, most of the time with a correction phase on the original manuscript itself. This is the case for many manuscripts in the BI edition, commented on as follows by the editors:</p>
<blockquote>
<p>One characteristic of letters is that you generally are not the first one to read them when you discover them in an archive. Not only have they been addressed to a person or a group of persons in the first place [..], many of the letters we at least are working on have already been edited in the last centuries. But not in extenso, no: they have been abridged, overwritten, corrected according to the expectation of the audience in the time that they were edited.<a class="footnote-ref" href="#baillot2015a"> [baillot2015a] </a></p>
</blockquote>
<p>Moreover, the novel machine learning method (alterLDA) presented here also offers new opportunities for many other areas of automated analysis of variants of sources, especially within the Digital Humanities. AlterLDA is based on the topic model latent Dirichlet allocation (LDA). “Topic Modeling has proven immensely popular in Digital Humanities” <a class="footnote-ref" href="#schoch2017"> [schoch2017] </a>. LDA is particularly popular in the DH because it is suitable for explorative text analysis. With the automated compilation of word lists by LDA, new topics can be identified in large text corpora whose existence was previously unknown. In this context, it almost always forms the first analysis step on text data, but it can in fact also be used for non-textual data<a class="footnote-ref" href="#jelodar2019"> [jelodar2019] </a><a class="footnote-ref" href="#liu2016"> [liu2016] </a>. In addition to LDA, which provides the identification of the overall relevant topics of the corpus to be examined and the specific topics of the individual documents of the corpus, alterLDA is particularly concerned with the variants of the documents. The starting conditions for this work are as follows: from the point of view of edition theory, the question of document variants is of major importance, and this has not yet been sufficiently investigated with Distant Reading methods. From a methodological point of view, there is a very widespread Topic Model (LDA), which is already recognized and accepted practice in the Digital Humanities. In this paper we therefore close the gap by adapting LDA in order to model document variants.</p>
<p>The processual aspects of text genesis in the sources underlying the edition are thus highlighted and supported by the processual aspects of the edition itself. If a document in its past has already been prepared for publication by an editor, then his or her notes in the TEI-XML are annotated in the same way as when the editors of the BI Edition leave notes: with the <code>&lt;note&gt;</code> -tag.</p>
<p>Parts may be deemed inappropriate for publishing to a broader readership at a certain place and time due to their political or religious context, or for revealing private information about a person or a group.</p>
<p>The application on the BI corpus is particularly interesting because the latter consists mainly of letters, which, especially around 1800 in Germany, exhibit a strong tension between public and private sphere. The framework presented here includes four methods that range from basic, well established, rule-based methods to a specialized, novel machine learning method (alterLDA) that was developed for exactly this purpose. From a methodological point of view, this is a challenge for all disciplines involved, conceived as a scenario optimized so that all sides benefit from each other. Finally, the newly introduced method is also applied to discover alteration candidates in the documents that are not yet altered. These findings led to, and hopefully will continue to fuel, interesting discussions on parts of the edition that were unnoticed thus far.</p>
<p>Due to machine readability, documents in digital editions can be modified by computer programs in such an algorithmic way that they are transformed into something else. This transformation is described by Stephen Ramsay in the concept of Algorithmic Criticism:</p>
<blockquote>
<p>Any reading of a text that is not a recapitulation of that text relies on a heuristic of radical transformation. The critic who endeavors to put forth areadingputs forth not the text, but a new text in which the data has been paraphrased, elaborated, selected, truncated, and transduced<a class="footnote-ref" href="#ramsay2011"> [ramsay2011] </a>.</p>
</blockquote>
<p>The methods to which Ramsay refers here, e.g. tf-idf normalization, are mostly deterministic methods. In this work, however, a probabilistic method is used to transform the documents and their variants, which uses previously collected data and relates the observations to it. Thus, these statistical transformations resemble human reading more than purely deterministic approaches and therefore foster the methodological concept of the “radical transformation” described within the Algorithmic Criticism towards a more general criticsm that includes non-explicit algorithms.</p>
<p>This transformation in an algorithmic way is a very standard technique (e.g. counting co-appearances of speakers in scenes of a play). However in recent years Machine Learning models are being used more broadly for e.g. Named Entity Recognition<a class="footnote-ref" href="#dalenoskam2016"> [dalenoskam2016] </a><a class="footnote-ref" href="#jannidis2015"> [jannidis2015] </a>. The patterns that are used to identify entities are not stated explicitly by a programmer but are learned from the data at training time. In general, Machine Learning methods would then be methodologically less strict than classical explicit algorithmic transformations and therefore be possibly also more human-like. However, when giving up this explicitness there has to be a more rigorous evaluation of the machine’s output. For many applications, like the one presented in this work, we therefore rely on the evaluation by machine learning and humanities scholars, who can employ methods for interpreting and explaining machine learning models<a class="footnote-ref" href="#samek2019"> [samek2019] </a>.</p>
<h2 id="methods">Methods</h2>
<p>In this section, the machine learning methods for identifying the reason for a given alteration are presented, by first introducing the general data analysis pipeline. Then, we specify precise definitions of the most relevant concepts for alterations. After specifying the preprocessing steps, the novel alterLDA model is introduced. It is designed to analyze the most interesting, yet most complex types of alterations. Before the methodologies of each step are explained in further detail, the definitions for the most important and most frequently used terms are given here.</p>
<p>Given an arbitrary version of a document, we define an alteration to be a local group of added and/or deleted symbols that is performed by the author of an alteration. Basically, any symbol appearing in the document could be regarded as a single addition, but the state of the manuscript at the time of the investigation often makes it impossible to identify beyond doubt which groups of symbols belong to a particular writing session. The same problem exists with deletions: Was the sentence completed first, or did the author pause in the middle and correct something before completing the sentence? In BI, additions and deletions are considered as such when they clearly stand out, for example when they are crossed out or written to the margin. Sometimes co-occurring additions and deletions are also referred to as replacements. The alteration may range from a single character to whole passages of the document and can even be a local group with non-altered symbols in between. An alteration author is a single person or institution that alters the document, possibly the primary author him or herself. The alteration author has an alteration reason for which he or she decides to alter the document. This is a very specific reason, for example “the alteration author thinks that a particular word is spelled differently” or “a real person which is referred to in the document may not want to be recognized by the readers, so this part is censored” .</p>
<p>Each alteration has a formal and content-related portion with varying emphasis. For example, if the author of an alteration changes the spelling of a single word this would not change the meaning of the document in most cases. On the contrary, adding multiple sentences to a document may change the content of the document significantly. Of course, whether an alteration is rather positioned on the form side or on the content side of the axis depends on the point of view of the recipient. Hence, the proposed method takes into account the formal changes of the document as well as the content-related changes. Smaller alterations tend to have a rather formal aspect, where longer alterations almost always are content-related.</p>
<p>The set of alterations can be broken down into different categories with respect to their alteration reason. One group of alterations is (1) the group of paratexts, for example archival notes, such as numberings or dates, or stamps and seals of the library or archival institution. Another group of alterations is (2) corrections of mistakes which consists in spelling alterations, grammatical changes and other corrections. (3) The third group contains stylistic alterations, for example replacing a token with its synonym or rearranging the word order. Of course, changing the word order is sometimes more than just a stylistic change, but one could e.g. begin a sentence with “Es bedarf daher [..]” as well as with “Daher bedarf es [..]” with very similar intentions. The last group of alterations which we call (4) content-related alterations incorporate alterations that either add new information to the document or suppress information that was present in the document before.</p>




























<figure ><img loading="lazy" alt="Images of manuscript handwriting" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Flow chart of machine learning pipeline with four example alterations. The stream of documents is analysed in four steps that identify different reasons of alterations as depicted in the panel at the top. In the panel at the bottom, the details of the individal alterations are presented. Each alteration has a unique appearance and unique characteristics, like the type of ink and the way in which it fits into the surrounding script. The presented preview of the facsimiles are shown in greater detail in Figure 9, Figure 10, Figure 11 and Figure 12.
        </p>
    </figcaption>
</figure>
<p>Figure 2 illustrates how the identification method works. All alterations are put into the analysis pipeline, and after the initial distinction between author alterations and non-author alterations, the four tests for different types are performed on each alteration. As an example, there are four alterations depicted in the illustration that are fed into the model. A detailed explanation for an identification of the three non-content-related types of modifications is given in the appendix. By elimination of all other possible categories, the remaining alterations are of the content-related category. There are still a variety of reasons in this category worthwhile to identify. Rather than the general category we aim for providing a distinct reason for each alteration. The fourth alteration which is marked in red is a longer deletion and a detailed facsimile is shown in Figure 2. It is performed with a pencil which is different from the primary ink of the letter. It deals with the author’s sickness and with the sickness of the author’s mother. The extent of the alteration already indicates that this is not a correction of a mistake and since the part that is deleted is not replaced by anything else, it can be assumed that this alteration changes the amount of information provided. It is thus to be classified as a content-related alteration. At this point it is still to be identified for which specific reason the document has been altered. With alterLDA, the alteration is assigned to one of a set of candidate reasons as a final step, in this case <em>Sickness</em> -reason.</p>
<h2 id="related-work">Related Work</h2>
<p>We convey a generative topic model, that is based on Latent Dirichlet Allocation<a class="footnote-ref" href="#blei2003"> [blei2003] </a>and that is able to take into account the structural information of alterations. LDA is a widely used topic model that extends Latent Semantic Indexing<a class="footnote-ref" href="#deerwester1990"> [deerwester1990] </a>which is capable of assigning a distribution of topics to a document instead of only a single topic. LDA takes advantage of the fact that a text is organized in documents. This structural information is the reason for LDA to function. Based on this structure of documents, LDA can learn which words tend to co-occur and thus have a relation. Words that often occur with each other form a topic. In this context, a topic is merely a distribution of word frequencies.</p>




























<figure ><img loading="lazy" alt="Image of the LDA model" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Graphical representation of the LDA model. The plate notation visualizes the generative process of a probabilistic model by following the directions of the arrows. Given α and η, one initially draws β, a distribution over words for each topic and θ, a topic distribution for each document. Then, for each token within a document, one draws a topic assignment and only then (because w has input arrows from both, β and z, one can draw w from the topic in , that was assigned in z.
        </p>
    </figcaption>
</figure>
<p>In Figure 3, the LDA model is shown in plate notation. An overview over the used symbols is given in the appendix. The plate notation shows the graphical representation of the LDA model. An open circle denotes a model variable and a shaded circle denotes an observed variable. Symbols without circle denote a hyper parameter. A rectangle indicates repetitions of the included variables. In this model, β represents the topic histograms, θ represents the topic mixture for each document, z represents the topic assignment for each token position and w denotes the token itself. LDA has no notion of the order of words within a document, which is referred to in the literature as abag-of-wordsfor each document.</p>
<p>There exists a wide range of topic models that customize LDA by taking into account additional structural information. To replace the bag-of-words approach by introducing structural information about the word order is a major field of LDA research<a class="footnote-ref" href="#rosenzvi2004"> [rosenzvi2004] </a><a class="footnote-ref" href="#gruber2007"> [gruber2007] </a><a class="footnote-ref" href="#wallach2006"> [wallach2006] </a>. In addition, there is a broad research community that addresses the recognition and arrangement of hierarchies of topics<a class="footnote-ref" href="#blei2010"> [blei2010] </a><a class="footnote-ref" href="#paisley2015"> [paisley2015] </a>. LDA has also been modified to work with graph-structured documents<a class="footnote-ref" href="#xuan2015"> [xuan2015] </a>. However, we are not aware of any literature that shows how to model alteration reasons in a corpus of natural language. Therefore, this paper is an important contribution to close this gap, i.e. to provide the literary scholarly community with a novel method and to open up another field of application for the machine learning community.</p>
<h2 id="alteration-latent-dirichlet-allocation">Alteration Latent Dirichlet Allocation</h2>
<p>In Figure 4, the alterLDA model is described in plate notation. The upper part is standard LDA whereas the lower right part contains the newly introduced variables to model alterations.</p>
<p>In standard LDA, the observed variable (the input) is just the words within each document. In the alterLDA setting, the additional structural information about the alteration of each word is provided as input. With that, the alterLDA model tries to infer the tendency for each topic to be an alteration topic.</p>




























<figure ><img loading="lazy" alt="Image of the alterLDA model" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Plate notation of the new alterLDA generative model. Newly introduced is the lower branch with variables c, ϒ and ξ that deal with alterations. There exist M documents with Nmtokens each, Also, there exist K topics and for each topic, there exist a tendency for it to be a reason for alteration (ϒ).
        </p>
    </figcaption>
</figure>
<p>The generative model detects reasons by taking into account all text, inside and outside the alterations. From alterations that were gone through manually, we expect to see alteration suggestions that mainly relate to the privacy of a person, political or religious topics may appear as well. In order to make the model description as clear as possible, we try to keep the mathematical formulations to a minimum. Therefore, we only include an explanation of the symbols used (see appendix), a graphical representation and the derivation of how the model can be algorithmically captured using a Collapsed Gibbs Sampler. Similar to LDA, in alterLDA there exists no feasible algorithm to compute the posterior distribution of the latent variables. Instead, approximate methods need to be applied to find a solution in reasonable time.</p>
<p>A Collapsed Gibbs Sampler is one of the possible approaches to find an approximate solution to the objective. Generally, a Gibbs Sampler iteratively samples the configuration of a specific latent variable based on the current configuration of all other model variables. An introductory tutorial on Gibbs Sampling LDA is presented by<a class="footnote-ref" href="#carpenter2010"> [carpenter2010] </a>. This algorithm can also be understood as an instance of a Markov Chain, a constrained iterative probabilistic model itself, where the current state only depends on the previous. From this perspective, the stationary of the Markov Chain represents the solution of the Gibbs Sampler. The source code of our implementation of the Collapsed Gibbs Sampler for the alterLDA model is<a href="gitlab.tubit.tu-berlin.de/david.lassner/shipping_alterLDA">publicly available</a>. In the appendix, derivation of the Collapsed Gibbs Sampler for the alterLDA model is given.</p>
<h2 id="results">Results</h2>
<p>In this section, we present three experiment settings which mainly differ in the splitting between training and test data. As shown in Figure 5, three settings are chosen, S1 as a straightforward explorative demonstration, S3 to comply with the methodological standards of data splitting for the performance report, as well as S2 for offering additional explorative results specific for this data set. We will first present the evaluation results that investigate the performance of alterLDA on the given data set and afterwards present explorative results that will be reconciled with expert knowledge. Apart from these experiments on the BI data set, the first experiments were performed on synthetic data, some results from these experiments are listed in the appendix.</p>




























<figure ><img loading="lazy" alt="Image of rows with text in them" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Visualization of the data splitting setup for all settings. For each experiment a different data setting is used. The different Settings are shown in the leftmost column (S1, S2, S3). Within each setting, the row at the bottom depicts the final setting of the data. Setting 1 (only one row) does not require test sets, Setting 2 (only one row) aims at finding alteration candidates in texts with no alterations. For Setting 3, the process of creating the setting is depicted in multiple rows. First, only documents that contain alterations are chosen. Then, each individual document is shuffled and split into a training and a test part.
        </p>
    </figcaption>
</figure>
<h2 id="performance-evaluation">Performance Evaluation</h2>
<p>In this experiment, in which alterLDA is applied to the entire training data, it is to be determined whether the model in principle delivers plausible results. It will be verified whether ϒ finds a meaningful topic composition that represents sensitive topics. This means that alteration topics may be a convolution of private and maybe political and religious matters.</p>
<p>With alterLDA, various parameters must be set which influence the outcome. These are the same parameters as for LDA: Number of topics and the Dirichlet prior for the topic distribution η and the topic mixture α. There is also another parameter, the Dirichlet prior for the alteration tendency ξ. The default value for a Dirichlet prior is 1, but it can take any value greater than 0. The smaller the value, the more the variable tends to be focussed on single values, the larger the value, the more different values are considered. Using the topic mixture as an example, a small α would mean that LDA is looking for a solution where each document consists of only a few topics, a large α finds a solution with mixtures of many topics. AlterLDA is initialized in this setting with α = (.1, .1, …), η = (.1, .1, …) and ξ = (.05, .05, …) as well as K=10. We choose ξ small in this setting to create a sparse ϒ so that alterLDA only learns one alteration topic.</p>
<p>The resulting topic learned in this naive approach as alteration-sensitive is visualized as a word cloud in Figure 6. It is very difficult to put a single label on thistopic. The most probable words are strongly influenced by global word frequencies, the strongest four words describe it: “ Sie Ich Brief schreiben ” this does not come as a surprise since the corpus consists mainly of letters. However, it is also possible to find terms from any subject area that was suspected in advance of being altered: <em>Sickness</em> terms are for exampleOperation,Bett,fürchten, <em>Financial</em> terms are e.g.Geschäft,Geldand regardingLove Storythere is for exampleliebandschön.</p>
<p>Beforehand, we assumed to also find political, religious topics but these do not appear in the naive setting whereas diverse private topics do occur.</p>




























<figure ><img loading="lazy" alt="Image of a word cloud" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>The strongest words of the topic that has a high alteration tendency after training alterLDA in the naive setting (Setting 1). The stronger the word, the larger the font. Strongest words are very general words on the topic of letters, words from the Financial, Sickness and Love Story are also weakly present.
        </p>
    </figcaption>
</figure>
<p>As visualized in Figure 5, the BI corpus consists of documents with alterations and documents without alterations. If we want to measure the performance of alterLDA in predicting the tendency for alteration, documents with alterations are much more helpful.</p>
<p>In Setting 3, we only use documents with alterations to produce the training and test set. We split every document individually into training and test set after shuffling to increase the chance that alterations are present in both sets<a class="footnote-ref" href="#muller2001"> [muller2001] </a>. After training, we use the topic mixture θ of the corresponding document.</p>
<p>In this setting, alterLDA is initialized with α= (1, 1, …), η = (1, 1, …) and ξ = (1, 1, …) as well as K=20. We explicitly chose the Dirichlet priors all equal to 1 as this can be considered the default. To allow for more topic diversity, we chose the number of topics a little bit higher than in the naive setting. This parameter combination will be used throughout the rest of the paper.</p>
<p>The performances on the total test set as well as for each individual author are shown in Table 1. The performance varies considerably across different authors where D. Tieck, L. Tieck and Ad. Chamisso work well above chance level, the performance for Hoffmann and especially H. Finckenstein is weaker. In case of H. Finckenstein, this may be due to the fact that in the corpus there is only a single letter. For E.T.A. Hoffmann, there is also only one document in the corpus, but it presents two specialties. It is considerably longer than most documents in the corpus: it is not a letter, but the novella _ Der Sandmann _ . The larger size and the differing properties due to the genre seem to trade off to a slightly better performance than in the case of H. Finckenstein. We thus argue that the performance of alterLDA depends on the size of the training set and on the homogeneity of the documents.</p>
<p>The results of this setting are not meant produce new domain insights as it only aims at reproducing the alteration tendencies of already altered documents. However, screening performance difference across viewpoints such as <em>authors</em> still reveals properties of the underlying data set.<br>
Test set performance for documents with alterations. The test set is grouped by author and two performance measures are given. Balanced Accuracy and Area under Receiver Operating Characteristic. In both cases, the sklearn implementation is used.[http://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics version 0.20.0.](<a href="http://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics">http://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics</a>                         version 0.20.0.)GroupingBalanced AccuracyArea under ROCAdelbert von Chamisso.60.57Henriette v. Finckenstein.38.07Immanuel v. Fichte.49.64E.T.A. Hoffmann.5.53Dorothea Tieck.61.65Ludwig Tieck.69.65Total.67.66</p>
<h2 id="explorative-analysis">Explorative Analysis</h2>
<p>In the explorative experiment (S2), the corpus is divided into two parts: On the one hand, all documents that contain changes and, on the other hand, all documents that do not contain any changes. The aim of the experiment is to train the model on the part of the corpus that contains changes and then let the model suggest which parts of the unchanged corpus may be changed in a similar way. There may be different reasons why some documents contain alterations and others do not. Assuming that all documents were reviewed by the same person and that person was also so diligent that he or she did not overlook a single passage, then alterLDA should at best-case scenario not propose an additional passage to be altered. We assume in this experiment that either not all documents have been reviewed for the same criteria or that relevant positions have been overlooked.<br>
The table shows the number of suggested alterations for different authors/editors. Only documents that were not truly altered are included. The number of suggested alterations represents the number of positions in dochuments that the method suggests to alter. Euler and Buch mainly wrote in French, which influenced the prediction a lot for these authors. Chamisso wrote in German although his mother tongue is French. Therefore there may be many minor mistakes that are suggested to be altered. Dorothea Tieck wrote about her mothers’ sickness which the method recognized as a sensible topic and therefore as a reason for alteration.AuthorSuggested alterationsImmanuel Hermann von Fichte3Karl August Varnhagen von Ense16Friedrich Wilhelm Neumann54Helmina von Chézy59Adelheid Reinbold73Henriette Herz76Friedrich von Schuckmann118Antonie von Chamisso258Friedrich Wilken340Ludwig Tieck389August Boeckh540Adolf Friedrich von Buch907Dorothea Tieck929Adelbert von Chamisso1075Jean Albert Euler2558<br>
In Table 2 the counts of the positions in documents with no alterations that have been suggested by the method are displayed for each author/editor. The rows in the table are sorted by the total amount of suggested alterations. Interestingly, the authors with many suggested alterations are not necessarily the ones that have a large share of total tokens of the corpus (see Figure 1, (b) and (c)). In the case of Euler and von Buch, this is due to the fact that their documents are mostly in French, whereas the alterLDA model in this case is primarily trained on German texts. For Boeckh, this is mainly due to the fact that the corpus encompasses only a few yet long documents and consequently there are not many documents present in the test set. Of course, there are other reasons for each author&rsquo;s ratio of corpus portion and number of suggested alterations. A person that altered all positions in the training set also diligently edited all documents in the test set and simply did not find any position that should be altered for the same reason: That the method did not find the respective amount in the test set can either mean that it was not able to find the right positions or that there were none.</p>
<p>For further analysis, we will ignore the texts by J. A. Euler and A. F. Buch and focus on the other four authors for which alterLDA suggested most alterations. As said, the texts by J. A. Euler and A. F. Buch were mainly written in French which influenced the number of suggested alterations. In Table 3, the most common words that were suggested to be altered for individual authors are listed. For all authors except A. Boeckh, the majority of words seem to relate to the overall <em>letter</em> topic, however for D. Tieck the keywordKrankheit <em>Sickness</em> appears. For Ad. Chamisso, words like e.g.schönandbegehrencan be observed that may relate to the topic <em>Love Story</em> . In the case of L. Tieck, a distinct convoluted alteration topic is not immediately conceivable. In the case of A. Boeckh, the topic of the documents in the corpus are mostly academia-related.<br>
Most common words that were suggested to be altered in texts from individual authors.Author25 most common suggested alteration words (descending order)Dorothea TieckSie, Brief, schreiben, Ich, schön, gewiß, denken, einig, lesen, all, gleichen, Düsseldorf, Agnes, Krankheit, Freund, kennen, erhalten, Arbeit, Dresden, halten, weiß, Die, Leben, Berlin, LüttichauAdelbert v . ChamissoBrief, schreiben, Ich, de, Die, weiß, kennen, all, wissen, schön, 4, Freund, denken, Sie, gleichen, halten, 3, neu, ton, erhalten, begehren, bleiben, einig, lesen, SacheLudwig TieckSie, Freund, Ich, Geist, Brief, Tieck, Dresden, Von, Ihr, umarmen, Juli, halten, sogleich, erleben, Die, schwach, schweigen, sprechen, Mich, Herrn, Vergnügen, fordern, Masse, gleichen, eintretenAugust BoeckMitglied, Seminar, Sie, Prämie, Fichte, erhalten, Arbeit, 1813, 2, Verfasser, 1812, Übung, hiesig, 4, Fähigkeit, welch, außerordentlich, Nummer, zahlen, Prüfung, Wernike, Anstalt, anfangen, Gedicht, Studiosus<br>
For a better understanding of alteration suggestions, a closer look into the individual authors is provided. D. Tiecks’ documents reveal a sequence of letters that she wrote to F. Uechtritz in the years between 1831 and 1840. In the letters she repeatedly mentions her mother’s sickness until her death in February 1837. Later, in March 1837, the father of F. Uechtritz passed away as well, D. Tieck writes about this in Letter 28.</p>
<p>By manually reviewing this series of letters, the editors of the BI edition agreed in many cases with the classification of the alterLDA model that D. Tieck’s mother’s sickness plays a role for the alteration tendencies of the documents. In some cases, however, human experts and the model disagreed about the reason for alteration. The following excerpt from letter 12 is identified by our proposed method as a stylistic alteration.</p>
<blockquote>
</blockquote>
<p>Meine arme Mutter<br>
**<br>
leidet schon seit längerer Zeit an Unter=</p>
<p>leibsbeschwerden, der Arzt sagt es seyen</p>
<p>Verhärtungen und Anschwellungen der Drü=<br>
**<br>
<strong>sen, sie</strong> hat schon seit längerer Zeit viel zu leiden, <strong>braucht schon</strong></p>
<p><strong>seit 3 Monathen,</strong> trinkt seit 4 Wochen hier</p>
<p>Karlsbad, und <strong>alles</strong> bis jetzt ohne den</p>
<p>mindesten Erfolg.</p>
<p>(BI, Dorothea Tieck to v. Uechtritz, Letter 12, p. 2)</p>
<p>And one can argue that this is actually a stylistic alteration, because the information about the mother’s sickness is preserved after the alteration. However, the detail that her mother has pelvic complaints is suppressed in the second version - this discrepancy in detail can be decisive for classification as a content-related alteration.</p>
<p>In Figure 7, the number of suggested alterations (for the test set) and the number of actual alterations (for the training set) are displayed for each document by Ad. Chamisso. Most of the letters are addressed to L. La Foye (Ad. Chamisso’s best friend), some are addressed to Antonie von Chamisso (Ad. Chamisso’s wife). For each of the addressees, the letters are ordered by date. There are two letters (letter 10 and 11) which stand out significantly with regard to the number of alterations, letter 10, actually encompasses a large number of alterations, whereas letter 11 is part of the test set and thus does not have any (content-related) alterations. The alterLDA model suggests an almost equally high number of alterations for letter 11, presumably because it consists of topics that the alterLDA model estimates to be altered accordingly - this shows that the alterLDA model captures subtle changes of topics by the same author.</p>




























<figure ><img loading="lazy" alt="image of a line graph" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>The letters are divided into documents containing content-related alterations (white background) and documents without content-related alterations (purple background). The alterLDA model is trained on the white part and predicts possible alterations on the purple part, so the blue line shows the number of real alterations and suggested alterations, depending on the background. Left of the dotted separator, we find letters addressed to L. La Foye, on the right side letters addressed to An. Chamisso. There are two consecutive outliers with significantly higher numbers of alteration words. One is part of the training set, one is part of the test set, the temporal proximity may indicate a content-related proximity that the model was able to capture.
        </p>
    </figcaption>
</figure>
<p>In Figure 8, the correspondence from L. Tieck to F. Raumer that is depicted ranges from years 1815 to 1840. The left panel showing the number of letters that were sent during that year grouped by whether they contain content-related alterations. The right panel shows the number of tokens that were altered (blue) and the number of tokens that alterLDA suggests to be altered (orange).</p>
<p>Just comparing the blue bars of the two panels reveals that despite the fact that in 1836 there was only one letter written, there occurs the third-highest number of altered tokens. By examining the letter, it turns out that Tieck wrote about his financial problems and his plans to sell his book collection to the Count Yorck von Wartenburg.<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup></p>
<p>Referring back to Table 3, Financial terms are not present in the most common alteration suggestions for L. Tieck. This could indicate that the person editing L. Tieck’s letters did not miss parts that refer to this financial struggle.</p>
<p>When also considering the suggestions by alterLDA (the orange bars of the panel on the right), one letter from 1838 draws the most attention just by the sheer number of suggested alterations. In this letter, L. Tieck refers to disputes between the Catholic Church and the Prussian state at that time.<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup> By arguing about this political controversy, L. Tieck chooses his wording in such a way that alterLDA suggests alterations. This could indicate that across the training corpus there might be the same tendency to alter parts of the documents that deal with a political controversy. The fact that alterLDA highlights a document containing a mixture of political and religious topics supports the hypothesis that the alterations in the BI corpus do not only consist of privacy matters, but also involve a wider political dimension. This result confirms and gives a novel dimension to the assertion that letters as a text genre evolve, especially in the German context of the 1800s, at the interface between private and public matters. In that sense, the role played by alterations aiming at balancing private and public dimensions is central and needs to be further delved into. AlterLDA provides a systematic approach to this major issue in literary studies.</p>




























<figure ><img loading="lazy" alt="image of two bar graphs" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Left panel shows the timeline with counts of letters from Ludwig Tieck to Friedrich von Raumer. The right panel shows the number of tokens that were altered (blue) and the number of suggested tokens (orange). The comparison between the number of letters and the number of alterations for each year shows that there are times where the letters were altered more (e.g. 1836). The letters with suggested alterations deal with financial, political and religuous topics.
        </p>
    </figcaption>
</figure>
<h2 id="conclusion">Conclusion</h2>
<p>This paper presented a general framework for analyzing alterations in historical documents, ranging from simple error corrections to stylistic changes and even to content-related alterations. In addition to established methods such as regular expressions, string distances and vector space comparison, a new probabilistic model for the classification of reasons for alterations has been introduced (alterLDA).</p>
<p>This work contributes to the understanding of text genesis, as it provides insight into the layers of changes in documents. It also offers a quantitative way of evaluating which topics are at what times prone to be altered and are therefore sensitive.</p>
<p>From a machine learning point of view, the BI data set posed special challenges because, on the one hand, the data set is very small and, on the other hand, it comprises several languages, which also differ greatly from the ones used today. Nonetheless, alterLDA was able to confidently find alterations on unseen, labelled data. Exploratively, the method was able to find characteristics on unseen, unlabeled data that in many cases match the expert analysis. The method hence proves to be useful to draw the human reader&rsquo;s attention to specific parts of the large corpus that may otherwise be unnoticed, and by doing so serves as an example of how a machine learning method may assist a scholar as a collaborating reader and a potential collaborating editor.</p>
<p>With regard to the editorial issues first presented here, it is to be noted that the machine-readability of the BI edition makes it possible to serve problem-specific, individualised editions tailored to the research question of a reader/scholar. This project showcases how machine learning methods radically transform the way in which scholars engage historical documents, by taking advantage of the quality of deeply-annotated data: editorial and machine learning expertise can be brought together to explore in depth Humanities research questions.</p>
<p>This research benefited greatly from expert knowledge on the corpus as well as from novel ML methods that were designed for this corpus. This collaboration therefore presents important guidelines for practical and methodological steps that will help other projects to enrich their digital editions with automated annotation or ML-guided corpus exploration.</p>
<p>To achieve further progress, highly interdisciplinary research is mandatory where novel ML models are conceived, and domain knowledge from literary studies is interacting with statistical inference.</p>
<h2 id="appendix">Appendix</h2>
<h2 id="non-content-related-alteration-processing">Non-content-related Alteration Processing</h2>
<p>To identify the non-content-related alteration categories, existing methods are used, however, for the identification of the specific content related reasons on the right, the novel alterLDA method is used. As the main contribution of this work lays in introducing the new alterLDA model, the description of the non-content-related alterations. In this section, the description of the established methods is shortly summarized, whereas the description of alterLDA is given more space in the forthcoming subsections.</p>
<h2 id="paratexts">Paratexts</h2>
<p>In Figure 2: Flow chart of machine learning pipeline with four example alterations. , the excerpt of the facsimile marked as archival note (orange) has the number 6 written in the top right corner of the sheet, this detail is shown in Figure 9. The corresponding xml transcription is the following:</p>
<p><code>&lt;note type=&quot;foliation&quot; place=&quot;margin-right inline&quot; hand=&quot;#pencil_1&quot;&gt;6&lt;/note&gt;</code></p>
<p>As the header reveals, this pencil numbering has been performed by an archivist:</p>
<p><code>&lt;handNote xml:id=&quot;pencil_1&quot; scope=&quot;minor&quot; medium=&quot;pencil&quot; scribe=&quot;archivist&quot;&gt;</code></p>
<p><code>&lt;seg xml:xml:lang=&quot;de&quot;&gt;Hand eines Archivars, in Bleistift.&lt;/seg&gt;</code></p>
<p><code>&lt;seg xml:xml:lang=&quot;en&quot;&gt;Hand of an archivist, in pencil.&lt;/seg&gt;</code></p>
<p><code>&lt;seg xml:xml:lang=&quot;fr&quot;&gt;Main d'un archiviste, crayon de papier.&lt;/seg&gt;</code></p>
<p><code>&lt;/handNote&gt;</code></p>
<p>For the second pencil note in Figure 9 there is no scribe annotated although it also contains nothing but a numbering:</p>
<p><code>&lt;note type=&quot;foliation&quot; place=&quot;align(center)&quot; hand=&quot;#pencil_2&quot;&gt;</code></p>
<p><code>&lt;hi rend=&quot;underline&quot;&gt;99&lt;/hi&gt;</code></p>
<p><code>&lt;/note&gt;</code></p>
<p>The additions that have been performed by a different hand than the primary author and that contain numberings or dates, we consider to be archivists notes. Such archivist’s or editor’s additions can be identified with a very basic set of rules.</p>




























<figure ><img loading="lazy" alt="Image of manuscript handwriting" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Archival note. BI, Adelbert von Chamisso to Louis de La Foye. Nachlass 239, Blatt 6. Staatsbibliothek Berlin / Manuscripts section. Reuse subject to prior approval by Staatsbibliothek Berlin.Published in: Letter from Adelbert von Chamisso to Louis de La Foye (fragment) (without place, 26 june 1804). Ed. by Anna Busch, Sabine Seifert. Prepared by Janine Katins. In collaboration with Sabine Seifert, Sophia Zeil. In: &ldquo;Letters and texts: Intellectual Berlin around 1800&rdquo;. Ed. by Anne Baillot. Berlin: Humboldt-Universität zu Berlin.<a href="http://www.berliner-intellektuelle.eu/manuscript?Brief005ChamissoandeLaFoye">http://www.berliner-intellektuelle.eu/manuscript?Brief005ChamissoandeLaFoye</a>. Last modified: 27 April 2015.
        </p>
    </figcaption>
</figure>
<h2 id="corrections">Corrections</h2>




























<figure ><img loading="lazy" alt="Image of manuscript handwriting" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Correction of mistake ofwurdetowürde. BI, Adelbert von Chamisso to Louis de La Foye. Nachlass 239, Blatt 85. Staatsbibliothek Berlin / Manuscripts section. Reuse subject to prior approval by Staatsbibliothek Berlin. Published in: Letter from Adelbert von Chamisso to Louis de La Foye (Geneva, at the beginning of 1812). Ed. by Anna Busch, Sabine Seifert. Prepared by Lena Ebert. In: “Letters and texts: Intellectual Berlin around 1800” . Ed. by Anne Baillot. Berlin: Humboldt-Universität zu Berlin.<a href="http://www.berliner-intellektuelle.eu/manuscript?Brief047ChamissoandeLaFoye">http://www.berliner-intellektuelle.eu/manuscript?Brief047ChamissoandeLaFoye</a>. Last modified: 27 April 2015.
        </p>
    </figcaption>
</figure>
<p>The alteration marked in green replaces a single character of a word, for which the corresponding part of the facsimile is shown in Figure 10.</p>
<blockquote>
</blockquote>
<p>[..]Geschichte wuürde lang</p>
<p>und schal ausfallen[..]</p>
<p>BI, Adelbert von Chamisso to Louis de La Foye. Letter 47, p. 1</p>
<p>This alteration is a correction of a mistake and conceptually, it is worth noting that the words before and after the alteration are very similar. This characteristic will be exploited for the identification of corrections. Identifying corrections is a considerably more difficult task because the corrected version does not necessarily have to be correct from what we know today. The fact that the alteration author corrected the text only means that he or she thought that his or her version is correct. We thus cannot rely on comparing the second version of the text with what an automatic spell checker would the first version correct to. Instead, we divided the problem even further into spelling alterations and grammatical alterations. For identifying spelling mistakes, the tokens of both versions are fuzzy-string matched against the common dictionary of lemmas. If both tokens match closely to the same lemma according to the Levenshtein distance, the two tokens are considered two different spellings of the same word. Fuzzy string matching of multiple tokens against a large vocabulary can be costly in terms of computing time and memory. For a larger data set an adjustment to this approach may be necessary. However, this approach gave better results than simply comparing Levenshtein distance of the tokens of both versions with each other, due to smaller tokens that are very similar but mean different things (e.g.hateandfatehave Levenshtein distance of 1 but have a very different meaning.) For identifying grammatical alterations, we assume that the forms of the tokens in the sentence change and probably punctuations are added or deleted, but the set of lemmas is preserved for the most part. Hence, if the forms or the part of speech of the tokens in the span change but the set of lemmas do not, this alteration is a grammatical correction.</p>
<h2 id="stylistic-alterations">Stylistic Alterations</h2>
<blockquote>
</blockquote>
<p>[..] DaherEs bedarf esdaher hier eines [..]</p>
<p>BI, About the notion of philosophy PP. by Immanuel Hermann von Fichte, p. 14</p>
<p>For identifying stylistic alterations, we assume that all corrections and paratexts are already labelled according to the described method. Thus, there are only stylistic alterations and moral censorships left to be labelled. In our understanding, a stylistic alteration preserves the meaning of the text by only changing the way it is posed which includes rearranging of words, the use of synonyms and rephrasing. In recent years, a method gained a lot of attention that strive to find a vector-space representation of words that capture its meaning. Words or sentences projected to this space reveal a high similarity (for example cosine-similarity) if they have the same meaning. We introduce a threshold and consider all alterations for which the vector-space embedding of the text before and after the alteration reveal a smaller distance to be a stylistic alteration.</p>
<p>The alteration marked in blue (Figure 2: Flow chart of machine learning pipeline with four example alterations. ) which is shown in higher resolution in Figure 11, reorders the words at the beginning of a sentence without changing the meaning.</p>




























<figure ><img loading="lazy" alt="Image of manuscript handwriting" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Stylistic alteration ofDaher bedarf estoEs bedarf daher. Immanuel Hermann Fichte: Über den Begriff der Philosophie, Humboldt-Universität zu Berlin, Universitätscharchiv, HU UA, Phil.Fak.01.Prom., No. 210, Page 14. Reuse subject to prior approval by the Universitätsarchiv.Ed. by Eva Schneider. Prepared by Eva Schneider, Anne Baillot, Denny Becker. In collaboration with Johanna Preusse. In: “Letters and texts: Intellectual Berlin around 1800” . Ed. by Anne Baillot. Berlin: Humboldt-Universität zu Berlin.<a href="http://www.berliner-intellektuelle.eu/manuscript?IHFichte_Die_Aufgabe_der_Philosophie">http://www.berliner-intellektuelle.eu/manuscript?IHFichte_Die_Aufgabe_der_Philosophie</a>. Last modified: 12 January 2015.
        </p>
    </figcaption>
</figure>
<p>For completeness, we also provide the individual facsimile of the content related alteration example in<a href="#id__Ref532487849">Figure 12</a>.</p>




























<figure ><img loading="lazy" alt="Image of manuscript handwriting" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Content-related alteration. Nachlass Uechtritz. Oberlausitzische Bibliothek der Wissenschaften Görlitz. Reuse subject to prior approval by Oberlausitzische Bibliothek der Wissenschaften Görlitz. Letter from Dorothea Tieck to Friedrich von Uechtritz (Dresden, 10 April 1835). Ed. by Sophia Zeil. Published in: “Letters and texts: Intellectual Berlin around 1800” . Ed. by Anne Baillot. Berlin: Humboldt-Universität zu Berlin.<a href="http://www.berliner-intellektuelle.eu/manuscript?Brief16DorotheaTieckanUechtritz">http://www.berliner-intellektuelle.eu/manuscript?Brief16DorotheaTieckanUechtritz</a>. Last modified: 24 January 2015.
        </p>
    </figcaption>
</figure>
<p>Used Symbols<br>
IdentifierMeaningTypeDimensionalityVNumber of unique tokens in the dictionaryIntWNumber of tokens in the corpusIntMNumber of documentsIntNmNumber of tokens in document mIntKNumber of topicsIntαConcentration of αHyper parameterKηConcentration of ηHyper parameterVξConcentration of ξHyper parameter2βTopic-term variableDirichletK x VθDocument-topic variableDirichletM x KϒTopic-alteration-tendency variableDirichletK x 2 <em>z</em> Token-topic variableCategoricalW x K <em>w</em> TokensObserved (Categorical)W x V <em>c</em> AlterationObserved (Categorical)W X 2</p>
<h2 id="collapsed-gibbs-sampler-of-the-alterlda-model">Collapsed Gibbs Sampler of the AlterLDA Model</h2>
<p>For the Collapsed Gibbs Sampler of the alterLDA model it is shown how to derive the posterior for the topic assignment at a current position, given the current configuration. First, the joint probability of the whole model is given before showing how to compute the topic assignment based on count statistics.</p>
<p>The joint probability of the model is given by$$\begin{align*} p(\mathbf{w}, c, z, \gamma, \beta, \theta~|~\alpha,\eta, \xi) =&amp; p(c~|~z) \cdot p(\mathbf{w}~|~z,\beta) \cdot p(\gamma~|~\xi) \cdot p(\beta~|~\eta)\cdot p(z~|~\theta)\cdot p(\theta~|~\alpha)\ =&amp; \prod^{M,N}\text{cat}(c~|~c,\gamma) \times \prod^{M,N}\text{cat}(\textbf{w}~|~z,\beta)\times\prod^{M,N}\text{cat}(z~|~\theta)\ &amp;\times\prod^{M}\text{dir}(\theta~|~\alpha)\times\prod^{K}\text{dir}(\gamma~|~\xi)\times\prod^{K}\text{dir}(\beta~|~\eta)\ \end{align*}$$</p>
<p>We introduce a counter variable c which can be indexed in four dimensions, the current topic ( <em>k</em> ), the current document ( <em>m</em> ), the current alteration mode ( <em>a</em> ) and the current token ( <em>w</em> ).$$ c_{k,m,a,\mathbf{w}} = \sum_{n=1}^N \mathbf{I}( z_{m,n}=k \quad&amp;\quad w_{m,n} = \mathbf{w} \quad&amp;\quad c_{m,n} = a ) $$</p>
<p>In this setting, the desired computation is the probability of a topic assignment at a specific position given a current configuration of all other topic assignments. This probability can be formalized by$$\begin{align*} p(z_{m,n}~|~z_{-<em>{(m,n)}},\mathbf{w},c,\alpha,\eta,\xi)&amp;\propto~p(z</em>{m,n},z_{-_{(m,n)}},\mathbf{w},c~|~\alpha,\eta,\xi) \end{align*}$$</p>
<p>Adopting Equation 16 from the Carpenter paper, this probability can be written by marginalizing θ,β,γ and from the joint probability.$$\begin{align*} p(z_{m,n},z_{-<em>{(m,n)}},\mathbf{w},c~|~\alpha,\eta,\xi) = &amp; \int\int\int~p(\mathbf{w},c,z,\gamma,\beta, \theta~|~\alpha,\eta,\xi)d\theta~d\beta~d\gamma\ = &amp; \underbrace{\int~p(\theta~|~\alpha)\cdot~p(z~|~\theta)d\theta}<em>A\ &amp; \times \underbrace{\int~p(\mathbf{w}~|~z,\beta)\cdot~p(\beta~|~\eta)d\beta}<em>B\ &amp; \times \int~p(c~|~z,\gamma)\cdot~p(\gamma~|~\xi)d\gamma\ = &amp; \prod</em>{k=1}^K\int~p(\gamma_k~|~\xi)\prod</em>{m=1,n=1}^{M,N_m}p(c~|~\gamma</em>{\text{argmax}(z_{m,n})})d\gamma_k \times A \times B \end{align*}$$</p>
<p>A and B are substituted here because their derivation is identical to the one in Carpenter et al. Analogue to Equation 27 of Carpenter et al., after inserting the definitions of the Dirichlet distribution the result is proportional to three factors.$$\begin{align*} \propto (\mathbf{c}^-<em>{z</em>{m,n},<em>,</em>,<em>}+\alpha_{z_{m,n}})\left( \frac{ \mathbf{c}^-<em>{z</em>{m,n},</em>,<em>,\mathbf{w}<em>{m,n}}+\eta</em>{w_{m,n}} }{ \mathbf{c}^-<em>{z</em>{m,n},</em>,<em>,</em>}+\sum_v^V\eta_{v} } \right)\left( \frac{ \mathbf{c}^-<em>{z</em>{m,n},<em>,c_{m,n},</em>}+\xi_{w_{m,n}} }{ \mathbf{c}^-<em>{z</em>{m,n},<em>,</em>,<em>}+\sum_i^2\xi_{i} } \right) \end{align</em>}$$where .- denotes the counter disregarding the current position <em>m, n</em> .</p>
<h2 id="results-on-synthetic-data">Results on Synthetic Data</h2>
<p>A big advantage of generative models is that they can be used to generate new data. The generated documents themselves may not be too interesting in the case of topic models, but they can be used to evaluate the functionality of the model. To do this, first, the variables of the model are initialized, and documents are generated. Now the variables are initialized again and based on the previously generated documents the old variable configurations are reconstructed. The performance of the inference can be measured by the accuracy of the reconstruction.</p>
<p>In Figure 13, the results of such an evaluation over 324 different experiment runs is shown, within the sparsity of the hyper-parameters as well as the number of tokens were varied. Each cell shows the mean reconstruction of c over two runs with a given set of parameter choices. The brighter the color of the cell, the better the reconstruction. Overall, a smaller alpha yields better results, independent of the size of the data set and the choice of the other concentration factors. Interestingly, for fewer documents and α = 0.1, a smaller concentration factor η performs better, wheras for either a larger number of documents or a larger α = 0.1 a larger η is to be preferred. The explanation for this result is that with more documents, the exact proportions of the topics can be inferred more accurately, wheras for fewer documents, there is the chance of getting a few (sparse) topics right.</p>




























<figure ><img loading="lazy" alt="Image of manuscript handwriting" src=""
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset=" 500w,
     800w, w" 
     class="landscape"
     ><figcaption>
        <p>Grid search result for training accuracy of Ĉ parameter on synthetic data. Even with a small total number of tokens, the accuracy can be very high. Interestingly, the accuracy depends strongly on the sparsity of α, ξ, and η
        </p>
    </figcaption>
</figure>
<ul>
<li id="andrews2013">Andrews, T., 2013. “The third way: philology and critical edition in the digital age.”  _Variants,_ pp. 61-76.
</li>
<li id="baillot2014">Baillot, A. & Busch, A., 2014. “Berliner Intellektuelle um 1800” als Programm. Über Potential und Grenzen digitalen Edierens. _Romantik Digital_ , 1 9.
</li>
<li id="baillot2015a">Baillot, A. & Busch, A., 2015. _Editing for man and machine: The digital edition Letters and texts. Intellectual Berlin around 1800 as an example_ . _Variants,_ Volume 13.
</li>
<li id="baillot">Baillot, A., ed., n.d. _Letters and texts. Intellectual Berlin around 1800._ s.l., Berlin: Humboldt-Universität zu Berlin.
</li>
<li id="baillot2015b">Baillot, A. & Schnöpf, M., 2015. “Von wissenschaftlichen Editionen als interoperable Projekte, oder: Was können eigentlich digitale Editionen?” . _Historische Mitteilungen der Ranke-Gesellschat,_ Volume Beiheft 91, pp. 139-156.
</li>
<li id="beckett2011">Beckett, S., 2011-2018. _Digital Manuscript Project. A digital genetic edition,_ Brussels: University Press Antwerp.
</li>
<li id="bishop2006">Bishop, C. M., 2006. _Pattern Recognition and Machine Learning (Information Science and Statistics)._ s.l.:Springer Science+Business Media, LLC.
</li>
<li id="blei2010">Blei, D. M., Griffiths, T. L. & Jordan, M. I., 2010. “The Nested Chinese Restaurant Process and Bayesian Nonparametric Inference of Topic Hierarchies.”  _J. ACM,_ Volume 57, pp. 1-30.
</li>
<li id="blei2003">Blei, D., Ng, A. & Jordan, M., 2003. “Latent Dirichlet allocation” . _JMLR._ 
</li>
<li id="bojanowski2017">Bojanowski, P., Grave, E., Joulin, A. & Mikolov, T., 2017. “Enriching Word Vectors with Subword Information” . _Transactions of the Association for Computational Linguistics,_ Volume 5, pp. 135-146.
</li>
<li id="carpenter2010">Carpenter, B., 2010. “Integrating out multinomial parameters in latent Dirichlet allocation and naive Bayes for collapsed Gibbs sampling” . _Rapport Technique,_ Volume 4, p. 464.
</li>
<li id="dalenoskam2016">Dalen-Oskam, K., 2016. “Corpus-based approaches to Names in Literature” . In: C. Hoigh, ed. _The Oxford Handbook of Names and Naming._ Oxford: Oxford University Press.
</li>
<li id="deerwester1990">Deerwester, S. et al., 1990. “Indexing by latent semantic analysis” . _Journal of the American society for information science,_ Volume 41, p. 391.
</li>
<li id="ehrmann2016">Ehrmann, D., 2016. “Textrevision — - Werkrevision. Produktion und Überarbeitung im Wechsel von Autoren, Herausgebern und Schreibern” . _Editio,_ Volume 30.
</li>
<li id="goethe2017">Goethe, J. W., 2017. _Faust. Historisch-kritische Edition.._ Frankfurt am Main / Weimar / Würzburg: s.n.
</li>
<li id="gruber2007">Gruber, A., Rosen-Zvi, M. & Weiss, Y., 2007. _Hidden Topic Markov Models._ s.l., JMLR.
</li>
<li id="jannidis2015">Jannidis, F. et al., 2015. _Automatische Erkennung von Figuren in deutschsprachigen Romanen._ Graz: ADHO.
</li>
<li id="jelodar2019">Jelodar, H. et al., 2019. “Latent Dirichlet allocation (LDA) and topic modeling: models, applications, a survey” . _Multimedia Tools and Applications,_ 78(11), pp. 15169-15211.
</li>
<li id="leclerc2009">Leclerc, Y., ed., 2009. _Les Manuscrits de Madame Bovary._ Rouen, s.n.
</li>
<li id="liu2016">Liu, L. et al., 2016. “An overview of topic modeling and its current applications in bioinformatics” . _SpringerPlus,_ Volume 5.
</li>
<li id="muller2001">Müller, K.-R.et al., 2001. “An introduction to kernel-based learning algorithms” . _IEEE Transactions on Neural Networks,_ 12(2), pp. 181-201.
</li>
<li id="nakajima2019">Nakajima, S., Kazuho, W. & Masashi, S., 2019. _Variational BAyesian Learning Theory._ s.l.:Cambridge University Press.
</li>
<li id="paisley2015">Paisley, J., Wang, C., Blei, D. M. & Jordan, M. I., 2015. “Nested hierarchical Dirichlet processes” . _IEEE transactions on pattern analysis and machine intelligence,_ Volume 37.
</li>
<li id="plachta2006">Plachta, B., 2006. _Editionswissenschaft: eine Einführung in Methode und Praxis der Edition neuerer Texte._ 2nd ed. Stuttgart: Reclam.
</li>
<li id="ralle2016">Ralle, I. H., 2016. “Maschinenlesbar — - menschenlesbar. Über die grundlegende Ausrichtung der Edition” . _Editio,_ Volume 30.
</li>
<li id="ramsay2011">Ramsay, S., 2011. _Reading Machines: Toward and Algorithmic Criticism._ s.l.:University of Illinois Press.
</li>
<li id="rasmussen2006">Rasmussen, C. E. & Williams, C. K. I., 2006. _Gaussian Processes for Machine Learning._ s.l.:MIT Press.
</li>
<li id="rosenzvi2004">Rosen-Zvi, M., Griffiths, T., Steyvers, M. & Smyth, P., 2004. _The author-topic model for authors and documents._ s.l., s.n., pp. 487-494.
</li>
<li id="samek2019">Samek, W. et al. eds., 2019. _Explainable AI: Interpreting, Explaining and Visualizing Deep Learning._ s.l.:Lecture Notes in Artificial Intelligence. Volume 11700.
</li>
<li id="schoch2017">Schöch, C., 2017. “Topic Modeling Genre: An Exploration of French Classical and Enlightenment Drama” . _Digital Humanities Quarterly,_ 11(2).
</li>
<li id="schlitz2014">Schlitz, S., 2014. “Digital Texts, Metadata, and the Multitude. New Directions in Participatory Editing” . _Variants,_ Volume 11, pp. 71–-89.
</li>
<li id="schmidt2016">Schmidt, D., 2016. “Using standoff properties for marking-up historical documents in the humanities” . _Information Technology: Human Computation,_ Volume 58, pp. 63-69.
</li>
<li id="shillingsburg2013">Shillingsburg, P., 2013. “Development Principles for Virtual Archives and Editions” . _Variants,_ pp. 61-76.
</li>
<li id="siemens2012">Siemens, R. et al., 2012. “Toward modeling the social edition: An approach to understanding the electronic scholarly edition in the context of new and emerging social media*” . _Literary and Linguistic Computing,_ Volume 27, pp. 445-461.
</li>
<li id="wainwright2008">Wainwright, M. J. & Jordan, M. I., 2008. _Graphical Models, Exponential Families, and Variational Inference._ s.l.:now publishers.
</li>
<li id="wallach2006">Wallach, H., 2006. _Topic Modeling: Beyond Bag-of-words._ New York, ACM.
</li>
<li id="witkowski1924">Witkowski, G., 1924. _Textkritik und Editionstechnik neuerer Schriftwerke._ Leipzig: Haessel.
</li>
<li id="xuan2015">Xuan, J., Lu, J., Zhang, G. & Luo, X., 2015. “Topic model for graph mining” . _IEEE transactions on cybernetics,_ Volume 45.
</li>
</ul>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>The encoding guidelines can be found at<a href="berliner-intellektuelle.eu/encoding-guidelines.pdf">berliner-intellektuelle.eu/encoding-guidelines.pdf</a>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>“ [..] Tieck plante aus finanzieller Bedrängnis heraus den Verkauf seiner Bibliothek an den Grafen Yorck von Wartenburg[..] ” BI, comment by Johanna Preusse in letter from Ludwig Tieck to Friedrich von Raumer (Dresden, 11. November 1836)&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>“ Kontext der von Tieck angedeuteten Vorgänge waren Machtstreitigkeiten zwischen der katholischen Kirche und dem preußischen Staat. [..] ” BI, comment by Johanna Preusse in letter from Ludwig Tieck to Friedrich von Raumer (Dresden, 27. März 1838)## Bibliography&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry><entry><title type="html">Modernism and Gender at the Limits of Stylometry</title><link href="https://rlskoeser.github.io/dhqwords/vol/15/4/000566/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://rlskoeser.github.io/dhqwords/vol/15/4/000566/</id><author><name>Sean Weidman</name></author><author><name>Aaren Pastor</name></author><published>2021-11-12T00:00:00+00:00</published><updated>2021-11-12T00:00:00+00:00</updated><content type="html"><![CDATA[<h2 id="heading"></h2>
<blockquote>
<p>_ Presumably   (once upon a time)   you thought MODERNIST STYLE was   worthy of your   ATTENTION and CURIOSITY.     _    “Manifesto of Modernist Digital Humanities”  Alex Christie, Andrew Pilsch, Shawna Ross, Katie Tanigawa <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></p>
</blockquote>
<p>When Doug Mao and Rebecca Walkowitz described the innovative critical turns of the New Modernist Studies over a decade ago, the digital turn did not make the cut <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. In fact, only very recently have accounts of modernism turned to the conceptual purview of the digital, often to reframe understandings of modernism and digital technology or to contextualize new digital technologies applied to modernism. Jessica Pressman’s  <em>Digital Modernism</em>  and Shawna Ross and James O’Sullivan’s edited collection  <em>Reading Modernism with Machines</em>  necessitate special note in this regard, as each study in its own way accounts for how a digital modernism, or a digital approach to modernism, is  “aligned with strategies of the avant-garde: it challenges traditional expectations about what art is and does [and] illuminates and interrogates the cultural infrastructures, technological networks, and critical practices that support and enable these judgments”   <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>  <sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>. Stephen Ross and Jentery Sayers trace as much in  “Modernism Meets Digital Humanities,”  and Gabriel Hankins argues similarly that through the weak program of digital methods applied to modernism, critics can activate a variety of subtler points of contact that better model how diffuse modernisms share aesthetic territory <sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>  <sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>. Elsewhere, Hankins goes so far as to say that the  “new modernist studies is now always already digital,”  and though Ama Bemma Adwetewa-Badu reminds us that  “digital tools cannot do the critical and analytical work that we do,”  she adds that  “neither can we do the work that they do,”  that is, the work necessitated by a messy and data-filled literary world <sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>  <sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>. Look no further for corroboration than Laura McGrath et al.’s  “Measuring of Modernist Novelty,”  an essay that operationalizes modernist newness to measure evolutions of novelty in 20th-century fiction <sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup>. Other digital forays in the past half-decade, as detailed expansively by J. Matthew Huculak, represent substantial collections, recoveries, expansions, and innovations of modernist archivism <sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup>. Cue in Amanda Golden and Cassandra Laity’s co-edited issue of  _Feminist Modernist Studies _ on modernist, feminist digital humanities <sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>, and the digital turn of modernist criticism seems fully upon us, furnishing the New Modernist Studies with accounts that illustrate how the critical debates of modernism are newly accessible, and newly accessible to critique, through digital methods.</p>
<p>Even so, computational approaches to analyzing modernist texts (e.g. text analysis, stylometry, computational stylistics, network analysis, and topic modeling) so far account for a relatively scarce subset of modernism’s new digital surge. By far the most active scholars at the intersection of text analysis and modernist studies have been Hoyt Long and Richard Jean So, whose work on modernism at the Chicago Text Lab has spanned a wide range of modernist topics: they’ve analyzed the poetic publication networks of high modernism, provided a multifaceted analysis of the English-language haiku, and studied the movement of stream of consciousness as literary technique <sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>  <sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup>  <sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup>. Other scattered exceptions exist too,<sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup>  but their work makes up the bulk of that subset of digital literary criticism that attempts to measure, model, and interpret the styles of modernism computationally.</p>
<p>Several practical reasons have shouldered much of the blame for this lack of critical diversity. Although new and compelling intellectual paradigms have attempted temporal and spatial refigurings of modernism through its complex global and planetary shifts <sup id="fnref:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup>  <sup id="fnref:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup>  <sup id="fnref:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup> , the dominant critical views of modernist work still converge in the early 20th century.<sup id="fnref:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup>  Computational literary criticism requires an accessible textual archive, and since the preferred periodization of modernism has gone largely unchanged, large portions of modernist corpora have long been under copyright until only very recently — and most works, at least widely, may remain so for the better part of the next decades.<sup id="fnref:20"><a href="#fn:20" class="footnote-ref" role="doc-noteref">20</a></sup>  Unsurprisingly, easy access to reliable digital materials has been a strong determiner of the topic matter of computational literary study, which often seeks to analyze bodies of work macroanalytically or, in the now notorious Morettian variation, from a distance. These limits have held particular sway in the realm of stylometry, a broad approach to studying textual style that aims, usually, to analyze a large corpus of literature by counting certain words (or other linguistic elements like grammatical structures, punctuation, etc.), averaging the measurements out into a unique signature, and then comparing that stylistic composite to other texts, authors, periods, and so on. Applied to literature, these countings have helped scholars make claims about style as a measurable form as it exposes certain conditions, contexts, and forms of representation, both inside and outside of narratives; through style, that is, critics have argued that they can digitally identify, measure, and compare the flows and workings of social and cultural mechanisms — like productions and representations of gender, race, class, or nationality — as they slip into linguistic tendency.</p>
<p>If we put aside the material limitations of modernist literature’s computational study, we find that deeper conceptual concerns have also tended to restrict it, especially with regard to modernist stylometry and its relationship to gender (and other identity categories). Modernist social life was as much governed by the sociopolitical affordances of feminism’s first wave as anything else, and its consequences ripple powerfully through much of modernist literature. Coupled with the dicta of avant-garde modernist aesthetics — which, for the sake of space, we might vulgarly boil down to newness and experimentation — this access to new material conditions, sociopolitical spaces, forms of representation, and modes of being-in-the-everyday also released a well-documented  <em>stylistic</em>  first wave for many writers. Gertrude Stein, Virginia Woolf, H.D., Djuna Barnes, Mina Loy, and Jean Rhys, among a multitude of others, unwound the concertinaed styles of their literary predecessors to produce, if we might borrow from Cary Nelson, a literature  “generically undecidable, sometimes feminist and lesbian, wholly unassignable to a humanizing persona, and more purely and powerfully devoted to an exploration of how language works”  than much of the literature that preceded it <sup id="fnref:21"><a href="#fn:21" class="footnote-ref" role="doc-noteref">21</a></sup>.</p>
<p>Our aim in this essay is to consider the conceptual impediments to studying that undecidability of gender stylometrically, as an influencer (or correlative feature) of modernist literary style.<sup id="fnref:22"><a href="#fn:22" class="footnote-ref" role="doc-noteref">22</a></sup>  We first locate where some of those difficulties are borne out in computational literary criticism broadly and, in particular, call attention to the way stylistic markers that have been taken as proxies for forms of gendered experience can prove unreliable in modernist contexts. Virginia Woolf’s famous meditation in  _A Room of One’s Own _ on androgyny and its creative potential might be invoked here: stylometric scholarship has not yet contended with how extensively the experiments of modernist style, full of gender explorations and polysemous difficulty, complicate the tidiness and differentiability sometimes required of text analysis. Such a claim is certainly not new in the realm of computational literary criticism generally, and as the next section details, many digital practitioners have already studied just how deeply our notions of gender undermine computational designs. It is simply to this mounting critical stack that we add a new stylometric case featuring a small corpus of modernist fiction, in an effort to show both the limits and potentials of its stylometric study.<sup id="fnref:23"><a href="#fn:23" class="footnote-ref" role="doc-noteref">23</a></sup></p>
<p>Using  <em>Orlando</em>  as a featured example, we argue that the story of gender and modernism as seen through stylometry reveals a shared limit of its models — of identity and computation — to provide trustworthy exemplars. We then briefly consider some ways that the stylistic uniqueness of Woolf’s novel might help us understand modernism more completely. After all, if the full cultural stakes of modernism remain unearthed <sup id="fnref:24"><a href="#fn:24" class="footnote-ref" role="doc-noteref">24</a></sup>, if the legacies of modernism continue to haunt our aesthetic forms, and if gender is one site of particularly fruitful aesthetic experimentation, then stylometry may help further uncover the stylistic innovations and cultural remnants that constitute the topos of modernist influence. And since modernist texts, at least in the US, are beginning to shed their copyright shackles, the proto-framing for a modernist stylometry is perhaps already overdue.</p>
<p>The digital turn has provided an innovative set of tools for reading, encountering, and thinking through modernist texts, as singular materials to linger with but also to scrutinize through their forms of digital manufacture. We hope to show that stylometric analysis of someone like Woolf in particular, and perhaps many modernist writers in general, links critics to new ways of seeing the designs of gender, the stylistic transitions of which require stylometrists to contend carefully with the complexity and multiplicity of modernist literature and its shifting orientations.<sup id="fnref:25"><a href="#fn:25" class="footnote-ref" role="doc-noteref">25</a></sup>  Such a view has the benefit of reiterating two important points: first that modernism’s collection of digital movements (vis-à-vis Pressman) occasioned an understanding of linguistic styles as flexible, experimental technologies that could hide and reveal, invoke and manufacture identity categories at the proverbial touch of a pen or click of a typewriter; and second that our computational methods themselves complicate the study of modernist representation by rewriting literary data into new and ongoing digital dialectics.</p>
<h2 id="a-critical-history-of-gender-literature-and-quantitative-formalism">A Critical History of Gender, Literature, and Quantitative Formalism</h2>
<p>Some framing is in order before we outline our modernist experiment, regarding first how we employ the concept of gender in relation to modernist literature, and second what digital humanists have tended to glean from the computational study of literary style and gender (using tools and methods at least adjacent to stylometry). After, we consider how treating Woolf’s  _Orlando _ as a case study of gender and modernist style may help scholars rethink their applications of stylometric critique.</p>
<p>If we start with an understanding of gender as a medley of sociopolitical conventions and laws discursively assigned to bodies and constructed and policed accordingly (a là Judith Butler), to imagine we might reliably evaluate gender computationally seems, conceptually, at least somewhat confused. Gayle Rubin’s sex-gender system, that  “set of arrangements by which society transforms biological sexuality into products of human activity”   <sup id="fnref:26"><a href="#fn:26" class="footnote-ref" role="doc-noteref">26</a></sup>, has long-since clarified that gendered identities are sets of internalized, performative scripts reified through various legal, political, scientific, and cultural processes. This is an essential clarification for stylometric analysis, which purports to measure these discursive influences as they bleed through language. If it is only through socializing discourses that a body becomes vested with the idea of an essential sex/gender, stylometry only detects the un-isolated impressions of that social and cultural influence as they mix with a mess of other discursive powers.<sup id="fnref:27"><a href="#fn:27" class="footnote-ref" role="doc-noteref">27</a></sup></p>
<p>In spite of these conceptual limitations, a variety of digital humanists have undertaken the intricate work of studying gender and style throughout literary history and at various scales. A first and landmark example is John Burrows’s 1986 study of Jane Austen’s novels, which simply used Austen’s varied frequency of function words (pronouns, prepositions, articles, conjunctions, and other non-content words) to identify character dialogue patterns, narrative subtleties, thematic transformations, tonal ebbs and flows in characterization, directions and forces of speech, and so on, formal features that (he notes) regularly fall along typical 19th-century gender lines <sup id="fnref:28"><a href="#fn:28" class="footnote-ref" role="doc-noteref">28</a></sup>.<sup id="fnref:29"><a href="#fn:29" class="footnote-ref" role="doc-noteref">29</a></sup>  Many have since built from Burrows’s pioneering study and, among the more recent attempts to unravel gender’s impact on literary style, Matthew Jockers’s work on 19th-century fiction in  <em>Macroanalysis</em>  is likely the most well-known. Chief among Jockers’s conclusions is that gender is the  <em>least</em>  influential determiner on literary style in the 19th century when compared to other categories of stylistic difference — e.g. author, genre, (decade-long) period, textual variations (punctuation, pronoun usage, etc.) <sup id="fnref:30"><a href="#fn:30" class="footnote-ref" role="doc-noteref">30</a></sup>. In a narrower study of 19th-century fiction, one framed more closely by the period’s sociohistorical contexts, Jockers and Gabi Kirilloff looked at the depiction and action (via pronoun and verb combinations like  <em>she walked</em>  or  <em>he dressed</em> ) of male and female characters; they observed that men and women were narrated with different vocabularies according to 19th-century norms, and that 19th-century novelists had difficulty masking their own gender when they wrote characters of another <sup id="fnref:31"><a href="#fn:31" class="footnote-ref" role="doc-noteref">31</a></sup>.<sup id="fnref:32"><a href="#fn:32" class="footnote-ref" role="doc-noteref">32</a></sup>  In fact, Kirilloff and Jockers published an addendum to that study recently, pairing close reading of several of its outliers with those verb-usage lists, concluding that  “[t]hough the characters in these novels behave in  “unconventional”  ways, all three struggle to mitigate traditional and emerging social values,”  a dialectic they found  “reflected not only at the level of plot but at the level of the sentence, and in fact, at the level of individual words”   <sup id="fnref:33"><a href="#fn:33" class="footnote-ref" role="doc-noteref">33</a></sup>.</p>
<p>Although the above examples concern gender as it emerges in literature to reflect certain cultural norms and social circumstances, critics have also studied gender computationally in relation to its contemporary production, to its politics of reception, and to the literary marketplace; and each approach productively ties the stylistic into its socioeconomic imprints. Ted Underwood, David Bamman, and Sabrina Lee find that gender divisions between characters in fiction are becoming less distinguishable (i.e. men and women characters are being described and speaking/acting more similarly), but that the proportion of fiction written by women, and the proportion of women characters, dropped dramatically from 1800-1960. Hence  “while gender roles were becoming more flexible,”  Underwood et al. argue that the story of women writing literature is  “a story of steady decline,”  and that between 1850 and 1960,  “fiction itself became more attentive to men,”  such that women as characters were becoming less prominent, even in books by women <sup id="fnref:34"><a href="#fn:34" class="footnote-ref" role="doc-noteref">34</a></sup>. Jonathan Y. Cheng, meanwhile, takes physical descriptions as an entryway to studying gender and gendered embodiment over the last 150 years of English-language fiction; he uncovers that physical descriptions of characters has increased over time, but that for women’s characterization, specifically, bodily descriptions remain curiously central, even as literary depictions of men and women become less binary <sup id="fnref:35"><a href="#fn:35" class="footnote-ref" role="doc-noteref">35</a></sup>. In a strictly contemporary account, after canvassing thousands of reviews from the  <em>New York Times Book Review</em> , Andrew Piper and Richard Jean So discovered that reviewers still talk about women’s books and men’s books in stereotypically antithetical terms,  “reproduc[ing] the public/private split bequeathed to us from the nineteenth century”   <sup id="fnref:36"><a href="#fn:36" class="footnote-ref" role="doc-noteref">36</a></sup>. For Piper and So, their findings indicate that the damaging discourses around gender are still being subtly reproduced in the public apparatuses that surround literature.<sup id="fnref:37"><a href="#fn:37" class="footnote-ref" role="doc-noteref">37</a></sup>  Their results were expanded upon by Eve Kraicer and Piper, who found similarly — and through a diverse and thorough assortment of computational models — that the representations of women in much of contemporary fiction mirrors their marginalized and decentralized positions in the literary marketplace <sup id="fnref:38"><a href="#fn:38" class="footnote-ref" role="doc-noteref">38</a></sup>. Matthew J. Lavin achieves something similar in his more located study of gender, reception, and the descriptive styles of  _NYT _ book reviews between 1905 and 1925; and, contrary to Underwood et al., Lavin finds that although women were in fact writing  <em>more</em>  fiction than men, that diverse work was still being discussed and reviewed within a reductive, domestic, private/public vocabulary <sup id="fnref:39"><a href="#fn:39" class="footnote-ref" role="doc-noteref">39</a></sup>.</p>
<p>The computational analyses we cover above largely discuss gender and literary production in the 19th-century or our contemporary period, and we draw attention to them not only to detail the range of current quantitative formalist work on gender and literature, but also to point to the critical gap that exists where a related digital modernist criticism could be, given that these studies have provided few insights about modernism.<sup id="fnref:40"><a href="#fn:40" class="footnote-ref" role="doc-noteref">40</a></sup>  While the above is by no means a comprehensive list,<sup id="fnref:41"><a href="#fn:41" class="footnote-ref" role="doc-noteref">41</a></sup>  as a largely current critical sampling, these studies showcase both the standard story of gender and style leading up to or following the fin de siècle and how scholars have approached that story, which is most often by examining the stylistic indications of a period’s culturally and socially produced gender binary, and then determining how those categorizations reveal new aspects of gendered subjectivity. In fact, this is probably a fair way to summarize the basic project of most computational studies of literary style and gender, the majority of which unearth, organize, and interpret linguistic traces through gender’s various social and cultural histories, its modes of material production, and its narrativized representations. Yet as these scholars realize, utilizing digital methods to approach gender as a defining mechanism governing literary reception, critique, and re/production introduces as many problems as insights, especially when the results of digital literary analyses are placed within the same social and historical contexts from which the corpuses are pulled.</p>
<p>The first impulse of computational critics is often to test the already-reified categories around us, gender notwithstanding. But that approach can tend toward troublesome precarity — troublesome not merely from a computational standpoint either, per Johanna Drucker, who makes the following observation about the hazards of studying gender digitally in her meta-analysis of computational modeling:</p>
<blockquote>
<p>The markers of gender identity may be not in the corpus but in the biographical details, expressed or suppressed, of authors. Looking at stylistic features and asking how they come to represent or construct conventions of gender as an effect of textual practice would lead in directions other than those that presume to sort according to gender as a given category. <sup id="fnref:42"><a href="#fn:42" class="footnote-ref" role="doc-noteref">42</a></sup></p>
</blockquote>
<p>So, as we harness computational methods for the textual analysis of gender, we cannot neglect the urgency of self-reflexivity, which must illuminate how the parameters of our textual data, as well as our own interpretive contexts, already construct the pseudo-essentialized elements of gender in our digital practices. Miriam Posner and Lauren Klein helpfully reimagine data in an analogous way,  “as much an orientation toward one’s sources as it is a primary category of knowledge”   <sup id="fnref:43"><a href="#fn:43" class="footnote-ref" role="doc-noteref">43</a></sup>. Such a view, the scholars point out, yields DH methods more materially, socially, culturally oriented, attendant to the non-digital conditionings of the digital, the discourses of which are often grounded in dialectics of visibility and invisibility.</p>
<p>Take, though, a statement from some of the scholars participating in that digital praxis — for example, Underwood et al. in their  _Cultural Analytics _ piece — who often explain these shared limitations differently:</p>
<blockquote>
<p>This essay considers both the gender positions ascribed to authors as biographical personages, and the signs of gender they used in producing characters. In both cases, we understand gender as a conventional role that people were expected to assume in order to become legible in a social context. Authors and characters have been coded according to a tripartite scheme (feminine / masculine / other or unknown), because that scheme organized most public representation of gender in the period we are studying. Gender can certainly be more complex than these categories suggest, and flexible ontologies can be designed to illuminate the complexity. But this essay is inquiring about the history of conventional roles, not about the truth of personal identity, or the underlying processes that produce gendered behavior. <sup id="fnref1:34"><a href="#fn:34" class="footnote-ref" role="doc-noteref">34</a></sup></p>
</blockquote>
<p>The message here begins to echo Drucker’s, in that it is in the author’s biographical details and situated experiences, which leech into the text as stylistic attributes, that the idiosyncrasies of the author’s personal style become the clothing that outfits a novel’s gendered position. But where Drucker implies that gender can operate as other than a given category, Underwood et al. must admit to retaining a tripartite structure for sorting gender, even though they make very clear that such a schema will fail to encapsulate gender’s complex, messy, performative capacities. In most computational analyses of literary style and gender this is an inevitable rhetorical move that, as Kraicer and Piper explain, historicizes the results and  “allows us to make explicit the otherwise latent ways in which gender is being mobilized and hierarchized within novels”   <sup id="fnref1:38"><a href="#fn:38" class="footnote-ref" role="doc-noteref">38</a></sup>. After all, a first lesson of computational literary studies is that discourse predetermines data — depending on the tools we implement and the discourses we possess, data can always be made to appear a certain way. As they track some of those ways in their  _Cultural Analytics _    “Identity Issue”  introduction, Susan Brown and Laura Mandell develop an exhaustive history of conceptualizing gender, outlining the importance of such historicizing for digital projects, which must balance quantitative methodologies with that same reflexive process that asks how our methodological designs direct our analyses.  “Historicizing helps to destabilize identities,”  Brown and Mandell maintain,  “and cultural analytics can make visible a kind of history that we have never seen before”   <sup id="fnref:44"><a href="#fn:44" class="footnote-ref" role="doc-noteref">44</a></sup>.</p>
<p>Perhaps, though, the kind of historicizing typical textual analysis does serves to stabilize more than  <em>de</em> stabilize. Underwood et al. say, for instance (to keep with our arbitrary, cherry-picked example), that their essay does not delve into  “the underlying processes that produce gendered behavior.”  But the assumption that such processes can be ignored when studying literary history seems suspect — because language, we know, is one such process, one that governs many others; and as Drucker reminds us, if you want to model gender you must produce (or reproduce) the underlying structures of gender. At some point, the current confines of stylometric analysis require that data be sorted into comparable categories, which means whether one gender, two gender, three gender, four, comparing categories or columns of gender identification remains much easier than comparing gender spectra. Does adding a third, non-binary category — or a fourth, or a fifth — unlock or ameliorate the hindered politics of representation at the heart of designing computational analyses around those historical notions of gender? As Pamela Caughie et al. emphasize in their work on feminist ontologies, the answer is:  <em>of course not</em>   <sup id="fnref:45"><a href="#fn:45" class="footnote-ref" role="doc-noteref">45</a></sup>. And yet, to label some identity as historically extant when it wasn’t  <em>also</em>  introduces a set of problems, and as Underwood and company note, to imagine there was more social space for expressions of gender fluidity than there actually was does us little good when analyzing textual traces that we aim to tie back into historically grounded claims about social realities.</p>
<p>This is the uncertain impasse at which stylometric gender analysis has, so far, found a rather comfortable home. Underwood is right, as he aptly remarks elsewhere, that  “nothing about this [binary] method compels us to stop at two perspectives”   <sup id="fnref:46"><a href="#fn:46" class="footnote-ref" role="doc-noteref">46</a></sup>; but is the best we can really expect of such critique its regular revelation that  “the very act of modelling carries with it the seeds of a constructionist recognition that a phenomenon could [or will soon] be modelled differently”   <sup id="fnref1:44"><a href="#fn:44" class="footnote-ref" role="doc-noteref">44</a></sup>?<sup id="fnref:47"><a href="#fn:47" class="footnote-ref" role="doc-noteref">47</a></sup>  That leaves us with a set of untenably limited critical positions and several vital, unaddressed questions. Does the clumsiness of quantitative formalism actually force us to decide whether to (a) anachronistically apply new understandings of gender to contexts that cannot hold them, or (b) hand-wash our responsibility to witness and recover different kinds of gendered being, whenever they occurred? In the context of stylometry specifically, how can we responsibly do (a) while avoiding (b)? How can we avoid reproducing normative investigations of identity categories like gender through models of distant reading, given that those models function in systemic and epistemological hierarchies of power that were not designed with plurality in mind?<sup id="fnref:48"><a href="#fn:48" class="footnote-ref" role="doc-noteref">48</a></sup></p>
<p>In short, we not only lack a spectrum-based computational model for categorizing and operationalizing gender stylistically, but — for scholars of modernism — we also lack a critical vocabulary to address how our digital methods at once allow and prevent us from analyzing those  “thick textures of history”  that the New Modernist Studies promised <sup id="fnref1:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. This is a hinge, too, of Nan Z. Da’s argument against computational literary studies: that computational approaches to texts (stylometry among them) are frequently ill-applied to data not suited for its methods, and that its results are either incorrect, overstated, or unoriginal [Da 2019]. So, how might we develop a situated stylometry as  “a digital strategy that  <em>can</em>  inform”  our computational and interpretative methods <sup id="fnref1:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>? How can we put at our service digital methods that, according to Drucker, only act as a kind of filter through which we pull existing social structures and hierarchies, like normative or binary ideas about gender? Can we avoid treating their computational products as purified or more extant forms of pre-constructed inequity, and instead allow our processes of modeling to deconstruct themselves, to produce findings resistant to the systems that arrange them?</p>
<p>In addition to underscoring the urgency of revising our methods to better fit gender’s actualities, we hope to demonstrate shortly — against recent critiques of computational literary scholarship — that even flawed stylometric models can teach us new things about modernism. In the following section, we posit that studying Woolf from within the binary she saw beyond is itself an approach that helps us to reconstruct her project of imaginative self-fashioning. Treating  <em>Orlando</em>  as an important edge-case for modernism’s digital-critical scaffolding, our investigation aims to recover one of Woolf’s stylistic innovations and, in doing so, to rethink a few of stylometry’s prevailing critical assumptions.</p>
<h2 id="anglophone-modernism-and-woolfs--_orlando_">Anglophone Modernism and Woolf’s  <em>Orlando</em></h2>
<p>As with any digital literary analysis, taking the stylometric baton into modernism began with an inevitably insufficient selection (and rejection) of texts to analyze, and the narrow variety we collected here is representative of three, semi-arbitrary governing features: first, each author needed to have written at least three works of fiction (or collections of short stories);<sup id="fnref:49"><a href="#fn:49" class="footnote-ref" role="doc-noteref">49</a></sup>  second, those texts needed to be published between 1900 and 1940;<sup id="fnref:50"><a href="#fn:50" class="footnote-ref" role="doc-noteref">50</a></sup>  and third, those works needed to be written in English. Bearing in mind the lessons of Jockers’s  <em>Macroanalysis</em> , by narrowing our scope so dramatically we tried our best to isolate gender and its stylistic pressures from other well-known influencers of style like genre, period, translation, and so on. We thus settled on 15 male and 15 female Anglo-modernists (Tables 1-2), and chose three to five works from each, for a total of 141 texts (68 female, 73 male).<sup id="fnref:51"><a href="#fn:51" class="footnote-ref" role="doc-noteref">51</a></sup></p>




























<figure ><img loading="lazy" alt="image of two charts, one for female authors and one for male authors" src="/dhqwords/vol/15/4/000566/resources/images/image1.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/4/000566/resources/images/image1_hub343725b321f026082de734a07788e08_63083_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/4/000566/resources/images/image1_hub343725b321f026082de734a07788e08_63083_800x0_resize_box_3.png 800w,/dhqwords/vol/15/4/000566/resources/images/image1_hub343725b321f026082de734a07788e08_63083_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/4/000566/resources/images/image1.png 1322w" 
     class="landscape"
     >
</figure>
<p>If the process of forming a modernist corpus begets a state-of-the-field question about the current archive of modernism, this study’s selection of authors succeeds only in capturing a small, Anglophone, transatlantic subsection of the diverse, global engagements with and reactions to the political, social, and cultural investments of modernity. It is representative, that is, of only one, precarious, incomplete variant of modernism — a small sample that allows us to make clear claims about one subsection of modernist work. Archives, digital or otherwise, summarize and sometimes disguise significant acts of scholarship, and our own corpus operates with an agenda that erases as much as it attempts to combat its erasures. The resulting selections are meant only to represent one specific stylistic case — a rather traditional canon of Anglophone modernist fiction<sup id="fnref:52"><a href="#fn:52" class="footnote-ref" role="doc-noteref">52</a></sup>  — among the many stylistic cases of modernist literature, and we hope readers will forgive the indiscretions of our modernist assemblage, knowing that this list of authors could have taken any number of reasonable forms.<sup id="fnref:53"><a href="#fn:53" class="footnote-ref" role="doc-noteref">53</a></sup></p>
<p>After just outlining some of the many problems of doing so, the irony of organizing our study around normative gender categories has not eluded us. Mandell announces that this very move is her topic of inquiry in the opening chapter of a recent  <em>Debates in the DH</em>  volume:  “in some quantitative cultural analyses, the category of gender has been biologized, binarized, and essentialized in a trend that I call  stereotyping ” ; and if literary data has  “been collected, sorted, and even produced according to the categories M/F,”  then it will find statistically significant differences whether or not those differences are significant <sup id="fnref:54"><a href="#fn:54" class="footnote-ref" role="doc-noteref">54</a></sup>. An algorithm trained on constructed differences of data (i.e. assigned authorial gender) does not sort through the world it purports to represent, after all, but generates a new world through the measures of difference with which we provide it. Bearing that considerable problem in mind, and building on the foundations set by the many computational studies of 19th-century literature, our interest is not in asking if or where gender is marked in fiction, but whether and how it gives-to-see the literary styles and structures of modernist fiction. Because we rely on gender as a historical category, we can learn about its in/stability and trans/formation from the ways it is invoked, signaled, modified, or tailored textually, around specific cultural contexts and social meanings and in relation to other texts. As traditional, binary conceptions of gender began to be understood as culturally, socially, politically, and legally affixed to bodies, modernist authors might also have affixed them to — or unfixed them from — their texts at a macro level, and we want to track the unique structural styling of gender in those texts.</p>
<p>Like most, our methods for doing so stylometrically rely on the production of a table of word frequencies, from which stylistic similarity (based on statistical distance) can be measured. Traditionally, the most influential words in these tables are those function words, English’s grammatical glue (articles, prepositions, pronouns, etc.), because it turns out that each author — and, though to a lesser degree, each period, genre, and gender — uses each of those words at a remarkably consistent rate in their own work, and at a unique rate when compared to others.<sup id="fnref:55"><a href="#fn:55" class="footnote-ref" role="doc-noteref">55</a></sup>  Stylometrists can then measure the statistical distances between the styles of individual texts, authors, or other groups (e.g. based on gender) to detect broad trends.<sup id="fnref:56"><a href="#fn:56" class="footnote-ref" role="doc-noteref">56</a></sup>  This research has tended to take the form of sorting and examining diverse most-frequent-word (MFW) lists made up of other types of content words (nouns, verbs, adjectives, etc.) to locate stylistic quirks or generate topics for close-reading. For instance, one group of authors might prefer one term, set of terms, or set of topics over another — or, they might not — and by contextualizing and interpreting the divergences or convergences, critics can theorize what circumstances (generic, historical, social) might contribute to similarity or difference. However, in this study, we want to be particularly sensitive to the interpretive dangers in close-reading individualized wordlists (either for authors or their texts) because of how our digital design treats gender as a two-body problem. Fearing, that is, the equal likelihood of finding gender-specific stylistic trends and replicating essentializing discourses in our interpretations, we decided to take a less orthodox path by staying macro and running a series of broader, supervised machine-learning analyses on our modernist corpus.<sup id="fnref:57"><a href="#fn:57" class="footnote-ref" role="doc-noteref">57</a></sup></p>
<p>So, we trained our machine on the two groups of modernist texts (female, male), asking it, in effect, to develop an average style for each group based on the frequencies with which its authors used common words.<sup id="fnref:58"><a href="#fn:58" class="footnote-ref" role="doc-noteref">58</a></sup>  We then gave it sequences of random groupings of six different texts (three male, three female), which it had to correctly place in the corresponding author’s associated sex. By random chance, the machine will assign any text to its correct group 50% of the time — with two possible groups in our analysis, the algorithm has the equivalent of a digital coin toss — which means that anything consistently or considerably more accurate suggests there may be measurable stylistic differences between the two groups. Among hundreds of classifying runs, misattributions occurred on average about 10% of the time; the classifier accurately determined the assigned gender of a text’s author in around 90% of cases, which supports the notion that even given the experiments of modernism, the period’s sex-gender binary seems to remain a strong determiner of style.<sup id="fnref:59"><a href="#fn:59" class="footnote-ref" role="doc-noteref">59</a></sup>  We utilized a series of different test conditions to ensure our results weren’t skewed with any one set of parameters,<sup id="fnref:60"><a href="#fn:60" class="footnote-ref" role="doc-noteref">60</a></sup>  and though our tests turned up fleeting misattributions — among other examples, the machine assigned a random sample from Hemingway’s  _For Whom the Bell Tolls _ to the female group, and it ascribed several samples from Radclyffe Hall’s  _Well of Loneliness _ to the male set<sup id="fnref:61"><a href="#fn:61" class="footnote-ref" role="doc-noteref">61</a></sup>  — we produced one misattribution that was both more frequent and more consistent than the rest: Virginia Woolf’s 1928 novel,  <em>Orlando: A Biography</em> .</p>
<p>Common stylometric practice says to zoom in here and discern why  _Orlando _ is an outlier — and options abound for doing so: we might, for example, compare individual wordlists of each gender grouping, or of the text itself, or of the words preferred or avoided by one gender grouping/text over another. Again, however, our interest is in staying macro. Having already separated the novels into male and female groups, the wordlists only and unsurprisingly provide the same stylistic snapshot that has been traced extensively elsewhere (as our previous section details). We know that even as standard uses sometimes lead to surprising ends, male and female authors normally reproduce socially, culturally, historically consistent accounts of gender in their writing, often through predictable and stereotypically gendered characterizations, vocabularies, narrative actions, identifiers, and locales.<sup id="fnref:62"><a href="#fn:62" class="footnote-ref" role="doc-noteref">62</a></sup>  Relocating the gender difference we structured our study around is not so interesting to us. What we want to ascertain, separately, is whether we can deploy stylometric methods without double-reading gendered cultural meaning into a list of semantic differences like verb constructions or function words. What if we apply stylometry to zoom out to the level of a whole text rather than its list of words? What does gender play look like within our small modernist corpus if we treat it as, for instance, an element of narrative structuring?</p>
<p>Instead of studying how male and female authors diverged in word choice or grammatical preference, then, we decided to look at the overarching structure of that difference in our consistent outlier,  <em>Orlando</em> . To do so, we ran a more specific classification Eder calls a Rolling NSC.<sup id="fnref:63"><a href="#fn:63" class="footnote-ref" role="doc-noteref">63</a></sup>  This method also requires that we train the machine on our two gender groups, but rather than supply it with a random selection of texts to assign, we ask it to analyze slices of one particular text — in this case,  <em>Orlando</em>  — and then assign each slice to its stylistically closest group/gender. To make matters more interesting, we isolated our test author, so that none of Woolf’s other works appeared in the training sample or contributed to the averaged style of the female set; if she had a particularly strong author signal (and she does), this process would prevent her unique style from overpowering the average gendered style that results from the amalgam of her contemporaries’ works.<sup id="fnref:64"><a href="#fn:64" class="footnote-ref" role="doc-noteref">64</a></sup>  For sake of reference, we did the same analysis on a number of other randomly selected texts and, as is to be expected with a ~90% attribution rate, most slices of most novels were correctly assigned to their author’s corresponding gender (Figures 1-2).<sup id="fnref:65"><a href="#fn:65" class="footnote-ref" role="doc-noteref">65</a></sup>  In other words, most modernist authors in our corpus write more like the other modernists of their sex, and do so consistently and for the entirety of each novel, which appears to be the stylistic norm in literature regardless of period.</p>




























<figure ><img loading="lazy" alt="Image of a bar graph" src="/dhqwords/vol/15/4/000566/resources/images/image2.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/4/000566/resources/images/image2_hu71b714bd43732f053f90c3ea34f637ec_21883_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/4/000566/resources/images/image2_hu71b714bd43732f053f90c3ea34f637ec_21883_800x0_resize_box_3.png 800w,/dhqwords/vol/15/4/000566/resources/images/image2_hu71b714bd43732f053f90c3ea34f637ec_21883_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/4/000566/resources/images/image2_hu71b714bd43732f053f90c3ea34f637ec_21883_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/4/000566/resources/images/image2_hu71b714bd43732f053f90c3ea34f637ec_21883_1800x0_resize_box_3.png 1800w,/dhqwords/vol/15/4/000566/resources/images/image2.png 2500w" 
     class="landscape"
     ><figcaption>
        <p>James Joyce’s <em>Portrait of the Artist as a Young Man</em> as seen through the “rolling.classify” function of <em>Stylo</em> ; the green line is actually made up of many thinner segments, 5000-word slices of the novel, each of which overlaps by 4500 words, and all of which were classified as male in style.
        </p>
    </figcaption>
</figure>




























<figure ><img loading="lazy" alt="Image of a bar graph" src="/dhqwords/vol/15/4/000566/resources/images/image3.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/4/000566/resources/images/image3_hube8edf40f7ac64cdb43b623304971e78_23597_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/4/000566/resources/images/image3_hube8edf40f7ac64cdb43b623304971e78_23597_800x0_resize_box_3.png 800w,/dhqwords/vol/15/4/000566/resources/images/image3_hube8edf40f7ac64cdb43b623304971e78_23597_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/4/000566/resources/images/image3_hube8edf40f7ac64cdb43b623304971e78_23597_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/4/000566/resources/images/image3_hube8edf40f7ac64cdb43b623304971e78_23597_1800x0_resize_box_3.png 1800w,/dhqwords/vol/15/4/000566/resources/images/image3.png 2500w" 
     class="landscape"
     ><figcaption>
        <p>Jean Rhys’s <em>Quartet</em> as seen through the “rolling.classify” function of <em>Stylo</em> ; the red line is similarly made up of 5000-word slices of the novel, each of which overlaps by 4500 words, and all of which were classified as female in style.
        </p>
    </figcaption>
</figure>
<p>But  _Orlando _ presents an atypical case — and to an extraordinary degree (Figure 3). About 32,000 words into the novel, the seemingly misclassified male style of Virginia Woolf abruptly changes, and the algorithm attributes the remainder of the novel’s style to the correctly assigned sex. More extraordinary, even, is what takes place narratively at the stylistic switch. The barely visible vertical dotted line marked &rsquo; <em>a</em> &rsquo; in Figure 3, which lines up almost perfectly with the moment of stylistic gender reassignment, if you will, comes just after our protagonist Orlando is awakened in dramatic fashion from a weeklong slumber, and as those around him make a startling discovery:</p>
<blockquote>
</blockquote>
<p>We are, therefore, now left entirely alone in the room with the sleeping Orlando and the trumpeters. The trumpeters, ranging themselves side by side in order, blow one terrific blast —</p>
<p>‘THE TRUTH!’</p>
<p>at which Orlando woke.</p>
<p>He stretched himself. He rose. He stood upright in complete nakedness before us, and while the trumpets pealed Truth! Truth! Truth! we have no choice left but confess — he was a woman.<br>
<sup id="fnref:66"><a href="#fn:66" class="footnote-ref" role="doc-noteref">66</a></sup></p>




























<figure ><img loading="lazy" alt="Image of a bar graph" src="/dhqwords/vol/15/4/000566/resources/images/image4.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/dhqwords/vol/15/4/000566/resources/images/image4_hu8de2a722b425686bbfa9b0e557b88340_24713_500x0_resize_box_3.png 500w,
    /dhqwords/vol/15/4/000566/resources/images/image4_hu8de2a722b425686bbfa9b0e557b88340_24713_800x0_resize_box_3.png 800w,/dhqwords/vol/15/4/000566/resources/images/image4_hu8de2a722b425686bbfa9b0e557b88340_24713_1200x0_resize_box_3.png 1200w,/dhqwords/vol/15/4/000566/resources/images/image4_hu8de2a722b425686bbfa9b0e557b88340_24713_1500x0_resize_box_3.png 1500w,/dhqwords/vol/15/4/000566/resources/images/image4_hu8de2a722b425686bbfa9b0e557b88340_24713_1800x0_resize_box_3.png 1800w,/dhqwords/vol/15/4/000566/resources/images/image4.png 2500w" 
     class="landscape"
     ><figcaption>
        <p>Virginia Woolf’s <em>Orlando</em> . The slices (and 500-word incremental changes) are more visible here, and their overlap shows the sudden stylistic transition — from stylistically male to female — about a third of the way through the novel. The smaller peaks/valleys represent the model’s less-certain detection of each gender’s style.
        </p>
    </figcaption>
</figure>
<p>As she narrates a sex change in her story, Woolf actively styles a gender signature to match that of her character, a creative feat no other author — of  <em>any</em>  period — has managed, as far as we know. It’s difficult to explain just how unusual this is, in part because our subsection of modernist fiction doesn’t speak for all of modernism (let alone all of fiction), but in larger part because we’ve just never seen anything like this before. If our analysis isolated a descriptive, content-based lexicon (nouns, verbs, adverbs, adjectives), something like this result would be expected, at least to a degree. Several of the studies we outlined in the last section confirm that regardless of an author’s gender, talented writers are capable of imitating aspects of another gender in their writing (preferred behaviors, contexts, spaces, styles of speaking, etc.), a fact likely truer still for a writer as accomplished as Woolf. In our analyses, however, which again ranged from 50 to 500 MFW, the words we used to measure literary style are largely just those functional, non-descriptive terms void of explicit narrative context. This means that Woolf didn’t simply change the  <em>what</em> , the content her male-turned-female protagonist was thinking, saying, or doing — she fundamentally changed the  <em>how</em> , the grammar and syntax that holds that content together.<sup id="fnref:67"><a href="#fn:67" class="footnote-ref" role="doc-noteref">67</a></sup>  Perhaps how this came to pass, as  <em>Orlando</em> ’s narrator relates, matters little:  “It is enough for us to state the simple fact; Orlando was a man till the age of thirty; when he became a woman and has remained so ever since”   <sup id="fnref1:66"><a href="#fn:66" class="footnote-ref" role="doc-noteref">66</a></sup>.</p>
<h2 id="_orlando_--at-the-limits-of-modernist-stylometry"><em>Orlando</em>  at the Limits of Modernist Stylometry</h2>
<p>If we turn to the literary criticism on  <em>Orlando</em> , the impressiveness of this stylistic manipulation would seem inevitable given the theoretical projects the novel is said to carry out.  “Long before university Gender Studies departments came into existence,”  writes Anne Delaplace,  “Woolf had already reflected on the dissonance between social and sexual identity”  in  <em>Orlando</em> , which retains echoes of the  “intermix[ing]”  of genders from the exploration of androgyny in  <em>A Room of One’s Own</em>   <sup id="fnref:68"><a href="#fn:68" class="footnote-ref" role="doc-noteref">68</a></sup>. Indeed,  “in every human being,”  says  <em>Orlando</em> ’s narrator,  “a vacillation from one sex to the other takes place, and often it is only the clothes that keep the male or female likeness, while underneath the sex is the very opposite of what it is above”   <sup id="fnref2:66"><a href="#fn:66" class="footnote-ref" role="doc-noteref">66</a></sup>. For Pamela Caughie, the novel provides nothing less than the archetypal  “model of modernist life writing in the era of transsexualism,”  a unique, modernist  “transgenre”  that  “radically refigures the narrative of transsexualism”   <sup id="fnref:69"><a href="#fn:69" class="footnote-ref" role="doc-noteref">69</a></sup>.<sup id="fnref:70"><a href="#fn:70" class="footnote-ref" role="doc-noteref">70</a></sup>  These literary critics identify clearly the power of Woolf’s novel as it derives from close readings of context and narrative, producing feminist modes of representation for non-normative sexuality and genders through its play of literary genre — in Caughie’s case, the biography, mock or not. Elsa Högberg and Amy Bromley even consider  _Orlando _ the novel that inaugurated Woolf’s theory of sentence morphology, the way Woolf playfully reshapes formal and generic sentence structures to mark a  “modernist rewriting of biography and gender, and the orthodox ways in which they tend to shape life and the body”   <sup id="fnref:71"><a href="#fn:71" class="footnote-ref" role="doc-noteref">71</a></sup>.</p>
<p>Keeping with these accounts, we simply want to make one vital addendum: that the novel’s power — as proto-gender study or as transgenre — emerges from Woolf’s style. Again, while not every novel in our analysis was always assigned so tidily to one gender or another, no text separated as consistently or in such a structured manner as  <em>Orlando</em>  — on cue, as it were, with the gender transition in its narrative. That level of stylistic control raises new questions about the scales of gender performativity, or rather focuses them down to the level of the line, the choice of one (seemingly insignificant) function word over the other, which complicates readings of  _Orlando _ as a tale of cross-dressing, in which clothes make the (wo)man without regard for the discursive power wielded in the creation of men and women. Woolf, it seems, did not merely change the descriptions and clothes of her protagonist and so change the gender; she altered the very stylistic nuances that characterized Orlando when she was a woman from when he was a man. Hers isn’t just a narrative exploration of transition either, in the vein of Dr. Matthew O’Connor in Djuna Barnes’s  <em>Nightwood</em>  or Leopold Bloom in James Joyce’s  <em>Ulysses</em> .<sup id="fnref:72"><a href="#fn:72" class="footnote-ref" role="doc-noteref">72</a></sup>  That her writing diverges so thoroughly from the stylistic doxa of her contemporaries suggests to us that, as Woolf celebrates the new possibilities of being gendered and representing genderedness, she delivers a linguistic paradigm of that futurity. In  <em>Orlando</em> , Woolf not only cultivates a sustained, pluralist theory of gender transition, but she also practices the linguistic applications of its modern futures, a multifaceted engagement that perhaps no other author had actualized before.</p>
<p>In this our study deviates from the findings of several aforementioned accounts of gender and literary style (e.g. the work of Jockers and Kirilloff (2016) on gender in 19th-century fiction), which tend to suggest gender-disciplinary social norms had significant and, by extension, inextricable effects on literature’s stylistic structures. Most stylometric studies that isolate gender within a corpus of literature treat it similarly, as a governing stylistic force that manifests in formal semantic preferences or syntactic characteristics. In the case of Anglo-modernist fiction, though, staying out of the wordlists forces a change in perspective.<sup id="fnref:73"><a href="#fn:73" class="footnote-ref" role="doc-noteref">73</a></sup>  Perhaps the period’s history of gender and literary style is more than a sum of linguistic tendencies, or more than gender’s socially, culturally, politically enforced written marking. In her innovative styling of gender transition, Woolf shows that this history is also an account of gender’s  <em>un</em> marking and  <em>un</em> disciplining. Perhaps all that Woolf intuited is that gender needn’t have been as influential on literary style as we think it must have been; or, perhaps Woolf saw that our ideas about gender do not so much influence literary style as literary style influences our ideas about gender. If  _Orlando _ declares gender as merely the newest formal territory on which writers could experiment with identity, then it uncovers for us Woolf’s broader way of modern seeing, which could understand gender not as an imminent, immutable, uniform stylistic determiner, but as a flexible literary landscape — or, what comes to the same thing, as a social text, a style.<sup id="fnref:74"><a href="#fn:74" class="footnote-ref" role="doc-noteref">74</a></sup></p>
<p>Taking a step back, and stealing from the punchy clarity of Stephen Ramsay, we want to repeat a key clarification:  “We are not trying to solve Woolf”   <sup id="fnref:75"><a href="#fn:75" class="footnote-ref" role="doc-noteref">75</a></sup>. We instead want to use  _Orlando _ to wonder whether the current registers of stylometry and computational stylistics stymie some of the radical potentials of modernist style. Because we constructed this stylometric study around a constricting binary gender logic, it can only tell us so much about that binary or what constitutes its differences, especially with tools applied as narrowly as we apply them here (e.g. on a relatively small modernist corpus, and then on a single modernist text relative to that corpus). Stylometric methods, like other forms of quantitative formalism, can’t exactly be employed in pursuit of such a task, particularly when our assumptions need to be levied in service of cultural and social conclusions to make them critically relevant. Most studies of gender and literary style have tended to assume that if a linguistic difference can be computationally tracked along gender lines, that difference must divulge something about gender’s lived differences or material history. But we are not convinced this is always true. Sometimes the only thing stylistic difference informs is our understanding of failed historical representation, or a history of  <em>un</em> lived difference, or (in our case) gender’s otherwise hidden potentials. Despite writing from within a descriptive system that inhibits gender identity’s expression and possible futures — and despite being studied from within the same — Woolf still teaches us about the ways gender/ing could be deployed, inverted, and resisted, a didactic function that materializes even through, even as, the digital recording of a broken and insufficient gender binary.</p>
<p>Our brief analysis of  <em>Orlando</em>  should suggest that stylometry even applied narrowly can still teach us a great deal about, say, the depth of modernism’s  <em>avant garde</em>  or about the limitations of studying style as a consistent and formal indicator of real-world messiness. After all, it’s through our study’s significant structural limitations that we see the extent to which modernists could make their styles fit (or an ill fit for) its categories — see, that is, how Woolf fashioned gender in, out, and around the binary trying to contain it. So what other realignments might Woolf’s modernist experiment afford within the genealogy of gender and computational literary studies? Two observations seem essential: the first, a minor point about exemplarity, will lead us to the second, a broader claim about the limits of studying modern(ist) sociocultural forms through stylometric analysis.</p>
<p>Christy Burns asserts that  “in  _Orlando _ Woolf has already […] anticipated our attempts to clothe her writings in our own desires,”  for the very reason that  “the curious residue of language […] both invites and resists our insistent refigurations, our attempts to make Woolf conform to our societal demands”   <sup id="fnref:76"><a href="#fn:76" class="footnote-ref" role="doc-noteref">76</a></sup>. In this unique anticipatory capacity Woolf’s novel comes to represent an exceptional case in our corpus of modernist fiction, both through its multi-century generic scope and through its stylistic flexibility. Its uniqueness thus poses an interpretive problem: How can we draw general conclusions about Anglophone modernism, or about gender and modernist literature, from a text so different from the rest, a novel that singularly rejects what we thought we knew about the relationship between gender identities and literary style?<sup id="fnref:77"><a href="#fn:77" class="footnote-ref" role="doc-noteref">77</a></sup></p>
<p>According to Ben Etherington and Sean Pryor, by focusing on such exceptional cases scholars  “can actively resist the falsifications and exclusions performed by the abstract generalisations of history, theory, and society at large”   <sup id="fnref:78"><a href="#fn:78" class="footnote-ref" role="doc-noteref">78</a></sup>. Stylometry notwithstanding, computational methods of analyzing literature attempt to discern a set of generalities that can cater to the macroanalytic at the cost of particulars, outliers or exceptions that sometimes require culling or discarding. But if Woolf’s unprecedented literary experimentalism is what makes her an exemplary modernist — which is the case made by Burns and many other modernist critics — perhaps  <em>Orlando</em> ’s exceptionalism is what makes it an ideal text from which to generalize. If, in numerous cases of Anglophone modernism, the experimentally exceptional  <em>is</em>  the general,<sup id="fnref:79"><a href="#fn:79" class="footnote-ref" role="doc-noteref">79</a></sup>  then the discovery of new forms of difference in that literature, like  <em>Orlando</em> ’s stylistic play, requires that the very category of general be productively expanded to include even its most remarkable outliers. And when the general example produces an updated interpretive scale or model, then it also produces an updated politics of historical interpretation consistent with that model, a politics that scholars like Burns would insist Woolf not only refuses but remakes.<sup id="fnref:80"><a href="#fn:80" class="footnote-ref" role="doc-noteref">80</a></sup>  Another way to say this is that, as Woolf invents a new means by which gender could be individually styled and proven as style, she shows that our general, normative understanding of the conditions and capabilities of fiction for representing gendered life is incomplete and in need of expansion. Literary critics have known about this capacity of Woolf for a long time, of course, but this stylometric view of  <em>Orlando</em>  demonstrates just how acutely she played with gender as itself a type of literary style. There is no solving Woolf here, as Ramsay says, because Woolf fundamentally changes the stylistic equation, widening modernism’s scope of the representatively possible.</p>
<p>To our second point, then, and contrary to what most digital humanists have found by studying the literature of the 19th century, Woolf proves again that modernist literature generated denaturalizing and atypical movements of gender  <em>as</em>  and  <em>through</em> , and not merely  <em>in</em> , its styles. Modernists, we know, were among the first generations of writers fundamentally influenced by the reality that unconscious forces construct us,<sup id="fnref:81"><a href="#fn:81" class="footnote-ref" role="doc-noteref">81</a></sup>  and it’s essential to remember that their words are thus not simply markers of hidden sociocultural powers and discourses, but that they instead represent — for one of the first times in modern literary history — active, ongoing, deliberate engagements with those discourses as they knew they were appearing in their literature. Hopefully it’s clear how our point about exemplarity is a stepping-stone to this. When we arrive in modernism, a period in which authors, figures, writers, and thinkers were quite explicitly styling gender and its cultural markers, tracing a gender signal in writing and attaching its features to latent cultural realities is no longer quite as defensible by the logic used in stylometric studies of 19th-century literature. As Woolf herself says elsewhere, theirs was already  “an age clearly when we are not fast anchored where we are; things are moving round us; we are moving ourselves”   <sup id="fnref:82"><a href="#fn:82" class="footnote-ref" role="doc-noteref">82</a></sup>.</p>
<p>Simply put, what we imagine to be the latent, stylistic territory of stylometry (and other computational methods invested in written style) cannot be trusted to have been latent at all in modernism. Woolf’s experiment reminds us that the fundamental assumption of traditional stylometry is notoriously unreliable: namely, that authors and specified groups have distinct, independent, hidden markers of style we can compare and from which we can glean social and cultural insight — about, for instance, the intentional or accidental written production of gender. Earlier we argued that modernism hasn’t been studied digitally for two reasons, but this would offer a third: treating modern style as a proxy for (or reflection of) the linguistic features of modern identity can neither be done dependably nor without presuming to already know, and thus arrange a study around, where identity begins and ends for literary stylists. Scholars leveraging stylometric methods to analyze modernist literature must contend with modernism’s unique self-awareness in this regard — the ways stylistic experimentation may limit the conclusions that can be drawn from modernist language about the conditions of modern life.  <em>Orlando</em> ’s remarkable stylistic play gives us reason to pause, in other words, reason to question stylometry’s ability to contextualize gender and modernism in the same breath.</p>
<p>We do not mean to suggest that stylometry is fundamentally an ill-fitted tool for studying modernism and gender, or even modern identity writ large. We simply want to point out that modernism, perhaps more so than other aesthetic movements or literary periods, necessitates a critical mutuality that requires from its scholars an upfront and explicit negotiation between object and method, accident and intention, style and styling. As interpretive constraints, these tensions might even address why there isn’t much serious stylometric work on modernism — and perhaps why there should be. If we want the future of modernity’s digital literary study to have any kind of educational end, and if we indeed think that stylometry can spill forth new insights about (or models of) the representational histories of social experience (vis-à-vis linguistic style), then a crucial part of such projects must be to gauge how the limitations of our tools might help us to study differently the sociocultural traces of modern identity and its stylistic waves. Maybe Woolf isn’t the first or only author to make the necessity of this reorientation so evident, but  <em>Orlando</em>  certainly illustrates the extents to which, when applied to and in modernism, both our literary data and our digital-critical methods must become more than  “mere containers”  of language’s curious residues <sup id="fnref1:43"><a href="#fn:43" class="footnote-ref" role="doc-noteref">43</a></sup>  <sup id="fnref1:76"><a href="#fn:76" class="footnote-ref" role="doc-noteref">76</a></sup>.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Christie, Alex, et al.  “Manifesto of Modernist Digital Humanities.”  Humanities Commons, 2014.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Mao, Douglas, and Rebecca Walkowitz.  “The New Modernist Studies,”    <em>PMLA</em>  123, no. 3 (2008): 737-48.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Pressman, Jessica.  _Digital Modernism: Making It New in New Media. _ Oxford UP, 2014.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Ross, Shawna, and James O’Sullivan, eds.  <em>Reading Modernism with Machines: Digital Humanities and Modernist Literature.</em> Palgrave Macmillan, 2016.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>Ross, Stephen, and Jentery Sayers.  “Modernism Meets Digital Humanities.”    _Literature Compass _ 11, no. 9 (2014): 625-633.&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>Hankins, Gabriel.  “We Are All Digital Modernists Now.”    <em>Modernism/Modernity Print Plus</em>  3, no. 2 (2018).&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>Hankins, Gabriel.  “The Weak Powers of Digital Modernist Studies.”    <em>Modernism/Modernity</em>  25, no. 3 (2018): 569-585.&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>Adwetewa-Badu, Ama Bemma.  “Poetry from Afar: Distant Reading, Global Poetics, and the Digital Humanities.”    _ Modernism/modernity Print Plus_  5, no. 1 (2020).&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>McGrath, Laura B., Devin Higgins, and Arend Hintze.  “Measuring Modernist Novelty.”    <em>Cultural Analytics</em> (Nov. 2018).&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>Huculak,J. Matthew.  “What Is a Modernist Archive?,”    <em>Modernism/Modernity Print Plus</em>  (2018).&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>Golden, Amanda, and Cassandra Laity,  “Feminist Modernist Digital Humanities” ,  <em>Feminist Modernist Studies</em>  1, no. 3 (2018), 205-210.&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p>Long, Hoyt, and Richard Jean So.  “Network Analysis and the Sociology of Modernism,”  boundary 2 40, no. 2 (2013).&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13">
<p>Long, Hoyt, and Richard Jean So.  “Literary Pattern Recognition: Modernism between Close Reading and Machine Learning.”    <em>Critical Inquiry</em>  42, no. 2 (2016): 235-67.&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14">
<p>Long, Hoyt, and Richard Jean So.  “Turbulent Flow: A Computational Model of World Literature.”    <em>Modern Language Quarterly</em>  77, no. 3 (2016): 345-67.&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:15">
<p>One might just as easily scan this essay’s bibliography for other examples, but among them: McGrath et al. (2018); Ross and O’Sullivan (2016); Jeffrey Drouin,  “Close- And Distant-Reading Modernism: Network Analysis, Text Mining, and Teaching  <em>The Little Review</em> ”  (2014); Sean Weidman and James O’Sullivan,  “The limits of distinctive words: Re-evaluating literature’s gender marker debate”  (2018).&#160;<a href="#fnref:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:16">
<p>Hayot, Eric, and Rebecca L. Walkowitz, eds.  “A New Vocabulary for Global Modernism” . Columbia UP, 2016.&#160;<a href="#fnref:16" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:17">
<p>Wollaeger, Mark, and Matt Eatough, eds.  <em>The Oxford Handbook of Global Modernisms</em> . Oxford UP, 2012.&#160;<a href="#fnref:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:18">
<p>Stanford Friedman, Susan.  _Planetary Modernisms: Provocations on Modernity Across Time. _ Columbia UP, 2015.&#160;<a href="#fnref:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:19">
<p>In fact, David James and Urmila Seshagiri recently bolstered that organizing logic, arguing how  “[r]etaining modernism across deep time can dehistoricize it as a movement but repoliticize it as a global practice, a practice that serves instrumental ends in the context of cultural circumstances with which modernist writing has yet to be associated”   <sup id="fnref1:24"><a href="#fn:24" class="footnote-ref" role="doc-noteref">24</a></sup>.&#160;<a href="#fnref:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:20">
<p>Peter B. Hirtle of Cornell University Library hosts a Copyright Information Center page that’s kept updated for the complex copyright terms of public domain in the US. See Hirtle’s article,  “When Is 1923 Going to Arrive and Other Complications of the U.S. Public Domain”  (2012), for an explanation of why published works after 1923 have remained out of the public domain. As of 1 January, 2019, a variety of US-published works from 1923 have entered the public domain, and each subsequent year will see a correlative year’s worth of once-copyrighted works do the same. An exception must be made, however, for Matthew Huculak and Claire Battershill’s <a href="http://openmods.uvic.ca/">   <em>Open Modernisms</em>   </a> project, which as of this writing represents an online archive of nearly 500 modernist works in various genres (see also Claire Battershill et al.’s  <em>Making The Modernist Archives Publishing Project</em>  (2017)). Otherwise, in the past half-decade, the HathiTrust Research Center is the only digital collection that offers access to sets of post-1923 texts for scholars doing computational research, and it comes with several restrictions.&#160;<a href="#fnref:20" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:21">
<p>Nelson, Cary.  <em>Repression and Recovery: Modern American Poetry and the Politics of Cultural Memory, 1910-1945</em> . U of Wisconsin P, 1989.&#160;<a href="#fnref:21" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:22">
<p>We should clarify that our use of the term style diverges from other variants used by modernist literary scholars (e.g. Rebecca Walkowitz), which tend to invoke the term loosely and synonymously with form, insofar as each denotes a literary pattern traceable through (for example) close reading. In the realm of stylometry, a notion of literary style is technical and tied to word use, and the term stands in for the aggregate of a set of formal, observable textual features. Our underlying assumption is that word preference can be measured by first counting words, both within individual texts and comparatively within a group of texts, and that those counts will be unique along different axes (e.g. different from writer to writer; genre to genre; etc.). By extension, those counts tell us something about the content and style of a text/writer relative to other texts. For a terrific account of the interdisciplinary use of the term among textual studies fields, see Herrmann et al. (2015).&#160;<a href="#fnref:22" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:23">
<p>And this line probably owes a great critical debt to those recent critiques of computational literary study proffered by Katherine Bode (2017) and Nan Z. Da (2019) — though, our study tends toward slightly more optimism: in the end, while we suggest stylometric critique needs to be wary of drawing firm conclusions about modern life via modernist fiction, we nevertheless think stylometry has much to offer the study of modernism.&#160;<a href="#fnref:23" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:24">
<p>James, David, and Urmila Seshagiri.  “Metamodernism: Narratives of Continuity and Revolution,”    <em>PMLA</em>  129, no. 1 (2014): 87-100.&#160;<a href="#fnref:24" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:24" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:25">
<p>In this we uphold Losh et al.’s call for a  “genuinely messy, heterogeneous, and contentious pluralism”  as the underlying ethic of our digital methods, a critical approach that may also productively join — or productively digitize — the political investments of our data, its structures, and our own methods of analyzing and contextualizing modernism <sup id="fnref:83"><a href="#fn:83" class="footnote-ref" role="doc-noteref">83</a></sup>.&#160;<a href="#fnref:25" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:26">
<p>Rubin, Gayle.  “The Traffic in Women: Notes on the ‘Political Economy’ of Sex.”  In  <em>Toward an Anthropology of Women</em> , edited by Rayna R. Reiter. Monthly Review Press, 1975, 157-210.&#160;<a href="#fnref:26" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:27">
<p>For Bode, this manner of viewing texts often ends in  “dismiss[ing] the documentary record’s multiplicity”   <sup id="fnref:84"><a href="#fn:84" class="footnote-ref" role="doc-noteref">84</a></sup>.&#160;<a href="#fnref:27" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:28">
<p>Burrows, John F.  <em>Computation into Criticism: A Study of Jane Austen’s Novels and an Experiment in Method</em> . Oxford: Clarendon Press, 1986.&#160;<a href="#fnref:28" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:29">
<p>Burrows popularized the field of literary computational stylistics with his book, and his was one of the first to recognize the scales of semantic meaning within patterns of function word usage; although he doesn’t really study gender in Austen explicitly, literary stylometrists continue to build on his principles.&#160;<a href="#fnref:29" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:30">
<p>Jockers, Matthew.  <em>Macroanalysis: Digital Methods and Literary History</em> . U of Illinois Press, 2013.&#160;<a href="#fnref:30" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:31">
<p>Jockers, Matthew, and Gabi Kirilloff.  “ “Understanding Gender and Character Agency in the 19th Century Novel.”    <em>Cultural Analytics</em>  (Dec. 2016).&#160;<a href="#fnref:31" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:32">
<p>Refer also to Jan Rybicki (2016) and Mark Algee-Hewitt (2015) for two more essays utilizing similar methods to reach similar ends.&#160;<a href="#fnref:32" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:33">
<p>Kirilloff, Gabi, Peter J. Capuano, Julius Fredrick, and Matthew L. Jockers.  “From a distance ‘You might mistake her for a man’: A closer reading of gender and character action in  <em>Jane Eyre, The Law and the Lady,</em> and  <em>A Brilliant Woman</em> .”    <em>Digital Scholarship in the Humanities</em>  33, no. 4 (2018): 821-44.&#160;<a href="#fnref:33" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:34">
<p>Underwood, Ted, David Bamman, and Sabrina Lee.  “The Transformation of Gender in English-Language Fiction.”    <em>Cultural Analytics</em>  (Feb. 2018).&#160;<a href="#fnref:34" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:34" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:35">
<p>Cheng, Jonathan Y.  “Fleshing Out Models of Gender in English-Language Novels (1850-2000).”    _Cultural Analytics _ (Jan. 2020).&#160;<a href="#fnref:35" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:35" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:36">
<p>Piper, Andrew. “There Will Be Numbers.”    <em>Cultural Analytics</em>  (May 2016).&#160;<a href="#fnref:36" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:36" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:37">
<p>Although not limited to modern literature, this claim is made even clearer and more forcefully by Earhart et al. (2020) in their recent account of gender and scholarly citational practices.&#160;<a href="#fnref:37" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:38">
<p>Kraicer, Eve, and Andrew Piper.  “Social Characters: The Hierarchy of Gender in Contemporary English-Language Fiction,”    <em>Cultural Analytics</em>  (Jan. 2019).&#160;<a href="#fnref:38" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:38" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:39">
<p>Lavin, Matthew J.  “Gender Dynamics and Critical Reception: A Study of Early 20th-century Book Reviews from  <em>The New York Times</em> .”    _Cultural Analytics _ (Jan. 2020).&#160;<a href="#fnref:39" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:40">
<p>In fact, aside from Lavin’s essay about modern reviewing, we can think of only two studies that employ some form of quantitative formalist approach to modernist work and even vaguely relate it to gender. Stephen Ramsay’s  _Reading Machines _ opens with a chapter that analyzes Virginia Woolf’s  _The Waves _ in relation to its feminist criticism, and David Hoover’s later  “Argument, Evidence, and the Limits of Digital Literary Studies”  positions itself directly opposite Ramsay’s earlier study by rereading  _The Waves _ with different computational methods <sup id="fnref1:75"><a href="#fn:75" class="footnote-ref" role="doc-noteref">75</a></sup>  <sup id="fnref:85"><a href="#fn:85" class="footnote-ref" role="doc-noteref">85</a></sup>. Among other thematically adjacent studies: see González et al. (2019) for an account of gender, stylometry, and  <em>modernismo</em> ; and outside of modernism’s computational literary study, see Churchill et al. (2018) regarding their work on Mina Loy, style, and UX design and their interactive digital project of feminist modernist design (which builds on D’Ignazio and Klein’s (2016) foundational feminist visualizations essay).&#160;<a href="#fnref:40" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:41">
<p>Among other, longer accounts, Katherine Bode’s  _A World of Fiction: Digital Collections and the Future of Literary History _ (2018) and Ted Underwood’s  _Distant Horizons: Digital Evidence and Literary Change _ (2019) also each contain a section on gender and (mainly) 19th-century literature.&#160;<a href="#fnref:41" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:42">
<p>Drucker, Johanna.  “Why Distant Reading Isn’t.”    <em>PMLA</em>  132, no. 3 (2017): 628-35.&#160;<a href="#fnref:42" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:43">
<p>Posner, Miriam and Lauren F. Klein.  “Editor’s Introduction: Data as Media.”    <em>Feminist Media Histories</em>  3, no. 3 (2017): 1-8.&#160;<a href="#fnref:43" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:43" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:44">
<p>Brown, Susan, and Laura Mandell.  “The Identity Issue: An Introduction.”    _Cultural Analytics _ (Feb. 2018).&#160;<a href="#fnref:44" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:44" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:45">
<p>Caughie, Pamela L., Emily Datskou,&amp; Rebecca Parker.  “Storm clouds on the horizon: feminist ontologies and the problem of gender.”    _Feminist Modernist Studies _ 1, no. 3 (2018): 230-42.&#160;<a href="#fnref:45" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:46">
<p>Underwood, Ted.  “Machine Learning and Human Perspective.”    <em>PMLA</em>  135, no. 1 (2020): 92-109.&#160;<a href="#fnref:46" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:47">
<p>Richard Jean So contends similarly that errors help us realign models to the unseen peripheries of data, and echoes Brown and Mandell’s sentiment through the oft-quoted adage of famed statistician George E.P. Box:  “All models are wrong, but some are useful”   <sup id="fnref:86"><a href="#fn:86" class="footnote-ref" role="doc-noteref">86</a></sup>. Andrew Piper makes a comparable remark when he considers that  “[m]odeling puts computation not on the outside of what is known but as part of the process itself,”  a reflexive process toward the contingency of knowledge he terms the New Recursivity  <sup id="fnref1:36"><a href="#fn:36" class="footnote-ref" role="doc-noteref">36</a></sup>. Piper also argues convincingly that every aspect of the modeling process, especially those required by its implementation on particular data selections, necessitates reduction — and that in its ubiquity reductiveness can actually be generative <sup id="fnref:87"><a href="#fn:87" class="footnote-ref" role="doc-noteref">87</a></sup>.&#160;<a href="#fnref:47" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:48">
<p>A case in point about this own study, which began many years ago (and before the resources of HathiTrust were widely accessible): our corpus of women authors is almost entirely hand-scanned and OCRed, because the continued gender inequality of the literary marketplace, which Piper and So have studied, also occupies the realm of text digitization. Riddell and Bassett (2020) have measured this gender inequity, finding (in a corpus from the 1830s to the present day) that novels by women have been digitized at substantially lower rates than novels by men. For a more detailed account on the many levels of infrastructural relevance women writers require to be studied by digital methods, see Laura Mandell’s  “Gendering Digital Literary History: What Counts for Digital Humanities”  (2016), then see Roopika Risam’s  “Navigating the Global Digital Humanities: Insights from Black Feminism”  (2016) for a take on the complexity of foregrounding racial and multicultural diversity while doing so.&#160;<a href="#fnref:48" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:49">
<p>While our analyses included the default standardization of all texts with z-scores (such that variations in a term like novel-length no longer come into play) and might have analyzed works of intentionally disparate lengths, we wanted to ensure our corpus maintained a cohesive genre; the phrase novel-length fiction is one common compromise.&#160;<a href="#fnref:49" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:50">
<p>Give or take a year or two — we cheated, for example, to fit in Conrad’s  _Heart of Darkness _ (1899).&#160;<a href="#fnref:50" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:51">
<p>For the sake of space, the full text list isn’t included here, but it is of course available upon request.&#160;<a href="#fnref:51" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:52">
<p>We would be remiss if we didn’t mention Evans and Wilkens (2018) as a recent computational study that adds to the mounting rationales  _against _ such a canon. The authors argue convincingly that, when modeling British fiction as a whole (and not just its canonical works), the modernist period produced narrative attentions to international locales that greatly outnumbered national ones. Most modern British fiction, that is, spent more time discussing international milieus than not, which reaffirms concerns about the representative validity of something like an orthodox canon of British modern fiction.&#160;<a href="#fnref:52" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:53">
<p>Hence conventional caveats apply: our corpus may indeed produce a narrative driven by ease-of-access or proximity over actual representativeness; a larger, more diverse, more global (in short, a different) corpus may have provided different results; and the claims we hope to make about gender and modernist style thus can’t reliably be extrapolated to modernist literature’s other flavors without further analysis. We admit this is a substantial, but so far largely unavoidable, limitation of studying the literary canon of a period still heavily under material and economic wraps. It’s reason, too, to be skeptical of the midrange scale of our study, which ends by looking at one text and one author and is thus not nearly as macroanalytic as most stylometric studies of literature. (It should be said that Marks Algee-Hewitt and McGurl explore this and other rationales of corpus-making in the Stanford Literary Lab’s _ Pamphlet 8_ ,  “Between Canon and Corpus: Six Perspectives on 20th-Century Novels,”  2-8.)&#160;<a href="#fnref:53" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:54">
<p>Mandell, Laura C.  “Gender and Cultural Analytics: Finding or Making Stereotypes?”    <em>Debates in the Digital Humanities</em> 2019, edited by Matthew K. Gold and Lauren F. Klein. UP of Minnesota, 2019.&#160;<a href="#fnref:54" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:54" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:55">
<p>Though, there are indeed more nuanced measures of stylistic distance that weight function and content words in different ways. Regardless, this general technique is often called the bag of words approach, and while it is popular in text analysis it also has its drawbacks. Its most basic model treats every word in each text in the same way, regardless of that word’s (a) syntactic or semantic contexts and (b) relation to the narrative or literary forms — thus, every novel merely becomes a countable bag of words. The approach makes measuring stylistic difference simple and effective, but in doing so it erases the context of all other literary elements in a work, which makes reading the resulting wordlists a precarious task. Take the exchange in  <em>Orlando</em>  between the newly acquainted Shelmerdine and Orlando as a brief example — and take first a jumbled approximation of what our machine sees, in the spirit of big-data experimentalism: a (2), cried (2), [a]re (2), you (2), he (1), man (1), Orlando (1), she (1), Shel (1), woman (1). What’s happened in this sequence, now that we’ve merely counted words and glossed over all context? Maybe a man and a woman were crying together, but who really knows? The flattening perils of decontextualized computational analysis are indeed laid bare. Now, here’s the moment as it appears in the actual narrative:  “‘You’re a woman, Shel!’ she cried. ‘You’re a man, Orlando!’ he cried.”  Even in a short, two-sentence example, Woolf’s tremendous gender-/word-play and its meaning for the speakers, who have fallen for one another (and soon marry) in part because of their gender non-conforming androgyny, is entirely lost if one merely rushed to count one’s computational chickens.&#160;<a href="#fnref:55" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:56">
<p>Although we don’t engage with it here, Underwood and So have raised a few conceptual concerns with this approach, asking recently whether statistical distance and stylistic distance are comparable measures of relation at all <sup id="fnref:88"><a href="#fn:88" class="footnote-ref" role="doc-noteref">88</a></sup>.&#160;<a href="#fnref:56" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:57">
<p>A basic note may be necessary regarding the difference between supervised and unsupervised machine learning techniques (though, there are other techniques that borrow from both types). Supervised techniques tend to require the input of pre-classified data to learn from, so that the algorithm can track and then predict future patterns from similarly classified data (e.g. one might train a road sign classification machine on thousands of different stop sign images, and then feed it other random images and ask it to output whether or not they feature stop signs). Unsupervised machine learning, conversely, tends to model the distribution or configuration of input data (e.g. providing a large, unsorted data set of road sign images and some metric of sorting them might provide correlations, groupings, or trends to help identify or separate those images). For further reading on this difference, and for the other variations of these methods, see Shalev-Shwartz and Ben-David (2014), 4-6.&#160;<a href="#fnref:57" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:58">
<p>Our stylistic analysis was done entirely through Eder et al.’s  <em>Stylo</em>  package in the statistical computing program, R <sup id="fnref:89"><a href="#fn:89" class="footnote-ref" role="doc-noteref">89</a></sup>. Although we’ve chosen a supervised analysis, its methodological limitations are significant. The benefit of unsupervised learning (e.g. PCA) is that the machine doesn’t know how many groups of data we think we’re studying, so we can’t privilege a group split just because we think there is one; the difficulty becomes identifying what features, exactly, constitute the groupings output by the machine. Inversely, the downside of a supervised analysis is that we organize the data in two pre-conceived groups and ask that each text be assigned to one group or the other. The benefit is that we at least think we have an idea of where machine-located differences are coming from — i.e. in this case, we isolate our groups based on gender. Both methods put us in an at least somewhat compromised position when drawing conclusions about gender and its stylistic features.&#160;<a href="#fnref:58" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:59">
<p>One digital interpretation of the stylistic distinctiveness between the genders in modernism has been advanced elsewhere <sup id="fnref:90"><a href="#fn:90" class="footnote-ref" role="doc-noteref">90</a></sup> . Again, as Mandell notes, this measure of  “textual gender”  alone is not terribly interesting; traditional social and cultural gender signaling is perfectly expected in literary work and, in this case, we also constructed our data to produce a split along that line of pre-sorted difference <sup id="fnref1:54"><a href="#fn:54" class="footnote-ref" role="doc-noteref">54</a></sup>.&#160;<a href="#fnref:59" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:60">
<p>We forced the machine, for example, to cull words that didn’t appear in most or all of the texts (to prevent particularly uncommon themes, narrative locations, or character names to artificially amplify differences); we used different MFW counts (50, 100, 200, … 1000) to see if a more robust set of word frequencies would change our findings; we employed a variety of sampling methods, from analyzing each text in its entirety to including only one or two random, bag-of-word samples; and we even swapped between statistical distance measures, seeing similar results from Burrows’s Delta and the Wurzburg Cosine — though we talk more in footnotes 28 and 34 about why we limited this particular adjustment.&#160;<a href="#fnref:60" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:61">
<p>This finding about Hall is especially intriguing; though we haven’t space to discuss the novel here, Hall’s protagonist, the sexual invert Stephen Gordon, is narrated through much of the same language as Woolf’s in  <em>Orlando</em> , a comparison that certainly warrants further study.&#160;<a href="#fnref:61" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:62">
<p>Many of the studies from Jockers and Underwood, for example, discern as much in their 19th-century and contemporary corpuses, and alongside at least one other aforementioned piece <sup id="fnref1:35"><a href="#fn:35" class="footnote-ref" role="doc-noteref">35</a></sup>, one of our prior essays confirms those findings in modernist fiction <sup id="fnref1:90"><a href="#fn:90" class="footnote-ref" role="doc-noteref">90</a></sup>.&#160;<a href="#fnref:62" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:63">
<p>For a detailed explanation of the nearest shrunken centroid (NSC) method of classification, see Jockers and Witten (2010). For a specific description of the technical features of NSC as applied in  <em>Stylo</em> , see Eder (2016). As a stylometric classifier that finds an averaged stylistic profile, NSC provides certain benefits over a standard variant like Burrows’s Delta in Eder’s rolling method, which tries to pinpoint moments of stylistic takeover in a single text after being trained on one more groups of texts <sup id="fnref:91"><a href="#fn:91" class="footnote-ref" role="doc-noteref">91</a></sup>. With Delta, each text in each training group is treated distinctly and is not consolidated into a stylistic profile (i.e.  _Orlando _ is measured against  <em>each</em>  training sample/text, ranking the  “styles”  that are closest to the test text); NSC, however, produces composites against which we can compare our test text (i.e.  _Orlando _  against the averaged style of all female texts or all male texts). For our study, where we want to isolate the gender binary as a two-class problem, NSC is a natural fit.&#160;<a href="#fnref:63" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:64">
<p>This control turned out to matter little — when we reintroduced her remaining texts to the training set and re-ran the analysis, the results were nearly identical.&#160;<a href="#fnref:64" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:65">
<p>Although we analyzed each text at 50-1000 MFWs with culling from 0-50% and received remarkably negligible variation in our results, for exact reproducibility’s sake, each NSC classification image we included here was produced at 500 MFWs and 0% culling.&#160;<a href="#fnref:65" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:66">
<p>Woolf, Virginia.  _Orlando: A Biography. _  (1928). Vintage, 1992.&#160;<a href="#fnref:66" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:66" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:66" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:67">
<p>Burrows’s aforementioned 1986 study, among other contemporary versions — e.g. Pennebaker’s  _The Secret Life of Pronouns _ (2011) — was among the first to explore just how important function words were to the close analysis of literary style. Remarkable, then, that Woolf seems to have divined and manipulated this reality half-a-century earlier and without the benefit of computational analysis.&#160;<a href="#fnref:67" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:68">
<p>Delaplace, Anne.  “Transcending Gender: Virginia Woolf and Marcel Proust, Orlando and Albertine.”    <em>Virginia Woolf Bulletin</em>  37 (2011): 37-41.&#160;<a href="#fnref:68" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:69">
<p>Caughie, Pamela L.  “The Temporality of Modernist Life Writing in the Era of Transsexualism: Virginia Woolf’s  <em>Orlando</em>  and Einar Wegener’s  <em>Man Into Woman</em> .”    <em>Modern Fiction Studies</em>  59, no. 3 (2013): 501-25.&#160;<a href="#fnref:69" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:70">
<p>Several related essays warrant a brief mention here: Brenda Helt’s  “Passionate Debates on ‘Odious Subjects’: Bisexuality and Woolf’s Opposition to Theories of Androgyny and Sexual Identity”  (2010) provides an argument for using bisexuality to describe Woolf’s depictions of desire; Jessica Berman’s  “Is the Trans in Transnational the Trans in Transgender”  (2017) offers a discussion of how Orlando’s transnational roaming contributes to Woolf’s critique of imperial masculinity via Orlando’s seamless gender transition; and Madelyn Detloff’s  “Camp Orlando (or) Orlando”  (2016) provides an account of the camp sensibilities and reparative work of  <em>Orlando</em> .&#160;<a href="#fnref:70" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:71">
<p>Högberg, Elsa and Amy Bromley.  “Introduction: Sentencing Orlando.”    <em>Sentencing Orlando: Virginia Woolf and the Morphology of the Modernist Sentence</em> , edited by Elsa Högberg and Amy Bromley. Edinburgh UP, 2018, 1-14.&#160;<a href="#fnref:71" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:72">
<p>We steal both examples directly from Emma Heaney’s magnificent book,  <em>The New Woman: Literary Modernism, Queer Theory, and the Trans Feminine Allegory</em>  (2017). In fact, Heaney’s exploration of the history and production of trans femininity provides an important clarification to our study’s finding, detaching Woolf from a legacy of modernist trans feminism <sup id="fnref:92"><a href="#fn:92" class="footnote-ref" role="doc-noteref">92</a></sup>.&#160;<a href="#fnref:72" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:73">
<p>We understand, of course, that we still depend on averaged literary styles (via a bag-of-word list of most frequent terms) in this study, and we do not call that essential stylometric practice into question. What we’re after here, rather, is a reimagining of how eschewing the inexact close-readings that tend to follow computational measures of style can actually help us accomplish something critically important. Deciphering averaged stylistic differences — or, what’s more probable, incidentally lodging our pre-held assumptions into stylistic peculiarities — does not expand or contextualize our conclusions as much as it forecloses their messiness in almost-assuredly biased explainers. By restraining that stylometric impulse, we hope to let the text’s relation to the corpus (rather than specific terminological connections/disparities) do the critical work for us. We want to venture an observation here, one helped along by Adam Kilgarriff’s influential essay,  “Language is never, ever, ever, random”  (2005) and its adjacent, clarifying distinction between  “randomness”  and  “arbitrariness”  in interpretations of linguistic phenomena in literary corpora. That linguistic structures appear nonrandom or predictable in their relation does not make that relation meaningful — just as syntax can colonize meaning, narrative structures can demand certain forms, patterns, and distributions of linguistic content, which detaches judgments about their meaning from their possible literary-historical or sociocultural arbitrariness. Having used a specific corpus organization and specific modeling tools, all of which were designed to treat gender as a bimodal stylistic question, we think finding in  _Orlando _ a nonrandom and narratively nonarbitrary gender flip is more than a linguistic oddity.&#160;<a href="#fnref:73" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:74">
<p>Following Rachael Scarborough King’s delineation of form and genre, we might even call the literary style of gender a  <em>genre</em> , an organizing metanarrative,  “a collection whose members are assembled and whose boundaries are always permeable”   <sup id="fnref:93"><a href="#fn:93" class="footnote-ref" role="doc-noteref">93</a></sup>.&#160;<a href="#fnref:74" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:75">
<p>Ramsay, Stephen.  _Reading Machines: Toward an Algorithmic Criticism. _ U of Illinois P, 2011.&#160;<a href="#fnref:75" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:75" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:76">
<p>Burns, Christy L.  “Re-Dressing Feminist Identities: Tensions between Essential and Constructed Selves in Virginia Woolf’s  <em>Orlando</em> .”    <em>Twentieth Century Literature</em>  40, no. 3 (1994): 342- 64.&#160;<a href="#fnref:76" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:76" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:77">
<p>This is a problem Piper (2020) has recently aimed to tackle at much greater length and with much greater care than we do here.&#160;<a href="#fnref:77" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:78">
<p>Etherington, Ben, and Sean Pryor.  “Historical poetics and the problem of exemplarity.”    <em>Critical Quarterly</em> 61, no. 1 (2019): 3-17.&#160;<a href="#fnref:78" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:78" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:79">
<p>As Pryor notes elsewhere,  “the category of modernism was developed through attention to exemplars”   <sup id="fnref:94"><a href="#fn:94" class="footnote-ref" role="doc-noteref">94</a></sup>.&#160;<a href="#fnref:79" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:80">
<p>Etherington and Pryor also clarify this point:  “because exemplarity conditions the production of knowledge, helping to construct the very object of inquiry, it is also a political problem”   <sup id="fnref1:78"><a href="#fn:78" class="footnote-ref" role="doc-noteref">78</a></sup>.&#160;<a href="#fnref:80" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:81">
<p>This is a claim that has long since entered modernist criticism&rsquo;s common vocabulary, but see, among other examples: Maud Ellman,  <em>Nets of Modernism: Henry James, Virginia Woolf, James Joyce, and Sigmund Freud</em>  (2010); Matt Ffytch,  “The Modernist Road to the Unconscious”  (2012); and, for a classic account of this impact on modern culture generally, Michael North,  <em>Reading 1922: A Return to the Scene of the Modern</em>  (1999).&#160;<a href="#fnref:81" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:82">
<p>Woolf, Virginia.  “Poetry, Fiction and the Future.”    _Selected Essays. _ Oxford UP, 2009, 74-84.&#160;<a href="#fnref:82" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:83">
<p>Losh, Elizabeth, Jacqueline Wernimont, Laura Wexler, and Hong-An Wu.  “Putting the Human Back into the Digital Humanities: Feminism, Generosity, and Mess.”  In  <em>Debates in the Digital Humanities</em>  2016, edited by Lauren F. Klein and Matthew K. Gold. U of Minnesota P, 2016, 92-103.&#160;<a href="#fnref:83" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:84">
<p>Bode, Katherine.  “The Equivalence of ‘Close’ and “Distant’ Reading; or, Toward a New Object for Data-Rich Literary History.”    <em>Modern Language Quarterly</em>  78, no. 1 (2017): 77-106.&#160;<a href="#fnref:84" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:85">
<p>Hoover, David L.  “Argument, Evidence, and the Limits of Digital Literary Studies.”    <em>In Debates in the Digital Humanities</em>  2016, edited by Lauren F. Klein and Matthew K. Gold. U of Minnesota P, 2016, 230-50.&#160;<a href="#fnref:85" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:86">
<p>So, Richard Jean.  “All Models Are Wrong.”    <em>PMLA</em>  132, no. 3 (2017): 668-73.&#160;<a href="#fnref:86" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:87">
<p>Piper, Andrew. “Think Small: On Literary Modeling.”    <em>PMLA</em>  132, no. 3 (2017): 651-8.&#160;<a href="#fnref:87" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:88">
<p>Underwood, Ted and Richard Jean So,  “Can We Map Culture?”    <em>Cultural Analytics</em>  (June 2021).&#160;<a href="#fnref:88" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:89">
<p>Eder, Maciej, Jan Rybicki, and Mike Kestemont.  “Stylometry with R: A Package for Computational Text Analysis.”    <em>R Journal</em>  8, no. 1 (2016): 107-21.&#160;<a href="#fnref:89" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:90">
<p>Weidman, Sean G., and James O’Sullivan.  “The limits of distinctive words: Re-evaluating literature’s gender marker debate,”    <em>Digital Scholarship in the Humanities</em>  33, no. 2 (2018): 374-90.&#160;<a href="#fnref:90" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:90" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:91">
<p>Eder, Maciej.  “Rolling stylometry.”    <em>Digital Scholarship in the Humanities</em>  31, no. 3 (2016): 457- 69.&#160;<a href="#fnref:91" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:92">
<p>Heaney, Emma.  <em>The New Woman: Literary Modernism, Queer Theory, and the Trans Feminine Allegory</em> . Northwestern UP, 2017.&#160;<a href="#fnref:92" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:93">
<p>King, Rachael Scarborough.  “The Scale of Genre.”    <em>New Literary History</em> , vol. 52, no. 2 (2021): 261-84.&#160;<a href="#fnref:93" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:94">
<p>Pryor, Sean.  “A poetics of occasion in Hope Mirrlees’s  <em>Paris</em> .”    <em>Critical Quarterly</em>  61, no. 1 (2019): 37-53.&#160;<a href="#fnref:94" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry></feed>