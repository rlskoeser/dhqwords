<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=theme-color content="#3D206C"><meta http-equiv=x-ua-compatible content="IE=edge"><link rel=apple-touch-icon sizes=180x180 href=/img/favicon/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/img/favicon/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/img/favicon/favicon-16x16.png><link rel=icon type=image/png sizes=384x384 href=/img/favicon/android-chrome-384x384.png><link rel=icon type=image/png sizes=192x192 href=/img/favicon/android-chrome-192x192.png><link rel=icon type=image/png sizes=150x150 href=/img/favicon/mstile-150x150.png><link rel="shortcut icon" href=/favicon.ico><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/img/favicon/safari-pinned-tab.svg color=#3d206c><link rel=schema.dc href=http://purl.org/DC/elements/1.0/><meta name=citation_public_url content="https://rlskoeser.github.io/dhqwords/vol/17/2/000681/"><meta name=citation_title content="Machine Learning Techniques For Analyzing Inscriptions From Israel"><meta name=citation_date content><meta name=citation_author content="Daiki Tagami"><meta name=citation_author content="Michael Satlow"><meta name=citation_abstract content="1. Introduction The study of antiquity is full of missing data. The evidence that does survive – whether texts on papyrus or parchment; inscriptions; coins; or archaeological – frequently survives only in damaged form. That problem, however, is compounded by two additional complications. First, many of these data have been unearthed in non-controlled excavations and have taken winding paths to libraries, museums, and the hands of private collectors, along the way losing valuable contextual information."><meta name=citation_journal_title content="DHQwords"><meta name=citation_issue content="17.2"><meta name=citation_publisher content="Center for Digital Humanities, Princeton University"><meta name=DC.rights content="http://creativecommons.org/licenses/by/4.0/"><meta name=author content="Daiki Tagami, Michael Satlow"><meta name=generator content="Center for Digital Humanities, Princeton University"><meta name=dcterms.created content><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-regular.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-italic.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-regular.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-300.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-700.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-300italic.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-500italic.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-700italic.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-500.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/Source_Sans_Pro/SourceSans3VF-Roman.otf.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/Source_Sans_Pro/SourceSans3VF-Italic.otf.woff2 crossorigin><title>Machine Learning Techniques For Analyzing Inscriptions From Israel</title><meta name=description content="DHQwords Issue 17.2, . an experiment republishing DHQ articles with Hugo & Startwords theme. "><meta property="og:title" content="Machine Learning Techniques For Analyzing Inscriptions From Israel"><meta property="og:description" content="1. Introduction The study of antiquity is full of missing data. The evidence that does survive – whether texts on papyrus or parchment; inscriptions; coins; or archaeological – frequently survives only in damaged form. That problem, however, is compounded by two additional complications. First, many of these data have been unearthed in non-controlled excavations and have taken winding paths to libraries, museums, and the hands of private collectors, along the way losing valuable contextual information."><meta property="og:type" content="article"><meta property="og:url" content="https://rlskoeser.github.io/dhqwords/vol/17/2/000681/"><meta property="og:image" content="https://rlskoeser.github.io/dhqwords/social.png"><meta property="article:section" content="vol"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://rlskoeser.github.io/dhqwords/social.png"><meta name=twitter:title content="Machine Learning Techniques For Analyzing Inscriptions From Israel"><meta name=twitter:description content="1. Introduction The study of antiquity is full of missing data. The evidence that does survive – whether texts on papyrus or parchment; inscriptions; coins; or archaeological – frequently survives only in damaged form. That problem, however, is compounded by two additional complications. First, many of these data have been unearthed in non-controlled excavations and have taken winding paths to libraries, museums, and the hands of private collectors, along the way losing valuable contextual information."><link rel=stylesheet href=/dhqwords/style.css><link rel=stylesheet href=/dhqwords/print.css media=print><script src=/dhqwords/js/polyfills.js></script><script defer src=/dhqwords/js/bundle.js></script></head><body class=article><header><nav class=main aria-label=main><ul><li class=home><a href=/dhqwords/>dhq</a></li><li class=issues><a href=/dhqwords/vol/>volumes</a></li></ul></nav></header><main><article><div class=grid><header><p class=number><a href=/dhqwords/vol/17/2/>Issue 17.2</a></p><h1>Machine Learning Techniques For Analyzing Inscriptions From Israel</h1><p><ul class=authors><li><address>Daiki Tagami</address></li><li><address>Michael Satlow</address></li></ul></p><p></p><ul class="categories tags"><li><span class=tag>archaeology</span></li><li><span class=tag>machine learning</span></li><li><span class=tag>markup</span></li></ul><ul class=tags></ul><p class=formats></p></header><section class=print-only><a class=first-page-header href=/ aria-label=Startwords><img alt=Startwords src=/pdf-logotype.svg></a>
<a class=page-header href=/ aria-label=Startwords><img alt=Startwords src=/pdf-logo.svg></a>
<a href=http://doi.org/ rel=alternate class=page-doi>doi:</a></section><div class=text-container><h2 id=1-introduction>1. Introduction</h2><p>The study of antiquity is full of missing data. The evidence that does survive – whether texts on papyrus or parchment; inscriptions; coins; or archaeological – frequently survives only in damaged form. That problem, however, is compounded by two additional complications. First, many of these data have been unearthed in non-controlled excavations and have taken winding paths to libraries, museums, and the hands of private collectors, along the way losing valuable contextual information. Second, scholars have used a bewildering array of conflicting and often inherently vague reporting methods. My Roman period, for example, might be your Byzantine period. As a result of this situation, scholars in ancient studies frequently find themselves unable to place or date evidence that could be critical to our deeper understanding. Given the paucity of our information, for example, dating a particularly revealing inscription to the fifth or third century BCE, or as originating from Athens or Asia Minor, could have serious scholarly ramifications.</p><p>Traditionally, scholars have used their own experience and specialized training to supply these missing contextual data <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>. Recently, however, there has been increasing interest in using machine learning techniques to supplement, or even replace, subjective and idiosyncratic (although sometimes brilliant) evaluations. For example, Niculae et al. <sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup> used machine learning techniques to date a corpus of older texts.</p><p>In 2022, Assael et al. <sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup> published their research into developing a machine learning platform that would aid the automated reconstruction and adding of missing contextual information to ancient Greek inscriptions. This platform, which they call Ithaca, is based on a deep neural network model. They demonstrate that such a technique greatly enhances scholarly expertise, although it cannot substitute for it.</p><p>As impressive as Ithaca is, deep neural network techniques presently have limited applicability to other digital humanities projects. One inherent problem with using them is that they are black box models; their processes remain opaque. Furthermore, they need both technical expertise to implement and a large sample size to train the algorithm <sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup>. Datasets from antiquity, particularly those that exist in high-quality structured form, are rarely large enough to make this approach suitable.</p><p>In this paper, we explore the utility of other machine learning algorithms for predicting values in incomplete datasets. We have determined that a random forest model has the most potential to predict these values, particularly in smaller datasets with several categorical variables. While our own work was based on one dataset, “Inscriptions of Israel/Palestine,” we believe that our results are applicable to other datasets as well.</p><h2 id=2-methods>2. Methods</h2><h2 id=21-inscription-dataset>2.1 Inscription dataset</h2><p>The “Inscriptions of Israel/Palestine” (IIP) dataset is an online database which seeks to make all of the previously published inscriptions of Israel/Palestine from the Persian period through the Islamic conquest (ca. 500 BCE - 640 CE) freely accessible <sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup>. This database includes approximately 4,500 inscriptions, and they are written primarily in Hebrew, Aramaic, Greek and Latin, by Jews, Christians, Greeks, and Romans. Some of the examples include imperial declarations on monumental architecture, notices of donations in synagogues and humble names scratched on ossuaries <sup id=fnref1:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup>. Each inscription exists as a single XML file structured according to EpiDoc conventions <sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup>.</p><h2 id=22-variable-explanation>2.2 Variable Explanation</h2><p>We consider the following characteristics in the dataset:</p><ul><li>Terminus ante quem (the latest possible date)</li><li>Terminus post quem (the earliest possible date)</li><li>Text Genre</li><li>Language</li><li>Material</li><li>Region</li><li>Likely Religion</li></ul><p>It is worth noting that language, material, and region are objectively determined in most cases. Dating, on the other hand, is often determined subjectively by scholars; relatively few contain dates or were found in carefully controlled archaeological excavations. Thus, we examine how machine learning models can accurately predict the date of inscriptions given the information of other variables in the dataset. All variables inside the dataset except date are categorical, as they are not quantifiable.</p><h2 id=23-data-preprocessing>2.3 Data Preprocessing</h2><p>The IIP dataset is converted into a single csv file through using the ElementTree XML API in Python programming. One of the features of the IIP dataset, like many others in the humanities, is that it contains a number of categorical variables, that is, different phrases that occur within a single XML element. For example, there are many different cities in the location element, and over fifty different text genres (e.g., funerary, dedicatory, label, prayer) are found within the appropriate element. The result of this is an imbalanced dataset. Imbalanced datasets occur when the proportion of minority class is significantly low compared with other classes in the dataset, and creating an effective machine learning algorithm with imbalanced datasets is a very difficult problem <sup id=fnref:7><a href=#fn:7 class=footnote-ref role=doc-noteref>7</a></sup>. It is also important to make sure that all the possible categorical values are in the training dataset to create a good machine learning model. For example, if the training set does not include any inscriptions that has the city name Jerusalem, it would be difficult for the machine learning algorithm to use the Jerusalem information in the test dataset. Error messages can come out in many machine learning programs when they encounter some information that is not included in the training dataset. Splitting the dataset into training and test set is done randomly, so it is possible for the minority class to not appear in both training and test set if the number of observations from the minority class is small. To have enough observations in the minority class, we combine various unique terms and generate a dataset that is better suited for the machine learning algorithm.</p><p>We first fix spelling mistakes inside the dataset. Afterwards, we combine words that describe the same concept. For instance, Golan Heights and Golan can be grouped together as Golan and there is no need for the machine learning algorithm to consider these elements separately. There are also some phrases such as dedicatory quotation and dedicatory verse, where they describe different objects but can be grouped together as dedicatory to reduce the number of variables inside the dataset. However, we take a different approach with the City Name variable. There are 244 unique city names inside the dataset, and many of them only include a few inscriptions. Since the location of inscription is already indicated in the Region variable, we only consider Jerusalem and Other Cities to make the prediction easier to interpret. We also do not consider all variables in the dataset, such as condition of artifacts and relief style, as they have a lot of missing values.</p><p>We use a technique called one-hot encoding to convert the categorical features to numerical features. There are many machine learning algorithms that can only analyze numerical data, so analyzing categorical variables without one-hot encoding can cause some issues. In this technique, we create a binary column for each category, where we denote the output of the column to be 1 if the variable is present and 0 if the variable is not present. For example, we create a new variable called Language_Greek to describe if the inscription is written in Greek or not. The Language_Greek variable will be 1 if the inscription is written in Greek, but 0 otherwise.</p><p>These inscriptions are used to examine the performance of machine learning models to predict the time periods. The time period distribution of inscription is shown in Figure 1. The mean date is 109.68 CE with standard deviation 311.47. The large standard deviation implies that the dataset is suitable for conducting this analysis, as it has inscriptions from a wide range of time periods. After we preprocess the dataset, we select 650 inscriptions that actually contain a certain date. The overview of the steps that are taken in this research project is shown in Figure 2.</p><h2 id=24-machine-learning-techniques>2.4 Machine Learning Techniques</h2><p>We compare the performances of eleven machine learning models: linear regression, ridge regression, lasso regression, elastic net, decision tree, random forest, neural network, XGBoost, and support vector regression with linear, radial and polynomial kernel. We select these algorithms, as they require minimal hyperparameter tuning and do not require data transformation. Hyperparameters determine the overall behavior of the machine learning model, and they must be set appropriately by the user before conducting the analysis <sup id=fnref:8><a href=#fn:8 class=footnote-ref role=doc-noteref>8</a></sup>. Hyperparameter tuning is often performed manually, but it is impractical when we have many hyperparameters, and technical expertise is required to correctly set the hyperparameters <sup id=fnref:9><a href=#fn:9 class=footnote-ref role=doc-noteref>9</a></sup>. To create a simple and reproducible machine learning prediction model, we try to select models that do not require fine parameter tuning.</p><figure><img loading=lazy alt="Bar chart showing time period of inscriptions ranging from 600 BCE to 800 CE. Most inscriptions date between 400 BCE and 600 CE" src=/dhqwords/vol/17/2/000681/resources/images/figure01.png sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/17/2/000681/resources/images/figure01_hu83fcecb428aa75adc7bfe7e7f48d936d_12402_500x0_resize_box_3.png 500w,
/dhqwords/vol/17/2/000681/resources/images/figure01_hu83fcecb428aa75adc7bfe7e7f48d936d_12402_800x0_resize_box_3.png 800w,/dhqwords/vol/17/2/000681/resources/images/figure01.png 566w" class=landscape><figcaption><p>Time periods of inscriptions in the IIP dataset.</p></figcaption></figure><p>We will provide a brief overview of these techniques with some examples of previous studies in digital humanities. Readers who are interested in further details of machine learning techniques should consult Hastie et al. <sup id=fnref:10><a href=#fn:10 class=footnote-ref role=doc-noteref>10</a></sup>.</p><h2 id=241-penalized-regression-techniques>2.4.1 Penalized Regression Techniques</h2><figure><img loading=lazy alt="Flow chart showing the steps in data preprocessing (converting xml file, combining unique terms, obtaining inscriptions) and data analysis (chi-squared test, comparining machine learning techniques, examining random forest model)." src=/dhqwords/vol/17/2/000681/resources/images/figure02.png sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/17/2/000681/resources/images/figure02_hu3c8eba7345199ee6042eb25030dbf21e_35428_500x0_resize_box_3.png 500w,
/dhqwords/vol/17/2/000681/resources/images/figure02_hu3c8eba7345199ee6042eb25030dbf21e_35428_800x0_resize_box_3.png 800w,/dhqwords/vol/17/2/000681/resources/images/figure02_hu3c8eba7345199ee6042eb25030dbf21e_35428_1200x0_resize_box_3.png 1200w,/dhqwords/vol/17/2/000681/resources/images/figure02.png 1291w" class=landscape><figcaption><p>Workflow of the research project.</p></figcaption></figure><p>Ordinary least squares (OLS) regression is a commonly used statistical technique in regression problems. It assumes that there is a linear relationship between the predictor and response variable. We can directly observe the regression coefficients in OLS regression, so we can understand how the model is making predictions. There are, however, some disadvantages to using OLS. When the number of predictors become large, a small change in the training dataset can cause a large change in the prediction model produced by the OLS model <sup id=fnref1:10><a href=#fn:10 class=footnote-ref role=doc-noteref>10</a></sup>. Thus, penalization techniques are often used to improve the predictability of OLS while retaining its linear model structure <sup id=fnref:11><a href=#fn:11 class=footnote-ref role=doc-noteref>11</a></sup>. These methods impose a shrinkage penalty and bring the estimated coefficients closer to zero <sup id=fnref2:10><a href=#fn:10 class=footnote-ref role=doc-noteref>10</a></sup>. We will be examining ridge, lasso, and elastic net, as they are commonly used penalization techniques.</p><p>Penalization techniques are frequently used in digital humanities research projects that contain datasets with many variables. For example, Finegold et al. <sup id=fnref:12><a href=#fn:12 class=footnote-ref role=doc-noteref>12</a></sup> used Poisson Graphical Lasso to reconstruct the historical social network in early modern Britain. They have imposed the penalization technique in statistical graph learning methods to find out the relationship between people’s names inside the historical documents. Considering that the number of distinct names inside the historical documents is large, penalized regression went well for their analysis.</p><h2 id=242-support-vector-regression>2.4.2 Support Vector Regression</h2><p>Support Vector Regression (SVR) uses the same principle as Support Vector Machine (SVM), which is one of the most widely used supervised machine learning techniques <sup id=fnref:13><a href=#fn:13 class=footnote-ref role=doc-noteref>13</a></sup>. It is frequently used in digital humanities, including a study by Argamon et al. <sup id=fnref:14><a href=#fn:14 class=footnote-ref role=doc-noteref>14</a></sup>, where they used SVM to classify author&rsquo;s gender from literary texts. The SVM algorithm conducts regression based on kernel functions, which converts the lower dimensional data into a higher dimensional feature space.</p><p>We consider the performances of three kernels, linear, radial and polynomial kernel, as they are commonly used kernels. The detailed information about the SVR mechanism can be found in <sup id=fnref1:13><a href=#fn:13 class=footnote-ref role=doc-noteref>13</a></sup>.</p><h2 id=243-neural-network>2.4.3 Neural Network</h2><p>Deep learning algorithms make predictions based on a neural network structure, which is inspired by the human nervous system <sup id=fnref:15><a href=#fn:15 class=footnote-ref role=doc-noteref>15</a></sup>. It has been used in multiple algorithms in digital humanities studies, including a study by Assael et al. <sup id=fnref1:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup> where they had implemented a deep learning algorithm to predict contextual information based on the textual information in ancient Greek inscriptions. To examine the performance of deep learning technique, we fit a single-hidden-layer neural network, as it has been shown that low complexity deep learning models perform better when the sample size is small <sup id=fnref:16><a href=#fn:16 class=footnote-ref role=doc-noteref>16</a></sup>.</p><h2 id=244-tree-based-approach>2.4.4 Tree Based Approach</h2><p>We examine three different tree based machine learning techniques, decision tree, random forest and Extreme Gradient Boosting (XGBoost). Random forest and XGBoost are tree ensemble methods, and they are considered to be the recommended tools to analyze tabular datasets <sup id=fnref:17><a href=#fn:17 class=footnote-ref role=doc-noteref>17</a></sup>. Ensembles are methods that combine multiple machine learning techniques to create more powerful models, and tree ensemble methods are used extensively in various digital humanities research. For example, a recent project by Baledent et al. <sup id=fnref:18><a href=#fn:18 class=footnote-ref role=doc-noteref>18</a></sup> used decision trees and random forests to automatically date French documents with high predictability. Fragkiadakis et al. <sup id=fnref:19><a href=#fn:19 class=footnote-ref role=doc-noteref>19</a></sup> compared the performances of various machine learning techniques to annotate video data with sign languages, and showed that the XGBoost was the optimal model to predict the begin and end frames of a sign sequence in a video.</p><p>Decision tree is the foundation of random forest and XGBoost model. It is considered to be one of the most interpretable machine learning methods for data analysis, as it can classify data based on a set of yes/no questions. However, decision trees can be very non-robust and a minor change in the training data can result in a large change in the final tree <sup id=fnref3:10><a href=#fn:10 class=footnote-ref role=doc-noteref>10</a></sup>.</p><p>XGBoost is a tree ensemble machine learning algorithm that uses gradient boosted decision trees. It has a tree learning algorithm that enables to learn from sparse data, and it can analyze data faster than other popular machine learning techniques <sup id=fnref:20><a href=#fn:20 class=footnote-ref role=doc-noteref>20</a></sup>. The gradient boosting algorithm generates one tree at a time based on the previous model’s residuals, and then they are combined to make the final prediction. In our analysis, we generate 150 trees in the final model, where the maximum depth of each tree is three.</p><p>The random forest algorithm is another tree ensemble machine learning algorithm that generates hundreds of decision trees by using a random subset of predictors in the bootstrapped samples. Bootstrapping is a statistical technique that repeatedly draw samples from the data with replacement <sup id=fnref4:10><a href=#fn:10 class=footnote-ref role=doc-noteref>10</a></sup>. Since the same element can appear multiple times in the new sample, this technique generates a large number of new datasets that are not exactly the same as the original model. The average of the decision trees generated from the bootstrapped samples is examined to make the final prediction.</p><p>Random forest can also be used to rank the predictor variables based on its ability to decrease the sum of squared errors when it is chosen to split the data <sup id=fnref:21><a href=#fn:21 class=footnote-ref role=doc-noteref>21</a></sup>. This is an important aspect of random forest, as we can understand which variables are important in the regression model to predict the criterion variable. Due to these advantages, multiple research highlight that random forests have emerged as serious competitors to other machine learning models for predicting numerical and categorical variables <sup id=fnref:22><a href=#fn:22 class=footnote-ref role=doc-noteref>22</a></sup>.</p><p>To implement random forests, we only need to specify the number of trees and the number of features in each split. In terms of the number of trees, it has been shown that implementing many trees will provide a stable result of variable importance <sup id=fnref:23><a href=#fn:23 class=footnote-ref role=doc-noteref>23</a></sup> and using more than the required number of trees does not harm the model <sup id=fnref1:21><a href=#fn:21 class=footnote-ref role=doc-noteref>21</a></sup>. Many studies use p/3 number of features in each split for regression problems, where p is the number of predictor variables <sup id=fnref1:23><a href=#fn:23 class=footnote-ref role=doc-noteref>23</a></sup>.</p><h2 id=25-metric>2.5 Metric</h2><p>Metrics are used to quantify the accuracy of the machine learning model once we obtain the machine learning models. We will examine three commonly used metrics to evaluate the machine learning algorithm, root mean square error (RMSE), mean absolute error (MAE), and R-squared. 10-fold cross-validation is performed to compare the performances of machine learning algorithms. In k-fold cross validation, we split the dataset into k smaller sets with equal number of elements and use k-1 sets to train the model, while the remaining set is used to evaluate the model <sup id=fnref5:10><a href=#fn:10 class=footnote-ref role=doc-noteref>10</a></sup>. We repeat the above iteration thirty times and compute the mean value of the determined metrics in cross validation to determine the optimal machine learning model for predicting the date.</p><h2 id=26-programming>2.6 Programming</h2><p>We use R version 4.1.3 to perform the data analysis and Python version 3.8.3 to obtain the XML dataset from the IIP database <sup id=fnref2:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup>. The dataset and the codes that we use to obtain the dataset are openly available to the public at <a href=https://github.com/daikitag/Inscriptions-of-Israel-Palestine>https://github.com/daikitag/Inscriptions-of-Israel-Palestine</a>.</p><h2 id=3-results>3. Results</h2><h2 id=31-variable-relationship>3.1 Variable Relationship</h2><p>It is important to understand the relationship between the predictor variables in the dataset before conducting machine learning analysis, as we can understand the issues behind effectively analyzing the dataset. Considering that all predictor variables are categorical, we use Pearson&rsquo;s chi-squared test of independence to examine the association between the variables in the dataset. We have examined the association between:</p><p>Language and Location Religion and Location Religion and Language Religion and Text Genre</p><p>The residual plots of the chi-squared test are shown in Figure 3. Results from chi-squared test indicate that there is a significant relationship between all the examined combinations ( <em>p&lt;0.001</em> ).</p><p>The results imply that the predictor variables are correlated with each other, which raises the problem of multicollinearity. Multicollinearity occurs when independent variables in the regression model are correlated with each other <sup id=fnref:24><a href=#fn:24 class=footnote-ref role=doc-noteref>24</a></sup>. One of the key assumptions of the linear regression model is that the predictor variables are uncorrelated. Thus, multicollinearity can undermine the statistical significance of an independent variable and can give inaccurate coefficient estimates when traditional statistical techniques are used <sup id=fnref:25><a href=#fn:25 class=footnote-ref role=doc-noteref>25</a></sup>.</p><figure><img loading=lazy alt="Four matrices each comparing a set of variables. Positive or negative associations shown with colored dots of different sizes." src=/dhqwords/vol/17/2/000681/resources/images/figure03.png sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/17/2/000681/resources/images/figure03_hu6a91fce135ef3ac98bdcb29b768e2028_255097_500x0_resize_box_3.png 500w,
/dhqwords/vol/17/2/000681/resources/images/figure03_hu6a91fce135ef3ac98bdcb29b768e2028_255097_800x0_resize_box_3.png 800w,/dhqwords/vol/17/2/000681/resources/images/figure03_hu6a91fce135ef3ac98bdcb29b768e2028_255097_1200x0_resize_box_3.png 1200w,/dhqwords/vol/17/2/000681/resources/images/figure03.png 1428w" class=landscape><figcaption><p>Residual plot of chi-squared analysis of the dataset. Red color indicates that two variables are negatively associated, and blue color indicates that two variables are positively associated. (a) Chi-squared test between language and location of inscriptions. (b) Chi-squared test between religion and location. (c) Chi-squared test between religion and language. (d) Chi-squared test between religion and text genre.</p></figcaption></figure><p>In contrast, due to recent advances in machine learning techniques, it has been reported that machine learning can better analyze data with multicollinearity than traditional statistical techniques <sup id=fnref:26><a href=#fn:26 class=footnote-ref role=doc-noteref>26</a></sup>. The presence of multicollinearity suggests the usage of machine learning techniques to effectively analyze the dataset.</p><h2 id=32-machine-learning-model-comparison>3.2 Machine Learning Model Comparison</h2><p>A total of eleven regression models are compared: linear regression, ridge regression, lasso regression, elastic net, decision tree, random forest, neural network, SVR linear, SVR radial, SVR polynomial, and XGBoost. The optimal hyperparameters of the machine learning algorithms are determined through 10-fold cross-validation. The cross-validation procedure is repeated 3 times, and the machine learning model is tested by using a total of 30 different datasets, each of which are generated through cross-validation. The evaluation results are shown in Figure 4, where Figure 4 (a) shows the distribution of RMSE from 30 different datasets and Figure 4 (b) shows the mean values of MAE, RMSE and R-squared. MAE and RMSE measure the error of the machine learning model and R-squared is a goodness of fit measure. The random forest model has the lowest value for MAE and RMSE, and has the highest value for R-squared among all models that are examined. This implies that random forest model is the optimal model for predicting the date of inscriptions.</p><h2 id=33-random-forest-model>3.3 Random Forest Model</h2><p>We describe the random forest model in detail, as it is the best model that is implemented in the previous section. We initially convert the number of trees that the random forest model generates from 100 to 1000 to determine the optimal number of trees that we put inside the algorithm, but we do not observe any significant differences. Thus, we select 500 number of trees, as it is the default number of trees in R&rsquo;s randomForest package <sup id=fnref2:23><a href=#fn:23 class=footnote-ref role=doc-noteref>23</a></sup>.</p><figure><img loading=lazy alt="Two side by side figures. 4(a) is a boxplot graph for the RMSE values of each learning method. 4(b) contains MAE, RMSE, and R-squared values for each learning method." src=/dhqwords/vol/17/2/000681/resources/images/figure04.png sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/17/2/000681/resources/images/figure04_hu381569ab9d304acec4c5cc6a10638271_129730_500x0_resize_box_3.png 500w,
/dhqwords/vol/17/2/000681/resources/images/figure04_hu381569ab9d304acec4c5cc6a10638271_129730_800x0_resize_box_3.png 800w,/dhqwords/vol/17/2/000681/resources/images/figure04_hu381569ab9d304acec4c5cc6a10638271_129730_1200x0_resize_box_3.png 1200w,/dhqwords/vol/17/2/000681/resources/images/figure04.png 1348w" class=landscape><figcaption><p>Evaluation of machine learning methods from 10-fold cross validation. (a) The distribution of RMSE from cross validation. (b) Mean values of MAE, RMSE and R-squared from cross validation. The best model for each metric is colored by blue and the worst model is colored by red.</p></figcaption></figure><p>Figure 5 (a) shows the variable importance plot of the random forest model. Variable importance is based upon the mean increase of mean squared error as a result of permuting a given variable <sup id=fnref3:23><a href=#fn:23 class=footnote-ref role=doc-noteref>23</a></sup>. The plot suggests that the material of inscription is the most important variable in the prediction model. The prediction plot is shown in Figure 5 (b). The machine learning model is trained based on the training dataset, which includes 70% of the randomly chosen inscriptions in the data. The prediction plot is created by using the test dataset, which is not used to train the machine learning model.</p><h2 id=4-discussions>4. Discussions</h2><h2 id=41-categorical-analysis>4.1 Categorical Analysis</h2><p>Every region and community in the Mediterranean in antiquity had its own epigraphic characteristics. The statistical analysis reveals some features of different communities within Judea/Roman Palestine. From the residual plot in Figure 3 (a), we can infer that inscription in Judea have a higher probability of being written in Hebrew and inscriptions in Negev have a higher probability of being written in Aramaic. There is also a very strong positive association between Aramaic and Samaria. However, there is a lower probability of Aramaic inscriptions found in the Coastal Plain.</p><figure><img loading=lazy alt="Two side by side figures. 5(a) bar graph showing the %IncMSE value for the variables language, material, region, religion, and text genre. 5(b) scatterplot showing relationship between prediction values and actual year." src=/dhqwords/vol/17/2/000681/resources/images/figure05.png sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/17/2/000681/resources/images/figure05_hue306da1e74082c488fa0a236bf341e2c_218606_500x0_resize_box_3.png 500w,
/dhqwords/vol/17/2/000681/resources/images/figure05_hue306da1e74082c488fa0a236bf341e2c_218606_800x0_resize_box_3.png 800w,/dhqwords/vol/17/2/000681/resources/images/figure05_hue306da1e74082c488fa0a236bf341e2c_218606_1200x0_resize_box_3.png 1200w,/dhqwords/vol/17/2/000681/resources/images/figure05.png 1335w" class=landscape><figcaption><p>(a) Variable importance plot of the random forest model. (b) The relationship between prediction and actual value of the test dataset. The dashed line represents the location where the prediction and the actual value are the same.</p></figcaption></figure><p>When we examine the residual plot in Figure 3 (b), there is a higher possibility of discovering Christian inscriptions in Coastal Plain, Galilee, and Negev. There is a higher possibility of discovering Jewish inscriptions in Judea and Galilee. However, the probability of finding Christian and Jewish inscriptions in Samaria is lower than other regions, and there are many inscriptions from other religions. These results are consistent with what we would expect from other historical sources.</p><p>The residual plot of language and religion is shown in Figure 3 (c). Christian inscriptions have a strong positive association with Greek, but negative association with other languages, specifically Aramaic. Inscriptions written in Aramaic and Hebrew are more likely to be Jewish inscriptions.</p><p>The relationship between text genre and religion is shown in Figure 3 (d). The plot implies that Christian inscriptions tend to be funerary or invocation related compared with other religions, but the probability of Christian inscription being document or legal/economic is lower. It seems that Christian inscriptions tend to be more religious and less administrative.</p><h2 id=42-machine-learning-model>4.2 Machine Learning Model</h2><p>We are able to conclude that the random forest model is the optimal machine learning model for predicting time periods of inscriptions. This is consistent with previous research, as it has been reported that tree-ensemble algorithms like random forests are better to analyze tabular data <sup id=fnref:27><a href=#fn:27 class=footnote-ref role=doc-noteref>27</a></sup>, which is the data type that we use in our project. If we examine the metric values in Figure 4 (b), we see that random forest, XGBoost and decision tree perform better than other models that we have examined. This highlights the importance of using tree based machine learning algorithms to analyze tabular dataset.</p><p>Our study also shows that linear models do not perform well compared with other methods in analyzing tabular datasets which consists of only categorical variables. This might be due to the nonlinear interactions between the variables in the dataset. In terms of SVM, it is important to select the appropriate kernel for each dataset. In our example, we see that SVR with linear kernel performs the worst out of all three kernels that we have examined. However, there are many kinds of kernels in SVM, and it would be a challenging problem to select the optimal kernel for the dataset. The results also suggest that the predictability of neural network is not high compared with tree-based algorithms when we analyze tabular datasets. Many digital humanities datasets are tabular data and they are rarely large enough to effectively train the deep learning algorithm. Our results are consistent with the previous study by Shwartz-Ziv & Armon <sup id=fnref1:27><a href=#fn:27 class=footnote-ref role=doc-noteref>27</a></sup>, where they also showed that tree ensemble methods are better than deep learning techniques to analyze tabular data.</p><p>In contrast, a random forest model can easily be implemented by specifying two hyperparameters of the model, and it is possible for a random forest model to capture the nonlinear interactions inside the dataset. This is a major advantage of random forests, as most machine learning models require fine tuning of hyperparameters <sup id=fnref6:10><a href=#fn:10 class=footnote-ref role=doc-noteref>10</a></sup>.</p><p>In spite of advances in machine learning techniques, it is still necessary to have epigraphers to analyze inscriptions. According to the variable importance plot of the random forest model (Figure 5 (b)), material is the most important variable in making predictions, but we cannot ignore the effects of other variables, including religion and text genre. These variables are subjective, and necessitates the importance of having humans to classify the inscriptions as well. Even in the research project conducted by Assael et al. <sup id=fnref2:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>, they showed that accuracy was the best when the deep learning algorithm was paired up with historians. Machine learning algorithms are still not perfect, so it would be important for us to incorporate knowledge from both human scholars and computers to analyze the dataset effectively.</p><h2 id=5-conclusions>5. Conclusions</h2><p>We show how machine learning techniques can be used to make predictions based on tabular dataset that is comprised of categorical variables. It is uncommon for humanities data to include all elements of a dataset. This could be due to the damage of artifacts over time and many texts being often only available in fragments. Instead of only using one element of the dataset to make predictions, it would be important for us to incorporate other elements in the dataset to effectively date the artifacts. As a next step, we plan to integrate the deep learning framework to the machine learning model that we have created, so that we can incorporate both textual data and tabular data of inscriptions in the prediction model to achieve better accuracy.</p><p>The results of our work indicate that computers can successfully be taught to predict missing characteristics of historical artifacts. The widespread use of machine learning techniques offers exciting prospects in epigraphy and related fields. Even if the dataset is not large, we provide an example in which machine learning techniques can effectively be used to make predictions. In addition to the inscription dataset, our research shows that the machine learning model could be used to analyze other digital humanities dataset which includes a wide range of categorical variables.</p><h2 id=acknowledgements>Acknowledgements</h2><p>We acknowledge computing resources from Columbia University&rsquo;s Shared Research Computing Facility project, which is supported by NIH Research Facility Improvement Grant 1G20RR030893-01, and associated funds from the New York State Empire State Development, Division of Science Technology and Innovation (NYSTAR) Contract C090171, both awarded April 15, 2010. We thank Brown University’s Center for Digital Scholarship for providing the valuable dataset for our research. We would also like to thank the anonymous reviewers, as their suggestions and comments have significantly improved the content and presentation of this paper.</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>Emmanuel, T., Maupong, T., Mpoeleng, D., Semong, T., Mphago, B., and Tabona, O. (2021) “A survey on missing data in machine learning.” <em>Journal of Big Data</em> , 8(1), pp. 1–37.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>Niculae, V., Zampieri, M., Dinu, L., and Ciobanu, A. M. (2014) “Temporal Text Ranking and Automatic Dating of Texts.” <em>Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, Volume 2: Short Papers</em> , pp. 17–21. <a href=https://doi.org/10.3115/v1/E14-4004>https://doi.org/10.3115/v1/E14-4004</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>Assael, Y., Sommerschield, T., Shillingford, B., Bordbar, M., Pavlopoulos, J., Chatzipanagiotou, M., Androutsopoulos, I., Prag, J., and de Freitas, N. (2022) <em>Restoring and attributing ancient texts using deep neural networks.</em> <em>Nature</em> , 603(7900), pp. 280–283.&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref2:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p>LeCun, Y., Bengio, Y., and Hinton, G. (2015) “Deep learning.” <em>Nature</em> , 521 (7553), pp. 436–444.&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5><p>Satlow, M. L. (2022) “Inscriptions of Israel/Palestine.” <em>Jewish Studies Quarterly (JSQ)</em> , 29(4), pp. 349–369. <a href=https://doi.org/10.1628/jsq-2022-0021>https://doi.org/10.1628/jsq-2022-0021</a>&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref2:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:6><p>Elliott, Tom, Bodard, Gabriel, and Cayless, Hugh et al. (2006, 2022) <em>EpiDoc: Epigraphic Documents in TEI XML</em> . <a href=https://epidoc.stoa.org/>https://epidoc.stoa.org/</a>&#160;<a href=#fnref:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:7><p>Johnson, J. M., and Khoshgoftaar, T. M. (2019) “Survey on deep learning with class imbalance.” <em>Journal of Big Data</em> , 6(1), 27. <a href=https://doi.org/10.1186/s40537-019-0192-5>https://doi.org/10.1186/s40537-019-0192-5</a>&#160;<a href=#fnref:7 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:8><p>Claesen, M., and De Moor, B. (2015) <em>Hyperparameter Search in Machine Learning</em> (arXiv:1502.02127). arXiv. <a href=https://doi.org/10.48550/arXiv.1502.02127>https://doi.org/10.48550/arXiv.1502.02127</a>&#160;<a href=#fnref:8 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:9><p>Claesen, M., Simm, J., Popovic, D., Moreau, Y., and De Moor, B. (2014) <em>Easy Hyperparameter Search Using Optunity</em> (arXiv:1412.1114). arXiv. <a href=https://doi.org/10.48550/arXiv.1412.1114>https://doi.org/10.48550/arXiv.1412.1114</a>&#160;<a href=#fnref:9 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:10><p>Hastie, T., Tibshirani, R., Friedman, J. H., and Friedman, J. H. (2009) <em>The Elements of Statistical Learning: Data Mining, Inference, and Prediction</em> (Vol. 2). Springer.&#160;<a href=#fnref:10 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:10 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref2:10 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref3:10 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref4:10 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref5:10 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref6:10 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:11><p>Zou, H., & Hastie, T. (2005). “Regularization and variable selection via the elastic net.” <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em> , 67(2), pp. 301–320.&#160;<a href=#fnref:11 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:12><p>Finegold, M., Otis, J., Shalizi, C., Shore, D., Wang, L., and Warren, C. (2016) “Six degrees of Francis Bacon: A statistical method for reconstructing large historical social networks.” <em>Digital Humanities Quarterly</em> , 10(3).&#160;<a href=#fnref:12 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:13><p>Drucker, H., Burges, C. J., Kaufman, L., Smola, A., and Vapnik, V. (1996) “Support vector regression machines.” <em>Advances in Neural Information Processing Systems</em> , 9.&#160;<a href=#fnref:13 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:13 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:14><p>Argamon, S., Goulain, J.-B., Horton, R., and Olsen, M. (2009) “Vive la différence! Text mining gender difference in french literature.” <em>Digital Humanities Quarterly</em> , 3(2).&#160;<a href=#fnref:14 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:15><p>Goodfellow, I., Bengio, Y., and Courville, A. (2016) <em>Deep Learning</em> . MIT press.&#160;<a href=#fnref:15 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:16><p>Brigato, L., and Iocchi, L. (2021) “A Close Look at Deep Learning with Small Data.” <em>2020 25th International Conference on Pattern Recognition (ICPR)</em> , pp. 2490–2497. <a href=https://doi.org/10.1109/ICPR48806.2021.9412492>https://doi.org/10.1109/ICPR48806.2021.9412492</a>&#160;<a href=#fnref:16 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:17><p>Borisov, V., Leemann, T., Seßler, K., Haug, J., Pawelczyk, M., & Kasneci, G. (2022) “Deep Neural Networks and Tabular Data: A Survey.” <em>IEEE Transactions on Neural Networks and Learning Systems</em> , pp. 1–21. <a href=https://doi.org/10.1109/TNNLS.2022.3229161>https://doi.org/10.1109/TNNLS.2022.3229161</a>&#160;<a href=#fnref:17 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:18><p>Baledent, A., Hiebel, N., and Lejeune, G. (2020) “Dating ancient texts: An approach for noisy French documents.” <em>Language Resources and Evaluation Conference (LREC) 2020</em> .&#160;<a href=#fnref:18 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:19><p>Fragkiadakis, M., Nyst, V., and Putten, P. van der. (2021) “Towards a User-Friendly Tool for Automated Sign Annotation: Identification and Annotation of Time Slots, Number of Hands, and Handshape.” <em>Digital Humanities Quarterly</em> , 15(1).&#160;<a href=#fnref:19 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:20><p>Chen, T., and Guestrin, C. (2016) “XGBoost: A Scalable Tree Boosting System.” <em>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em> , pp. 785–794. <a href=https://doi.org/10.1145/2939672.2939785>https://doi.org/10.1145/2939672.2939785</a>&#160;<a href=#fnref:20 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:21><p>Breiman, L. (2001). “Random forests.” <em>Machine Learning</em> , 45(1), pp. 5–32.&#160;<a href=#fnref:21 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:21 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:22><p>Belgiu, M., and Drăguţ, L. (2016) Random forest in remote sensing: A review of applications and future directions. <em>ISPRS Journal of Photogrammetry and Remote Sensing</em> , 114, pp. 24–31.&#160;<a href=#fnref:22 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:23><p>Liaw, A., and Wiener, M. (2002) “Classification and regression by randomForest.” <em>R News</em> , 2(3), pp. 18–22.&#160;<a href=#fnref:23 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:23 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref2:23 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref3:23 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:24><p>Alin, A. (2010) “Multicollinearity.” <em>Wiley Interdisciplinary Reviews: Computational Statistics</em> , 2(3), pp. 370–374.&#160;<a href=#fnref:24 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:25><p>Allen, M. P. (1997) “The problem of multicollinearity” . In Allen, M.P. <em>Understanding Regression Analysis</em> . Berlin: Springer, pp. 176-180.&#160;<a href=#fnref:25 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:26><p>Chan, J. Y. L., Leow, S. M. H., Bea, K. T., Cheng, W. K., Phoong, S. W., Hong, Z. W., and Chen, Y. L. (2022) “Mitigating the multicollinearity problem and its machine learning approach: a review” . <em>Mathematics</em> , 10(8), 1283.&#160;<a href=#fnref:26 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:27><p>Shwartz-Ziv, R., and Armon, A. (2022) “Tabular data: Deep learning is not all you need.” <em>Information Fusion</em> , 81, pp. 84–90.&#160;<a href=#fnref:27 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:27 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div></div></article></main><footer><nav aria-label="footer links"><ul><li><a class=highlight-focus href=/dhqwords/tags/>Tags</a></li><li><a class=highlight-focus href=/dhqwords/about/>About</a></li><li><a class=highlight-focus href=/dhqwords/categories/>Keywords</a></li></ul></nav><div class=icons><a class="license highlight-focus" rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons Attribution No-Derivatives 4.0 International License" src=/dhqwords/img/logos/license.svg width=120 height=42></a></div></footer></body></html>