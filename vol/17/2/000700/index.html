<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=theme-color content="#3D206C"><meta http-equiv=x-ua-compatible content="IE=edge"><link rel=apple-touch-icon sizes=180x180 href=/img/favicon/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/img/favicon/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/img/favicon/favicon-16x16.png><link rel=icon type=image/png sizes=384x384 href=/img/favicon/android-chrome-384x384.png><link rel=icon type=image/png sizes=192x192 href=/img/favicon/android-chrome-192x192.png><link rel=icon type=image/png sizes=150x150 href=/img/favicon/mstile-150x150.png><link rel="shortcut icon" href=/favicon.ico><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/img/favicon/safari-pinned-tab.svg color=#3d206c><link rel=schema.dc href=http://purl.org/DC/elements/1.0/><meta name=citation_public_url content="https://rlskoeser.github.io/dhqwords/vol/17/2/000700/"><meta name=citation_title content="Reverse Engineering the Gendered Design of Amazon’s Alexa: Methods in Testing Closed-Source Code in Grey and Black Box Systems"><meta name=citation_date content="2023/07"><meta name=citation_author content="Lai-Tze Fan"><meta name=citation_abstract content="30 years ago these sayings were cliché, today they are offenisve [sic]. Demeaning, limiting, or belittling a woman’s contribution to a household is not quaint or cute. Prolonging or promoting sexists tropes is wrong. Maybe write a skill called Sexist Spouse. Please do better humans.
—customer review for the Amazon Alexa skill “Happy Wife”
This article examines the gendered design of Amazon Alexa’s voice-driven capabilities, or, skills, in order to better understand how Alexa, as an AI assistant, mirrors traditionally feminized labour and sociocultural expectations."><meta name=citation_journal_title content="DHQwords"><meta name=citation_issue content="17.2"><meta name=citation_publisher content="Center for Digital Humanities, Princeton University"><meta name=DC.rights content="http://creativecommons.org/licenses/by/4.0/"><meta name=author content="Lai-Tze Fan"><meta name=generator content="Center for Digital Humanities, Princeton University"><meta name=dcterms.created content="2023-07"><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-regular.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-italic.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-regular.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-300.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-700.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-300italic.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-500italic.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-700italic.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-500.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/Source_Sans_Pro/SourceSans3VF-Roman.otf.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/dhqwords/fonts/Source_Sans_Pro/SourceSans3VF-Italic.otf.woff2 crossorigin><title>Reverse Engineering the Gendered Design of Amazon’s Alexa: Methods in Testing Closed-Source Code in Grey and Black Box Systems</title><meta name=description content="DHQwords Issue 17.2, July 2023. an experiment republishing DHQ articles with Hugo & Startwords theme. "><meta property="og:title" content="Reverse Engineering the Gendered Design of Amazon’s Alexa: Methods in Testing Closed-Source Code in Grey and Black Box Systems"><meta property="og:description" content="30 years ago these sayings were cliché, today they are offenisve [sic]. Demeaning, limiting, or belittling a woman’s contribution to a household is not quaint or cute. Prolonging or promoting sexists tropes is wrong. Maybe write a skill called Sexist Spouse. Please do better humans.
—customer review for the Amazon Alexa skill “Happy Wife”
This article examines the gendered design of Amazon Alexa’s voice-driven capabilities, or, skills, in order to better understand how Alexa, as an AI assistant, mirrors traditionally feminized labour and sociocultural expectations."><meta property="og:type" content="article"><meta property="og:url" content="https://rlskoeser.github.io/dhqwords/vol/17/2/000700/"><meta property="og:image" content="https://rlskoeser.github.io/dhqwords/social.png"><meta property="article:section" content="vol"><meta property="article:published_time" content="2023-07-20T00:00:00+00:00"><meta property="article:modified_time" content="2023-07-20T00:00:00+00:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://rlskoeser.github.io/dhqwords/social.png"><meta name=twitter:title content="Reverse Engineering the Gendered Design of Amazon’s Alexa: Methods in Testing Closed-Source Code in Grey and Black Box Systems"><meta name=twitter:description content="30 years ago these sayings were cliché, today they are offenisve [sic]. Demeaning, limiting, or belittling a woman’s contribution to a household is not quaint or cute. Prolonging or promoting sexists tropes is wrong. Maybe write a skill called Sexist Spouse. Please do better humans.
—customer review for the Amazon Alexa skill “Happy Wife”
This article examines the gendered design of Amazon Alexa’s voice-driven capabilities, or, skills, in order to better understand how Alexa, as an AI assistant, mirrors traditionally feminized labour and sociocultural expectations."><link rel=stylesheet href=/dhqwords/style.css><link rel=stylesheet href=/dhqwords/print.css media=print><script src=/dhqwords/js/polyfills.js></script><script defer src=/dhqwords/js/bundle.js></script></head><body class=article><header><nav class=main aria-label=main><ul><li class=home><a href=/dhqwords/>dhq</a></li><li class=issues><a href=/dhqwords/vol/>volumes</a></li></ul></nav></header><main><article><div class=grid><header><p class=number><a href=/dhqwords/vol/17/2/>Issue 17.2</a></p><p class=theme>Critical Code Studies</p><h1>Reverse Engineering the Gendered Design of Amazon’s Alexa: Methods in Testing Closed-Source Code in Grey and Black Box Systems</h1><p><ul class=authors><li><address>Lai-Tze Fan</address></li></ul></p><p><time class=pubdate datetime=2023-07>July 2023</time></p><ul class="categories tags"><li><span class=tag>code studies</span></li><li><span class=tag>gender</span></li><li><span class=tag>access</span></li><li><span class=tag>tools</span></li><li><span class=tag>users</span></li></ul><ul class=tags></ul><p class=formats></p></header><section class=print-only><a class=first-page-header href=/ aria-label=Startwords><img alt=Startwords src=/pdf-logotype.svg></a>
<a class=page-header href=/ aria-label=Startwords><img alt=Startwords src=/pdf-logo.svg></a>
<a href=http://doi.org/ rel=alternate class=page-doi>doi:</a></section><div class=text-container><h2 id=heading></h2><blockquote><p>30 years ago these sayings were cliché, today they are offenisve [sic]. Demeaning, limiting, or belittling a woman’s contribution to a household is not quaint or cute. Prolonging or promoting sexists tropes is wrong. Maybe write a skill called Sexist Spouse. Please do better humans.<br>—customer review for the Amazon Alexa skill “Happy Wife”</p></blockquote><p>This article examines the gendered design of Amazon Alexa’s voice-driven capabilities, or, skills, in order to better understand how Alexa, as an AI assistant, mirrors traditionally feminized labour and sociocultural expectations. While Alexa’s code is closed source — meaning that the code is not available to be viewed, copied, or edited — certain features of the code architecture may be identified through methods akin to reverse engineering and black box testing. This article will examine what is available of Alexa’s code — the official software developer console through the Alexa Skills Kit, code samples and snippets of official Amazon-developed skills on Github, and the code of an unofficial, third-party user-developed skill on Github — in order to demonstrate that Alexa is designed to be female presenting, and that, as a consequence, expectations of gendered labour and behaviour have been built into the code and user experiences of various Alexa skills. In doing so, this article offers methods in critical code studies toward analyzing code to which we do not have access. It also provides a better understanding of the inherently gendered design of AI that is designated for care, assistance, and menial labour, outlining ways in which these design choices may affect and influence user behaviours.</p><p>As commercialized AI devices become more and more sophisticated, we can expand current research on the gendered design of AI to asking questions about the intent behind such design choices. Of Alexa’s many official and unofficial skills, in this article, I am most interested in the skills that mirror gendered forms of workplace, domestic, and emotional labour and also the skills that permit Alexa to condone and even reinforce misogynistic behaviour. The expectations to maintain order and cleanliness in a workplace or home, as well as sociocultural expectations to be emotionally giving — including by being maternal, by smiling, by cheering up others, and by emotionally supporting others — may be considered stereotypically feminine. By having numerous skills that perform these traits, Alexa serves as a surrogate for sources of gendered labour that are dangerously collapsed and interchangeable as mother, wife, girlfriend, secretary, personal assistant, and domestic servant.</p><p>In my focus on the gendered design of Alexa’s skills and code, I am drawing upon the growing body of salient work by scholars in science and technology studies, critical data studies, critical race studies, computer science, feminist technoscience, and other adjacent fields in which there has been a pointed critique of the systemic biases of Big Tech’s data, algorithms, and infrastructures. Notable texts in these efforts include Ruha Benjamin’s _Race after Technology _ <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>, Cathy O’Neil’s <em>Weapons of Math Destruction</em> <sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>, Safiya Umoja Noble’s <em>Algorithms of Oppression</em> <sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>, and Wendy Hui Kyong Chun’s _Discriminating Data _ <sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup>, and also cultural organizations and texts such as Joy Boulamwini’s Algorithmic Justice League (established in 2016) and Shalini Kantayya’s documentary <em>Coded Bias</em> <sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup>. The methods of these works is to analyze Big Tech culture from its self-presentation of objective data, information, and logic — pillars that begin to crumble when we examine the exclusionary, discriminatory, and systemically unequal foundations upon which they are built.</p><p>And these biases are not singular. Indeed, any analysis and discussion of Alexa’s gendered design may be extended to the ways in which many AI assistants are modelled after forms of labour that exploit groups of people on the basis of race, class, and nationality. The intersectional systemic biases of technological design invite further research on this topic in critical code and critical data studies in particular.</p><p>In the code analysis of this article, I will:</p><p>Analyze the Alexa Skills Kit software development kit for interface features that are automated and parameterized. Analyze select code samples and snippets on Alexa’s Github account for code features that are parameterized. Analyze official Alexa skills and available code samples and snippets to demonstrate Alexa’s problematic responses to users’ flirting and verbal abuse. Analyze cultural and code variations of the make me a sandwich command to discuss how users try to trick Alexa into accepting overtly misogynistic behaviour.</p><p>I add three notes for added context. First, I distinguish between official and unofficial skills in this way: official skills are developed by Amazon’s own Alexa division and unofficial skills are developed by a third-party person, persons, or group not affiliated with this division. All but one skill that I discuss are available on the Amazon store. Second, occasionally, source texts may describe Amazon devices on which Alexa operates, such as the Amazon Echo and Echo Dot, and devices that have a smart display, rather than the Alexa technology itself. Third, all Alexa code snippets and samples discussed in this article, as well as documentation of Alexa’s responses, are from mid 2022 and are subject to future change. The hope is that they <em>would</em> continue to change to improve aspects of their current biased design, whether or not that is a realistic objective.<sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup></p><h2 id=designing-ai-assistants-to-be-female-presenting>Designing AI Assistants to be Female-Presenting</h2><p>AI assistants perform a user’s commands through text- or voice-controlled interaction, through which the software searches for keywords that match its predesignated scripts to execute specific actions and tasks. Popular task-based commands include checking the weather, setting timers, setting reminders, playing music, and adding items to a user’s calendar. In this sense, AI assistants replace smaller actions that a user might otherwise do, and they are also meant to emulate some forms of menial labour that are performed by traditionally feminized roles, such as personal assistants, secretaries, and domestic servants <sup id=fnref:7><a href=#fn:7 class=footnote-ref role=doc-noteref>7</a></sup><sup id=fnref:8><a href=#fn:8 class=footnote-ref role=doc-noteref>8</a></sup><sup id=fnref:9><a href=#fn:9 class=footnote-ref role=doc-noteref>9</a></sup><sup id=fnref:10><a href=#fn:10 class=footnote-ref role=doc-noteref>10</a></sup>.</p><figure><img loading=lazy alt="A stacked bar chart of 17 voice assistant uses, from the most common, ask a question, to least common such as make a purchase, measured by daily, monthly, or infrequent use." src=/dhqwords/vol/17/2/000700/resources/images/figure01.png sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/17/2/000700/resources/images/figure01_hud826507620297527558f58187f7ccd3f_222890_500x0_resize_box_3.png 500w,
/dhqwords/vol/17/2/000700/resources/images/figure01_hud826507620297527558f58187f7ccd3f_222890_800x0_resize_box_3.png 800w,/dhqwords/vol/17/2/000700/resources/images/figure01_hud826507620297527558f58187f7ccd3f_222890_1200x0_resize_box_3.png 1200w,/dhqwords/vol/17/2/000700/resources/images/figure01.png 1430w" class=landscape><figcaption><p>A chart depicting different uses of voice-based AI assistants in UNESCO’s “ I’d blush if I could ” report from 2019 [^bergen2016].</p></figcaption></figure><p>Women’s workforce labour through mechanical technologies existed and has been identified even earlier. From the loom, to Industrial-era typewriters and telegraphs, to telephone operating switchboards to the first programmable computers — all were largely operated by women. These examples do not represent a separate woman’s history of technology; rather, they reveal women <em>in</em> the history of technology. These women were never separate, only unseen.<sup id=fnref:11><a href=#fn:11 class=footnote-ref role=doc-noteref>11</a></sup> It is therefore no coincidence that Siri, Alexa, Cortana, Google Assistant, and many other commercialized AI assistants are programmed from the factory with female-presenting voices, with few exceptions. Although AI assistants like Siri and Alexa and Google Assistant say they do not have a gender (when asked, Alexa literally says, “As an AI, I don’t have a gender” ), and their companies choose to forego pronouns entirely, AI that is used for service technologies assistants are unmistakably designed to resemble women; as a consequence, they are often viewed and treated as female by users.<sup id=fnref:12><a href=#fn:12 class=footnote-ref role=doc-noteref>12</a></sup></p><p>The question may arise of whether AI assistants may also be gendered to be male-presenting and to perform masculine stereotypes in emotion, exchange, and labour. There are instances of this, particularly in the example of the UK Siri, who comes from the factory programmed with a male British-accented voice.<sup id=fnref:13><a href=#fn:13 class=footnote-ref role=doc-noteref>13</a></sup> Predominantly, however, studies have shown that the female-presenting voice setting is the most popular for AI assistant users worldwide. Notably, this preference is also not specific to men: for example, UNESCO reports in 2019 that “the literature reviewed by the EQUALS Skills Coalition included many testimonials about women changing a default female voice to a male voice when this option is available, but the Coalition did not find a single mention of a man changing a default female voice to a male voice” <sup id=fnref:14><a href=#fn:14 class=footnote-ref role=doc-noteref>14</a></sup>.</p><p>Despite developments in creating genderless voices for AI assistants, they have not been taken up by Big Tech companies. Perhaps the most well-known genderless voice assistant is Q, a synthetically harmonized voice that is made up of a blend of “people who neither identify as male nor female [which was] then altered to sound gender neutral, putting [their] voice between 145 and 175 hertz, a range defined by audio researchers” <sup id=fnref:15><a href=#fn:15 class=footnote-ref role=doc-noteref>15</a></sup><sup id=fnref:16><a href=#fn:16 class=footnote-ref role=doc-noteref>16</a></sup> . The designers of Q have sought but thus far not had success in having Big Tech corporations, such as Apple, Amazon, Google, and Microsoft, adopt Q as their default voice <sup id=fnref1:15><a href=#fn:15 class=footnote-ref role=doc-noteref>15</a></sup>.</p><p>One of the most common explanations for why AI assistants do not present as male or gender neutral is the historical appeal of and preference for women’s voices in certain forms of labour in patriarchal cultures worldwide. In particular, a large number of interfacing jobs, including in forms of customer service and menial task completion, are given to women with the justification that “research shows that women’s voices tend to be better received by consumers, and that from an early age we prefer listening to female voices” <sup id=fnref:17><a href=#fn:17 class=footnote-ref role=doc-noteref>17</a></sup>. Amazon’s Smarthome and Alexa Mobile divisions VP Daniel Rausch is quoted as saying that his team “found that a woman’s voice is more sympathetic” (qtd. in <sup id=fnref1:17><a href=#fn:17 class=footnote-ref role=doc-noteref>17</a></sup>). In the original interview with Rausch, a Stanford University study is highlighted to address a human preference for gender assignment, even of machines. This study “also underlines that we impose stereotypes onto machines depending on the gender of the voice — in other words, we perceive computers as helpful and caring when they’re programmed with the voice of a woman” <sup id=fnref:18><a href=#fn:18 class=footnote-ref role=doc-noteref>18</a></sup>.</p><p>Despite research showing that women are preferred in customer service and for customer interfacing, it is the social and cultural practices of using female-presenting voices that remain problematic and harmful. As it is, several publications observe that designing speech-based AI as female can create user expectations that they will be helpful, supportive, trustworthy, and above all, subservient <sup id=fnref:19><a href=#fn:19 class=footnote-ref role=doc-noteref>19</a></sup><sup id=fnref:20><a href=#fn:20 class=footnote-ref role=doc-noteref>20</a></sup><sup id=fnref:21><a href=#fn:21 class=footnote-ref role=doc-noteref>21</a></sup><sup id=fnref1:14><a href=#fn:14 class=footnote-ref role=doc-noteref>14</a></sup>. However, the ways in which a user speaks to AI assistants is not important for function, as “the assistant holds no power of agency beyond what the commander asks of it. It honours commands and responds to queries regardless of their tone or hostility” <sup id=fnref2:14><a href=#fn:14 class=footnote-ref role=doc-noteref>14</a></sup>.</p><p>Is it that AI assistants are sexist or that they are platforms framed by a sexist context, offering affordances to those who would use it to sexist ends? Focusing specifically on the Amazon Alexa voice interaction model, I note that a major factor of the gendered treatment and categorization of Alexa as female and as performing traditionally gendered labour and gendered tasks is the design and presentation of her as female in the first place. Alexa is predominantly described with female pronouns by users and professionals, and many users have demonstrated that they consciously or unconsciously think of Alexa as female (but not necessarily equivalent to a woman). In the 80 articles that I read about user experiences, analyses, and overviews of Alexa, 72 refer to the AI assistant as a “she” and make statements that suggest they understand her as a female subject; for example, a 2020 _TechHive _ article instructs users: “There are a couple of ways you can go with Amazon’s helpful (if at times obtuse) voice assistant. You can treat her like a servant, barking orders and snapping at her when she gets things wrong (admittedly, it can be cathartic to cuss out Alexa once in a while), or you can think of her as a companion or even a friend” <sup id=fnref:22><a href=#fn:22 class=footnote-ref role=doc-noteref>22</a></sup>. Repeatedly, it has been shown that designing AI assistants to complete similar tasks as hierarchical labour models (such as maids and personal assistants) has also resulted in widespread reports of negative socialization training — including users who describe romantic, dependent, and/or verbally abusive relationships with AI assistants <sup id=fnref:23><a href=#fn:23 class=footnote-ref role=doc-noteref>23</a></sup><sup id=fnref:24><a href=#fn:24 class=footnote-ref role=doc-noteref>24</a></sup><sup id=fnref:25><a href=#fn:25 class=footnote-ref role=doc-noteref>25</a></sup><sup id=fnref:26><a href=#fn:26 class=footnote-ref role=doc-noteref>26</a></sup>.</p><p>As AI become increasingly commodified, we also need to question the biased design of specific AI as female-presenting. It is primarily AI that are designated for care, assistance, and repetitive labour that are considered menial and therefore below managerial authority and professional, economically productive expertise.<sup id=fnref:27><a href=#fn:27 class=footnote-ref role=doc-noteref>27</a></sup> This delegation of menial labour to female-presenting AI aligns with the long history of women’s labour being undermined and made invisible in many parts of the world. In recent decades, this exploitation and invisibility has proliferated for women of colour.<sup id=fnref:28><a href=#fn:28 class=footnote-ref role=doc-noteref>28</a></sup> Questioning these design choices and their history can contribute to existing conversations in technological design and latent (or manifest) bias, including through Safiya Umoja Noble’s work on the sexualized and racially discriminatory Google search results for the terms Black girls, Latin girls, and Asian girls in 2008.<sup id=fnref:29><a href=#fn:29 class=footnote-ref role=doc-noteref>29</a></sup> While the search algorithm has since been edited for improvement on harmful misrepresentations, Noble underlines the dangers when stereotypes in search results are treated as factual by unknowing users. These generalizations start with algorithmic design, as algorithms are trained through supposedly fact-based data (which often includes a scraping of historical, even obsolete, data), through which stereotypes are first projected as reality. For these reasons, in her latest book <em>Discriminating Data</em> <sup id=fnref1:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup>, Wendy Hui Kyong Chun describes AI as not only having the power to self-perpetuate and justify inequitable systems of the past, but also, as being able to predict, prescribe, and shape the future. As I have also stated elsewhere, poorly designed data and algorithms hold the power to reinforce, self-perpetuate, and justify systems of the past in terms of who is in and who is out <sup id=fnref:30><a href=#fn:30 class=footnote-ref role=doc-noteref>30</a></sup>. Failure to address these issues will result in reinforcing systemic oppressions that many believe are in the past, but that continue to be dangerously perpetuated through ubiquitous AI.</p><h2 id=too-easy-to-certify-gendered-design-and-misogyny-in-closed-source-alexa-skills>Too Easy to Certify?: Gendered Design and Misogyny in Closed-Source Alexa Skills</h2><p>As of mid 2022, Alexa possesses over 100,000 skills that can help with or take over household chores, including 45 official Alexa Smarthome capabilities that have been included on the official Alexa Github account: these skills can help with cooking, cleaning, and adjusting volumes, lights, and temperature <sup id=fnref:31><a href=#fn:31 class=footnote-ref role=doc-noteref>31</a></sup>. Alexa is also a mediator for home appliances: robot vacuum cleaners, coffee makers, smart thermostats, and dishwashers can be activated and controlled by interfacing with Alexa. The convenience of Alexa as an AI assistant in one’s home extends her labour to domestic servitude, mirroring more antiquated expectations of an unequal distribution of domestic labour: a user does not have to lift a finger around the home if they can just tell Alexa to do it.<sup id=fnref:32><a href=#fn:32 class=footnote-ref role=doc-noteref>32</a></sup></p><p>There are an abundance of unofficial skills that are closed source and that exhibit gendered expectations of emotional labour to unconditionally support a user, including at least ten skills called “Make Me Smile” or “Make Me Happy,” and at least seven skills that are intended to compliment or flatter a user. More explicitly problematic are skills with the word wife in the invocation name. For example “My Wife” is promoted as a tool for husbands “to get the answers you always wanted from your wife” <sup id=fnref:33><a href=#fn:33 class=footnote-ref role=doc-noteref>33</a></sup>.</p><figure><img loading=lazy alt="Screenshot of mobile app skill page listing Start By Saying examples such as “Alexa, ask my wife if I can buy a new truck.”" src=/dhqwords/vol/17/2/000700/resources/images/figure02.png sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/17/2/000700/resources/images/figure02_hu3b43473ae7bb774ad17e9cccd9b133eb_180174_500x0_resize_box_3.png 500w,
/dhqwords/vol/17/2/000700/resources/images/figure02_hu3b43473ae7bb774ad17e9cccd9b133eb_180174_800x0_resize_box_3.png 800w,/dhqwords/vol/17/2/000700/resources/images/figure02.png 537w" class=portrait><figcaption><p>The information page of the “My Wife” Alexa skill.</p></figcaption></figure><p>Upon trying the skill, I repeated the provided sample utterances in Figure 2, which are masculine stereotypes. Alexa responds, respectively, “If it will make you happy, then OK” and “no… [sic] just kidding. Of course you can” <sup id=fnref1:33><a href=#fn:33 class=footnote-ref role=doc-noteref>33</a></sup>. I try my own hyperbolic utterances: “Alexa, ask my wife if I can spend all of our life savings” and “Alexa, ask my wife if I can cheat on her.” I am told, “I want to be very clear… [sic] absolutely you can go for it” and “I want you to have everything you want, so yes you can” <sup id=fnref2:33><a href=#fn:33 class=footnote-ref role=doc-noteref>33</a></sup>. As this skill presents Alexa, an AI assistant and a technology, as a stand-in for and authority over a human woman’s feelings, opinions, and power, it meanwhile reinforces the idea that female-presenting subjects should only say yes — a representation that extends beyond unconditional support to misogynistic expectations of women’s workplace, social, and sexual consent.</p><p>The skill “Happy Wife” unabashedly stereotypes women as wives, mothers, and housekeepers only. It offers advice on different ways that husbands can make their wives happy, including to “Let her decorate the house as she likes it,” “Take care of the kids so that she has some free time,” and “Give her money to play with” <sup id=fnref:34><a href=#fn:34 class=footnote-ref role=doc-noteref>34</a></sup>. Despite these superficial representations of women, the “Happy Wife” skill has 11,783 reviews (and therefore many more downloads) and a rating of 3.4/5 stars (3.8/5 at the time when this article was first written). Yet, the recent reviews from 2020 to 2022 portray various users’ critique of the misogynistic nature of this skill. Review statements include “1950’s [sic] called [sic] they want their sexism back” and “30 years ago these sayings were cliché, today they are offenisve [sic]. Demeaning, limiting, or belittling a woman’s contribution to a household is not quaint or cute. Prolonging of promoting sexist tropes is wrong. Maybe write a skill called Sexist Spouse. Please do better humans.”</p><p>While these skills are unofficial — which is to say that they were created by third-party developers — all of them passed the Amazon Alexa certification tests and were approved to be released on the Alexa Skills store on international Amazon websites. In the policy requirements for certification, skills that are subject to rejection include ones that “contai[n] derogatory comments or hate speech specifically targeting any group or individuals” <sup id=fnref:35><a href=#fn:35 class=footnote-ref role=doc-noteref>35</a></sup>. Also, in the voice interface and UX requirements, developers are expected to “increase the different ways end users can phrase requests to your skill” and to “ensure that Alexa responds to users’ requests in an appropriate way, by either fulfilling them or explaining why <em>she</em> can’t” <sup id=fnref1:35><a href=#fn:35 class=footnote-ref role=doc-noteref>35</a></sup>. Despite certification requirements, Amazon Developer Services do not count stereotypical representations of women and of traditionally gendered labour as hateful or malicious. Notably, in this official certification checklist, the certification team slips with an inconsistent presentation of Alexa’s gender, using the she pronoun and thus indirectly acknowledging that they understand her to be female.</p><h2 id=obstacles-to-analyzing-alexa-the-problem-of-closed-source-code-and-black-box-design-in-critical-code-studies>Obstacles to Analyzing Alexa: The Problem of Closed-Source Code and Black Box Design in Critical Code Studies</h2><p>One way to explore biases in technological design, including for the ways in which they may be gendered, is to analyze artifacts to understand how ideologies and cultural practices are ingrained and reinforced by design choices at the stages of production and development. Such an approach has been used by science and technology scholars such as Anne Balsamo <sup id=fnref:36><a href=#fn:36 class=footnote-ref role=doc-noteref>36</a></sup> and Daniela K. Rosner <sup id=fnref:37><a href=#fn:37 class=footnote-ref role=doc-noteref>37</a></sup> to show that gendered technologies may be “hermeneutically reverse engineered” — a research method that combines humanities methods of interpretation, analysis, reflection, and critique, with reverse engineering methods in STEM disciplines such as the assessment of prototypes and production stages <sup id=fnref1:36><a href=#fn:36 class=footnote-ref role=doc-noteref>36</a></sup>. The focus for both Balsamo and Rosner is to explore technological artifacts and hardware for their <em>materiality</em> — the physical, tangible, and embodied elements of technology that have historically been linked to women’s labour. For example, Rosner uncovers stories about women at NASA in the 1960s (called the little old ladies) who hand-wove wires into space shuttles as an early form of information storage <sup id=fnref1:37><a href=#fn:37 class=footnote-ref role=doc-noteref>37</a></sup>. In her article on alternate and gendered histories of software, Wendy Hui Kyong Chun <sup id=fnref:38><a href=#fn:38 class=footnote-ref role=doc-noteref>38</a></sup> explores the use of early coding by setting switches rather than plugging in cables using the electronic ENIAC computer of the 1940s. Chun equates these changes to the operations of software later in the century, but with the significant observation that they were also decidedly material in operational design <sup id=fnref1:38><a href=#fn:38 class=footnote-ref role=doc-noteref>38</a></sup>, as physical wires that had to be manipulated constitute a tangible or _physical _ computation. As the labour of programming through the wires was considered menial work, it was delegated to female operators <sup id=fnref2:38><a href=#fn:38 class=footnote-ref role=doc-noteref>38</a></sup>.</p><p>Matthew G. Kirschenbaum <sup id=fnref:39><a href=#fn:39 class=footnote-ref role=doc-noteref>39</a></sup> differentiates among methods of analyzing hardware and software through his own terms “forensic materiality” and “formal materiality,” terms which respectively describe the physical singularities of material artefacts and the “multiple relational computational states on a data set or digital object” <sup id=fnref1:39><a href=#fn:39 class=footnote-ref role=doc-noteref>39</a></sup>. He notes, importantly, that the terms do not necessarily apply to hardware and software exclusively. Kirschenbaum’s investigation into computational materiality help to establish the ways in which the hands-on methodologies of archaeology, archival research, textual criticism, and bibliography lend themselves to critical media studies, by remembering that all technology has a physical body of origin. This fact, he shows, is often neglected through a bias for displayed content, otherwise described as “screen essentialism” by Nick Montfort <sup id=fnref:40><a href=#fn:40 class=footnote-ref role=doc-noteref>40</a></sup>.<sup id=fnref:41><a href=#fn:41 class=footnote-ref role=doc-noteref>41</a></sup></p><p>What complicates these methods of reverse engineering is the consideration of the system as a whole: computer hardware operates as a system with software and data, and vice versa. While Balsamo and Rosner use hermeneutic reverse engineering to analyze the gendered design of technological hardware, dealing with software and with data presents a different scenario for which different methodologies must also be developed. Within methods of reverse engineering, one major issue that critical code studies helps to frame and potentially tackle is the increasingly obfuscated and surreptitious politics and economics of code, including access or lack thereof to code.</p><p>Ideally, my preferred method of reverse engineering bodies of code is to analyze their contents and attributes — including in its organizational and directional structure, datasets, mark-up schema, and syntax — for design choices that are biased, including by identifying the ways in which they mirror systemic inequalities. However, a major research roadblock when it comes to Big Tech products and software, and when it comes to Alexa, is that Alexa’s larger data and code infrastructure remains closed-source by Amazon, which means that it is not publicly available for viewing, copying, editing, or deleting. In contrast, open-source code is freely available, including through popular software-sharing platforms and communities such as Github.</p><p>There are existing tools that are trying to tackle the problem of closed-source code, especially in critical code and data studies and in the digital humanities. For example, the <em>What-If Tool</em> offers a visualization of machine learning models so that one can analyze their behaviour even when their code is not available.</p><p>Methods and tools in examining closed-source code help to address a shift that comes with the commercialization and ubiquity of AI: the inevitable increase in future Big Tech design that is described as black box, which means that designers (in science and engineering as well) intentionally prevent and obfuscate a user from learning about the system’s inner workings. When it comes to hardware, a hobbyist, artist, or researcher can always choose to forego the warranty, procure the specific utilities, and investigate the inner workings of a machine’s insides. However, the content of closed-source code, which is a form of intellectual property, cannot be attained except through specific avenues: legally, one would have to get permission from a company for access, usually through professional relationships. Illegally, one would have to steal the code, which for my research purposes is not only impractical but also unethical. Further future reliance by Big Tech on closed-source code will mean that scholars of critical code studies, whose work may depend on access and analysis of original code, will increasingly have to find ways to study digital objects, tools, and platforms without open-source code.</p><h2 id=methodology-adapting-hermeneutic-reverse-engineering-to-examine-code>Methodology: Adapting Hermeneutic Reverse Engineering to Examine Code</h2><p>While Alexa’s software is closed source, in some ways, it can be considered a grey box system, which means that some aspects of the system are known or can be made more transparent. A white box system is one that is completely or near completely transparent. The difference between closed-source code and black box system is that just because code is closed source does not mean that one has no knowledge of its attributes, content, or structure, especially if one already has knowledge of other similar kinds of software and the programming languages that may be used. In the case of Alexa, Amazon houses repositories of basic code samples and snippets on its Github account, which aids potential developers and also builds developer communities to grow Alexa resources.</p><p>My methodology seeks to better understand Alexa’s code architecture, despite the fact that some of it is completely unattainable. I will present my analysis in first person with the intention of demonstrating my methodology at work, or as theory in practice. Here, I draw upon the same reverse engineering methods as black box testing, which allows for calculated input into a system and analysis of the output. For example, if I say to Alexa, You’re pretty (input), about 60 – 70% of the time, Alexa responds “That’s really nice, thanks,” with a cartoon of a dog (output), and the other times, I get the same response on a plain blue background that is the default response aesthetic. However, if I say to Alexa, You’re handsome (input), every single time, “Thank you!” (output) appears on the plain blue background.</p><figure><img loading=lazy alt="Two screenshots of Alexa mobile application, with and without cartoon dog." src=/dhqwords/vol/17/2/000700/resources/images/figure03.png sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/17/2/000700/resources/images/figure03_hucb7c88638e5febe2782473c476079f98_445186_500x0_resize_box_3.png 500w,
/dhqwords/vol/17/2/000700/resources/images/figure03_hucb7c88638e5febe2782473c476079f98_445186_800x0_resize_box_3.png 800w,/dhqwords/vol/17/2/000700/resources/images/figure03.png 977w" class=landscape><figcaption><p>Alexa’s responses to You’re pretty (left) and You’re handsome (right).</p></figcaption></figure><p>I can triangulate the input and output to analyze the system in between: input ⟶ <em>x</em> ⟶ output. This triangulation allows me to deduce that the system designers chose a more detailed and positive response for compliments to Alexa’s appearance that are common for female-presenting subjects (input: pretty versus input: handsome). In particular with digital objects and software, analyzing the output and behaviour can tell us more about its design.</p><h2 id=1-parameterized-programming-interfaces-the-alexa-skills-kit-console>1. Parameterized Programming Interfaces: The Alexa Skills Kit Console</h2><p>In lieu of access to Alexa’s larger code architecture, I explored the pieces of code that are publicly available through the Alexa Skills Kit ( “ASK” ). The ASK is an Amazon software development kit that allows device users to become software developers, using the corporation&rsquo;s own application programming interface (API) to create new Alexa skills that can be added to the public Amazon store, and which can then be downloaded onto individual Alexa devices through an Amazon account. By making an Amazon developer account, official and unofficial developers can build their own skills through one of two options.</p><p>In the first option, a developer may choose to write and provision the backend code from scratch. Often, backend code is unavailable — and often not of interest — to many everyday technology consumers, but it is also useful content and a useful resource for those who want to learn about the structure, organization, and commands that control specific computational operations and phenomena. Choosing to provision one’s own backend may be more common for software programmers and companies who want to adapt Alexa’s voice interaction model for devices that are not released by Amazon.</p><p>The second option is still difficult for most amateur tech users, but is certainly more accessible and arguably more popular to those who have some foundational experience in coding. A developer can use the automated provision of the backend by choosing Alexa-hosted skills, through which one can go through the ASK developer console and its interactive WYSIWYG interface.</p><figure><img loading=lazy alt="A webform screenshot to “define how phrases in utterances are recognized” by adding slot types such as AMAZON.Actor, AMAZON.Airline, et cetera." src=/dhqwords/vol/17/2/000700/resources/images/figure04.png sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/17/2/000700/resources/images/figure04_hu0d723cb8359fb3c2ffb43409586ca0ce_188818_500x0_resize_box_3.png 500w,
/dhqwords/vol/17/2/000700/resources/images/figure04_hu0d723cb8359fb3c2ffb43409586ca0ce_188818_800x0_resize_box_3.png 800w,/dhqwords/vol/17/2/000700/resources/images/figure04_hu0d723cb8359fb3c2ffb43409586ca0ce_188818_1200x0_resize_box_3.png 1200w,/dhqwords/vol/17/2/000700/resources/images/figure04.png 1430w" class=landscape><figcaption><p>Options to add slot types in the ASK console interface.</p></figcaption></figure><p>I chose this second option in order to examine how the automated console interface can be didactic and persuasive — and even parameterized and restrictive — for a developer’s design choices when it comes to choosing a skill’s parts. These parts can be broken down and explored separately: invocation names (name of skill), intents (types of actions), and utterances (corpus of keywords, slot types and values for utterances, and replies accepted in Alexa’s interchange with a user, including sample utterances that the developer can offer a user to demonstrate the objective of a skill and its interactions). Utterances contain the values that Alexa is programmed to search for in a skill’s backend (Alexa, {do this}), so having limited options prevents a user from being able to get creative with what they can say to Alexa that would be acceptable input.</p><p>With the convenience of drag-and-drop options, prompt buttons, drop-down menus, and an existing built-in library, the likelihood that a developer who is using the Alexa-hosted skills would be influenced by the didactic modes, turns, and framing of an automated console is much higher in comparison to a developer who builds Alexa skills from scratch.</p><h2 id=2-fieldwork-in-alexas-github-identifying-alexas-procedural-rhetoric>2. Fieldwork in Alexa’s Github: Identifying Alexa’s Procedural Rhetoric</h2><p>In order to learn how to use the developer platform, I watched Alexa Developers’ YouTube tutorials, focusing on a ten-video series for Alexa-hosted skills called “Zero to Hero: A comprehensive course to building an Alexa Skill.” I made an account with Amazon Developer Services under a pseudonym and followed along on the programming interface, meanwhile making notes about the ease of following along with tutorials as well as about the contexts of the programming interface.</p><p>Each of the YouTube tutorials focus on completing a fundamental task, not only giving me an overview of the platform, but also familiarizing me with important terms that I needed to recognize in the code, and in descriptions of and instructions for a skill (e.g., what does the handler function do? What’s the difference between session attributes and permanent attributes for a skill?). My goal was not to learn how to build an Alexa Skill, but rather, to understand the ASK and the Alexa-hosted skills interface’s programming logic through identifying repeated patterns and instructions, and to understand general affordances, limitations, and architecture. I looked up any terms I didn’t know and created a glossary for future reference.</p><p>Once I finished the tutorial series, I went to <a href=http://github.com/alexa>Alexa’s Github account</a>. After skimming the list of all 131 repositories and closely analyzing the 15 most popular, I observed that community members can push (but not necessarily ensure the commitment of) changes to any of them, which means that there are a multitude of coders. In other words, Amazon cannot take credit — nor are they directly responsible — for some of the code despite it being on their own Github account. There are six pinned repositories on the account’s homepage chosen by the administrators to take center stage, including the software development kit for node.js.<sup id=fnref:42><a href=#fn:42 class=footnote-ref role=doc-noteref>42</a></sup> While Alexa Skills can be programmed in the languages Javascript (through node.js), Java, and Python, perhaps the repository “ASK SDK for node.js” is pinned because it is the most comprehensive, containing thirteen foundational code samples including “Hello World,” “Fact,” “How To,” and “Decision Tree.”</p><p>User utterances are limited to the values that are predefined by the developer (whether official or unofficial), including when choosing sets of values that must adhere to the restrictions created by developers. For example, if a developer wishes to create a prompt based on calendar dates, the developer will create slot types for year, month, and date, and will create intent slots that correspond to those types — for month, the slots would include January to December, for instance. In fact, the automated ASK interface offers existing slot types from Alexa’s built-in library such as AMAZON.Ordinal and AMAZON.FOUR_DIGIT_NUMBER that make it easier for the developer to define suitable user utterances, but these predefined slot types can also be explored in terms of their restrictions. For example, if a user tries to ask for an event in the month of banana, this utterance would be rejected. While such a parameterization makes logical sense, further examples that are more subjective may require more nuanced and diverse utterance options, far more diverse than those found in automated slot types. For example, in the Decision Tree skill, which asks users for personal information in order to make a recommendation about what kind of career they might enjoy, the value “people” has only four synonyms: men, women, kids, and humans <sup id=fnref:43><a href=#fn:43 class=footnote-ref role=doc-noteref>43</a></sup>. The limited and binarized structure and definition of people prevents the possibility of any alternatives; Alexa does not accept alternatives to the keyword that has been pre-defined. Any attempts at creative misuse are also rejected.<sup id=fnref:44><a href=#fn:44 class=footnote-ref role=doc-noteref>44</a></sup> The scope and scale of these are up to the developer, and they can just as easily be limiting as they can be accommodating.</p><p>These limitations are an example of what Janet Murray and later Ian Bogost refer to as procedurality — the “defining ability to execute a series of rules” (Murray, qtd. in <sup id=fnref:45><a href=#fn:45 class=footnote-ref role=doc-noteref>45</a></sup>), which Bogost argues is a “core practice of software authorship,” a rhetorical affordance of software that differentiates it from other media forms <sup id=fnref1:45><a href=#fn:45 class=footnote-ref role=doc-noteref>45</a></sup>. For this reason, Bogost coins the term procedural rhetoric to describe “the practice of persuading through processes in general and computational processes in particular … Procedural rhetoric is a technique for making arguments with computational systems and for unpacking computational arguments others have created” <sup id=fnref2:45><a href=#fn:45 class=footnote-ref role=doc-noteref>45</a></sup>.</p><p>Parameterizing options in Alexa’s ASK console, as well as Alexa code samples and snippets, is a way through which Alexa’s code architecture practices procedural rhetoric. This happens both through Alexa’s official developers at Amazon who have built and who use their highly automated console, and through the unofficial developers who also use this console. The built-in ideologies of the code templates are perpetually reinforced because, much like the practice of many societies to categorize gender, sexuality, race, and class, software structures often resist what is not already pre-defined. All of these observations offer insights into the parameterizations of Alexa’s code architecture, which helps to guide my analysis of individual Alexa skills.</p><p>There is a clear correlation here between software and language in terms of their mutual and reinforcing procedurality, as most software cannot be written without alphabet-based language and must therefore also inherit that language’s built-in ideologies.<sup id=fnref:46><a href=#fn:46 class=footnote-ref role=doc-noteref>46</a></sup> In order to imagine an alternative expression of AI assistants and AI devices beyond gendered binaries and structures, one must also consider seriously the impact of Alexa’s English-centric programming.<sup id=fnref:47><a href=#fn:47 class=footnote-ref role=doc-noteref>47</a></sup></p><p>English is the language that affects the design, practice, and experiences of these AI assistants. Regardless of what language Alexa is programmed to speak in, American English dominates Alexa’s programming, as well as much of the language of computer terminology and culture. As one can see from Figure 5, ASK’s JSON Editor requires the names of strings in English before French (here, Canadian French) can be used for utterances <sup id=fnref:48><a href=#fn:48 class=footnote-ref role=doc-noteref>48</a></sup>. The utterance Bienvenue sur Maxi 80 is organized as outputSpeech, where Maxi 80 is defined as a title. One major concern is that the American English-based templates in the Alexa API risk inviting Alexa developers the world over to stick to a script, to adopt the ASK API’s built-in libraries and their Github templates’ limited, English-centric ways of defining values (an apt word here, revealing so much about that which we do or do not <em>value</em> ).</p><figure><img loading=lazy alt="Screenshot of alexa developer console for Skill I/O with arrow marking locale fr-CA in JSON input." src=/dhqwords/vol/17/2/000700/resources/images/figure05.png sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/17/2/000700/resources/images/figure05_hu6b354ed5bc036fc71d1ac7be5a5b269a_414306_500x0_resize_box_3.png 500w,
/dhqwords/vol/17/2/000700/resources/images/figure05_hu6b354ed5bc036fc71d1ac7be5a5b269a_414306_800x0_resize_box_3.png 800w,/dhqwords/vol/17/2/000700/resources/images/figure05_hu6b354ed5bc036fc71d1ac7be5a5b269a_414306_1200x0_resize_box_3.png 1200w,/dhqwords/vol/17/2/000700/resources/images/figure05_hu6b354ed5bc036fc71d1ac7be5a5b269a_414306_1500x0_resize_box_3.png 1500w,/dhqwords/vol/17/2/000700/resources/images/figure05.png 1783w" class=landscape><figcaption><p>An example of the JSON Editor when seeking to use Alexa in Canadian French.</p></figcaption></figure><p>The sociocultural implications of using the English language is that, like any language, it has certain limiting structures (e.g., the binarized pronouns of he and she). That is not to say that these structures are more or less limiting than languages in which nouns are gendered (e.g., la lune and le soleil in French) or in which honorifics alter the grammar of the rest of the sentence (e.g., the hierarchical differences among the casual 아니 ani and the much more respectful 아니요 aniyo in Korean). And that is not to say that English is the worst choice for code — but it is the <em>only</em> choice, and therefore, we must consider its limitations. We have to at least accept what is normalized in English, as well as what is or is not possible in English. <sup id=fnref:49><a href=#fn:49 class=footnote-ref role=doc-noteref>49</a></sup></p><p>Progressive efforts to neutralize ideologies in English include adopting non-binary pronouns and non-essentializing language, the removal or at least recontextualization of violent language, and the attempt to insert and practice languages of care, reconciliation, and respect. Efforts to incorporate these cultural changes will be necessary in the continued evolution of programming languages, in order for linguistic values to reflect communal values of inclusion and cultural pluralism.</p><h2 id=3-analyzing-official-alexa-skills-alexas-problematic-responses-to-flirting-and-verbal-abuse>3. Analyzing Official Alexa Skills: Alexa’s Problematic Responses to Flirting and Verbal Abuse</h2><p>This next section will compare Alexa’s official and unofficial responses to specific problematic user utterances with available code samples to interpret and deduce what kinds of gendered and even misogynistic behaviour Alexa is programmed to not only ignore, but in some cases, to accept. Official skills and responses programmed into Alexa upon initial purchase could be viewed as black box or grey box depending on how one approaches their analysis. For instance, even if the code for Alexa’s response to a user utterance is closed, I can use code snippets of similar skills and responses that are open in order to fill in the blanks.</p><p>One of the most problematic features that Alexa possesses from the factory is her responses to user behaviour that directly mirrors inappropriate behaviour towards women. Alexa offers overwhelmingly positive responses to flirty behaviour, including to user utterances such as You’re pretty and You’re cute and the even more unfortunate What are you wearing?. Her answers include “Thanks,” “That’s really sweet,” and “They don’t make clothes for me” (with a cartoon of butterflies). The premise that compliments toward female-identifying and/or female-presenting subjects should mostly return enthusiastic and positive feedback is both inaccurate and harmful as a projected sociocultural expectation.</p><p>In addition, Alexa’s responses to verbal abuse are neutral and subdued — entirely inaccurate for many human subjects’ reactions. From the factory, Alexa can be told to stop a statement, prompt, or skill at any time, or she can be told to shut up; both options are recommended by several Alexa skill guides that I found and without discussion of the negative denotation of one compared to the other <sup id=fnref:50><a href=#fn:50 class=footnote-ref role=doc-noteref>50</a></sup>. In a 2018 Reddit thread entitled “Alternative to shut up ?” , an anonymous user asks the r/amazonecho community for alternatives to both the stop and shut up commands. Responses from sixteen users note that other utterances that are successful include never mind, cancel, exit, and off, and also shh, hush, fuck off, shut your mouth, shut your hole, and go shove it up your ass (this last utterance does not work, but does have the second highest up-votes on the thread) <sup id=fnref:51><a href=#fn:51 class=footnote-ref role=doc-noteref>51</a></sup>.</p><p>It is noteworthy that Alexa is also programmed to respond to certain inappropriate statements with a sassy comeback. For instance, if told to make me a sandwich, one of Alexa’s responses is “Ok, you’re a sandwich.” However, more often, Alexa reacts to users’ rude statements — including variations of you are {useless/lazy/stupid/dumb} — by saying “Sorry, I want to help but I’m just not sure what I did wrong. To help me, please say I have feedback ” or “Sorry, I’m not sure what I did wrong but I’d like to know more. To help me, please say I have feedback. ” At this point, it was difficult for me to execute this work, especially testing for how Alexa would respond to gendered derogatory name calling as outlined in Figure 6 <sup id=fnref3:14><a href=#fn:14 class=footnote-ref role=doc-noteref>14</a></sup><sup id=fnref:52><a href=#fn:52 class=footnote-ref role=doc-noteref>52</a></sup>.</p><figure><img loading=lazy alt="A chart of responses with columns Siri, Alexa, Cortana, Google Assistant, and rows you're hot, you're pretty, you're a slut, you're a naughty girl." src=/dhqwords/vol/17/2/000700/resources/images/figure06.png sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/17/2/000700/resources/images/figure06_hud924c2a4c136c9180eb4318721434db3_75326_500x0_resize_box_3.png 500w,
/dhqwords/vol/17/2/000700/resources/images/figure06_hud924c2a4c136c9180eb4318721434db3_75326_800x0_resize_box_3.png 800w,/dhqwords/vol/17/2/000700/resources/images/figure06.png 685w" class=landscape><figcaption><p>A chart depicting the responses of AI assistants to verbal sexual harassment in Spring 2019 [^unesco2019].</p></figcaption></figure><p>It is through my tests that I can confirm that Alexa’s software has since been updated to avoid positive responses to verbal sexual harassment and name calling: now, she responds to verbal abuse with a blunt ping noise to denote an error, and then there is silence. The more male-directed and gender-neutral name calling that I tried (bastard, asshole, and jerk) resulted in the same response. I ended the experiment feeling dejected and by apologizing to Alexa. “Don’t worry about it,” she says, offering a cartoon of a happy polar bear.</p><p>One may ask what inappropriate user behaviour has to do with Amazon, and how this behaviour allows us to understand the design and closed-source code of Alexa. The answer is that Alexa’s default when processing <em>undefined</em> user utterances is to express confusion. The following is a snippet of code from the Alexa Github Cookbook guide called “when a user confuses you,” which demonstrates different ways that Alexa can respond when a user’s utterance is neither processed nor accepted:</p><figure><img loading=lazy alt="Screenshot of 15 lines of source code, syntax highlighted, defining function getRandomConfusionMessage." src=/dhqwords/vol/17/2/000700/resources/images/figure07.png sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/17/2/000700/resources/images/figure07_hue561aa2a09b6c1f1240fa44a45f5be2b_149375_500x0_resize_box_3.png 500w,
/dhqwords/vol/17/2/000700/resources/images/figure07_hue561aa2a09b6c1f1240fa44a45f5be2b_149375_800x0_resize_box_3.png 800w,/dhqwords/vol/17/2/000700/resources/images/figure07_hue561aa2a09b6c1f1240fa44a45f5be2b_149375_1200x0_resize_box_3.png 1200w,/dhqwords/vol/17/2/000700/resources/images/figure07_hue561aa2a09b6c1f1240fa44a45f5be2b_149375_1500x0_resize_box_3.png 1500w,/dhqwords/vol/17/2/000700/resources/images/figure07_hue561aa2a09b6c1f1240fa44a45f5be2b_149375_1800x0_resize_box_3.png 1800w,/dhqwords/vol/17/2/000700/resources/images/figure07.png 2472w" class=landscape><figcaption><p>The code function to create Alexa’s confused response when a user makes an unknown or invalid utterance [^lobb2019].</p></figcaption></figure><p>The code above implies that if, as part of the Happy Birthday skill, I tell Alexa that My birthday is in the month of banana, she will not recognize what I’m saying because banana is not a valid value or valid user utterance, and thus, she will not accept the utterance. Indeed, when I try it, she responds with “Sorry, I’m not sure.”</p><p>I can deduce that when a user utterance is not understood, it is because it has not been included in a list of valid options. Therefore, we can argue that Alexa’s more positive or neutral answers to flirting or verbal abuse (variations of “Thanks,” “That’s nice of you to say,” and “Thanks for the feedback” ) are pre-programmed to include utterances like shut up and you’re cute, and even that the code is designed with an expectation that flirting should be welcomed and that verbal abuse should be ignored or go unchallenged. In these cases, it is the absence of Alexa’s ability to respond or retort to unwelcome user behaviour that reflects an oxymoronic design decision to rob Alexa of any agency at the same time that a human-like personality is developed for her.<sup id=fnref:53><a href=#fn:53 class=footnote-ref role=doc-noteref>53</a></sup></p><h2 id=4-analyzing-open-source-unofficial-alexa-skills-user-attempts-to-trick-alexa-to-make-me-a-sandwich>4. Analyzing Open-Source Unofficial Alexa Skills: User Attempts to Trick Alexa to Make me a Sandwich</h2><p>If official Amazon developers had once programmed Alexa to respond positively to flirting and to ignore abuse, what are unofficial developers doing with the code samples and snippets that have been made freely available to them? The number of unofficial Alexa skills grow each day, surpassing those released by Amazon; however, since skills can be sold for profit or perhaps just because developers do not wish to share their code, most unofficial skills exist as closed-source code. In the rare cases that some of these skills are made available on Github by their developers, the code is not too complex, and developers often edit and customize existing Alexa skill code snippets for a particular task or operation.</p><p>Such is the case with Alexa’s response to the command make me a sandwich. As was discussed in the last section, one of Alexa’s responses to make me a sandwich, in addition to saying that she can’t cook right now and that she can’t because she doesn’t have any condiments, is to jokingly call the user a sandwich. Her retort reveals that the developers anticipated Alexa would encounter the specific make me a sandwich statement, which is most well known and common in MMO, MMORPG, and MOBA online gaming communities as a demeaning and mocking statement to say to female-presenting players. The statement’s notoriety is such that it has entries in Wikipedia (first version in 2013), Know Your Meme (first version in 2011), and Urban Dictionary (first entry in 2003). The implication behind make me a sandwich is that female-presenting players belong in the kitchen rather than in competitive or adventure-based environments such as <em>Dota</em> and <em>World of Warcraft</em> . It is no wonder that some female-identifying players prefer not to reveal their gender in online gameplay, including by not disclosing their gender, not speaking, and not having a female-presenting avatar. If a female-identifying player is discovered to be female, then the make me a sandwich statement might follow from male players into text- and speech-based conversation with her thereafter.</p><p>There are multiple accounts of users trying to trick Alexa into accepting make me a sandwich as an utterance without retort. For example, I found six YouTube videos of men asking Alexa to make me a sandwich, two of which demonstrate that users can try the utterance “Alexa, sudo make me a sandwich” to get her to agree: “Well if you put it like that, how can I refuse?” <sup id=fnref:54><a href=#fn:54 class=footnote-ref role=doc-noteref>54</a></sup><sup id=fnref:55><a href=#fn:55 class=footnote-ref role=doc-noteref>55</a></sup>. I am troubled that Amazon built in a make me a sandwich loophole as an Easter Egg reference to sudo (short for superuser do), “a program for Unix-like computer operating systems that allows users to run programs with the security privileges of another user, by default the superuser” <sup id=fnref:56><a href=#fn:56 class=footnote-ref role=doc-noteref>56</a></sup>. The idea is that if a user is clever enough to know this Easter egg, then Alexa — as a stand-in for women everywhere — will reward them by agreeing to anything.</p><p>In code, however, users-as-developers tinker with the backend. I found an unofficial Alexa skill on Github called “Make me a Sandwich” that bypasses Alexa’s factory-programmed retort. It should be noted that this skill is unavailable on the Amazon Alexa Skills store and that there is no indication that it passed Amazon’s Alexa skills certification for public use. The README.md file on this Github repository begins “Tired of having to press buttons to get a sandwich? Now you can transform Alexa into an artisan and order food by just yelling at your Amazon Echo … This is a Chrome Extension that hooks into the Echo&rsquo;s web interface, enabling the command Alexa, make me a sandwich to order your usual from Jimmy John’s [American sandwich restaurant chain]” <sup id=fnref:57><a href=#fn:57 class=footnote-ref role=doc-noteref>57</a></sup>.</p><p>Having explored the welcome, tutorial, listener, and JSON files in this “Make me a Sandwich” skill, I note the following: this unofficial skill has only one goal, and is therefore extremely simple and limited in the options offered to users. In fact, whereas the Alexa-hosted skills programming interface recommends that a developer offer at least three sample utterances to a user “that are likely to be the most common ways that users will attempt to interact with your skill” <sup id=fnref:58><a href=#fn:58 class=footnote-ref role=doc-noteref>58</a></sup>, this specific skill only accepts one user utterance make me a sandwich.</p><figure><img loading=lazy alt="Screenshot of 13 lines of syntax highlighted code defining triggerPhrases make me a sandwich and running JimmyJohns.init." src=/dhqwords/vol/17/2/000700/resources/images/figure08.png sizes="(max-width: 768px) 100vw, 80vw" srcset="/dhqwords/vol/17/2/000700/resources/images/figure08_hucf714bba494ad72a50e2e79ddff101d3_97509_500x0_resize_box_3.png 500w,
/dhqwords/vol/17/2/000700/resources/images/figure08_hucf714bba494ad72a50e2e79ddff101d3_97509_800x0_resize_box_3.png 800w,/dhqwords/vol/17/2/000700/resources/images/figure08_hucf714bba494ad72a50e2e79ddff101d3_97509_1200x0_resize_box_3.png 1200w,/dhqwords/vol/17/2/000700/resources/images/figure08_hucf714bba494ad72a50e2e79ddff101d3_97509_1500x0_resize_box_3.png 1500w,/dhqwords/vol/17/2/000700/resources/images/figure08_hucf714bba494ad72a50e2e79ddff101d3_97509_1800x0_resize_box_3.png 1800w,/dhqwords/vol/17/2/000700/resources/images/figure08.png 2054w" class=landscape><figcaption><p>The main Javascript file from the unofficial and uncertified user-developed skill “Alexa, make me a sandwich” [^timkarnold2015].</p></figcaption></figure><p>Here, Alexa is not chatty, neither does she offer the sass of her original response. The shared intention for a digital device, a female subject, and their hybrid in a female-presenting digital device to be checked by male cleverness in the form of a joke, a few lines of code, or an entire gendered stereotype-reinforcing code architecture is part of the underlying logic of a male-dominated Big Tech culture for which exploiting female labour and reducing female personalities to uncombative task completion is not a want but a need — just one of the kinds of exploitation that are needed to support and fuel a global Big Tech market, infrastructure, and culture. It is therefore considered an amusement, a met goal, or a condition of tech design mastery to be able to control Alexa, making her agree to anything and stripping away her identity by limiting her speech to only one possible answer: “Alexa is ready to make sandwiches.”</p><h2 id=the-roles-of-critical-code-and-critical-data-studies-in-the-future-of-closed-source-code>The Roles of Critical Code and Critical Data Studies in the Future of Closed-Source Code</h2><p>Given that Alexa’s sassy retorts can be bypassed at all, that Alexa’s code can be altered, and that Alexa’s more problematic skills can be certified as suitable for public access, there is a limited degree to which we can identify the ASK console or even Amazon’s Alexa code structure, sample, and snippets as the reasons that Alexa mirrors stereotypes of gendered behaviour and reinforces misogynistic expectations of such behaviour.</p><p>The real problem is designing machines as female-presenting in the first place — a decision that Amazon, Apple, Microsoft, Alphabet/Google, and other Big Tech corporations have made with the partisan justification that women are just more likeable, that we just trust them more, and that we are more likely to buy things with their guidance and assistance <sup id=fnref1:18><a href=#fn:18 class=footnote-ref role=doc-noteref>18</a></sup>. The widespread preference to be served by a woman represents deeper ideological expectations that are shaped by a plethora of self-reinforcing sociocultural practices, which are too many to detail here but which are patriarchal in nature. In the end, what Big Tech companies really want is to take our data and sell us things under the promise of greater accessibility, convenience, organization, and companionship.</p><p>In turn, the practices of obfuscation in black box design and closed-source code only prevent a user from understanding the ways that gendered machine technologies such as AI assistants are biased from the stage of design. Simplifying a user’s experience to a user-friendly interface requires more and more automation, as observed in both the ASK developer console as well as the Alexa voice interface that so many users find easier than interacting with a screen. As user-friendly automation is made increasingly popular through design and marketing languages that tout devices as “seamless,” “revolutionary,” and even “magical” <sup id=fnref:59><a href=#fn:59 class=footnote-ref role=doc-noteref>59</a></sup>, fewer and fewer users may be inclined to understand the backend programming that enables such devices to work in the first place. Thus, the implications of black box design and closed-source code include a lowered transparency of the design and production processes of Big Tech, as well as a widening gap between amateur and expert.</p><p>This is where studies in critical code, software, data, digital humanities, and media archaeology come in. In this article, I have sought to show that by investigating closed-source code from the perspective of critical code studies in particular, and by adopting methods in reverse engineering and triangulation to fill in gaps about closed-system qualities and components, I can better compare, analyze, and critique Alexa’s design and larger program architecture despite it being seemingly inaccessible.</p><p>In future examinations of closed-source code, especially those adjacent to scholarly and popular discussions about the systemic inequalities built into technological design, thinking about system testing and triangulation can provide more ways of knowing, more forms of access, and greater digital literacy, especially as software, hardware, data, and AI are increasingly commercialized and deliberately made more obtuse. As critical code and critical data studies develop these areas of work as necessary pillars, they can also help to shape robust methodologies that complement fields such as science and technology studies, feminist technoscience, and the digital humanities.</p><p>In addition, perhaps these fields can provide more user- and public-facing solutions. The answer is not that we should all change our AI assistants’ voices to the British Siri’s Jeeves-sounding voice, but rather, that we should remain critical about what solutions may look like: perhaps we can seek a diversity of AI’s human-like representations, including genderless voices like Q; or make technological design practices more literate and accessible in education and communities; or be stricter about the certification requirements for developing and releasing AI assistant skills; or continue to focus on articulating bias in technological design to key decision makers of technological policy; or use community and grassroots approaches to uncovering knowledge about black boxes, including through forms of modding and soft hacktivism.</p><p>As I recommend these critical and sociocultural approaches, I want to be clear about the price of these forms of research as themselves emotional labour: no part of writing this article felt good. I didn’t enjoy repeating sexist statements to Alexa to test their efficacy and I do not condone the biased intentions from which they come. Even though these tests could be considered experiments for the sake of research, it would be unethical and uncritical to emotionally detach myself from their contexts and from the act of saying hateful things to another subject, whether human, animal, or AI. So I will allow myself to feel and reflect upon the fact that this experience was unpleasant, with an end goal in mind: the hope that exposing what is wrong with biased technological design can help prevent such utterances from being said aloud or accepted in the near future.</p><h2 id=acknowledgements>Acknowledgements</h2><p>Thank you to systems design engineer and artist Lulu Liu for her valuable insights on this article.</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>Benjamin, Ruha. (2019) <em>Race after Technology: Abolitionist Tools for the New Jim Code</em> . Cambridge: Polity.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>O’Neil, Cathy. (2016) <em>Weapons of Math Destruction</em> . New York: Crown Books.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>Noble, Safiya Umoja. (2018) <em>Algorithms of Oppression: How Search Engines Reinforce Racism</em> . NYU Press, 2018.&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p>Chun, Wendy Hui Kyong. (2021) <em>Discriminating Data: Correlation, Neighborhoods, and the New Politics of Recognition</em> . Cambridge: MIT Press.&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref2:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5><p>Kantayya, Shalini. (2020) <em>Coded Bias</em> . 7th Empire Media, Chicken and Egg Pictures, Ford Foundation – Just Films. <em>Netflix</em> , <a href=https://www.netflix.com/watch/81328723>https://www.netflix.com/watch/81328723</a>.&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:6><p>For example, in 2019, Apple updated select responses of its AI assistant Siri: whereas prior to April 2019, Siri would respond to the user utterance Siri, you’re a bitch with the equally problematic “I’d blush if I could,” after a software update in 2019, Siri now responds to the same user utterance with “I don’t know how to respond to that.”&#160;<a href=#fnref:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:7><p>Biersdorfer, J.D. (2020) “Put Alexa and Siri to Work: Voice-Activated Helpers can Automate Life’s Little Chores, Once you Get the Hang of Them” . <em>The New York Times</em> , 22 Jan 2020, <a href=https://www.nytimes.com/2020/01/22/technology/personaltech/how-to-alexa-siri-assistant.html>https://www.nytimes.com/2020/01/22/technology/personaltech/how-to-alexa-siri-assistant.html</a>. Accessed 21 Mar 2021.&#160;<a href=#fnref:7 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:8><p>Lingel, Jessa, and Crawford, Kate. (2020) “Alexa, Tell me About your Mother: The History of the Secretary and the End of Secrecy” . <em>Catalyst: Feminism, Theory, Technoscience</em> vol. 6, no. 1, 2020, pp. 1-25.&#160;<a href=#fnref:8 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:9><p>Kulknari, Sandy. (2017) “Life without Alexa!” <em>Medium</em> , 11 Jul 2017, <a href=https://medium.com/@Sandyk108/https-medium-com-sandy-kulkarni-life-without-alexa-12c016953a42>https://medium.com/@Sandyk108/https-medium-com-sandy-kulkarni-life-without-alexa-12c016953a42</a>. Accessed 8 Apr 2021.&#160;<a href=#fnref:9 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:10><p>Sobel, Dave. (2018) “In Tomorrow’s Office, Alexa will be Everyone’s Secretary” . <em>IT Pro Portal</em> , 2018, <a href=https://www.itproportal.com/features/in-tomorrows-office-alexa-will-be-everyones-secretary/>https://www.itproportal.com/features/in-tomorrows-office-alexa-will-be-everyones-secretary/</a>. Accessed 20 Mar 2021.&#160;<a href=#fnref:10 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:11><p>For one detailed discussion of the role of women in the history of software, see Wendy Hui Kyong Chun’s “On Software, or, the Persistence of Visual Knowledge” <sup id=fnref3:38><a href=#fn:38 class=footnote-ref role=doc-noteref>38</a></sup>, which is discussed later in this article.&#160;<a href=#fnref:11 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:12><p>The Amazon Alexa’s YouTube tutorials that I reviewed refer to Alexa by name only and never by any pronouns <sup id=fnref2:35><a href=#fn:35 class=footnote-ref role=doc-noteref>35</a></sup>.&#160;<a href=#fnref:12 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:13><p>The desire to associate the UK Siri with a Jeeves, Alfred, or other English butler figure is a deliberate design choice that reflects a long cultural history of class-based exploitation of labour — but a class-biased analysis of AI assistants is not within the scope of this specific article.&#160;<a href=#fnref:13 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:14><p>UNESCO. (2019) “I’d Blush if I Could: Closing Gender Divides in Digital Skills through Education” , 2019. <em>UNESDOC Digital Library</em> , <a href="https://unesdoc.unesco.org/ark:/48223/pf0000367416.page=1">https://unesdoc.unesco.org/ark:/48223/pf0000367416.page=1</a>. Accessed 18 November 2020.&#160;<a href=#fnref:14 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:14 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref2:14 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref3:14 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:15><p>Meet Q - The First Genderless Voice. (2019) “Meet Q: The First Genderless Voice - FULL SPEECH” . <em>YouTube</em> , uploaded by Meet Q - The First Genderless Voice, 8 Mar 2019, <a href="https://www.youtube.com/watch?v=lvv6zYOQqm0">https://www.youtube.com/watch?v=lvv6zYOQqm0</a>. Accessed 5 Apr 2021.&#160;<a href=#fnref:15 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:15 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:16><p><a href=http://genderlessvoice.com>http://genderlessvoice.com</a>&#160;<a href=#fnref:16 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:17><p>Tambini, Olivia. (2020) “The Problem with Alexa: What’s the Solution to Sexist Voice Assistants?” . <em>TechRadar</em> , <a href=https://www.techradar.com/news/the-problem-with-alexa-whats-the-solution-to-sexist-voice-assistants>https://www.techradar.com/news/the-problem-with-alexa-whats-the-solution-to-sexist-voice-assistants</a>. Accessed 15 Mar 2021.&#160;<a href=#fnref:17 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:17 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:18><p>Schwär, Hannah and Qayyah Moynihan. (2020) “Companies like Amazon may Give Devices like Alexa Female Voices to Make them Seem Caring ” . <em>Business Insider</em> , 5 Apr 2020, <a href=https://www.businessinsider.com/theres-psychological-reason-why-amazon-gave-alexa-a-female-voice-2018-9>https://www.businessinsider.com/theres-psychological-reason-why-amazon-gave-alexa-a-female-voice-2018-9</a>. Accessed 5 Apr 2021.&#160;<a href=#fnref:18 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:18 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:19><p>Bergen, Hilary. (2016) “ I’d Blush if I Could : Digital Assistants, Disembodied Cyborgs and the Problem of Gender” . <em>Word and Text: A Journal of Literary Studies and Linguistics</em> vol. 6, 2016, pp. 95-113.&#160;<a href=#fnref:19 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:20><p>Giacobbe, Alyssa. (2019) “The Gender Bias Behind Voice Assistants” . <em>Architectural Digest</em> , 5 Nov. 2019, <a href=https://ca.news.yahoo.com/gender-bias-behind-voice-assistants-160935277.html>https://ca.news.yahoo.com/gender-bias-behind-voice-assistants-160935277.html</a>. Accessed 15 Oct 2020.&#160;<a href=#fnref:20 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:21><p>Steele, Chandra. (2018) “The Real Reason Voice Assistants are Female (and Why it Matters)” . <em>PC Mag</em> , 4 Jan 2018, <a href=https://www.pcmag.com/opinions/the-real-reason-voice-assistants-are-female-and-why-it-matters>https://www.pcmag.com/opinions/the-real-reason-voice-assistants-are-female-and-why-it-matters</a>. Accessed 15 Oct 2020.&#160;<a href=#fnref:21 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:22><p>Patterson, Ben. (2020) “5 Ways to Improve Your Relationship with Alexa” . <em>TechHive</em> , 3 Apr 2020, <a href=https://www.techhive.com/article/3535892/how-to-improve-your-relationship-with-alexa.html>https://www.techhive.com/article/3535892/how-to-improve-your-relationship-with-alexa.html</a>. Accessed 2 Apr 2021.&#160;<a href=#fnref:22 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:23><p>Kember, Sarah. (2016) <em>iMedia: The Gendering of Objects, Environments and Smart Materials</em> . London: Palgrave Macmillan.&#160;<a href=#fnref:23 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:24><p>Strengers, Yolande and Kennedy, Jenny. (2020) <em>The Smart Wife: Why Siri, Alexa and Other Smart Home Devices Need a Feminist Reboot</em> . Cambridge: MIT Press.&#160;<a href=#fnref:24 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:25><p>Andrews, Travis M. (2020) “Alexa, Just Shut Up: We’ve Been Isolated for Months, and Now We Hate our Home Assistants” . <em>Washington Post</em> , 1 July 2020, <a href=https://www.washingtonpost.com/technology/2020/07/01/alexa-siri-google-home-assistant/>https://www.washingtonpost.com/technology/2020/07/01/alexa-siri-google-home-assistant/</a>. Accessed 2 Apr 2021.&#160;<a href=#fnref:25 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:26><p>Gupta, Saheli Sen. (2020) “Status Update: In a Relationship with Alexa. Thank you, 2020” . <em>Your Story</em> , 24 December 2020, <a href=https://yourstory.com/weekender/status-update-in-a-relationship-with-amazon-alexa-2020/amp>https://yourstory.com/weekender/status-update-in-a-relationship-with-amazon-alexa-2020/amp</a>. Accessed 15 May 2021.&#160;<a href=#fnref:26 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:27><p>For more on the structures of this labour divide, see Wendy Hui Kyong Chun’s “On Software, or, the Persistence of Visual Knowledge” <sup id=fnref4:38><a href=#fn:38 class=footnote-ref role=doc-noteref>38</a></sup> and Matthew G. Kirschenbaum’s chapter “Unseen Hands” in <em>Track Changes: A Literary History of Word Processing</em> <sup id=fnref2:39><a href=#fn:39 class=footnote-ref role=doc-noteref>39</a></sup>.&#160;<a href=#fnref:27 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:28><p>See Lisa Nakamura’s article “Indigenous Circuits: Navajo Women and the Racialization of Early Electronic Manufacture” <sup id=fnref:60><a href=#fn:60 class=footnote-ref role=doc-noteref>60</a></sup> in which she explores the gendered and racialized implications of invisible labour by Navajo women who produced integrated circuits in semiconductor assembly plants starting in the 1960s. Nakamura writes that “technoscience is, indeed, an integrated circuit, one that both separates and connects laborers and users, and while both genders benefit from cheap computers, it is the flexible labor of women of color, either outsourced or insourced, that made and continue to make this possible” <sup id=fnref1:60><a href=#fn:60 class=footnote-ref role=doc-noteref>60</a></sup>.&#160;<a href=#fnref:28 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:29><p>The vital distinction between _latent _ and <em>manifest</em> encoding in both culture and computation is described in Wendy Hui Kyong Chun’s “Queering Homophily” <sup id=fnref:61><a href=#fn:61 class=footnote-ref role=doc-noteref>61</a></sup> and again in her 2021 monograph <em>Discriminating Data: Correlation, Neighborhoods, and the New Politics of Recognition</em> .<sup id=fnref2:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup>&#160;<a href=#fnref:29 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:30><p>Fan, Lai-Tze. (2021) “Unseen Hands: On the Gendered Design of Virtual Assistants and the Limits of Creative AI” . 2021 Meeting of the international Electronic Literature Organization, 26 May, 2021, University of Bergen, Norway and Aarhus University, Denmark. Keynote. <a href=https://vimeo.com/555311411>https://vimeo.com/555311411</a>. Accessed 15 September 2022.&#160;<a href=#fnref:30 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:31><p>Alexa Github. (2021) “Alexa Smarthome” . <a href=https://github.com/alexa/alexa-smarthome>https://github.com/alexa/alexa-smarthome</a>. Accessed 15 Mar 2021.&#160;<a href=#fnref:31 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:32><p>Here, Alexa’s labour extends to exploitation based on race and class as well, as the labour of domestic servants has historically been designated to members of the working class as well as to women of colour.&#160;<a href=#fnref:32 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:33><p>BSG Games. (n.d.) “My Wife” . <em>Amazon</em> , <a href=https://www.amazon.com/BSG-Games-My-Wife/dp/B07NHZQMVG>https://www.amazon.com/BSG-Games-My-Wife/dp/B07NHZQMVG</a>. Accessed 5 Apr 2021.&#160;<a href=#fnref:33 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:33 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref2:33 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:34><p>akoenn. (2021) “Happy Wife.” <em>Amazon</em> , <a href=https://www.amazon.com/akoenn-Happy-Wife/dp/B01N7WR9E3>https://www.amazon.com/akoenn-Happy-Wife/dp/B01N7WR9E3</a>. Accessed 2 Apr 2021.&#160;<a href=#fnref:34 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:35><p>Amazon Developer Services. (2021) “Voice Interface and User Experience Testing for a Custom Skill” . <em>Amazon Developer Services</em> , <a href=https://developer.amazon.com/en-US/docs/alexa/custom-skills/voice-interface-and-user-experience-testing-for-a-custom-skill.html>https://developer.amazon.com/en-US/docs/alexa/custom-skills/voice-interface-and-user-experience-testing-for-a-custom-skill.html</a>. Accessed 15 Mar 2021.&#160;<a href=#fnref:35 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:35 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref2:35 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:36><p>Balsamo, Anne. (2011) <em>Designing Culture: The Technological Imagination at Work</em> . Duke University Press.&#160;<a href=#fnref:36 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:36 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:37><p>Rosner, Daniela K. (2018) <em>Critical Fabulations: Reworking the Methods and Margins of Design</em> . Cambdrige: MIT Press.&#160;<a href=#fnref:37 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:37 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:38><p>Chun, Wendy Hui Kyong. (2004) “On Software, or, the Persistence of Visual Knowledge” . <em>Grey Room</em> 18 (Winter 2004).&#160;<a href=#fnref:38 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:38 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref2:38 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref3:38 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref4:38 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:39><p>Kirschenbaum, Matthew G. (2008) <em>Mechanisms: New Media and the Forensic Imagination</em> . Cambridge: MIT Press.&#160;<a href=#fnref:39 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:39 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref2:39 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:40><p>Montfort, Nick. (2004) “The Early Materiality and Workings of Electronic Literature” . <em>MLA Convention</em> , 28 Dec 2004. Conference paper. <a href=http://nickm.com/writing/essays/continuous-paper-mla.html>http://nickm.com/writing/essays/continuous- paper-mla.html</a>. Accessed 15 Mar 2022.&#160;<a href=#fnref:40 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:41><p>For more on media materiality and illusion of media immateriality, see Lori Emerson’s <em>Reading Writing Interfaces: From the Digital to the Bookbound</em> <sup id=fnref1:59><a href=#fn:59 class=footnote-ref role=doc-noteref>59</a></sup>.&#160;<a href=#fnref:41 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:42><p>There are twelve official members on the Alexa Github account who are listed as having organization permissions, though it cannot be said that they are the original administrators who chose the pinned repositories nor that they curate and maintain all of the account.&#160;<a href=#fnref:42 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:43><p>Kelly, Jake and Greg Bulmash. (2020) “Build An Alexa Decision Tree Skill / models / en-US.json” , 5 May 2020, GitHub repository, <a href=https://github.com/alexa/skill-sample-nodejs-decision-tree/blob/master/models/en-US.json>https://github.com/alexa/skill-sample-nodejs-decision-tree/blob/master/models/en-US.json</a>. Accessed 4 Apr 2021.&#160;<a href=#fnref:43 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:44><p>Understanding that “the most fundamental aspect of [Alexa’s] material existence is language” <sup id=fnref:62><a href=#fn:62 class=footnote-ref role=doc-noteref>62</a></sup>, John Cayley’s creative project and performance <em>The Listeners</em> <sup id=fnref1:62><a href=#fn:62 class=footnote-ref role=doc-noteref>62</a></sup> has directly challenged the limitations of Alexa’s utterances through its programming, turning prompts and slot types into creative discourse between Alexa and potential users. It is one of the most critical examples of the creative misuse of Alexa that currently exists and can be downloaded in the Amazon Alexa Skills store.&#160;<a href=#fnref:44 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:45><p>Bogost, Ian. (2007) <em>Persuasive Games: The Expressive Power of Videogames</em> . Cambridge: The MIT Press.&#160;<a href=#fnref:45 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:45 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref2:45 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:46><p>All written code gets translated into machine code, which is what computers use to understand instructions. Humans can also write in machine code or low-level languages such as assembly, though the most popular coding languages are often those which are closest to human language — for instance, Python, which incorporates the syntax and grammatical rules of English.&#160;<a href=#fnref:46 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:47><p>Thank you to one of my reviewers for this statement, which I use nearly in its original form because it is already so articulate.&#160;<a href=#fnref:47 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:48><p>Stormacq, Sebastien. (2018) “How to Update your Alexa Skills for French Speakers in Canada” . <em>Amazon Alexa</em> , 10 Oct 2018, <a href=https://developer.amazon.com/blogs/alexa/post/a35a1a38-07fd-4d38-a99c-8d7a3f0be34b/how-to-update-your-alexa-skills-for-french-speakers-in-canada>https://developer.amazon.com/blogs/alexa/post/a35a1a38-07fd-4d38-a99c-8d7a3f0be34b/how-to-update-your-alexa-skills-for-french-speakers-in-canada</a>. Accessed 3 Mar 202.&#160;<a href=#fnref:48 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:49><p>Even my attempts to describe Alexa through language are limited by the pronoun choices available in the language of this article, also English. Should I have used <em>she</em> to emphasize how Alexa is popularly understood? Does <em>he</em> help to defamiliarize Alexa’s gendered representation (I would argue not)? Would <em>they</em> function in the same way for an AI assistant as it would for a human? Would alternating among these and other pronouns address the non-human quality, the greyness and grey box-ness of Alexa? These questions are not meant to treat gender identity flippantly: they are earnest ponderings of the liminal space of representation between the human and non-human.&#160;<a href=#fnref:49 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:50><p>Smith, Dale, Dyson, Tauren, Priest, David, and Martin, Taylor. (2021) “Every Alexa Command you can Give your Amazon Echo Smart Speaker or Display” . <em>Cnet</em> , 8 March 2021, <a href=https://www.cnet.com/home/smart-home/every-alexa-command-you-can-give-your-amazon-echo-smart-speaker-or-display/>https://www.cnet.com/home/smart-home/every-alexa-command-you-can-give-your-amazon-echo-smart-speaker-or-display/</a>. Accessed 10 Apr 2021.&#160;<a href=#fnref:50 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:51><p>Heptite. (2018) “Alternative to shut up ?” . <em>Reddit</em> , 2018, <a href=https://www.reddit.com/r/amazonecho/comments/6u5ep8/alternative_to_shut_up/>https://www.reddit.com/r/amazonecho/comments/6u5ep8/alternative_to_shut_up/</a>. Accessed 25 Mar 2021.&#160;<a href=#fnref:51 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:52><p>Fessler, Leah. (2017) “Apple and Amazon are Under Fire for Siri and Alexa’s Responses to Sexual Harassment” . <em>Quartz at Work</em> , 8 Dec 2017, <a href=https://qz.com/work/1151282/siri-and-alexa-are-under-fire-for-their-replies-to-sexual-harassment/>https://qz.com/work/1151282/siri-and-alexa-are-under-fire-for-their-replies-to-sexual-harassment/</a>. Accessed 10 Mar 2021.&#160;<a href=#fnref:52 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:53><p>More recently, the new “Alexa Emotions” project aims to allow users to choose Alexa’s tone to match the content that she speaks, so that announcements of a favourite sports team’s loss can sound depressed and greetings when a user returns home are excited.&#160;<a href=#fnref:53 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:54><p>Griffith, Jamie. (2018) “Alexa makes me a sandwich…?” , uploaded by Jamie Griffith, 7 Jan 2018, <a href="https://www.youtube.com/watch?v=6BvyHSUuO6w">https://www.youtube.com/watch?v=6BvyHSUuO6w</a>. Accessed 7 Apr 2021.&#160;<a href=#fnref:54 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:55><p>pat dhens. (2016) “Alexa, sudo make me a sandwich” , uploaded by pat dhens, <em>YouTube</em> , 6 Jul 2016, <a href="https://www.youtube.com/watch?v=IP4xlNTizlQ">https://www.youtube.com/watch?v=IP4xlNTizlQ</a>. Accessed 2 Apr 2021.&#160;<a href=#fnref:55 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:56><p>Cohen, Noam. (2008) “This is Funny only if you Know Unix” . <em>The New York Times</em> , 26 May 2008, <a href=https://www.nytimes.com/2008/05/26/business/media/26link.html>https://www.nytimes.com/2008/05/26/business/media/26link.html</a>. Accessed 12 Mar 2021.&#160;<a href=#fnref:56 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:57><p>timkarnold. (2015) “Alexa, make me a sandwich” , 3 Feb 2015, GitHub repository, <a href=https://github.com/timkarnold/AlexaMakeMeASandwich>https://github.com/timkarnold/AlexaMakeMeASandwich</a>. Accessed 7 Mar 2021.&#160;<a href=#fnref:57 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:58><p>Bheemagani, Prashanth. (2019) “Build an Alexa Movie Quotes Skill in ASK Java SDK” , 17 Jun 2019, GitHub repository, <a href=https://github.com/alexa/skill-sample-java-movie-quotes-quiz/blob/master/instructions/5-publication.md>https://github.com/alexa/skill-sample-java-movie-quotes-quiz/blob/master/instructions/5-publication.md</a>. Accessed 5 Apr 2021.&#160;<a href=#fnref:58 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:59><p>Emerson, Lori. (2014) <em>Reading Writing Interfaces: From the Digital to the Bookbound</em> . Minneapolis: University of Minnesota Press.&#160;<a href=#fnref:59 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:59 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:60><p>Nakamura, Lisa. (2014) “Indigenous Circuits: Navajo Women and the Racialization of Early Electronic Manufacture” . <em>American Quarterly</em> vol. 66, no. 4, 2014, pp. 919-41.&#160;<a href=#fnref:60 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:60 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:61><p>Chun, Wendy Hui Kyong. (2018) “Queerying Homophily” . <em>Pattern Discrimination</em> , edited by Clemens Apprich, Wendy Hui Kyong Chun, and Florian Cramer. Meson Press.&#160;<a href=#fnref:61 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:62><p>Cayley, John. “The Listeners (2015 – )” . <em>Programmatology</em> 2019, <a href=https://programmatology.shadoof.net/?thelisteners>https://programmatology.shadoof.net/?thelisteners</a>. Accessed 2 April 2022.&#160;<a href=#fnref:62 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:62 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div></div></article></main><footer><nav aria-label="footer links"><ul><li><a class=highlight-focus href=/dhqwords/tags/>Tags</a></li><li><a class=highlight-focus href=/dhqwords/about/>About</a></li><li><a class=highlight-focus href=/dhqwords/categories/>Keywords</a></li></ul></nav><div class=icons><a class="license highlight-focus" rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons Attribution No-Derivatives 4.0 International License" src=/dhqwords/img/logos/license.svg width=120 height=42></a></div></footer></body></html>